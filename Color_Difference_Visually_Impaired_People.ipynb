{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jianfeng0Liu/Color-Difference_Visually-Impaired-People/blob/main/Color_Difference_Visually_Impaired_People.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8XduP_K8BC2",
        "outputId": "d383615f-02f7-418a-bdb3-3f49b9538f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This is for google colab, which used for training now.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')    "
      ],
      "id": "v8XduP_K8BC2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KweRiGk9SoV"
      },
      "source": [
        "#Packages#"
      ],
      "id": "0KweRiGk9SoV"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W5ubR-gR6bV6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras import layers, Model, Input\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "id": "W5ubR-gR6bV6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-OQiTO_KaOV"
      },
      "source": [
        "#GPU Training Setting (Selected)#\n",
        "Setting to use GPU for Accelerating training.  \n",
        "  \n",
        "CPU training is also available, the program of this module is not required when using CPU training."
      ],
      "id": "F-OQiTO_KaOV"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F7Ki2VAWIqbn"
      },
      "outputs": [],
      "source": [
        "# Setting to use GPU for training\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.backend import set_session\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Assign which GPU for training\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.allow_soft_placement=True \n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.4 # 40% GPU memory for training\n",
        "config.gpu_options.allow_growth=True   # GPU memory not fully occupied during initialization, allocated on demand\n",
        "sess = tf.compat.v1.Session(config = config)\n",
        "set_session(sess)"
      ],
      "id": "F7Ki2VAWIqbn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Quhtsl9cU9"
      },
      "source": [
        "#1. Preliminary Try#\n",
        "Based on the preliminary study code (Given by Prof. Damien).\n",
        "\n",
        "**Changing Parameters:**  \n",
        "Layer: 10  \n",
        "Unit(nn): 256  \n",
        "Epochs: 1000  \n",
        "Learning Rate: 1e-4\n",
        "\n",
        "**Result**  \n",
        "Around Train_loss=0.03 and Validation_loss=0.04"
      ],
      "id": "Q2Quhtsl9cU9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsLDEkmiQdeD",
        "outputId": "a95e67d6-dd14-43b9-9834-a8f4b2830dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "80/80 [==============================] - 9s 12ms/step - loss: 0.6690 - val_loss: 0.4341\n",
            "Epoch 2/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.4106 - val_loss: 0.3608\n",
            "Epoch 3/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3920 - val_loss: 0.4178\n",
            "Epoch 4/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3722 - val_loss: 0.3690\n",
            "Epoch 5/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.3721 - val_loss: 0.3600\n",
            "Epoch 6/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3452 - val_loss: 0.3954\n",
            "Epoch 7/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3532 - val_loss: 0.4108\n",
            "Epoch 8/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3171 - val_loss: 0.3110\n",
            "Epoch 9/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3173 - val_loss: 0.3648\n",
            "Epoch 10/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.3061 - val_loss: 0.3195\n",
            "Epoch 11/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3212 - val_loss: 0.3524\n",
            "Epoch 12/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.2963 - val_loss: 0.2918\n",
            "Epoch 13/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.2951 - val_loss: 0.2804\n",
            "Epoch 14/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.2852 - val_loss: 0.2969\n",
            "Epoch 15/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.2759 - val_loss: 0.2903\n",
            "Epoch 16/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2848 - val_loss: 0.2927\n",
            "Epoch 17/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2784 - val_loss: 0.2689\n",
            "Epoch 18/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2564 - val_loss: 0.2658\n",
            "Epoch 19/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2413 - val_loss: 0.2595\n",
            "Epoch 20/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2352 - val_loss: 0.2690\n",
            "Epoch 21/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2411 - val_loss: 0.3340\n",
            "Epoch 22/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2391 - val_loss: 0.2357\n",
            "Epoch 23/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2328 - val_loss: 0.2368\n",
            "Epoch 24/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2230 - val_loss: 0.2347\n",
            "Epoch 25/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2259 - val_loss: 0.2326\n",
            "Epoch 26/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2180 - val_loss: 0.2414\n",
            "Epoch 27/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2075 - val_loss: 0.2327\n",
            "Epoch 28/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2040 - val_loss: 0.2174\n",
            "Epoch 29/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1933 - val_loss: 0.2119\n",
            "Epoch 30/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1904 - val_loss: 0.2057\n",
            "Epoch 31/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1956 - val_loss: 0.1820\n",
            "Epoch 32/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1847 - val_loss: 0.1831\n",
            "Epoch 33/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.1752 - val_loss: 0.1953\n",
            "Epoch 34/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1730 - val_loss: 0.2006\n",
            "Epoch 35/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1999 - val_loss: 0.1978\n",
            "Epoch 36/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1814 - val_loss: 0.1703\n",
            "Epoch 37/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1699 - val_loss: 0.1790\n",
            "Epoch 38/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1648 - val_loss: 0.2024\n",
            "Epoch 39/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1760 - val_loss: 0.1873\n",
            "Epoch 40/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1706 - val_loss: 0.1978\n",
            "Epoch 41/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1599 - val_loss: 0.1794\n",
            "Epoch 42/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1445 - val_loss: 0.1879\n",
            "Epoch 43/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1552 - val_loss: 0.1566\n",
            "Epoch 44/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1466 - val_loss: 0.1845\n",
            "Epoch 45/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1551 - val_loss: 0.1662\n",
            "Epoch 46/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1480 - val_loss: 0.1795\n",
            "Epoch 47/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1370 - val_loss: 0.1692\n",
            "Epoch 48/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1470 - val_loss: 0.1587\n",
            "Epoch 49/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1387 - val_loss: 0.1640\n",
            "Epoch 50/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1380 - val_loss: 0.1557\n",
            "Epoch 51/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1327 - val_loss: 0.1581\n",
            "Epoch 52/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1481 - val_loss: 0.1667\n",
            "Epoch 53/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1525 - val_loss: 0.1569\n",
            "Epoch 54/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1350 - val_loss: 0.1543\n",
            "Epoch 55/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1282 - val_loss: 0.1492\n",
            "Epoch 56/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1141 - val_loss: 0.1533\n",
            "Epoch 57/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1291 - val_loss: 0.1451\n",
            "Epoch 58/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1225 - val_loss: 0.1413\n",
            "Epoch 59/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1200 - val_loss: 0.1523\n",
            "Epoch 60/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1367 - val_loss: 0.1363\n",
            "Epoch 61/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1231 - val_loss: 0.1452\n",
            "Epoch 62/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1170 - val_loss: 0.1438\n",
            "Epoch 63/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1284 - val_loss: 0.1390\n",
            "Epoch 64/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1092 - val_loss: 0.1245\n",
            "Epoch 65/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1242 - val_loss: 0.1437\n",
            "Epoch 66/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1305 - val_loss: 0.1464\n",
            "Epoch 67/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1314 - val_loss: 0.1316\n",
            "Epoch 68/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1459 - val_loss: 0.1396\n",
            "Epoch 69/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1282 - val_loss: 0.1294\n",
            "Epoch 70/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1315 - val_loss: 0.1305\n",
            "Epoch 71/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1165 - val_loss: 0.1276\n",
            "Epoch 72/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1051 - val_loss: 0.1232\n",
            "Epoch 73/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.1276\n",
            "Epoch 74/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.1511\n",
            "Epoch 75/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1338 - val_loss: 0.1467\n",
            "Epoch 76/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1418 - val_loss: 0.1433\n",
            "Epoch 77/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1266 - val_loss: 0.1282\n",
            "Epoch 78/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.1115 - val_loss: 0.1295\n",
            "Epoch 79/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1054 - val_loss: 0.1399\n",
            "Epoch 80/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1201 - val_loss: 0.1219\n",
            "Epoch 81/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1159 - val_loss: 0.1211\n",
            "Epoch 82/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.1027\n",
            "Epoch 83/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.1229\n",
            "Epoch 84/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1146 - val_loss: 0.1085\n",
            "Epoch 85/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0965 - val_loss: 0.1137\n",
            "Epoch 86/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1019 - val_loss: 0.1182\n",
            "Epoch 87/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.1192\n",
            "Epoch 88/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1201 - val_loss: 0.1222\n",
            "Epoch 89/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.1083\n",
            "Epoch 90/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0887 - val_loss: 0.1221\n",
            "Epoch 91/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0907 - val_loss: 0.1037\n",
            "Epoch 92/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0865 - val_loss: 0.0955\n",
            "Epoch 93/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1119 - val_loss: 0.1151\n",
            "Epoch 94/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1018 - val_loss: 0.1025\n",
            "Epoch 95/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0888 - val_loss: 0.1168\n",
            "Epoch 96/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0859 - val_loss: 0.1193\n",
            "Epoch 97/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.1035 - val_loss: 0.1111\n",
            "Epoch 98/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0995 - val_loss: 0.1162\n",
            "Epoch 99/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0914 - val_loss: 0.0989\n",
            "Epoch 100/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1017 - val_loss: 0.0977\n",
            "Epoch 101/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0930 - val_loss: 0.1168\n",
            "Epoch 102/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0868 - val_loss: 0.1039\n",
            "Epoch 103/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0849 - val_loss: 0.0959\n",
            "Epoch 104/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.0895 - val_loss: 0.0923\n",
            "Epoch 105/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0883 - val_loss: 0.0956\n",
            "Epoch 106/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1067 - val_loss: 0.0993\n",
            "Epoch 107/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0793 - val_loss: 0.1124\n",
            "Epoch 108/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0918 - val_loss: 0.0946\n",
            "Epoch 109/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0942 - val_loss: 0.0998\n",
            "Epoch 110/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0919 - val_loss: 0.1021\n",
            "Epoch 111/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0805 - val_loss: 0.0956\n",
            "Epoch 112/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.0894\n",
            "Epoch 113/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0804 - val_loss: 0.1001\n",
            "Epoch 114/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0940 - val_loss: 0.0951\n",
            "Epoch 115/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0916 - val_loss: 0.1082\n",
            "Epoch 116/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0935 - val_loss: 0.0971\n",
            "Epoch 117/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0819 - val_loss: 0.1012\n",
            "Epoch 118/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0836 - val_loss: 0.0899\n",
            "Epoch 119/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0877 - val_loss: 0.1013\n",
            "Epoch 120/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0932 - val_loss: 0.1054\n",
            "Epoch 121/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0874 - val_loss: 0.0939\n",
            "Epoch 122/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0907 - val_loss: 0.0948\n",
            "Epoch 123/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0870 - val_loss: 0.1010\n",
            "Epoch 124/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0827 - val_loss: 0.1077\n",
            "Epoch 125/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0774 - val_loss: 0.0943\n",
            "Epoch 126/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0851 - val_loss: 0.1155\n",
            "Epoch 127/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0907 - val_loss: 0.1101\n",
            "Epoch 128/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0873 - val_loss: 0.1012\n",
            "Epoch 129/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0863 - val_loss: 0.0919\n",
            "Epoch 130/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0853 - val_loss: 0.1042\n",
            "Epoch 131/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0822 - val_loss: 0.1027\n",
            "Epoch 132/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0785 - val_loss: 0.1008\n",
            "Epoch 133/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0813 - val_loss: 0.0848\n",
            "Epoch 134/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0706 - val_loss: 0.0885\n",
            "Epoch 135/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.0825\n",
            "Epoch 136/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0774 - val_loss: 0.0940\n",
            "Epoch 137/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0800 - val_loss: 0.0902\n",
            "Epoch 138/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0719 - val_loss: 0.1010\n",
            "Epoch 139/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0849 - val_loss: 0.1031\n",
            "Epoch 140/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0861 - val_loss: 0.1009\n",
            "Epoch 141/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0841 - val_loss: 0.1124\n",
            "Epoch 142/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.1087\n",
            "Epoch 143/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0826 - val_loss: 0.1013\n",
            "Epoch 144/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.1028\n",
            "Epoch 145/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0745 - val_loss: 0.1002\n",
            "Epoch 146/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0767 - val_loss: 0.1060\n",
            "Epoch 147/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0703 - val_loss: 0.1033\n",
            "Epoch 148/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0792 - val_loss: 0.0949\n",
            "Epoch 149/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0842 - val_loss: 0.0979\n",
            "Epoch 150/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.0993\n",
            "Epoch 151/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.0911\n",
            "Epoch 152/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0762 - val_loss: 0.0840\n",
            "Epoch 153/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0808 - val_loss: 0.0968\n",
            "Epoch 154/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0788 - val_loss: 0.0987\n",
            "Epoch 155/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0820 - val_loss: 0.0854\n",
            "Epoch 156/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0751 - val_loss: 0.0831\n",
            "Epoch 157/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0661 - val_loss: 0.0874\n",
            "Epoch 158/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0750 - val_loss: 0.0857\n",
            "Epoch 159/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0683 - val_loss: 0.0806\n",
            "Epoch 160/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0653 - val_loss: 0.1048\n",
            "Epoch 161/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0740 - val_loss: 0.0813\n",
            "Epoch 162/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0810 - val_loss: 0.0882\n",
            "Epoch 163/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0834 - val_loss: 0.0944\n",
            "Epoch 164/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0810 - val_loss: 0.0903\n",
            "Epoch 165/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0714 - val_loss: 0.0776\n",
            "Epoch 166/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0744 - val_loss: 0.0827\n",
            "Epoch 167/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0642 - val_loss: 0.0679\n",
            "Epoch 168/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0658 - val_loss: 0.0967\n",
            "Epoch 169/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0697 - val_loss: 0.0878\n",
            "Epoch 170/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.0848\n",
            "Epoch 171/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0822 - val_loss: 0.0804\n",
            "Epoch 172/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0725 - val_loss: 0.0821\n",
            "Epoch 173/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0704 - val_loss: 0.0886\n",
            "Epoch 174/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0771 - val_loss: 0.1019\n",
            "Epoch 175/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0844 - val_loss: 0.0864\n",
            "Epoch 176/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0755 - val_loss: 0.0947\n",
            "Epoch 177/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0668 - val_loss: 0.0924\n",
            "Epoch 178/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0801 - val_loss: 0.1077\n",
            "Epoch 179/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0786 - val_loss: 0.0814\n",
            "Epoch 180/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0675 - val_loss: 0.0826\n",
            "Epoch 181/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0901 - val_loss: 0.0916\n",
            "Epoch 182/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0725 - val_loss: 0.0708\n",
            "Epoch 183/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0727 - val_loss: 0.0951\n",
            "Epoch 184/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0843 - val_loss: 0.0888\n",
            "Epoch 185/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0690 - val_loss: 0.0849\n",
            "Epoch 186/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0639 - val_loss: 0.0746\n",
            "Epoch 187/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0647 - val_loss: 0.0739\n",
            "Epoch 188/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0662 - val_loss: 0.0793\n",
            "Epoch 189/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0628 - val_loss: 0.0838\n",
            "Epoch 190/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0638 - val_loss: 0.0904\n",
            "Epoch 191/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0675 - val_loss: 0.0757\n",
            "Epoch 192/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0826 - val_loss: 0.0933\n",
            "Epoch 193/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0773 - val_loss: 0.0922\n",
            "Epoch 194/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0674 - val_loss: 0.0840\n",
            "Epoch 195/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0672 - val_loss: 0.0783\n",
            "Epoch 196/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0722 - val_loss: 0.0853\n",
            "Epoch 197/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0588 - val_loss: 0.0807\n",
            "Epoch 198/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0596 - val_loss: 0.0825\n",
            "Epoch 199/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0715 - val_loss: 0.1015\n",
            "Epoch 200/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0678 - val_loss: 0.0841\n",
            "Epoch 201/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0709 - val_loss: 0.1002\n",
            "Epoch 202/1000\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0725 - val_loss: 0.0857\n",
            "Epoch 203/1000\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0718 - val_loss: 0.0753\n",
            "Epoch 204/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0569 - val_loss: 0.0734\n",
            "Epoch 205/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0640 - val_loss: 0.0707\n",
            "Epoch 206/1000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0668 - val_loss: 0.0788\n",
            "Epoch 207/1000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0663 - val_loss: 0.0720\n",
            "Epoch 208/1000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0690 - val_loss: 0.0812\n",
            "Epoch 209/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0691 - val_loss: 0.0745\n",
            "Epoch 210/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0752 - val_loss: 0.0772\n",
            "Epoch 211/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0654 - val_loss: 0.0776\n",
            "Epoch 212/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0704 - val_loss: 0.0647\n",
            "Epoch 213/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0591 - val_loss: 0.0751\n",
            "Epoch 214/1000\n",
            "80/80 [==============================] - 2s 18ms/step - loss: 0.0712 - val_loss: 0.0729\n",
            "Epoch 215/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0517 - val_loss: 0.0706\n",
            "Epoch 216/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0584 - val_loss: 0.0775\n",
            "Epoch 217/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0689 - val_loss: 0.0741\n",
            "Epoch 218/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0604 - val_loss: 0.0802\n",
            "Epoch 219/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0618 - val_loss: 0.0802\n",
            "Epoch 220/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0785 - val_loss: 0.0792\n",
            "Epoch 221/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0674 - val_loss: 0.0729\n",
            "Epoch 222/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0582 - val_loss: 0.0659\n",
            "Epoch 223/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0653 - val_loss: 0.0732\n",
            "Epoch 224/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0636 - val_loss: 0.0736\n",
            "Epoch 225/1000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0575 - val_loss: 0.0658\n",
            "Epoch 226/1000\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0549 - val_loss: 0.0837\n",
            "Epoch 227/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0651 - val_loss: 0.0712\n",
            "Epoch 228/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0672 - val_loss: 0.0706\n",
            "Epoch 229/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0605 - val_loss: 0.0852\n",
            "Epoch 230/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0600 - val_loss: 0.0710\n",
            "Epoch 231/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0677 - val_loss: 0.0675\n",
            "Epoch 232/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0701\n",
            "Epoch 233/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0686 - val_loss: 0.0792\n",
            "Epoch 234/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0721 - val_loss: 0.0732\n",
            "Epoch 235/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0671 - val_loss: 0.0716\n",
            "Epoch 236/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0647 - val_loss: 0.0721\n",
            "Epoch 237/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0717 - val_loss: 0.0815\n",
            "Epoch 238/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0617 - val_loss: 0.0679\n",
            "Epoch 239/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0716\n",
            "Epoch 240/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.0690\n",
            "Epoch 241/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0618 - val_loss: 0.0749\n",
            "Epoch 242/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0529 - val_loss: 0.0628\n",
            "Epoch 243/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0596 - val_loss: 0.0633\n",
            "Epoch 244/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0487 - val_loss: 0.0681\n",
            "Epoch 245/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0734\n",
            "Epoch 246/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0492 - val_loss: 0.0676\n",
            "Epoch 247/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.0650\n",
            "Epoch 248/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0627 - val_loss: 0.0893\n",
            "Epoch 249/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0685 - val_loss: 0.0702\n",
            "Epoch 250/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0549 - val_loss: 0.0723\n",
            "Epoch 251/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0666 - val_loss: 0.0672\n",
            "Epoch 252/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0710 - val_loss: 0.0694\n",
            "Epoch 253/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0588 - val_loss: 0.0690\n",
            "Epoch 254/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0564 - val_loss: 0.0763\n",
            "Epoch 255/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0725 - val_loss: 0.0783\n",
            "Epoch 256/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0546 - val_loss: 0.0640\n",
            "Epoch 257/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0488 - val_loss: 0.0852\n",
            "Epoch 258/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0507 - val_loss: 0.0685\n",
            "Epoch 259/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0667\n",
            "Epoch 260/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.0789\n",
            "Epoch 261/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0631 - val_loss: 0.0836\n",
            "Epoch 262/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0641 - val_loss: 0.0827\n",
            "Epoch 263/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0554 - val_loss: 0.0611\n",
            "Epoch 264/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0576 - val_loss: 0.0697\n",
            "Epoch 265/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0670 - val_loss: 0.0769\n",
            "Epoch 266/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.0764\n",
            "Epoch 267/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.0780\n",
            "Epoch 268/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0615 - val_loss: 0.0695\n",
            "Epoch 269/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0583 - val_loss: 0.0804\n",
            "Epoch 270/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0633 - val_loss: 0.0763\n",
            "Epoch 271/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.0802\n",
            "Epoch 272/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.0744\n",
            "Epoch 273/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0576 - val_loss: 0.0656\n",
            "Epoch 274/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0554 - val_loss: 0.0754\n",
            "Epoch 275/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0498 - val_loss: 0.0609\n",
            "Epoch 276/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.0737\n",
            "Epoch 277/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0634 - val_loss: 0.0706\n",
            "Epoch 278/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0692 - val_loss: 0.0848\n",
            "Epoch 279/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0664 - val_loss: 0.0894\n",
            "Epoch 280/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0543 - val_loss: 0.0787\n",
            "Epoch 281/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0670\n",
            "Epoch 282/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0581 - val_loss: 0.0763\n",
            "Epoch 283/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0587 - val_loss: 0.0804\n",
            "Epoch 284/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0556 - val_loss: 0.0810\n",
            "Epoch 285/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0557 - val_loss: 0.0605\n",
            "Epoch 286/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0543 - val_loss: 0.0710\n",
            "Epoch 287/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0566 - val_loss: 0.0806\n",
            "Epoch 288/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0552 - val_loss: 0.0702\n",
            "Epoch 289/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.0709\n",
            "Epoch 290/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0677\n",
            "Epoch 291/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0535 - val_loss: 0.0584\n",
            "Epoch 292/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0551 - val_loss: 0.0724\n",
            "Epoch 293/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0525 - val_loss: 0.0718\n",
            "Epoch 294/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0558 - val_loss: 0.0680\n",
            "Epoch 295/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0601 - val_loss: 0.0622\n",
            "Epoch 296/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0590 - val_loss: 0.0584\n",
            "Epoch 297/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.0636\n",
            "Epoch 298/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0648 - val_loss: 0.0666\n",
            "Epoch 299/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0566 - val_loss: 0.0694\n",
            "Epoch 300/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0639\n",
            "Epoch 301/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0613 - val_loss: 0.0610\n",
            "Epoch 302/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0639 - val_loss: 0.0666\n",
            "Epoch 303/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0575 - val_loss: 0.0572\n",
            "Epoch 304/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0492 - val_loss: 0.0593\n",
            "Epoch 305/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.0727\n",
            "Epoch 306/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0542 - val_loss: 0.0728\n",
            "Epoch 307/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0538 - val_loss: 0.0638\n",
            "Epoch 308/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0636\n",
            "Epoch 309/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0545 - val_loss: 0.0652\n",
            "Epoch 310/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0624 - val_loss: 0.0659\n",
            "Epoch 311/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0470 - val_loss: 0.0595\n",
            "Epoch 312/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0533 - val_loss: 0.0626\n",
            "Epoch 313/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0442 - val_loss: 0.0628\n",
            "Epoch 314/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0532 - val_loss: 0.0625\n",
            "Epoch 315/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0627\n",
            "Epoch 316/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.0740\n",
            "Epoch 317/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0654\n",
            "Epoch 318/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0480 - val_loss: 0.0585\n",
            "Epoch 319/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0727 - val_loss: 0.0691\n",
            "Epoch 320/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.0662\n",
            "Epoch 321/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0547 - val_loss: 0.0573\n",
            "Epoch 322/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0558 - val_loss: 0.0686\n",
            "Epoch 323/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0536 - val_loss: 0.0672\n",
            "Epoch 324/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0502 - val_loss: 0.0641\n",
            "Epoch 325/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0482 - val_loss: 0.0672\n",
            "Epoch 326/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0533 - val_loss: 0.0632\n",
            "Epoch 327/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.0626\n",
            "Epoch 328/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0501 - val_loss: 0.0644\n",
            "Epoch 329/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0641\n",
            "Epoch 330/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0544 - val_loss: 0.0635\n",
            "Epoch 331/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0560 - val_loss: 0.0624\n",
            "Epoch 332/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0507 - val_loss: 0.0739\n",
            "Epoch 333/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0508 - val_loss: 0.0610\n",
            "Epoch 334/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0584\n",
            "Epoch 335/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0613\n",
            "Epoch 336/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0476 - val_loss: 0.0673\n",
            "Epoch 337/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0491 - val_loss: 0.0556\n",
            "Epoch 338/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0491 - val_loss: 0.0588\n",
            "Epoch 339/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0465 - val_loss: 0.0641\n",
            "Epoch 340/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0623\n",
            "Epoch 341/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0493 - val_loss: 0.0609\n",
            "Epoch 342/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0633\n",
            "Epoch 343/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0565 - val_loss: 0.0564\n",
            "Epoch 344/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.0593\n",
            "Epoch 345/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0496 - val_loss: 0.0604\n",
            "Epoch 346/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0486 - val_loss: 0.0569\n",
            "Epoch 347/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.0557\n",
            "Epoch 348/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0491 - val_loss: 0.0558\n",
            "Epoch 349/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0471 - val_loss: 0.0577\n",
            "Epoch 350/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0554 - val_loss: 0.0523\n",
            "Epoch 351/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0492 - val_loss: 0.0601\n",
            "Epoch 352/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0440 - val_loss: 0.0575\n",
            "Epoch 353/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0436 - val_loss: 0.0519\n",
            "Epoch 354/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0476 - val_loss: 0.0599\n",
            "Epoch 355/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0505 - val_loss: 0.0703\n",
            "Epoch 356/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0473 - val_loss: 0.0612\n",
            "Epoch 357/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0428 - val_loss: 0.0551\n",
            "Epoch 358/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0420 - val_loss: 0.0655\n",
            "Epoch 359/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.0568\n",
            "Epoch 360/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0577 - val_loss: 0.0746\n",
            "Epoch 361/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.0542\n",
            "Epoch 362/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.0467\n",
            "Epoch 363/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.0559\n",
            "Epoch 364/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0537\n",
            "Epoch 365/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0478 - val_loss: 0.0541\n",
            "Epoch 366/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0574 - val_loss: 0.0538\n",
            "Epoch 367/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0471 - val_loss: 0.0610\n",
            "Epoch 368/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0443 - val_loss: 0.0545\n",
            "Epoch 369/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0511 - val_loss: 0.0588\n",
            "Epoch 370/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0433 - val_loss: 0.0556\n",
            "Epoch 371/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0486 - val_loss: 0.0617\n",
            "Epoch 372/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0482 - val_loss: 0.0649\n",
            "Epoch 373/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0639\n",
            "Epoch 374/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0541 - val_loss: 0.0590\n",
            "Epoch 375/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0416 - val_loss: 0.0632\n",
            "Epoch 376/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0468 - val_loss: 0.0543\n",
            "Epoch 377/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0445 - val_loss: 0.0650\n",
            "Epoch 378/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.0631\n",
            "Epoch 379/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.0633\n",
            "Epoch 380/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0488 - val_loss: 0.0629\n",
            "Epoch 381/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.0500\n",
            "Epoch 382/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0506 - val_loss: 0.0583\n",
            "Epoch 383/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0458 - val_loss: 0.0585\n",
            "Epoch 384/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0710\n",
            "Epoch 385/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0588\n",
            "Epoch 386/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0606\n",
            "Epoch 387/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.0554\n",
            "Epoch 388/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0702\n",
            "Epoch 389/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0531 - val_loss: 0.0640\n",
            "Epoch 390/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0491 - val_loss: 0.0529\n",
            "Epoch 391/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0518 - val_loss: 0.0583\n",
            "Epoch 392/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0459 - val_loss: 0.0597\n",
            "Epoch 393/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.0555\n",
            "Epoch 394/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0503 - val_loss: 0.0585\n",
            "Epoch 395/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0488 - val_loss: 0.0575\n",
            "Epoch 396/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0395 - val_loss: 0.0597\n",
            "Epoch 397/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.0554\n",
            "Epoch 398/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.0574\n",
            "Epoch 399/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0447 - val_loss: 0.0631\n",
            "Epoch 400/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0551 - val_loss: 0.0568\n",
            "Epoch 401/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0618 - val_loss: 0.0673\n",
            "Epoch 402/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0505 - val_loss: 0.0620\n",
            "Epoch 403/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0484 - val_loss: 0.0499\n",
            "Epoch 404/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0484 - val_loss: 0.0577\n",
            "Epoch 405/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0473 - val_loss: 0.0530\n",
            "Epoch 406/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.0634\n",
            "Epoch 407/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0457 - val_loss: 0.0645\n",
            "Epoch 408/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0622\n",
            "Epoch 409/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0471 - val_loss: 0.0575\n",
            "Epoch 410/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0500 - val_loss: 0.0619\n",
            "Epoch 411/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0423 - val_loss: 0.0566\n",
            "Epoch 412/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0454 - val_loss: 0.0558\n",
            "Epoch 413/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0526\n",
            "Epoch 414/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0464 - val_loss: 0.0524\n",
            "Epoch 415/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.0526\n",
            "Epoch 416/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.0464\n",
            "Epoch 417/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0429 - val_loss: 0.0547\n",
            "Epoch 418/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.0561\n",
            "Epoch 419/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0431 - val_loss: 0.0648\n",
            "Epoch 420/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0396 - val_loss: 0.0541\n",
            "Epoch 421/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0457\n",
            "Epoch 422/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0482 - val_loss: 0.0549\n",
            "Epoch 423/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0467 - val_loss: 0.0533\n",
            "Epoch 424/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0488\n",
            "Epoch 425/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.0567\n",
            "Epoch 426/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0502 - val_loss: 0.0649\n",
            "Epoch 427/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0579 - val_loss: 0.0677\n",
            "Epoch 428/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0529 - val_loss: 0.0525\n",
            "Epoch 429/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0567 - val_loss: 0.0643\n",
            "Epoch 430/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0527 - val_loss: 0.0513\n",
            "Epoch 431/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0561\n",
            "Epoch 432/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0511\n",
            "Epoch 433/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0458 - val_loss: 0.0593\n",
            "Epoch 434/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0502 - val_loss: 0.0609\n",
            "Epoch 435/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.0544\n",
            "Epoch 436/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0499 - val_loss: 0.0502\n",
            "Epoch 437/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0673\n",
            "Epoch 438/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0477 - val_loss: 0.0681\n",
            "Epoch 439/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0537 - val_loss: 0.0642\n",
            "Epoch 440/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0548\n",
            "Epoch 441/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0496 - val_loss: 0.0564\n",
            "Epoch 442/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.0568\n",
            "Epoch 443/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0478 - val_loss: 0.0538\n",
            "Epoch 444/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0596 - val_loss: 0.0656\n",
            "Epoch 445/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0441 - val_loss: 0.0588\n",
            "Epoch 446/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0504\n",
            "Epoch 447/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0535\n",
            "Epoch 448/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0408 - val_loss: 0.0543\n",
            "Epoch 449/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0370 - val_loss: 0.0519\n",
            "Epoch 450/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0425 - val_loss: 0.0506\n",
            "Epoch 451/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.0493\n",
            "Epoch 452/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0332 - val_loss: 0.0517\n",
            "Epoch 453/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0403 - val_loss: 0.0462\n",
            "Epoch 454/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.0469\n",
            "Epoch 455/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0452 - val_loss: 0.0541\n",
            "Epoch 456/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0493 - val_loss: 0.0575\n",
            "Epoch 457/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.0557\n",
            "Epoch 458/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0396 - val_loss: 0.0506\n",
            "Epoch 459/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0379 - val_loss: 0.0502\n",
            "Epoch 460/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0383 - val_loss: 0.0549\n",
            "Epoch 461/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0413 - val_loss: 0.0532\n",
            "Epoch 462/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.0568\n",
            "Epoch 463/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0519\n",
            "Epoch 464/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0403 - val_loss: 0.0547\n",
            "Epoch 465/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0609\n",
            "Epoch 466/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.0571\n",
            "Epoch 467/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0457 - val_loss: 0.0555\n",
            "Epoch 468/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0376 - val_loss: 0.0628\n",
            "Epoch 469/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0455 - val_loss: 0.0560\n",
            "Epoch 470/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0441 - val_loss: 0.0512\n",
            "Epoch 471/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.0594\n",
            "Epoch 472/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.0491\n",
            "Epoch 473/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.0570\n",
            "Epoch 474/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.0518\n",
            "Epoch 475/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.0576\n",
            "Epoch 476/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0404 - val_loss: 0.0542\n",
            "Epoch 477/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0444 - val_loss: 0.0544\n",
            "Epoch 478/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0459 - val_loss: 0.0630\n",
            "Epoch 479/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0570 - val_loss: 0.0671\n",
            "Epoch 480/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0508 - val_loss: 0.0612\n",
            "Epoch 481/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0509\n",
            "Epoch 482/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0495 - val_loss: 0.0608\n",
            "Epoch 483/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0420 - val_loss: 0.0631\n",
            "Epoch 484/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0579 - val_loss: 0.0604\n",
            "Epoch 485/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.0509\n",
            "Epoch 486/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0379 - val_loss: 0.0535\n",
            "Epoch 487/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0447 - val_loss: 0.0658\n",
            "Epoch 488/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0510 - val_loss: 0.0522\n",
            "Epoch 489/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0545 - val_loss: 0.0536\n",
            "Epoch 490/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0451 - val_loss: 0.0542\n",
            "Epoch 491/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0414 - val_loss: 0.0536\n",
            "Epoch 492/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0386 - val_loss: 0.0558\n",
            "Epoch 493/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0538\n",
            "Epoch 494/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0452\n",
            "Epoch 495/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0471 - val_loss: 0.0735\n",
            "Epoch 496/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0386 - val_loss: 0.0535\n",
            "Epoch 497/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.0506\n",
            "Epoch 498/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.0548\n",
            "Epoch 499/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0385 - val_loss: 0.0454\n",
            "Epoch 500/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0499\n",
            "Epoch 501/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0431 - val_loss: 0.0620\n",
            "Epoch 502/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0371 - val_loss: 0.0563\n",
            "Epoch 503/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0451 - val_loss: 0.0580\n",
            "Epoch 504/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0423 - val_loss: 0.0567\n",
            "Epoch 505/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0392 - val_loss: 0.0457\n",
            "Epoch 506/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0354 - val_loss: 0.0551\n",
            "Epoch 507/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0493\n",
            "Epoch 508/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.0556\n",
            "Epoch 509/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0432 - val_loss: 0.0527\n",
            "Epoch 510/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0476\n",
            "Epoch 511/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0579\n",
            "Epoch 512/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0469 - val_loss: 0.0501\n",
            "Epoch 513/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0375 - val_loss: 0.0514\n",
            "Epoch 514/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0376 - val_loss: 0.0557\n",
            "Epoch 515/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0428 - val_loss: 0.0516\n",
            "Epoch 516/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0469 - val_loss: 0.0518\n",
            "Epoch 517/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0515\n",
            "Epoch 518/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0414 - val_loss: 0.0469\n",
            "Epoch 519/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0376 - val_loss: 0.0506\n",
            "Epoch 520/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.0523\n",
            "Epoch 521/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0444 - val_loss: 0.0564\n",
            "Epoch 522/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0413 - val_loss: 0.0484\n",
            "Epoch 523/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0387 - val_loss: 0.0449\n",
            "Epoch 524/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0495 - val_loss: 0.0488\n",
            "Epoch 525/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0484 - val_loss: 0.0430\n",
            "Epoch 526/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0427 - val_loss: 0.0531\n",
            "Epoch 527/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0455\n",
            "Epoch 528/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0365 - val_loss: 0.0452\n",
            "Epoch 529/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.0479\n",
            "Epoch 530/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0471\n",
            "Epoch 531/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0443\n",
            "Epoch 532/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.0584\n",
            "Epoch 533/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0423 - val_loss: 0.0507\n",
            "Epoch 534/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0363 - val_loss: 0.0536\n",
            "Epoch 535/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0425 - val_loss: 0.0607\n",
            "Epoch 536/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0387 - val_loss: 0.0526\n",
            "Epoch 537/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0482\n",
            "Epoch 538/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.0547\n",
            "Epoch 539/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0408 - val_loss: 0.0629\n",
            "Epoch 540/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0584\n",
            "Epoch 541/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0459\n",
            "Epoch 542/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0453 - val_loss: 0.0461\n",
            "Epoch 543/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0432 - val_loss: 0.0424\n",
            "Epoch 544/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0385 - val_loss: 0.0591\n",
            "Epoch 545/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0352 - val_loss: 0.0448\n",
            "Epoch 546/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0470\n",
            "Epoch 547/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0451\n",
            "Epoch 548/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0518\n",
            "Epoch 549/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0490\n",
            "Epoch 550/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0423 - val_loss: 0.0492\n",
            "Epoch 551/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.0481\n",
            "Epoch 552/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0496\n",
            "Epoch 553/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0543\n",
            "Epoch 554/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0498\n",
            "Epoch 555/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0524\n",
            "Epoch 556/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0459 - val_loss: 0.0527\n",
            "Epoch 557/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0496\n",
            "Epoch 558/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.0534\n",
            "Epoch 559/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.0412\n",
            "Epoch 560/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0470\n",
            "Epoch 561/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0382 - val_loss: 0.0465\n",
            "Epoch 562/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0386 - val_loss: 0.0525\n",
            "Epoch 563/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0385 - val_loss: 0.0565\n",
            "Epoch 564/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0454 - val_loss: 0.0581\n",
            "Epoch 565/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0382 - val_loss: 0.0480\n",
            "Epoch 566/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0327 - val_loss: 0.0438\n",
            "Epoch 567/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.0467\n",
            "Epoch 568/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0421 - val_loss: 0.0425\n",
            "Epoch 569/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0447\n",
            "Epoch 570/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.0518\n",
            "Epoch 571/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.0472\n",
            "Epoch 572/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0380 - val_loss: 0.0557\n",
            "Epoch 573/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0534\n",
            "Epoch 574/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0431 - val_loss: 0.0498\n",
            "Epoch 575/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0516\n",
            "Epoch 576/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0413\n",
            "Epoch 577/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0381 - val_loss: 0.0509\n",
            "Epoch 578/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0458\n",
            "Epoch 579/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0436\n",
            "Epoch 580/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0322 - val_loss: 0.0380\n",
            "Epoch 581/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0335 - val_loss: 0.0452\n",
            "Epoch 582/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0374 - val_loss: 0.0499\n",
            "Epoch 583/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0346 - val_loss: 0.0419\n",
            "Epoch 584/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0429\n",
            "Epoch 585/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0417\n",
            "Epoch 586/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0482\n",
            "Epoch 587/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0457 - val_loss: 0.0560\n",
            "Epoch 588/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.0453\n",
            "Epoch 589/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.0479\n",
            "Epoch 590/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0543\n",
            "Epoch 591/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0516\n",
            "Epoch 592/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0438\n",
            "Epoch 593/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0491\n",
            "Epoch 594/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0468\n",
            "Epoch 595/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.0474\n",
            "Epoch 596/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.0529\n",
            "Epoch 597/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0483\n",
            "Epoch 598/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0367 - val_loss: 0.0450\n",
            "Epoch 599/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0444 - val_loss: 0.0449\n",
            "Epoch 600/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0356 - val_loss: 0.0437\n",
            "Epoch 601/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0375 - val_loss: 0.0445\n",
            "Epoch 602/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0474 - val_loss: 0.0469\n",
            "Epoch 603/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.0433\n",
            "Epoch 604/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0456\n",
            "Epoch 605/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0502 - val_loss: 0.0477\n",
            "Epoch 606/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0396 - val_loss: 0.0424\n",
            "Epoch 607/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0503\n",
            "Epoch 608/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0410 - val_loss: 0.0457\n",
            "Epoch 609/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0380 - val_loss: 0.0447\n",
            "Epoch 610/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0419\n",
            "Epoch 611/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0457\n",
            "Epoch 612/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0421\n",
            "Epoch 613/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0398\n",
            "Epoch 614/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.0408\n",
            "Epoch 615/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0413\n",
            "Epoch 616/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0336 - val_loss: 0.0420\n",
            "Epoch 617/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.0462\n",
            "Epoch 618/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0450 - val_loss: 0.0470\n",
            "Epoch 619/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0352 - val_loss: 0.0436\n",
            "Epoch 620/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0450 - val_loss: 0.0423\n",
            "Epoch 621/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0454\n",
            "Epoch 622/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0473\n",
            "Epoch 623/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0393\n",
            "Epoch 624/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0343 - val_loss: 0.0392\n",
            "Epoch 625/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0408 - val_loss: 0.0501\n",
            "Epoch 626/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.0538\n",
            "Epoch 627/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.0445\n",
            "Epoch 628/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0524\n",
            "Epoch 629/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0401\n",
            "Epoch 630/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0414 - val_loss: 0.0454\n",
            "Epoch 631/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0326 - val_loss: 0.0439\n",
            "Epoch 632/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0343 - val_loss: 0.0425\n",
            "Epoch 633/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0409\n",
            "Epoch 634/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0377\n",
            "Epoch 635/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0379 - val_loss: 0.0417\n",
            "Epoch 636/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0382 - val_loss: 0.0433\n",
            "Epoch 637/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0406 - val_loss: 0.0472\n",
            "Epoch 638/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0492\n",
            "Epoch 639/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0344 - val_loss: 0.0466\n",
            "Epoch 640/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0371 - val_loss: 0.0436\n",
            "Epoch 641/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0417 - val_loss: 0.0441\n",
            "Epoch 642/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0317 - val_loss: 0.0413\n",
            "Epoch 643/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0394\n",
            "Epoch 644/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0471\n",
            "Epoch 645/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0465\n",
            "Epoch 646/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0364 - val_loss: 0.0529\n",
            "Epoch 647/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0402\n",
            "Epoch 648/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0383\n",
            "Epoch 649/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0332 - val_loss: 0.0429\n",
            "Epoch 650/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.0373\n",
            "Epoch 651/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0323 - val_loss: 0.0400\n",
            "Epoch 652/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.0393\n",
            "Epoch 653/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.0398\n",
            "Epoch 654/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0329 - val_loss: 0.0440\n",
            "Epoch 655/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0336 - val_loss: 0.0463\n",
            "Epoch 656/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0412\n",
            "Epoch 657/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0371 - val_loss: 0.0442\n",
            "Epoch 658/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.0583\n",
            "Epoch 659/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.0502\n",
            "Epoch 660/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0468\n",
            "Epoch 661/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0429\n",
            "Epoch 662/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0331 - val_loss: 0.0432\n",
            "Epoch 663/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0407\n",
            "Epoch 664/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0388\n",
            "Epoch 665/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0359 - val_loss: 0.0380\n",
            "Epoch 666/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0336 - val_loss: 0.0361\n",
            "Epoch 667/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0385 - val_loss: 0.0517\n",
            "Epoch 668/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.0547\n",
            "Epoch 669/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0416 - val_loss: 0.0463\n",
            "Epoch 670/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0476\n",
            "Epoch 671/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.0478\n",
            "Epoch 672/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0368 - val_loss: 0.0438\n",
            "Epoch 673/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0301 - val_loss: 0.0415\n",
            "Epoch 674/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0365 - val_loss: 0.0485\n",
            "Epoch 675/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0368 - val_loss: 0.0377\n",
            "Epoch 676/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0326 - val_loss: 0.0467\n",
            "Epoch 677/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0379 - val_loss: 0.0417\n",
            "Epoch 678/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0407\n",
            "Epoch 679/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.0439\n",
            "Epoch 680/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0510\n",
            "Epoch 681/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0424\n",
            "Epoch 682/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.0443\n",
            "Epoch 683/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0443\n",
            "Epoch 684/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0455 - val_loss: 0.0474\n",
            "Epoch 685/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0425\n",
            "Epoch 686/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0479\n",
            "Epoch 687/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.0479\n",
            "Epoch 688/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0365 - val_loss: 0.0389\n",
            "Epoch 689/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0341 - val_loss: 0.0422\n",
            "Epoch 690/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0329 - val_loss: 0.0431\n",
            "Epoch 691/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0281 - val_loss: 0.0415\n",
            "Epoch 692/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0334 - val_loss: 0.0413\n",
            "Epoch 693/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0317 - val_loss: 0.0417\n",
            "Epoch 694/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0380 - val_loss: 0.0546\n",
            "Epoch 695/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0371 - val_loss: 0.0431\n",
            "Epoch 696/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0344 - val_loss: 0.0378\n",
            "Epoch 697/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.0408\n",
            "Epoch 698/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0422\n",
            "Epoch 699/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0406 - val_loss: 0.0478\n",
            "Epoch 700/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0329 - val_loss: 0.0456\n",
            "Epoch 701/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.0464\n",
            "Epoch 702/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0485\n",
            "Epoch 703/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0379 - val_loss: 0.0489\n",
            "Epoch 704/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0414\n",
            "Epoch 705/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0341 - val_loss: 0.0445\n",
            "Epoch 706/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0517\n",
            "Epoch 707/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0483\n",
            "Epoch 708/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.0541\n",
            "Epoch 709/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0393 - val_loss: 0.0471\n",
            "Epoch 710/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.0428\n",
            "Epoch 711/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0380 - val_loss: 0.0499\n",
            "Epoch 712/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0341 - val_loss: 0.0373\n",
            "Epoch 713/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0328 - val_loss: 0.0435\n",
            "Epoch 714/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0443\n",
            "Epoch 715/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0313 - val_loss: 0.0420\n",
            "Epoch 716/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.0432\n",
            "Epoch 717/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.0461\n",
            "Epoch 718/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0329 - val_loss: 0.0457\n",
            "Epoch 719/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0327 - val_loss: 0.0505\n",
            "Epoch 720/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0456\n",
            "Epoch 721/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0477\n",
            "Epoch 722/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0520\n",
            "Epoch 723/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0382 - val_loss: 0.0462\n",
            "Epoch 724/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0381\n",
            "Epoch 725/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0435\n",
            "Epoch 726/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0432\n",
            "Epoch 727/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0453\n",
            "Epoch 728/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.0366\n",
            "Epoch 729/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0398\n",
            "Epoch 730/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0331 - val_loss: 0.0465\n",
            "Epoch 731/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0345 - val_loss: 0.0415\n",
            "Epoch 732/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0292 - val_loss: 0.0445\n",
            "Epoch 733/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0445\n",
            "Epoch 734/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0423\n",
            "Epoch 735/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0501\n",
            "Epoch 736/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0582\n",
            "Epoch 737/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0363 - val_loss: 0.0436\n",
            "Epoch 738/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.0470\n",
            "Epoch 739/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.0498\n",
            "Epoch 740/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.0436\n",
            "Epoch 741/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0365\n",
            "Epoch 742/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0352 - val_loss: 0.0379\n",
            "Epoch 743/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0448\n",
            "Epoch 744/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0379\n",
            "Epoch 745/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0399\n",
            "Epoch 746/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0322 - val_loss: 0.0395\n",
            "Epoch 747/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0503\n",
            "Epoch 748/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.0503\n",
            "Epoch 749/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0394 - val_loss: 0.0467\n",
            "Epoch 750/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0389 - val_loss: 0.0480\n",
            "Epoch 751/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0425 - val_loss: 0.0477\n",
            "Epoch 752/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0367 - val_loss: 0.0396\n",
            "Epoch 753/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0299 - val_loss: 0.0361\n",
            "Epoch 754/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0378 - val_loss: 0.0458\n",
            "Epoch 755/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0370 - val_loss: 0.0452\n",
            "Epoch 756/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0385 - val_loss: 0.0456\n",
            "Epoch 757/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0472\n",
            "Epoch 758/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0339 - val_loss: 0.0469\n",
            "Epoch 759/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.0427\n",
            "Epoch 760/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0363 - val_loss: 0.0400\n",
            "Epoch 761/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0422\n",
            "Epoch 762/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0418\n",
            "Epoch 763/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0353 - val_loss: 0.0496\n",
            "Epoch 764/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0475\n",
            "Epoch 765/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0248 - val_loss: 0.0391\n",
            "Epoch 766/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.0387\n",
            "Epoch 767/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0375 - val_loss: 0.0457\n",
            "Epoch 768/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0412 - val_loss: 0.0424\n",
            "Epoch 769/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0412 - val_loss: 0.0382\n",
            "Epoch 770/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0334 - val_loss: 0.0375\n",
            "Epoch 771/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0252 - val_loss: 0.0385\n",
            "Epoch 772/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0384\n",
            "Epoch 773/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0348\n",
            "Epoch 774/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0297 - val_loss: 0.0400\n",
            "Epoch 775/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.0364\n",
            "Epoch 776/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0332 - val_loss: 0.0446\n",
            "Epoch 777/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0249 - val_loss: 0.0312\n",
            "Epoch 778/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0244 - val_loss: 0.0357\n",
            "Epoch 779/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0370 - val_loss: 0.0350\n",
            "Epoch 780/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0482 - val_loss: 0.0389\n",
            "Epoch 781/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0423\n",
            "Epoch 782/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0386 - val_loss: 0.0404\n",
            "Epoch 783/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.0388\n",
            "Epoch 784/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0429\n",
            "Epoch 785/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0366 - val_loss: 0.0425\n",
            "Epoch 786/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0224 - val_loss: 0.0366\n",
            "Epoch 787/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0280 - val_loss: 0.0342\n",
            "Epoch 788/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0313 - val_loss: 0.0435\n",
            "Epoch 789/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0296 - val_loss: 0.0371\n",
            "Epoch 790/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.0434\n",
            "Epoch 791/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0319 - val_loss: 0.0378\n",
            "Epoch 792/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.0402\n",
            "Epoch 793/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0396\n",
            "Epoch 794/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0362\n",
            "Epoch 795/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0383\n",
            "Epoch 796/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0325 - val_loss: 0.0453\n",
            "Epoch 797/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0421\n",
            "Epoch 798/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0423\n",
            "Epoch 799/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.0410\n",
            "Epoch 800/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0344\n",
            "Epoch 801/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0417\n",
            "Epoch 802/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0335 - val_loss: 0.0390\n",
            "Epoch 803/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0336\n",
            "Epoch 804/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0346 - val_loss: 0.0397\n",
            "Epoch 805/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0317 - val_loss: 0.0360\n",
            "Epoch 806/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0359 - val_loss: 0.0437\n",
            "Epoch 807/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0277 - val_loss: 0.0445\n",
            "Epoch 808/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0304 - val_loss: 0.0371\n",
            "Epoch 809/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0462\n",
            "Epoch 810/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0366 - val_loss: 0.0468\n",
            "Epoch 811/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0342\n",
            "Epoch 812/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0249 - val_loss: 0.0329\n",
            "Epoch 813/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0303 - val_loss: 0.0448\n",
            "Epoch 814/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0368\n",
            "Epoch 815/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0235 - val_loss: 0.0391\n",
            "Epoch 816/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0398\n",
            "Epoch 817/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0388\n",
            "Epoch 818/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.0427\n",
            "Epoch 819/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0406 - val_loss: 0.0462\n",
            "Epoch 820/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.0377\n",
            "Epoch 821/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.0359\n",
            "Epoch 822/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0389\n",
            "Epoch 823/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0305 - val_loss: 0.0310\n",
            "Epoch 824/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0287 - val_loss: 0.0353\n",
            "Epoch 825/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0353 - val_loss: 0.0368\n",
            "Epoch 826/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0267 - val_loss: 0.0284\n",
            "Epoch 827/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.0421\n",
            "Epoch 828/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0381 - val_loss: 0.0434\n",
            "Epoch 829/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0350 - val_loss: 0.0392\n",
            "Epoch 830/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0370 - val_loss: 0.0332\n",
            "Epoch 831/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0392\n",
            "Epoch 832/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0400\n",
            "Epoch 833/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0326 - val_loss: 0.0375\n",
            "Epoch 834/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.0385\n",
            "Epoch 835/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0423\n",
            "Epoch 836/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.0405\n",
            "Epoch 837/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.0391\n",
            "Epoch 838/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0382\n",
            "Epoch 839/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0377\n",
            "Epoch 840/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0290 - val_loss: 0.0357\n",
            "Epoch 841/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0446\n",
            "Epoch 842/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0374\n",
            "Epoch 843/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0313 - val_loss: 0.0401\n",
            "Epoch 844/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0337 - val_loss: 0.0408\n",
            "Epoch 845/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0339 - val_loss: 0.0348\n",
            "Epoch 846/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0356 - val_loss: 0.0307\n",
            "Epoch 847/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0292 - val_loss: 0.0434\n",
            "Epoch 848/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0381\n",
            "Epoch 849/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0246 - val_loss: 0.0443\n",
            "Epoch 850/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0262 - val_loss: 0.0366\n",
            "Epoch 851/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0287 - val_loss: 0.0433\n",
            "Epoch 852/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.0429\n",
            "Epoch 853/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0415\n",
            "Epoch 854/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.0490\n",
            "Epoch 855/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0325 - val_loss: 0.0555\n",
            "Epoch 856/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0309 - val_loss: 0.0405\n",
            "Epoch 857/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0423\n",
            "Epoch 858/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0436\n",
            "Epoch 859/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.0382\n",
            "Epoch 860/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.0359\n",
            "Epoch 861/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0370\n",
            "Epoch 862/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0372 - val_loss: 0.0390\n",
            "Epoch 863/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0335 - val_loss: 0.0461\n",
            "Epoch 864/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0386 - val_loss: 0.0388\n",
            "Epoch 865/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0287 - val_loss: 0.0378\n",
            "Epoch 866/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0385\n",
            "Epoch 867/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0389\n",
            "Epoch 868/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0391\n",
            "Epoch 869/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0422\n",
            "Epoch 870/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0309 - val_loss: 0.0405\n",
            "Epoch 871/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0356\n",
            "Epoch 872/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0499\n",
            "Epoch 873/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0265 - val_loss: 0.0393\n",
            "Epoch 874/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0344 - val_loss: 0.0383\n",
            "Epoch 875/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0317 - val_loss: 0.0388\n",
            "Epoch 876/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0350 - val_loss: 0.0394\n",
            "Epoch 877/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.0386\n",
            "Epoch 878/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0426\n",
            "Epoch 879/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0303 - val_loss: 0.0351\n",
            "Epoch 880/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0392\n",
            "Epoch 881/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0280 - val_loss: 0.0362\n",
            "Epoch 882/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0295 - val_loss: 0.0423\n",
            "Epoch 883/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0228 - val_loss: 0.0348\n",
            "Epoch 884/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.0379\n",
            "Epoch 885/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0384\n",
            "Epoch 886/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0383 - val_loss: 0.0310\n",
            "Epoch 887/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0352 - val_loss: 0.0349\n",
            "Epoch 888/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0352\n",
            "Epoch 889/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.0462\n",
            "Epoch 890/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.0371\n",
            "Epoch 891/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0250 - val_loss: 0.0342\n",
            "Epoch 892/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0299 - val_loss: 0.0363\n",
            "Epoch 893/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0349\n",
            "Epoch 894/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0322 - val_loss: 0.0345\n",
            "Epoch 895/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.0360\n",
            "Epoch 896/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0432\n",
            "Epoch 897/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0246 - val_loss: 0.0379\n",
            "Epoch 898/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0232 - val_loss: 0.0320\n",
            "Epoch 899/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0277 - val_loss: 0.0360\n",
            "Epoch 900/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0312 - val_loss: 0.0315\n",
            "Epoch 901/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0351 - val_loss: 0.0331\n",
            "Epoch 902/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0358 - val_loss: 0.0375\n",
            "Epoch 903/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0298 - val_loss: 0.0364\n",
            "Epoch 904/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0386 - val_loss: 0.0412\n",
            "Epoch 905/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0309 - val_loss: 0.0389\n",
            "Epoch 906/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0680\n",
            "Epoch 907/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0325 - val_loss: 0.0448\n",
            "Epoch 908/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0373\n",
            "Epoch 909/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0336 - val_loss: 0.0357\n",
            "Epoch 910/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0310 - val_loss: 0.0345\n",
            "Epoch 911/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.0382\n",
            "Epoch 912/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0333\n",
            "Epoch 913/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0358\n",
            "Epoch 914/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0386\n",
            "Epoch 915/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0331\n",
            "Epoch 916/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0350\n",
            "Epoch 917/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0397\n",
            "Epoch 918/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0393\n",
            "Epoch 919/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0325 - val_loss: 0.0374\n",
            "Epoch 920/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0253 - val_loss: 0.0384\n",
            "Epoch 921/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0318 - val_loss: 0.0380\n",
            "Epoch 922/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0388\n",
            "Epoch 923/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0383\n",
            "Epoch 924/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0368\n",
            "Epoch 925/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0308 - val_loss: 0.0385\n",
            "Epoch 926/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0328 - val_loss: 0.0463\n",
            "Epoch 927/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.0426\n",
            "Epoch 928/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0381\n",
            "Epoch 929/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0391\n",
            "Epoch 930/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0397\n",
            "Epoch 931/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0260 - val_loss: 0.0404\n",
            "Epoch 932/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0461\n",
            "Epoch 933/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0317 - val_loss: 0.0375\n",
            "Epoch 934/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0415\n",
            "Epoch 935/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0398 - val_loss: 0.0348\n",
            "Epoch 936/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0277 - val_loss: 0.0338\n",
            "Epoch 937/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0342 - val_loss: 0.0433\n",
            "Epoch 938/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0333 - val_loss: 0.0409\n",
            "Epoch 939/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0336 - val_loss: 0.0369\n",
            "Epoch 940/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0299 - val_loss: 0.0399\n",
            "Epoch 941/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0480\n",
            "Epoch 942/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0287 - val_loss: 0.0434\n",
            "Epoch 943/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0364 - val_loss: 0.0365\n",
            "Epoch 944/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0297 - val_loss: 0.0409\n",
            "Epoch 945/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0381\n",
            "Epoch 946/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0308 - val_loss: 0.0386\n",
            "Epoch 947/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0418\n",
            "Epoch 948/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0511\n",
            "Epoch 949/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0258 - val_loss: 0.0341\n",
            "Epoch 950/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0346 - val_loss: 0.0365\n",
            "Epoch 951/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0249 - val_loss: 0.0362\n",
            "Epoch 952/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0438\n",
            "Epoch 953/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0464\n",
            "Epoch 954/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.0419\n",
            "Epoch 955/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0375 - val_loss: 0.0472\n",
            "Epoch 956/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0327 - val_loss: 0.0400\n",
            "Epoch 957/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0243 - val_loss: 0.0384\n",
            "Epoch 958/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0306 - val_loss: 0.0420\n",
            "Epoch 959/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0420\n",
            "Epoch 960/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0228 - val_loss: 0.0446\n",
            "Epoch 961/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0304 - val_loss: 0.0374\n",
            "Epoch 962/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0406\n",
            "Epoch 963/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0438\n",
            "Epoch 964/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0411\n",
            "Epoch 965/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.0362\n",
            "Epoch 966/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0375 - val_loss: 0.0382\n",
            "Epoch 967/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.0349\n",
            "Epoch 968/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0380\n",
            "Epoch 969/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0297 - val_loss: 0.0366\n",
            "Epoch 970/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0313 - val_loss: 0.0369\n",
            "Epoch 971/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0354\n",
            "Epoch 972/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0256 - val_loss: 0.0428\n",
            "Epoch 973/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0296 - val_loss: 0.0370\n",
            "Epoch 974/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0317 - val_loss: 0.0406\n",
            "Epoch 975/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0353 - val_loss: 0.0383\n",
            "Epoch 976/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0430\n",
            "Epoch 977/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0350\n",
            "Epoch 978/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0333\n",
            "Epoch 979/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0341\n",
            "Epoch 980/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0386\n",
            "Epoch 981/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0252 - val_loss: 0.0336\n",
            "Epoch 982/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0291 - val_loss: 0.0402\n",
            "Epoch 983/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.0361\n",
            "Epoch 984/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0262 - val_loss: 0.0353\n",
            "Epoch 985/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0407\n",
            "Epoch 986/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0323 - val_loss: 0.0388\n",
            "Epoch 987/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0251 - val_loss: 0.0368\n",
            "Epoch 988/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0396\n",
            "Epoch 989/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0390\n",
            "Epoch 990/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0215 - val_loss: 0.0393\n",
            "Epoch 991/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0402\n",
            "Epoch 992/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0313 - val_loss: 0.0411\n",
            "Epoch 993/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0240 - val_loss: 0.0376\n",
            "Epoch 994/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0266 - val_loss: 0.0448\n",
            "Epoch 995/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0405\n",
            "Epoch 996/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0484\n",
            "Epoch 997/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.0406\n",
            "Epoch 998/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0317 - val_loss: 0.0419\n",
            "Epoch 999/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0255 - val_loss: 0.0408\n",
            "Epoch 1000/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0244 - val_loss: 0.0340\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0340\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f47630e9160>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHMCAYAAAAzqWlnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACHsUlEQVR4nO3dd3gUxRsH8O9u7tI7SUhIIJVOKFJUQKpKlaIICCpNxK74U1QERQUUC4IiNoqiIE16s9Cldwg9hJDeSC4h/e52f38cueS4S0hIrhC+n+fhMTs7uzf3JubezMzOCLIsyyAiIiKqpURrN4CIiIjInJjsEBERUa3GZIeIiIhqNSY7REREVKsx2SEiIqJajckOERER1WpMdoiIiKhWY7JDREREtRqTHSIiIqrVmOwQWZEgCOjWrVu179OtWzcIglD9BtUyNRVfIrq7Mdmhe5ogCFX698svv1i7yWQGtvBz8Msvv9zxvUvaRUSmKazdACJr+vDDD43K5syZg+zsbLz++uvw9PQ0ONe6desaff3z58/D2dm52vdZsmQJ8vPza6BF9yZr/xwQkXkJ3AiUyFBISAiuXbuGq1evIiQkxNrNoWoQBAFdu3bFrl27qnytpX8OfvnlF4wZMwaLFy/G6NGjq3RtSa8Of50TmcZhLKJKKpkXU1xcjI8//hiNGzeGg4OD/oMpOzsbX3zxBXr06IGgoCDY29vD19cXAwYMwIEDB0ze09SckmnTpkEQBOzatQurV69Ghw4d4OzsDG9vbwwfPhyJiYnltq2sXbt2QRAETJs2DSdPnkS/fv3g6ekJZ2dndO3aFfv37zfZpuTkZIwZMwZ+fn5wcnJC69at8euvvxrcrzKqE4+MjAw8//zzCAgIgIODA5o3b47FixebvKa4uBiffPIJwsPD4eDggNDQUEyZMgVFRUWVauedOHToEIYMGQJ/f3/Y29ujfv36mDBhApKSkozqxsTE4Pnnn0dERAScnJzg7e2NyMhIvPDCC7h+/ToA3fdvzJgxAIAxY8YYDJnFxsbWaNuLiorw2WefITIyEs7OznB3d8dDDz2ElStXmqy/YcMG9OzZU/+9qFevHrp27Yr58+dX+X2W9ccff6B79+7w9PSEo6MjmjZtiunTp5v8vu3duxePPfYYgoKC4ODgAH9/fzzwwAP46KOPaiYoVOtxGIuoip544gkcOXIEffr0waBBg+Dn5wdANyT1/vvvo0uXLujXrx+8vLwQFxeHDRs2YOvWrdi4cSN69+5d6deZP38+NmzYgAEDBqBr1644dOgQVqxYgVOnTuHkyZNwcHCo1H2OHj2Kzz//HA8++CCee+45xMXF4c8//0TPnj1x8uRJNG7cWF83LS0NDz74IK5du4YuXbqgY8eOSElJwUsvvYRHH320SnG603ioVCp06tQJ9vb2GDJkCIqKirBq1SqMHTsWoihi1KhR+rqyLGPo0KFYv349wsPD8corr6C4uBiLFi3CmTNnqtTeylq0aBGef/55ODg4YMCAAahfvz4uX76MBQsWYOPGjTh48CAaNGgAQJc4tm/fHjk5Oejbty+eeOIJFBYW4urVq/jtt9/wyiuvoE6dOhg9ejQ8PT2xfv16DBw40GCY7NYhtOooLi5Gr169sHv3bjRp0gQvv/wy8vPzsXr1agwbNgwnT57EzJkz9fV/+uknTJgwAf7+/njsscfg4+ODtLQ0nD59GosXL8ZLL71UpfdZYuzYsVi8eDGCgoLwxBNPwNPTEwcPHsTUqVOxfft2/PPPP1AodB9P27ZtQ79+/eDu7o4BAwYgMDAQmZmZOH/+PObPn29yCJLIiExEBoKDg2UA8tWrVw3Ku3btKgOQIyMj5fT0dKPrVCqVyfL4+Hg5ICBAbtKkidE5AHLXrl0Nyj788EMZgOzm5iafPn3a4NxTTz0lA5BXrFhhsm1l7dy5UwYgA5AXL15scO6HH36QAcgvvviiQfnYsWNlAPKkSZMMyk+ePCnb29vLAOQPP/zQ6H2YcqfxACCPGzdO1mg0+vKzZ8/KdnZ2ctOmTQ3qL126VAYgP/DAA3JBQYG+/Pr163JYWJjJ+FaWqZ+DixcvykqlUg4PD5cTEhIM6v/777+yKIryoEGD9GXffPONDECeM2eO0f1zc3Pl/Px8/fHixYtNfq8qoyRutzNz5kwZgNynTx9ZrVbry1NTU/Xvd9++ffry++67T7a3t5dTU1ON7lX2e3sn73Pw4MEG5bJc+rNf9j6PP/64DEA+efJkhW0gqgiHsYiq6JNPPoGPj49RuYeHh8nyoKAgDBkyBBcuXEBcXFylX+e1115DZGSkQdn48eMBAIcPH670fTp16mQ0B2Ts2LFQKBQG9ykuLsYff/wBDw8PTJkyxaB+q1at8Oyzz1b6NYE7j4ezszNmz54NOzs7fVmzZs3QqVMnnD9/Hrm5ufrykqGtmTNnwtHRUV/u7e2NqVOnVqm9lfH9999DrVZj7ty5CAwMNDjXs2dPDBgwABs3bsSNGzcMzjk5ORndy8XFxWS5OS1atAiCIGD27Nn6nhMA8PPz08drwYIFBtcoFAoolUqje5n63lbmfc6dOxcKhQKLFi0yqj916lTUqVMHS5curdS9TbWByBQOYxFVUYcOHco9t2/fPsydOxcHDhxAWloaiouLDc4nJibqhzhup127dkZl9evXBwBkZWVVur2m7qNUKlG3bl2D+1y8eBEFBQVo164d3NzcjK7p3Lmz0Qfh7dxJPBo2bAh3d3eje5V9766urgCA48ePQxRFdO7c2ai+OdbXKZlrtHv3bhw5csTofFpaGrRaLS5duoS2bdtiwIABmDx5Ml5++WX89ddf6NWrFzp16oRmzZpZ/FHxGzduIDo6GoGBgWjSpInR+R49egAATpw4oS8bOXIk/ve//6FZs2YYPnw4unbtik6dOsHX19fg2sq+z/z8fJw6dQo+Pj6YM2eOyXY6ODjg/PnzBm1Ys2YN7r//fgwbNgzdu3dHp06dEBQUVJ1w0D2GyQ5RFfn7+5ssX7t2LYYMGQJHR0c88sgjCA8Ph4uLC0RRxK5du7B79+4qTZo1NVej5K9xrVZbrfuU3KvsfbKzswEAdevWNVm/vPLy3Gk8KmovAKM2e3t7m+x5KO/7VB0lE22/+OKLCuuV9D4FBwfj8OHDmDZtGrZt24Y1a9YA0CVub731Fl577bUab2N5Sr6/AQEBJs+XlKtUKn3Zm2++CR8fH8yfPx/ffPMN5syZo3/C7YsvvtAn0pV9n1lZWZBlGenp6ZWeXPz4449j06ZN+Oqrr7Bo0SL8+OOPAIC2bdvi008/xSOPPFL1YNA9h8kOURWV9xf51KlTYW9vj6NHj6Jp06YG5yZMmIDdu3dbonl3rKQ3JTU11eT58srLY4l4eHh4IDMzE2q12ijhSUlJqfb9Tb0eoEscTPU+mdK0aVOsWLECGo0Gp06dwr///otvv/0Wr7/+OlxcXDBu3Lgab6cpJW0vLy7JyckG9Uo8++yzePbZZ6FSqbB//36sXbsWixYtQq9evXDhwgV9L09l3mfJvdu0aYPjx49Xuu39+vVDv379kJeXh0OHDmHTpk34/vvv0b9/f5w4cQLNmjWrcjzo3sI5O0Q1JDo6Gs2aNTP6YJckCf/995+VWlV5TZo0gZOTE06fPm005wRAld+DJeJx3333lXu/O1lb53YeeOABALpHoatKoVCgbdu2eOedd/DHH38AANatW6c/XzJHqSq9dlXh5uaG8PBwJCYm4vLly0bnd+7cCUAXU1M8PT3Rt29f/Pzzzxg9ejQyMzOxZ88eo3oVvU9XV1c0b94cZ8+eRWZmZpXfg4uLC3r06IHZs2dj8uTJKC4uxtatW6t8H7r3MNkhqiEhISG4fPmywVorsixj2rRpOHfunBVbVjn29vYYNmwYsrOzMX36dINzp06dwpIlS6p0P0vEo2Rtmvfffx+FhYX68szMTKP3UBNeeeUVKJVKTJw4EZcuXTI6X1xcbJAIHTt2TD98VFZJL1nZ1bNLHs2uyiT2qho7dixkWcbbb79tkFRlZGTgk08+0dcpsXPnTpMLFaalpQEobX9V3uebb76J4uJijB071mDIrERWVpZBr8+ePXug0WgqdW+i8nAYi6iGTJw4ES+88ALatGmDJ554AkqlEvv27cO5c+fw2GOPYePGjdZu4m199tln2LFjBz7//HMcOnQIHTt2RHJyMlauXIm+ffti3bp1EMXK/Y1kiXg89dRTWLFiBTZs2IAWLVpg4MCBUKvVWL16Ndq3b48rV65U+zXKatKkCRYtWoSxY8eiefPm6N27Nxo1agS1Wo24uDjs3bsXvr6+uHDhAgDgt99+w48//ojOnTsjPDwcXl5euHLlCjZu3AgHBwe88cYb+ns/+OCDcHZ2xpw5c3D9+nX9nKNXX33VaGipPBWtvDx//ny89dZb2Lp1K9avX49WrVqhb9++yM/Px6pVq5CWloZJkyYZTPYePHgwXF1d8cADDyAkJASyLGPv3r04cuQI2rZti4cffrjK73Ps2LE4duwY5s+fj/DwcPTq1QsNGjRAZmYmrl69ij179mDMmDH44YcfAOieSkxMTESnTp0QEhICe3t7HDt2DDt27EBwcDCGDx9eqdjQPc6az70T2aLbrbNTkcWLF8utWrWSnZ2d5Tp16siDBg2ST58+rV8/ZOfOnQb1UcE6O7fWlWVZvnr1qgxAHjVq1G3bVrLOTnnr4gQHB8vBwcFG5QkJCfKzzz4r+/j4yI6OjnKrVq3kX375RV61apUMQP76668rjEFZNRGPEqNGjTL5fSkqKpI/+ugjOTQ0VLa3t5eDg4PlyZMny4WFhTW+zk6J06dPy6NGjZIbNGgg29vby15eXnLz5s3l559/Xt6+fbu+3sGDB+UXXnhBbtmypezl5SU7OjrK4eHh8ujRo+UzZ84Y3Xfr1q3yAw88ILu4uOjXzjH1+rcqqVvRv6ysLFmWZbmgoECeMWOG3Lx5c9nR0VF2dXWVO3XqJC9btszovt9//708aNAgOTQ0VHZycpK9vLzk1q1by7NmzZJzcnLu+H3Ksixv3LhR7tevn+zr6ysrlUq5bt26cvv27eX3339fPn/+vL7eihUr5OHDh8sRERGyi4uL7ObmJjdv3lyePHmynJaWdtvYEMmyLHNvLCKqlPfffx8zZ87Etm3b0KtXL2s3h4io0pjsEJGBpKQk1KtXz6DszJkz6NixI+zt7ZGYmGiwgB8Rka3jnB0iMtCuXTtERESgRYsWcHFxweXLl7F582ZIkoQff/yRiQ4R3XXYs0NEBj766COsW7cOsbGxuHHjBjw9PfHAAw/grbfeMsuqxERE5sZkh4iIiGo1mxzG2rZtGzZu3AiVSoXg4GCMHTsWERERJuuWt2ZHmzZt8N5775m7qURERGTjbK5nZ//+/Zg3bx7Gjx+Phg0bYvPmzTh48CDmzJljcq2J3NxcgwWnbty4gbfffhsvvPACu9yJiIjI9lZQ3rRpE3r27Inu3bsjKCgI48ePh729vX4p81u5urrC09NT/+/06dNwcHDQL+tORERE9zabSnY0Gg1iYmIQGRmpLxNFEZGRkSaXZjdlx44d6NixY7lPjKjVauTn5+v/lV1inoiIiGofm5qzk5OTA0mS4OnpaVDu6elpsL9OeaKjoxEfH48XX3yx3Dpr167F6tWr9ceNGjUyyx46REREZBtsKtmprh07dqBBgwblTmYGdHu99O/fX38sCAIAID093eRmc9UhCAL8/f2RkpJicjM9qhmMs2UwzpbDWFsG42wZ5oqzQqGAr69v5erW2KvWAHd3d4iiaLQTrkqlMurtuVVhYSH27duHYcOGVVhPqVRCqVSaPGeuH3ZZlvk/kgUwzpbBOFsOY20ZjLNlWDPONjVnR6FQICwsDFFRUfoySZIQFRWFRo0aVXjtwYMHodFo8NBDD5m7mURERHQXsalkBwD69++P7du3Y9euXUhISMCCBQtQVFSkf4x83rx5WLZsmdF1O3bsQPv27eHm5mbhFhMREZEts6lhLADo2LEjcnJysHLlSqhUKoSEhGDy5Mn6YayMjAz9PJsSSUlJuHDhAqZMmWKFFhMREZEts7lFBa0lPT0darW6Ru8pCAICAgKQnJzM8WAzYpwtg3G2HMa6+jQaDfLz829bz97eHsXFxRZo0b3tTuIsyzIUCgVcXFxMnlcqlXfnBGUiIqLq0mg0yMvLg5ubG0Sx4tkaSqWyxv/QJWN3Gue8vDwUFRXBwcGhWq9vc3N2iIiIqiM/P79SiQ7ZPmdnZxQVFVX7PvxJICKiWoeJTu1w6xzdO8WfBiIiIqrVmOwQERFRrcZkh4iIqJa5//778fPPP9fIvfbv34/AwEBkZ2fXyP2sgU9jERER2YAhQ4agWbNm+Pjjj6t9ry1btsDZ2bkGWlU7MNkxE1mtBm6ooFHaWbspRERUC8iyDK1WC4Xi9h/dderUsUCL7h4cxjKXuCvQvjMOae+Mt3ZLiIjIxr3xxhs4cOAAFi5ciMDAQAQGBmLFihUIDAzEjh070Lt3b4SGhuLw4cOIjY3FmDFj0KpVKzRs2BB9+/bFnj17DO536zBWYGAgli1bhnHjxiE8PBydOnXC33//fcft3bx5M7p3747Q0FDcf//9+OGHHwzO//LLL+jUqRPCwsLQqlUrjB07Vn9u06ZN6NmzJ8LDw9G8eXMMGzasUgtAVgd7dsyNi58SEVmVLMtAsem1WmRJq+uJNxd7h0o9Pv3xxx8jJiYGTZo0wVtvvQUAuHjxIgBg5syZ+OCDD9CgQQN4eHggKSkJPXr0wDvvvAN7e3usXr0aY8aMwZ49exAYGFjua8yePRtTpkzBlClTsHjxYrzyyis4dOgQvLy8qvSWTp8+jRdeeAFvvvkmBgwYgKNHj2Ly5Mnw8vLCsGHDcOrUKXzwwQf45ptv0K5dO6hUKhw9ehQAkJqaipdffhnvv/8++vTpg9zcXBw6dMjsK4Uz2TE7ZjtERFZVXATplaEmT1V/ubqKifNWAg6Ot63n7u4Oe3t7ODo6ws/PDwAQHR0NAHj77bfRpUsXfV0vLy80b95cfzxp0iRs27YNf//9N8aMGVPuawwdOhSDBg0CALz77rtYuHAhTp48ie7du1fpPf3000/o3LkzJk6cCAAIDw/H5cuX8cMPP2DYsGFITEyEs7MzHn74Ybi6uiIoKAht2rSBWq1GWloaNBoN+vbti6CgIABA06ZNq/T6d4LDWOZSkslzXxsiIqqGli1bGhzn5eXh448/RteuXdG0aVM0bNgQly9fRmJiYoX3KZtUODs7w83NDRkZGVVuz+XLl9G+fXuDsvbt2+Pq1avQarXo0qULgoKC8OCDD+LVV1/FmjVr9MNUzZo1Q+fOndGzZ088//zzWLp0KVQqVZXbUFXs2TEXJjtERLbB3kHXw2KC2ffGsq/enk4AjJ6q+vjjj7F3715MnToVISEhcHR0xPPPP3/bjTaVSqXBsSAIkCSp2u27laurK7Zt24b9+/djz549+PLLLzF79mxs3rwZHh4eWL58OY4ePYrdu3dj8eLFmDVrFjZt2oQGDRrUeFtKMNkxm5IxWiY7RETWJAhCuUNJglIJQbSNp2aVSmWlko+jR4/iySefRJ8+fQDoenoSEhLM3Ty9hg0b4siRIwZlR44cQVhYGOzsdLFUKBTo0qULunTpgjfffBNNmzbFvn370LdvXwiCgPbt26N9+/aYOHEiOnTogK1bt2LChAlmazOTHXNhrkNERFVQv359nDhxAvHx8XBxcSk38QkNDcXWrVvxyCOPQBAEfPHFF2bpoSnPhAkT0LdvX3z99dcYMGAAjh07hsWLF2PmzJkAgH/++QdxcXG4//774enpie3bt0OSJISHh+P48eP477//0LVrV/j4+OD48ePIzMxEw4YNzdpmJjvmUkOblxER0b1hwoQJeOONN9CtWzcUFhZi9uzZJut9+OGHePPNNzFw4EB4e3vj5ZdfRm5ursXaGRkZiR9++AFffvkl5s6dCz8/P7z99tsYNmwYAMDDwwNbt27F7NmzUVhYiNDQUPz4449o3LgxLl++jEOHDmHBggXIzc1FYGAgPvjgA/To0cOsbRZkcz/vdZdIT0+v0XFb+doVSNMnwq6OH4RZC83+WN29TBAEBAQEIDk5mXE2I8bZchjr6snJyYG7u3ul6pp9zg4BqF6cy/t+KpVK+Pr6VuoefBrL7PiLioiIyJo4jGUu+oexZHBAi4iIbNU777yDNWvWmDz3+OOPY9asWRZuUc1jsmM2fPSciIhs39tvv40XXnjB5Dk3NzcLt8Y8mOyYi36dHes2g4iIqCI+Pj7w8fGxdjPMinN2zEU/dsVsh4iIyJqY7JgNZ+oQERHZAiY75sLtIoiIiGwCkx1zY7JDRERkVUx2zIb7RRAREdkCJjvmwlyHiIjuIvHx8QgMDERUVJS1m1LjmOyYC+fsEBFRFQwZMgQffPBBjd3vjTfewNixY2vsfnczJjtmo0t2uK8NERGRdTHZMRc+eU5ERJX0xhtv4MCBA1i4cCECAwMRGBiI+Ph4XLhwAU8//TQaNmyIVq1a4dVXX0VmZqb+uk2bNqFnz54IDw9H8+bNMWzYMOTn5+Orr77CqlWr8Ndff+nvt3///iq368CBA+jXrx9CQ0PRpk0bzJw5ExqN5ravDwD79+9Hv379EBERgYiICAwcOBAJCQnVD9Yd4ArKZsNJO0REtkCWZRRpTf8u1kKCWiOZ7bUd7AQIwu3/+v34448RExODJk2a4K233gIAKBQK9OvXD0899RSmTZuGwsJCzJgxAxMmTMCqVauQmpqKl19+Ge+//z769OmD3NxcHDp0CLIs44UXXsDly5eRm5uL2bNnAwA8PT2r1Pbk5GQ888wzGDp0KObOnYvo6Gi8/fbbcHBwwP/+978KX1+j0WDcuHEYMWIEvvvuO8iyjCNHjlQqFubAZMdcOGeHiMgmFGllDFtxySqvvWJYIzgqbv8B7+7uDnt7ezg6OsLPzw8AMGfOHLRo0QLvvfeevt5XX32F9u3b48qVK8jPz4dGo0Hfvn0RFBQEAGjatKm+rqOjI4qLi/X3q6pff/0V9erVw4wZMyAIAiIiIpCSkoKZM2di4sSJSEtLK/f1s7KykJOTg4cffhghISFQKpUIDQ29o3bUBCY75sZch4iI7sC5c+ewf/9+NGzY0OjctWvX0LVrV3Tu3Bk9e/ZE165d0bVrV/Tr16/KPTjliY6ORtu2bQ16Y9q3b4+8vDwkJyejWbNm5b6+l5cXhg4dipEjR+Khhx5Ct27d0LdvX9StW7dG2lZVTHbMhXtjERHZBAc7ASuGNTJ5TqlQQq1Rm/W171R+fj4eeeQRTJ482ehc3bp1YWdnh+XLl+Po0aPYvXs3Fi9ejFmzZmHTpk1o0KBBdZpdKbd7/a+//hrjxo3Dzp07sW7dOnz66af4448/0LZtW7O37VacoGw2HMYiIrIFgiDAUSGa/qcsp7yG/lVljopSqYQklc4fatGiBS5evIj69esjNDTU4J+zs7P+vbVv3x5vvfUW/vrrLyiVSmzduhUAYG9vD61We8dxi4iIwLFjxwyeKj5y5AhcXV0REBBw29cveQ+vvvoqtmzZgsaNG2PdunV33J7qYLJjLpyzQ0REVVC/fn2cOHEC8fHxyMzMxOjRo6FSqfDSSy/h5MmTiI2Nxa5duzBx4kRotVocP34c33zzDU6dOoXExERs2bIFmZmZ+mGvoKAgnD9/HtHR0cjMzIRaXbUerFGjRiEpKQlTpkxBdHQ0/vrrL3z11Vd4/vnnIYpiha8fFxeHTz/9FEePHkVCQgJ27tyJq1evIiIiwhyhuy0OY5kdkx0iIrq9CRMm4I033kC3bt1QWFiIgwcPYt26dZg5cyZGjBiBoqIiBAUFoVu3bhBFEW5ubjh06BAWLFiA3NxcBAYG4oMPPkCPHj0AACNHjsSBAwfQt29f5OXlYdWqVejYsWOl2xMQEIDffvsN06dPxyOPPAJPT0889dRTeP311wGgwtdPT09HdHQ0Vq1ahaysLNStWxejR4/GM888Y5bY3Y4gc9U7AEB6enqVs96KyBmpkN4bD8HBAXbfrebigmYkCAICAgKQnJzMOJsR42w5jHX15OTkwN3dvVJ1lUpljf7uJ9OqE+fyvp9KpRK+vr6VugeHscxFP4xl3WYQERHd6ziMZWb8q4yIiGzBN998g2+//dbkufvvvx+///67hVtkOTaX7Gzbtg0bN26ESqVCcHAwxo4dW+GEpry8PPzxxx84fPgwcnNz4evri1GjRuG+++6zYKtN4QRlIiKyHc888wwee+wxk+ccHR0t3BrLsqlkZ//+/ViyZAnGjx+Phg0bYvPmzZgxYwbmzJkDDw8Po/oajQbTp0+Hu7s73nzzTXh7eyMjI0P/SJ5VcZ0dIiKyIV5eXvDy8rJ2M6zCppKdkg3FunfvDgAYP348jh8/jp07d2LQoEFG9Xfs2IHc3Fx88sknUCh0b+VOl8WueezZISIisgU2k+xoNBrExMQYJDWiKCIyMhKXLpne0+TYsWNo2LAhFi5ciKNHj8Ld3R2dOnXCoEGDIIpWnnvNCcpERFbBuZJ0K5tJdnJyciBJktGeHp6enkhKSjJ5TWpqKtLT09G5c2e89957SElJwYIFC6DVavHkk0+avEatVhs8/iYIApycnPRf1xix9F7W2uX1XlESX8bZvBhny2Gsq0ehUCAvLw/Ozs6M4V2uuLgYglC5neMrYjPJzp2QZRnu7u6YMGECRFFEWFgYMjMzsWHDhnKTnbVr12L16tX649DQUMyaNavSz+pXltZBCV2KJsPf379G702mMc6WwThbDmN951QqFbKysm7by1NYWGihFt3b7iTOgiBAqVSiQYMG1R6tsZlkx93dHaIoQqVSGZSrVKpyd3D19PSEQqEwCEJgYCBUKhU0Go1+Hk9ZgwcPRv/+/fXHJdlieno6NBpN9d/ITbIq8+YXMlJSUtitakaCIMDf359xNjPG2XIY65pxuyeMGGfLqG6cU1NTTZYrFIpKd1TYTLKjUCgQFhaGqKgodOjQAQAgSRKioqLQu3dvk9c0btwY+/btgyRJ+oQnOTkZXl5eJhMdQLfiolKpNHmuJn/Y9XeSZcg3/5F5Mc6WwThbDmNtGYyzZVgzzja1gnL//v2xfft27Nq1CwkJCViwYAGKiorQrVs3AMC8efOwbNkyff1HH30Uubm5+OWXX5CUlITjx49j7dq16NWrl5XeQRkcJiYiIrIJNtOzAwAdO3ZETk4OVq5cCZVKhZCQEEyePFk/jJWRkWEwScnHxwfvv/8+fv31V7z99tvw9vZGnz59TD6mbnml7eRfDERERNZjU8kOAPTu3bvcYatp06YZlTVq1AgzZswwc6vuQNmZ47JseExEREQWY1PDWLULkxsiIiJbwGTHXAxyHQ5jERERWQuTHXMxGMayXjOIiIjudUx2LIETlImIiKyGyY7ZlB3HYrJDRERkLUx2zIW5DhERkU1gsmM2zHaIiIhsAZMdc+G6OkRERDaByY65GHTssGeHiIjIWpjsmM0tKygTERGRVTDZISIiolqNyY653Lo3FhEREVkFkx2z4dNYREREtoDJjrkw1yEiIrIJTHbMhY+eExER2QQmO2bDOTtERES2gMmOuRh07DDZISIishYmO2ZTtmfHeq0gIiK61zHZsQQOYxEREVkNkx1zEfg4FhERkS1gsmMu3BuLiIjIJjDZMRs+ek5ERGQLmOyYC9fZISIisglMdsxE4N5YRERENoHJjiUw2SEiIrIaJjtERERUqzHZMaeSoSz27BAREVkNkx2zKpm3w2SHiIjIWpjsmBMfyCIiIrI6JjvmpB/Gsm4ziIiI7mVMdsyKc3aIiIisjcmOOemHsZjsEBERWQuTHbNizw4REZG1MdkhIiKiWo3JjjlxnR0iIiKrY7JjTnz0nIiIyOqY7JgVe3aIiIisjcmOOQns2iEiIrI2JjtmxZ4dIiIia2OyY076rbGY7BAREVkLkx0iIiKq1RTWboAp27Ztw8aNG6FSqRAcHIyxY8ciIiLCZN1du3Zh/vz5BmVKpRJLly61RFMrxkfPiYiIrM7mkp39+/djyZIlGD9+PBo2bIjNmzdjxowZmDNnDjw8PExe4+TkhLlz51q4pZXBCcpERETWZnPDWJs2bULPnj3RvXt3BAUFYfz48bC3t8fOnTvLvUYQBHh6ehr8swncG4uIiMjqbKpnR6PRICYmBoMGDdKXiaKIyMhIXLp0qdzrCgsL8dJLL0GWZYSGhuKpp55C/fr1LdDi2+EwFhERkbXZVLKTk5MDSZKMemY8PT2RlJRk8pp69erhxRdfRHBwMPLz87FhwwZMmTIFs2fPRp06dYzqq9VqqNVq/bEgCHByctJ/XaNu3k+AwDV3zKjk+1bj3z8ywDhbDmNtGYyzZdhCnG0q2bkTjRo1QqNGjQyOJ06ciH/++QfDhw83qr927VqsXr1afxwaGopZs2bB19e3xtuWaCdCAuDjUwfKgIAavz8Z8vf3t3YT7gmMs+Uw1pbBOFuGNeNsU8mOu7s7RFGESqUyKFepVJWeh6NQKBAaGoqUlBST5wcPHoz+/fvrj0syzfT0dGg0mjtqd3kkSTd8lZGeDiidavTeVEoQBPj7+yMlJQUyhwzNhnG2HMbaMhhnyzBXnBUKRaU7Kmwq2VEoFAgLC0NUVBQ6dOgAAJAkCVFRUejdu3el7iFJEuLi4tCmTRuT55VKJZRKpclz5vphl2WZ83YsQJZl/sKyAMbZchhry2CcLcOacbapZAcA+vfvj++++w5hYWGIiIjAli1bUFRUhG7dugEA5s2bB29vb4wYMQIAsHr1ajRs2BD+/v7Iy8vDhg0bkJ6ejp49e1rxXdzEcWAiIiKrs7lkp2PHjsjJycHKlSuhUqkQEhKCyZMn64exMjIyDCY55ebm4scff4RKpYKLiwvCwsIwffp0BAUFWekdlMWnsYiIiKzN5pIdAOjdu3e5w1bTpk0zOB49ejRGjx5t/kbdCe6NRUREZHU2t6hgrcJhLCIiIqtjsmNWHMYiIiKyNiY75sTtIoiIiKyOyY4lsGeHiIjIapjsmBPn7BAREVkdkx2zKpmzY91WEBER3cuY7JgT5+wQERFZHZMds+LTWERERNbGZMecBA5jERERWRuTHXPiMBYREZHVMdkxKw5jERERWRuTHSIiIqrVmOyYE9fZISIisjomO2bFYSwiIiJrY7JjTiUdO0x2iIiIrIbJjjnph7GY7BAREVkLkx2z4jo7RERE1sZkx5y4zg4REZHVMdkhIiKiWo3JjlnxaSwiIiJrY7JjTjcnKDPXISIish4mO+bEOTtERERWx2THrDiMRUREZG1MdsyJ20UQERFZHZMds2LPDhERkbUx2SEiIqJaTWHtBtRW+Wotrjr5Q+GmQSP27BAREVkNkx0zuaYqwuSgx+FfJwM/8GksIiIiq+EwlpmINycnSxD55DkREZEVMdkxE/Hm3GRJEMBsh4iIyHqY7JiJXUnPjiDyaSwiIiIrYrJjJqU9OxzGIiIisiYmO2ZSOmeHw1hERETWxGTHTAx6doiIiMhq+ElsJnY3sx0t5+wQERFZFZMdMynp2SlQOOJgNpczIiIishYmO2YiltkE9LM4Jyu2hIiI6N7GZMdMRG54TkREZBOY7JhJ2Z4dAChQS1ZqCRER0b2NyY6Z3NqzM3zlJWy9lGWdxhAREd3DmOyYya09OwDww5FUK7SEiIjo3maTjwlt27YNGzduhEqlQnBwMMaOHYuIiIjbXrdv3z7MnTsX7dq1w6RJkyzQ0vJxzg4REZFtsLmenf3792PJkiUYMmQIZs2aheDgYMyYMQPZ2dkVXpeWlobffvsNTZs2tVBLK2aqZ4eIiIgsz+aSnU2bNqFnz57o3r07goKCMH78eNjb22Pnzp3lXiNJEr799lsMHToUfn5+Fmxt+Uz17CjY3UNERGRxNpXsaDQaxMTEIDIyUl8miiIiIyNx6dKlcq9bvXo13N3d0aNHD0s0s1JM9ew4KZjsEBERWZpNzdnJycmBJEnw9PQ0KPf09ERSUpLJay5cuIAdO3bg888/r9RrqNVqqNVq/bEgCHByctJ/XVPsTKSRjkqxRl+DdEpiytiaF+NsOYy1ZTDOlmELcbapZKeqCgoK8O2332LChAlwd3ev1DVr167F6tWr9cehoaGYNWsWfH19a7RtsiwDuGBQ5ubkgICAgBp9HSrl7+9v7SbcExhny2GsLYNxtgxrxtmmkh13d3eIogiVSmVQrlKpjHp7ACA1NRXp6emYNWuWvky+uenm8OHDMWfOHKPgDh48GP3799cfl2Sa6enp0Gg0NfROTFPIWiQnJ5v1Ne5FgiDA398fKSkp+u8/1TzG2XIYa8tgnC3DXHFWKBSV7qiwqWRHoVAgLCwMUVFR6NChAwDd5OOoqCj07t3bqH69evXw5ZdfGpQtX74chYWFGD16NHx8fIyuUSqVUCqVJl/f3D/sSjuB/0OZkSzLjK8FMM6Ww1hbBuNsGdaMs00lOwDQv39/fPfddwgLC0NERAS2bNmCoqIidOvWDQAwb948eHt7Y8SIEbC3t0eDBg0MrndxcQEAo3JbYMdxYSIiIouzuWSnY8eOyMnJwcqVK6FSqRASEoLJkyfrh7EyMjLu2slkIh89JyIisjibS3YAoHfv3iaHrQBg2rRpFV778ssvm6FFNYNPnhMREVmeTa2zU9uxZ4eIiMjymOwQERFRrcZkx4I425+IiMjymOxYkMRch4iIyOKY7FgQO3aIiIgsj8mOBUnMdoiIiCyOyY4FSdZuABER0T2oWuvsZGRkICMjA02aNNGXxcbGYtOmTVCr1ejUqZN+2wfiMBYREZE1VKtnZ9GiRVi1apX+WKVS4aOPPsKhQ4dw/vx5fPXVVzh06FC1G1lb8GksIiIiy6tWsnPlyhVERkbqj/fs2YPi4mJ88cUX+OGHHxAZGYmNGzdWu5G1BZ/GIiIisrxqJTu5ubnw8PDQHx87dgzNmjWDv78/RFFEhw4dkJiYWO1G1haJOcXs3SEiIrKwaiU77u7uSE9PBwDk5eXh8uXLaNWqlf68JEmQJE7LLZFdpMXGi1nWbgYREdE9pVoTlCMjI7F161Y4Ozvj7NmzkGXZYEJyQkIC6tSpU+1G1ia/n0zHgCbe1m4GERHRPaNayc6IESOQnJyM3377DQqFAs888wz8/PwAAGq1GgcOHECnTp1qpKFEREREd6JayY6npyc++eQT5Ofnw97eHgpF6e1kWcbUqVPh4+NT7UberV7s4I/vD6dYuxlERET3tBpZVNDZ2dkg0QEAe3t7hISEwNXVtSZe4q7Up5EX9g6pj6Gx/+jLOD2ZiIjIsqrVs3PmzBlcvXoVAwYM0Jft2LEDq1atgkajQadOnfDss89CFO/dhZrtHBzRNPuqtZtBRER0z6pWFrJq1SrExsbqj+Pi4vDzzz/D3d0dzZo1w9atW7Fhw4bqtvGuJtjbQyzzuHmxVkZWgcaKLSIiIrq3VCvZSUxMRHh4uP54z549cHJywscff4yJEyeiZ8+e2LNnT7UbeTcTlA4GyQ4ALDqeZqXWEBER3XuqlewUFhbCyclJf3zy5Em0bt0aDg4OAICIiAj9Ojz3KsHBAcItW4Cq2LNDRERkMdVKdnx8fHDlyhUAQEpKCuLj49GyZUv9+dzcXCiVyuq18C4nKJUQbunZEQQrNYaIiOgeVK0Jyp07d8bq1auRmZmJhIQEuLi4oH379vrzMTExCAgIqHYj72aCnQLiLc9g2THbISIisphqJTuPP/44NBoNTpw4AR8fH7z00ktwcXEBoOvVOXv2LPr27VsjDb2b3TpnR2SuQ0REZDHVSnbs7Ozw1FNP4amnnjI65+rqip9//rk6t6817B4bBiSVHovMdoiIiCymWslOWYWFhcjIyACgm8vj6OhYU7e+6wl2dgbHzHWIiIgsp9rJTnR0NJYuXYoLFy7odzgXRRFNmjTB008/bfBo+r1KtDMMs8g5O0RERBZTrWTn8uXLmDZtGhQKBXr06IHAwEAAuvV39u3bhw8//BDTpk1DREREjTT2biUoyu/ZSc0thpuDHZyVdiAiIqKaV61kZ/ny5fD29sYnn3wCT09Pg3NPPvkkpk6dij/++ANTp06tzsvc9cRbhrFKnsZKuVGMCRtiYG8nYNXwxtZoGhERUa1XrXV2Ll++jEceecQo0QF0O6I//PDDuHz5cnVeolYob87O6dR8ALotJIiIiMg8qpXsCIIArVZb7nlJkiBwfgrsFIYLK3LODhERkeVUK9lp3Lgx/vrrL5NbQmRkZODvv/9GkyZNqvMStYJYzpwdmR06REREZletOTtPPfUUPvzwQ7zxxhvo0KGDfrXkpKQkHD16FKIomlyD514jKBQAivXHJT07ErMdIiIis6tWshMaGoqZM2fijz/+wNGjR1FcrPtAt7e3R+vWrfHkk0/Czc2tRhp6NxPsbk12rNcWIiKie02119kJCgrC22+/DUmSkJOTAwBwd3eHKIpYs2YNVqxYgRUrVlS7oXczUWEYZq1Gt+s5+3WIiIjMr1pzdgxuJIrw9PSEp6cnRLHGblsriErDZEeK1e0Uz1EsIiIi82NWYgHCrT07+blWagkREdG9h8mOJdg7GBxqRV3ywwnKRERE5sdkxxJuSXYkgWEnIiKylCpPUI6Jial03czMzKrevlaSbunA0d5MdtivQ0REZH5VTnbee+89c7SjVlPc8qy5JOoWGeQoFhERkflVOdl58cUXzdGOWs3LSYHhXjewO7EIyc4+JoexZFnm1hpERERmUOVkp1u3bmZohqFt27Zh48aNUKlUCA4OxtixYxEREWGy7qFDh7B27VqkpKRAq9XC398fjz32GLp06WL2dlbFMN9C+BzYgXlNhqFk30+5zECWJAN2zHWIiIhqXLUXFaxp+/fvx5IlSzB+/Hg0bNgQmzdvxowZMzBnzhx4eHgY1Xd1dcXjjz+OevXqQaFQ4Pjx45g/fz7c3d3RunVry7+BcgjhTWEn/wsApclOmWEsjmgRERGZh809FrRp0yb07NkT3bt3R1BQEMaPHw97e3vs3LnTZP3mzZujQ4cOCAoKgr+/P/r27Yvg4GBcuHDBwi2vmODtA7uW7QCUTlg2SHY4gYeIiMgsbKpnR6PRICYmBoMGDdKXiaKIyMhIXLp06bbXy7KMqKgoJCUlYeTIkSbrqNVqqNVq/bEgCHByctJ/XZNK7lfyX7s6vkAKoC2Zn1Pm9WQInLNzh26NM5kH42w5jLVlMM6WYQtxtqlkJycnB5IkwdPT06Dc09MTSUlJ5V6Xn5+PCRMmQKPRQBRFjBs3Di1btjRZd+3atVi9erX+ODQ0FLNmzYKvr2+NvAdT/P39AQDuHp5Aim6dnYCAALjFFQNIAwD41a0LZ3ub+nbcdUriTObFOFsOY20ZjLNlWDPOteLT1dHREV988QUKCwtx5swZLFmyBHXr1kXz5s2N6g4ePBj9+/fXH5dkmunp6dDc3KCzpgiCAH9/f6SkpECWZRQUFQJwxAWlL+b8fcZg9/Pk5BQ429vV6OvfK26NM5kH42w5jLVlMM6WYa44KxSKSndU2FSyU7JbukqlMihXqVRGvT1liaKozxhDQkKQmJiIdevWmUx2lEollEqlyfuY64ddlmXIsgzR1RWALqFaeiodT7fy0dfRSjL/Z6umkjiTeTHOlsNYWwbjbBnWjLNNTVBWKBQICwtDVFSUvkySJERFRaFRo0aVvo8kSQbzcmyFwi/A4Ph0Sr7+a8nSjSEiIrpH2FTPDgD0798f3333HcLCwhAREYEtW7agqKhIv77PvHnz4O3tjREjRgDQzcEJDw9H3bp1oVarceLECezduxfPPfecFd+Fabeuo3M6tTTZ4V8VRERE5mFzyU7Hjh2Rk5ODlStXQqVSISQkBJMnT9YPY2VkZBjM6C4qKsKCBQtw/fp12NvbIzAwEK+++io6duxopXdQPpcK5uQw1yEiIjIPm0t2AKB3797o3bu3yXPTpk0zOB4+fDiGDx9ugVZVn1sFyY6W2Q4REZFZ2NScndrO3bGCnh0LtoOIiOhewmTHghwq2PyKHTtERETmwWTHgipaPVJitkNERGQWTHZshMRch4iIyCyY7FjYjG7+6JARZVTOjh0iIiLzYLJjYS0CPfHslS1G5RKnKBMREZkFkx0rcOjU3aiMw1hERETmwWTHChwHjTAq4zAWERGReTDZsQJTj6BzuwgiIiLzYLJjBUo7AaJsuPUnh7GIiIjMg8mOFYiCgIbFGQZlTHaIiIjMg8mOlTTXpBscy3wai4iIyCyY7FhJPaHA4Jg9O0RERObBZMdKfEW1wTG3iyAiIjIPJjtWEqQwTHaY6xAREZkHkx0r8XYQMO3kT/pjJjtERETmwWTHWuwd0FIVjfrIA8DtIoiIiMyFyY61OLkAAARJt94OJygTERGZB5Mda3FyBgCIshYAh7GIiIjMhcmOtdxMdkp7dpjtEBERmQOTHSsRSnp2JPbsEBERmROTHWvRz9nRJTucs0NERGQeTHaspaRnR6sBANwo1iI9T13RFURERHQHFNZuwD2rpGfnZrIz90AyAODXxyPg6cRvCxERUU1hz461uHsCAMRbJutczCgwUZmIiIjuFJMdKxGUSgBApoO7QbmGM5WJiIhqFJMdK7u1Z0dVoLVSS4iIiGonJjtWJIx8EWOiNxiUZRZorNQaIiKi2onJjhWJ3fqgw9PDMCT2X32ZqpDJDhERUU1ismNtHl54KvZvtMqJAQDkqyUrN4iIiKh2YbJjbe6eEAB0STwMgMkOERFRTWOyY21uHgAAF00hACC/mBOUiYiIahKTHSsT7OwAAM7am8kOe3aIiIhqFJMdG+GkKQLAZIeIiKimMdmxAeKU2XDR6FZOzldzGIuIiKgmMdmxBfVD4QTdI+eFGhlaboFORERUY5js2ABBtIOLoz1EWTeElV3E3h0iIqKawmTHRihc3eBbmAUASMoptnJriIiIag8mO7bCzR2B+ekAgKQbTHaIiIhqCpMdGyG4eSCgQJfsJLJnh4iIqMYw2bEVbh6ol58BgD07RERENUlh7QaYsm3bNmzcuBEqlQrBwcEYO3YsIiIiTNb9999/sWfPHsTHxwMAwsLC8NRTT5Vb32Z5eCMw/zwA9uwQERHVJJvr2dm/fz+WLFmCIUOGYNasWQgODsaMGTOQnZ1tsv65c+fQqVMnfPjhh5g+fTrq1KmD6dOnIzMz08Itrx4hrDH8CnVtTs9TW7k1REREtYfNJTubNm1Cz5490b17dwQFBWH8+PGwt7fHzp07TdZ/7bXX0KtXL4SEhCAwMBAvvPACZFnGmTNnLNzyagprBEetrkenWCvr3kNqHp5bG42jiblWbhwREdHdy6aGsTQaDWJiYjBo0CB9mSiKiIyMxKVLlyp1j6KiImg0Gri6upo8r1aroVaX9pwIggAnJyf91zWp5H6Vua9g7wAHB6X++Knl51Eg6XLRT3YlYMPTTWu0bbVJVeJMd45xthzG2jIYZ8uwhTjbVLKTk5MDSZLg6elpUO7p6YmkpKRK3WPp0qXw9vZGZGSkyfNr167F6tWr9cehoaGYNWsWfH1977jdt+Pv71+pelp3d/3XJYlOCSdPH3g6KW+9hMqobJypehhny2GsLYNxtgxrxtmmkp3qWrduHfbt24dp06bB3t7eZJ3Bgwejf//++uOSTDM9PR0ajaZG2yMIAvz9/ZGSkgJZvv0WELKTMxSSBhrR+Nvy8cZTePuhwBptX21R1TjTnWGcLYextgzG2TLMFWeFQlHpjgqbSnbc3d0hiiJUKpVBuUqlMurtudWGDRuwbt06TJ06FcHBweXWUyqVUCpN95CY64ddluXK3dvNHfaS2mSyE5WWz/8Zb6PScaZqYZwth7G2DMbZMqwZZ5uaoKxQKBAWFoaoqCh9mSRJiIqKQqNGjcq9bv369fjzzz8xefJkhIeHW6KpZiG4ecBeMt275GDHMWUiIqI7YVPJDgD0798f27dvx65du5CQkIAFCxagqKgI3bp1AwDMmzcPy5Yt09dft24dVqxYgRdffBF+fn5QqVRQqVQoLCy00juohnoN4KA1vcaOPZMdIiKiO2JTw1gA0LFjR+Tk5GDlypVQqVQICQnB5MmT9cNYGRkZBjO6//nnH2g0GsyePdvgPkOGDMHQoUMt2fRqE+7rCPuYKJPn4rKLodbKUDLpISIiqhKbS3YAoHfv3ujdu7fJc9OmTTM4/u677yzQIssQvH2gdfMCJNPnt1zKwsCm3pZtFBER0V3O5oax7nVqO9NPkQHAouNpUGs5iY6IiKgqmOzYmExNxcNU2y5nWaglREREtQOTHRvTxrf8nh2Am4QSERFVFZMdGzO8dV30S/gPHdJNT1TeelmFvGKthVtFRER092KyY2Ma+rpgXPQG1Ck2vcs7ABxK4MagRERElcVkx0bl2znqv+4fv9fgnMSVPomIiCqNyY6NylU667+2l9QG5/hEFhERUeUx2bFBwrg3katw0h8rZMM5Ormcs0NERFRpTHZskPhAN/hFhJV7PruIyQ4REVFlMdmxUeMeCkO3lKP49Pg8yDBceye7UIsDcTdwNesu3P+LiIjIwmxyuwgCvJwUeO3CSgDAMe8mBucOxt/AntgcAMD6kU2MriUiIqJS7NmxZY1a6P4rGPbsFJeZoCzzySwiIqIKMdmxYeKESUBoI1SUzuSry9k1lIiIiAAw2bFpgrsnxOHjK6yjKuRkZSIiooow2bF1Lm5GE5TLyi7UWLAxREREdx8mO7bOyRkumgL94YN1lQanU3PVt15BREREZTDZsXVOzuiTuB/3Xb+AFy7+iZe3ToejovTbNudAMlJucCd0IiKi8jDZsXGC0h6OkhpTzizCo8mH4JxzHd/0C8Fbwnl9nQkbYjBtR7zRk1myLONkch7S89j7Q0RE9y4mO3chPxclWu35w6DsRHIebtyysvKF9AJ8uCMez627YsnmERER2RQmO3ejC6fhojVePblQY9izE6sq0n9dwEfUiYjoHsVk5y4kzZ4KAHj9nGHvTkJOEX49kYb3Vp9E4cYVcLG305+7fL0ARERE9yImO3exh9JOGhx/tDMBa85l4lyRI87uPYzizOv6cxczmOwQEdG9icnOXUDoPwxQGG9jJlawtrJWsENRcenE5MuJWWZpGxERka1jsnMXEAeOhPjNcsDd0+icn2y6x+aUV0MUa0rn6RzK0Oo3DyUiIrqXMNm5SwhKe8DEpp8f5O03WX9T/YeQlm/4dNZX+5KQwxWXiYjoHsNk525i72BUVK8wvdzqifnGT2AVaPhUFhER3VuY7NxFxHFvAs4uhoWFBXgo9YTJ+pkmFlYu0lS0hzoREVHtw2TnLiI0bAZxzjLDwuwsvHhxNV49v9yofrra+NvLnh0iIrrXMNm5ywiCAOH+rqUFmRlwlNTonHbKqG4h7IzKuLggERHda5js3IWE0a9B/HAuYKcAtLoJx0pZe5urdArZs0NERPcYJjt3IUGhhBAUCnj7mDzfIDe53GuZ7BAR0b2Gyc7dzMTTWQDQNPsqnri2XX88NPYf/dccxiIionuN8bK8dPcQBJPFjR2KoZVL19N5LGEv0hy9sMu/HX44kgp7OwFNfZ1xVVWICG9H1HW1t1SLiYiILI7Jzt3slmTn20Nf4IJHMLr4CbihzoV7cS5C8pLhoimEk6Z0B/RvDqYYXLd6eGMo7UwnTkRERHc7Jjt3s1uSncCCdAQWpAP1HoSHg4gfDn4KO1k3bHVf5kVsDepk8jaJOUX44UgqHgn3QM9wT3O3moiIyKKY7NRCgiBCdnKBo1S6EWjT7Kvl1v90TyJSctU4n14ArQy42dvhwQZulmgqERGR2XGC8l2tTM9OeJPSr0URgpNz6bGbBxy0JpZTvikltzQp+u5QCj7bmwitdPuVltedv46JW64im/ttERGRDWOyczdzc9d/Kb72YWm5KAIuZXpmfOrCDlXbJiKvWItLGQUVPqq++Hg6YrKK8MfpDKNzqbnFSL5RfoJFRERkKUx27mLiiAlA/VAIz/0PQtk9s0QRcC1NhMpbj6cimy9l4e2/ruGjHfG3rRufY5jUaCUZz6+PwQsbYpCvrtxih0RERObCOTt3McGvHuw+mGt8QrQDXEt7dgTfgCr26wCrz14HAJxLL9CXFWsl2NsZ58fpeWqD4xtFpQnOxYxCtAlwufUSIiIii2HPTm0i3vx2NmsNKJT6YiE4vMq3unX0asGxVIxcdRkJObpH2PfG5ujPlU1uACCnzPHljALciZxCDaZuj8Puq9l3dD0REVEJm+vZ2bZtGzZu3AiVSoXg4GCMHTsWERERJuvGx8djxYoVuHr1KtLT0zFq1Cj069fPwi22HeLMn4Br0UCbByEf2Fl6IijEqO4Hp37Gx63GV/reGy9kAQDmHUxBiKcDtl5W6c/lqyXIsgzh5qPwZZOfpaczsPdaDj7oXh++LqUJmEaScSY1H419HOGsNN6wdOnpDJxOycfplHx0DfWodDuJiIhuZVM9O/v378eSJUswZMgQzJo1C8HBwZgxYways03/dV9UVIS6detixIgR8PT0tGxjbZBQxw/CfR11O6MHBJWe8KlrUK9efjpaZ11G15RjVX6N8+kFBolOidc2X9XPzzmenGdwLi67GD8fTTUoWxWVgWk74jFrT6LJ18ks4BNeRERUM2wq2dm0aRN69uyJ7t27IygoCOPHj4e9vT127txpsn5ERASeeeYZdOrUCUql0mSde5UQ2gjC6Nchvv0pBIUS4pTZ+nM+RSoAgLO2sFL3+nyv6YSkrLjsYqyKuo58tVY/36esq1lFBsdbL+nacDIl36BckmV8ticBhxNyK9W2E8l5WHYqHZJc1VlJRER0r7CZYSyNRoOYmBgMGjRIXyaKIiIjI3Hp0qUaex21Wg21unRCrSAIcHJy0n9dk0ruV9P3rSy7zg+XtiWkIT46vx3rLuVgwuW1AACP4tIemJ7Jh7E9oIPJ++yLu1Gp1/vnSjZCvRxNnkvLU+N4ch4a13GCg0I0WCKobHzOpxXgQLxholNR/KbdfFps+ZmdWDOiKRQit70wF2v/PN9LGGvLYJwtwxbibDPJTk5ODiRJMhqO8vT0RFJSUo29ztq1a7F69Wr9cWhoKGbNmgVfX98ae41b+fv7m+3eVeHbpxsi/xykP3bRlE4efu7yunKTncq6UaTFDZTuxN6nWV1sPVc6fFXyGHubIA/YiSIA3bBXQEAAtJKMArUWzgXZAK4Z3DcgIKCCVz2v/ypJ7YD7Q7yr9R7o9mzl5/lewFhbBuNsGdaMs80kO5YyePBg9O/fX39ckmmmp6dDo6nZeSKCIMDf3x8pKSmQbWKYxQ7C/V0hH9oNwDDZsZc0CM+JxxX3+tV6hejkTADAwCbeGHefN8a29MSTyy8a1DmRkA0Px9JJycnJyZi+Mx6HE3Mxvp3h/CIAOHbpGhSCAD/Xiocq069fR7JDUYV16M7Z3s9z7cVYWwbjbBnmirNCoah0R4XNJDvu7u4QRREqlcqgXKVS1ejkY6VSWe78HnP9sMuybDP/IwmDnoZ89jjgG4DgtOTScgAzTn6PzYGd8Fv4nT/RtiNGN5nc3cEOsizDvpzd1NXa0nh8+V8iDifqhq7WnjOe7/PC+isAgDVPNYadKCBOVQSNJCPUy8GgniTZTpxrM1v6ea7tGGvLYJwtw5pxtpkJygqFAmFhYYiKitKXSZKEqKgoNGrUyIotq10En7oQv/oN4tMvIjQvGRPPLcMnJ74HoOvdGRS/u0Zex71Mz834dn5G5/PVpQv57CmzZk9Gfvm9axn5amglGZP/jcPErbE4cctTX2XvSUREVMJmkh0A6N+/P7Zv345du3YhISEBCxYsQFFREbp16wYAmDdvHpYtW6avr9FoEBsbi9jYWGg0GmRmZiI2NhYpKSlWegd3B0EUAUfdRqEPpZ1E8zI7ogsA+ib8B1HW4rNj32JS1BL9Oe+bT3EBwDeHv8SS+ol4Mvk/k69Rtuemf2NvPNGs+nNpkm+oseh4mn4dn7+jVQbnazLZKdJIiMksrPJfIdmFGpxIzuPTYURENsRmhrEAoGPHjsjJycHKlSuhUqkQEhKCyZMn64exMjIyDGZzZ2ZmYtKkSfrjjRs3YuPGjWjWrBmmTZtm4dbfZcruin6LMVc2YVjsP3DTFOCKXJpA9E48gGVhfdAgNxlB+WnAb3PxpCCiYcZlLHjgeaSV2TYi0t/w/mUXFLxTCTlF2HQxS3+clmfYC1ST+3BN+TcOl64X4t2HAvFgA7fbX3DTpL+uISVXjXe7BOLB+pW/joiIzMemkh0A6N27N3r37m3y3K0JjJ+fH1auXGmBVtVCLuV8EDdvA7uzJ+B2c/JyeG4ihl/9C17FN9A95SjqFWSgmSpGX10hS2h3/Txmlkl0WtZRImjlPEhu7hAGPwPBwRE+ztVPdm59JP1KpuE6QbnFle/ZURVo4OZgB7tyHlW/dF1373+uqPBgAzdIsozr+ZoKkzZZlpGSq4vD0cRcJjtERDbCpoaxyHIEUYT45a+Ghb7+EAKMn8Yaem07Hkk+DIUsoWP6GXiq84zqlFUv5gTkw7shb98IectqyLk58Ny9rtptjkrNr/B8XvHte3ZyirRYeiodo9ZE49NyVm8uS7o5GvXdoRQ8t+4KNl7IxMVy9vsqu+qzp6PN/R1BRHTP4m/ke5jg4QVx9m+AQgl5/w4IzVoDLq6QTx8ByjypZcTbBxBE4Hqavmjc5XVY2HAQACCvsLSXR/53HeTUBPicPgl0alfptr1yYQXmNRlWpfeTU6SFLMso1sq6hQtN+GhHPKJv9ggdufkEmEaSkZqrRqC7vVH9krk3/17RPWW24JjuPb/6gD8eDvc0qJteZlit6NadVImIyGrYs3OPE9w8IDg5Q+zZH0JAEAR3T9jN+BGILD8xEbr0BoLLbM4a2Q79EvcjIicOANA19XjpOY0GOHkY7up8TDm9EK+dXw6/gszbtuuB9Kjb1rlVTqEWPxxJxdOrLyP5RjEAQCvJWH32Otadvw61VtInOiW0kowv/0vESxtjcCjeeKVoqZx5xv9EZ0MjyfjxSAqm/BuH+Owig56ljRez8PTqy3h61SWsMfE4PRERWQ6THTLNxFpEwgPdIAwfD6HPExBcS+ejCB26AAA+OvUTvjg6F20yyywiKEmAVtfjcV/mRXRLPY4fDn0GF1mN8nRIj4KztgjPXNlcqaYOaV4HABCVlo9tl1Uo1sr4en8ytJKMwwm5+O1kOhYfT8fSXeeNrh2+8pJ+LtC688ZJmCzLyDKxKamjUsSPR1Kw5ZIKZ1Lz8fa2a8i75WmwG0Va3CiW8OuJdK7hQURkRRzGIpPEJ0ZDSk6A0KMfBG9fQLQDmrWCIOrWz5E96+jrCs1aQwbgpC1GeG6ZeTAubkCe6X21fizahdzcfMQnXceMlmP15R+c+hnNb06AHhy/+7YLHPaK8ETXUHejzUcvZhTghyMp+Ds6W1925lIi4N7AoF5xmUfkTS2AKMnAixtijMpP3rLGT4FGQnZh+WsE5asluNjblXueiIjMh8kOmST4BcDu4+/KP9+jH+Rj+yC0eQCCuyeEhx6FvPdvw0rOLuUmO84H/4EzAG/BsHOxddblSrfxTZxDx3aDkKc23WtSNtEBgBSnOibrlTiZko+Pd8bjvS6B+rICjYSCSs6/KZnPY0p2obbSyU58tm7Li/oeDkbnbhRpkZ6nRpi36Q1XbyXLMv44k4EQTwd0bOBeqWtqg2KtBKUocINHIgLAZIfukODiBrtp35YWNI4Ebk120m+/uKNCltAhIwqHfVpU+rXvu34Bj8ftQLO8RIjd2sHdxRU+ogYZUsU/zrlKl9ve+1hSHoYsv6Q/vppVM3ttHUq4AXs7EY9GeEBpV5rgbb6YhTOpeXimtR8C3e2h1kp4ZZNukcfl0k44Dh4JwbU0SXlt81VkFmgw69FgNPF1uu3rnkrJx4ozul6v9SPvjWTner4az6+/gvuD3DDpocDbX0BEtR7n7FCNEFxcjcseHWxc1qmnUZlCqtxigD2zz+GdwBuYcmYRmmXHAho1pGmvQH57NDrHml7J2Vb8ciIdPx1NxfzDpbvAayQZPx1NxYH4XGy6qJsvlF1UGouMI0cgr1hocJ+Sx9sPJ5juMStRrJWwIybbaC2iiqgKNfhgexx2xGTjUMKNu/aJsn+uZEMjAfviKo4REd07mOxQzfALKP06sh3E1z+EMPhpg+RGfPdzCE9NMLrUXip/snKfxH0AgO7JR/By3BY8oMw2Wa9LmSfABsftNDg3/tJaiLJtfHDviMnG1ktZeHHDFYNEJLtQl+TcKJPsvN7+f/ipMBBXs4wTFvE2wzPLT2dg7oFkLDmZri+73STppafScSolH3MPJGPm7kTMP1y9bVe0JjZmzchXIzGnuFr3vZ1y1okkqlWKNBJis6q+pc29isNYVCMEv3oQ3/wE8KwDISCo9MTIFwE3Twit2kMIb2LyWq8G9YGSz7/IdhAaNYf8p27Bw9HRm/BAehQa51wDJA1w8YzJe4TkpeCzY9/CRVMIpaTG2gbdAQA9kw/jkeRDWBzxGCTBdG7/6vnl+Lbp8Cq/ZxEy3B0VUBVWbZuKH47oencm/XVNX5ZVoEFWgQY5ZZIdrWiHrZ4tsXVLLLqEuOPl+/3151advY6kG8V4u3M9k/NS9l7LMSrTrT9UfiZw61Nnu67mYGLHevrj3VezsWbrNbzV0R9BJtYkKkuWZUzbEY+kG8WY/1gYHBQiEnKK8OaWWADAgsERcLMX73hOTXqeGguOpWJI8zpoWMcJsixDI8lQ2om3TQSJaoP3/43D5euFmNwlEPdztfbbYs8O1RihaSvDRAeAoLSH+MQoCBHNyr3uiTqFaJEVjRcvrobQthPE3k8ANz+wlLIWLXv3gL2sSwLkAzvLvU+jG/EILEiHq6Z0heOHb6787FeYZVD382PfwElTiFHRm/Bg+hm0Lvu4fCU9E70Zi/sGws/F8G8GV3XFKz2bci69AKPXRON4kunVqffE5mDeQcOFHvfF3cDpclaVNvWBX1hmWOpsWj5+PZGGOFUR5h9KqVRvy1f7knD1ej7mHkjC5esFOJ1i2Nbr+Wr8cTodmQUaxOcU43RqPjLyNTicoHu0/2hiLoq0Moq0Mg7G38Dz66/g1xPlT+quyKd7EnEwPhfv/6Nb22n2vmSMWhMNVaHGoGeHf/VSbXX55pY2/8aY7u0mQ0x2yOKE8W/pVmAGIAx+Bi5uLvj41E94JPkwhOZtAADijB8hPD8J4g9rIfZ8DKjjZ3iP5/5ncCy6e+q/dmrZRv91nSLdL4KJ55ahXr5uSKdL6nFE3EjA0v8+wMCEPXCU1Pjg9EIMyTVcyLBefjp+OjADn5z4Xl/2skvpo/Vumjxg8wq8Fr/F4LpvDn+J/3nd2Ye4qbV+Suy9ZjwH5YPt8Ri49AL2xBr25Nwu2Zl/KAVrzmXi1c1X8Ve0Ci9tjMGRRONEqyRZuKYqnah9KaMQ7/59DR/uiEdCTmn5rL2JWH7mOr7al4RzaaVJ2Jf7klCslVBQZh2i7w6lIC1PgzXnbr/AZL5ai5VRGfqFIoHSfdGKtDLmHkjGnms5yCuWsOtqNuzKvPdirYzcouptEJuv1uLXE2mIqcL8JyJLkcpb+ZQMcBiLLE7s0AXo0AVy1nXAw1O3yvID3YCWHSB4egMABF9/CL6lwzZC7ycg/z6/9Lhle5T9X1zKUem/Vrw0GR+9PRF5Ckf43Ex2wnMTMe/wFxW2q07iRaCx7qmwGfFr4JtwHj5F2fAuykHHtFOoW5iJ7jHb8F23WQAAV3UB5L/WohkAdHtUfx9PdS5aaNMB6BK0Ok4KXDexMGFN+mpfEho4SQj2cEC+aK/f5qKsIo2MHTHZSM0tRkIl580UamQ4KQV8uCPeoLwkb3p541V80rM+Wvq74GKGLhmISs1HUx/DJ8WOJ+Vh+RnTK0mrtZLBE2plXcksxJtbYwEAWy6p8MvjEUZ1dpT5y1aAYNCzs+2yCouOp+GlDv7o1dCzordq0oH4G/g3WoWjSXlYcy4T60eaHoolshamOpXDZIesRvC6ue6NvR2EcW9WWFfs2hty50cg794KoVkbCE7OFdaPVF2pfDseHgj53/W6bS4EAc1VVxDkIAM3EyURMt46t9ToupDcJP3X7TLO4ahPM7TPOAsAcLxwHKjXHABQx7niZKddPRfUPfwXNgd1BgA4KUSDtX36NPREiJcDFKKARcfSjFZqLvHW3/GYfnYxpraeYLBYYonsIg3mHqhgzzMThq+8BHs7weT9Sny9PxmLb0lCrhcYTjqvaNPV9DwN6pUzB6gk0QF0c4qWnkrHE83LXy9JFABtmURv0XFdD9v8wyn6ZKekt6qi+UKSLOPfK9n47pDhJO21567Dx1mJh0LKf4y/UKNb48fOxmdKayQZSTeKUd/d3ibWI8op1MDNwc4m2nI34Uht5XAYi+4agp0dxB79IfjfXDulaSvdfxs1N657cwuLSqmre5LMUVKjV9JBBOWnA1kZ5Vb/+shXmHFiPvyKVPqy1y6swIsXV+O1CysAAA6XzmBs9AaM9ctDI1fj30azjn2j/1qZn4NHkw7CTtKiI9LwQXfDeU8ygF4OmejpmIPvHwstt11qUYl3Ip8vNzGJSyz/PVWkokQHANQmutEz8ivfk5WWV5oYbb6Yhc/2JCJfrcWZVONhtZVR1zFsxSWj8rJtKa+9++JyIMsy3v07Dv/bdg3am+3WSjJiMgv1xwCw8sx1o0QH0C0h8OW+JKPyEqpCDV7aEIO3y0w+r4jWikMQ3x5IxqubrmJ3rPFkdkvbG5uDZ/6MrtSwJhky1YtLxtizQ3ctccgYyGePQ+zeD+LsqVBfvQS0aAsAEJ55CfLhPaV1P5wL6fvPSndzb9QCuKSboyPUqVulruDgvFSjMldNAR5JPmxQ1j/hP2Dlf8h1rYOUrm+g6+FVuOpaD/aSGg1vJKBuwXWkOtVB350/o35+Gn4+MAMuPXrDwfch9E34D1tu9vS0u/IfpJ+X6F7H1R1j2z+FRXJ4FVqs89P5gttXugM3irRG6/mcTqn8JO247CK08neGJAM/HdXF9kRyLgo1Vf8lvvn8dWQUmu75+nyvYZIydm00Pu8VjL2xN/DbqXSMbOmDoZE+AIA/zlScGKq1MpR2AtRaCYAA5c2tRv48ex3XCzS4XqBBkUZCSq4a8w4m45nWvmjpb7io5YozGVh7LhMf9ghCozpOFu8J2nUzyVl+JgPdQj0AAHGqIjjbi/BxNt4b71ZaScaFjAKEezvCUSEanROFinvPyppzQPe9WXKy4p67u82hhBvYG5uDFzv4m227GNtYVMP2Mdmhu5bQIEz3TxDg88FXSF37B9Ctr+6co+EwlxAUCrsZP0I7foDuOKwx5JvJjqlNT6vUjm59Ie/aUu5519zrmLx5KgCgU/ppffkHpxcgR+mCxjd3i/dU5wJ/rYbs5Y3nojdgyLXtSHCpq98rDACQm4P+O3+EduJi/HoiHcOaecJ77QLs9WuDK26BKFBUbhsJAJgincR0sbVBma+THdILKjehd+zl9djcZghSc9UGw01VtfBYGgo1ElqXSQbuJNEBUG6iY4qqUIv5h1P1+5wtPZ0BD0cFIurcPob5ai2cIeLVzVeRU6RFn4ZeKNZKuJRhuHbSkhNpuHS9EFO3xxvN91l2WpdQvft3HAY08cLQFj6ISsvH/UGuBpOsK6KVZHy2NxGOdiJeezBAn3Td6kxqHhzsRDTyMV51uyTWcdlFeHXzVfi5KPDzIOO5UbfaejkLPx9Nw30BLviwR319uVor4fUtsajjpMAnDzeo4A6l7tL1K29r5m7d8G09d3uMaOlrltdgx07lcBiLagWFXwDEAU9BcDMxl8Ku9C8q8bMFEB5/FkL/YaXnK/htIQwcAfG7VfpH4QFA6NEf4rR5EIaPh/jKFIgjXzB9sWvFa18EFFzXJzplyct/AgB4qvPQQhUDUx9f/Xf/hNldfTHUTYVHkg/j41M/4qcDM43qveSRhkgT+429Uz8f9+1ZhkeSDqJlmfOOGYn4IfY3/XGQWIilg0Mxsxmwcve7BvcIzkvB8Js9IVXRPuMs7LWGc3qWnsood+hHYcYej1s3dJ1/OAVvbo01uSlsWZP+uobFJ9KRfEONvGIJq89ex4YLWbiQUdp7Fp1ZgKNllhKYuOUqUm6Ynhi+4UIWZu5OwGd7Eg2GcmRZrnCo60RyHg4n5GLPtRy8/6/xzxKgWxJgyr/xePuvayYfxS+8OQds182J3ml5GmjKvOb1fDUOxN0wGi7ZfFG3nMPxW2L40c4EJN5ceiBfXZo456u1WH46A69tvgpVBZvmlpWv1lp1qK+m3KjmE4EVqYnlFZJyirEjJrtWD4kx2aFaS+jWR/ffJ0t3VRfq+EHsMwSCQ5m/3v0CIE6ZDaH/LQsLhjeB2H84BHsHg4RIeOhRCIENIPZ8DEKrDuW/fr+hECd+VL338NCjJsvtjh9AyEdjIMyapC9z0RbixYur9cePJh3Ew+u/hE+hSl8279DnWL3rHdz/2zQAwIuX1mDaqZ/15x20xfCLPYOvj8xGn4R9+GTvLDi9PxZNU85Cccsq1C1UV9B+2QxUsE6hSd1TjmHp3inonnK0UvU/7lEfrQMMh4BmpG5GnTLvq8TAuN1Va0w5HFHxh1NKrlr/YV+e+bfM+YnJKsKEDTFYfDwNM3cnGNU/l65LlDZd0CU7ebu2YdVn3+GpFRdw+brpIciyq2tfLJNopeep8ddlFbSSbPDknalNbUvKyk6izyrQ6BOV/227hs/2JmL3VcO5PbcOUam1MvZdy8GZMms/jVt7BaoCDXKLtHhhQwz+OJOBa6oibLxQfuxWR+me2jsUfwMjV13GxK2xRglPyVYrRxNzy72PtZVdasFFaTyEJcsyvtqXhJ+OVH2l8rIJTk3kgi9ujMHcA8nYW8Pzt7SSbDMJFIexqNYShj8PoWtvoF6wyfPih98AebkQ6vgBdfwgBEdAG3UMiNX1dAiNW5ZW9vYBMjOAoFAIQSHGrzXoacjrfocwbJx+PyshKBRo1ALCY0/pjvs+CaTEQ5ozDcgu88veLwBCu4cgb1lpfN+W7Q13k2/zAHDyULm9UY8kH8YjyYeRrXSB283FDRVy6Qd3vQLTc1GeuLYdfwb3xNjojQB0vTbjo9frTqoBeY1uReuOaaew368Vvjg6FwIA52sXMM91HV7wHWTyvr/aH8HqmEJAlpHk7IsrbkFooboCO8h46upfuOpaDz6FKhz1MVx00lmU8MK1rWjrXATXupPxPw97PPNnNACgma8TmsYm4cNrC7C+fhc0V8UgwdkPw2L/gVLWYn2Dribbcqt2dR1wNNX0Rq852ur/HXij2PTYTEVrKQG6CeEZp05h/7Jl+D1yNCDp1iWa01c3OT2nSIuYzEK42ItGSYMkyxAATP4nDml5av0TV/o2FWnhbOKDt0hjuA7Sc+t0TzP2b+ylX1l74bFU/Hw0Ff5uSrz6QIBBj+N3h5Lxd7Tx4nb5agmrz11HM18n/ZYoJa8H6Ia8bl0T6rdT6RjY1AtRafmQZN0aT6PWROPbfqFIzClGWp4aAnQ9S5svZmHlsEZwuGXOkHz0P0grFkKcMAlCRFPTgTYhPU8NO1GAt1P1PxrT88rfBgcAkm+o9etjjb7PD/blLL9gStmHAm6XSsiybHLu1K8n0hCfXYx3u5RulnvpeiFaBbjgaGIuHgp2N4prVRTf3NTY11mBmY+G3PF9agqTHaq1BDs7IKj8p5dMJS3iqFch/fQFhKAQXXJSUj7+Lci7t0F4YrTpe/UZAqFdZ13i0rwtkBgLNI6EIAgQBjxVWjEoFOIXv0A+ug+CT13IF05D6PAQ5JOHTDeyZTsgohmQdwPie19AcHKGXFQE6e1RQEH5k4A91KVDCyULK1ZkxNW/8HjcLjhpK97lfeK5ZXju8nrd/KKS+589AHQbZFT3zbNL4fHGixjzz0vAzV4hrSDC7ubXPkXZmH10DgBgXq/J2FHkCQB4/fwf6OotAVdOAQCk//6By77tmN+xL3a5N8WAJt7A9iwE5Wfg5TI9WVXRISMK7+xagie6fV5hvXcfCsTX+5NQVM4TXu4Odng/QIWFZ7Jwya1y81NuJ08t4bnTdpAiR+vLrmYVYcnRZAxp5Yc3t8QgPd90z9Pl64VwUor6J9z+uqzCtezS72lesQStJGPqdsMhr+jrhcg3saTBpjK9VyXJ25XMIuyJzSk7smsy0SlRqJYw65bJ4Uk3irHpYiZ+PZGOADfjZQfOpRfgepkn+m4UaTF6TbT+2N2hNGF7bfNVTO0WhCAPBwC6D1m7H3XfV+mnL2D3+SKDe2sl3XpT99VzgY9L6WufTsnD1O3xcHeww+e9guHvqsQX/yUhXy3hg+5BFW5DUqSRIAowWC+q7FOG+WotNJKMmbsTUN/DAWPu8zNIWLILtfB1MUwsYrMKMWN3AoZF+uDhcE+Dc2WfOKyoZ2fNuetYffY6Zj7cAOsvZCGvWKtPbkqGS0+UGWo9m5aPTX/qvucJ2cV4prXvHU+cP5Wcj9RcNVJz1TaxqTCTHaIyhKAQ2H38nXF5RLMKt7wQRBGoe3MfqYAg3b/y6goChPa6J62E0Ia6wocehbz8Z4N64v+mQxDtIE761OAvM8HBAeKkzyD/9w/kU4eB/DwgX5d8lO1ZKvFYwn+44BGCB9NN7ysGAAJw20QHADwffxr2DVtAO+sdfZkdZHRNOYaLvo3gpcnHeYe6ePbKJnROPwX41AXcPfQ9WXblbMj69I65OHj/JPgUqtAh4xxQpsNC/vVbAIB/9Dk89dz/IH/7t66XrZI8NXlwUBci1akOJuAS3KMOomVWNAQAr55fgYv1WuBvD+PlCwAgzNsBvz/ZEPuuZGHOkXSDcy/f74+uIe5QvDgIERED9MlOQH4Gkp1L5zK1cpdxKqdqHxiSYNz78ufFbGTHXkV6kXe510366xqea1u62njZRAcAYrIKoSrU4Gya4bDYqrPXTSY75TmZnFfp+v9cMU6EjiXl4djND9myq3OX+GB7vFFZWWX3kEvJVeO1zVexZkQTnEvLx9TtcRhevxsej9+FotwbyI2JRZyDD/bE5uCRCA/EqYrx09FUuDvY4fcnG+naGK3Ctze3Y8m5OeT2fLu62BenW7U8PU+Nuq72OJuWj5wiLR4ssxeVWivjpY0xcFKKmN0nBN8fTsGOGMPhoM2XVAh0d9C/72db+yK/uPQ95BRp4e2kMEgsfjyYiLQ8Db49mIKHwz1xOiUP9dzt4eOshKZMslPRnKZfT+h+ZmftTdL38EWl5htsD3OlzFDo1azS78Xa85nYfCkLClHA4GbeeLJ5HQiCgGKtBLVWRlahBi5KO3iZ6AXTSjKmlxmqzchXI6TcVlqGIHPzGABAeno61OqKux2rShAEBAQEIDk5mXv0mFFtibN8MQrSl5P1x+KshRC8b/8Eh7RpBeT1SwE3D9jN/g3ar6YAF04bVlLaA+oq7jZeNxBINVwMsO43v+O6swe0m1ZAXve7YfsBFNo54LJbfTTPjoGdLMHu5w2Q9m2H/Mvc275ckaiEUtJArOaasDGu9fCfXyv0TdyPPX5t0Cn9FBy0akS718d91y+YvP8lt/p4t+2r+mNRluBfcB1zjnwFZcce0F48gyEtSrco6eCrwP+O/gSHvo9D+m4mvm76FPbW1W1TMvfwl3i9w1v6ur/sm4bRnaaV297OqSfxX93W6OyUiy75VzBTaFWt9387Des46vdVMqVP4j5sDexU5fuWXXyyvIUoh7aog5VRplfSLk8TqHABnret9+eQMLy8LR4pubrf460zL+Kkd2Ojeu0DXfRbozTxdcLYjhGYtL78PwQA4LNHG8DTUYEXNuiejPxhQBiOJuZi77UbGB5ZBx/t1H2w92noia2XVbdt60c96husSv5Ma1+sOJOBhnUccaNIi5fu98ePW0/jqp2n/vXf/TsOLkoRy4Y20j05t+kqAN2ipYsGm356buDSC7dtS5C7faVWVP+6TwjCvB3x5tZY/VITbvaiPmGUZRmSDNiJAqKvF+J/22L113YNcceXT7ar8d/RSqUSvr6Ve8qNPTtENkJo3ALinKWQbvZkwKtyTzoJvQYDru4QInVrDIkvvqfr8REAedFcwN0TcHEFknRDF+KU2ZAP7QZEEfJfawHPOoDq5geQtw9g7wBIEoSBIyD/VLrFhvjkWNiHNwGSkyH0fRJCw2aQvihNzkp6h1qqooGmrSAOHKm7rlNPaM8cAY7t19V75mWIXXpBvp4G6d3n9Nc7SDXzx0ZYbhLCbq5u/Xj8Ln15u+vny72m0Y14fHjqJ2z374Bx0evhqimAKEsQAMj7/oUI4KULq/BTo8F4O34T2u/SvRfpO90TcHXKLDBZPz8Nb539DV82fwb9EvbCvYKNYZ+/tObmPKtDaJgTB8fINvjhwmac9wzFOY8w/FPv/jsNA947sxifRo4xKi+b6NRxVhgMFwFAl9QTJpMdz6IcqBzKXzn6xfzjmOugS/jeaFcHnx8y7H37vFcw6roqy012mmRfxQ2FMxJd6hqUhyaexYWb7XmssRc2ljMxPHrHLqTklg4lmkp0ABjsAXchveC2iQ4AJMfEI8ap9L3/b1ss8m4O65UkOgAqlegAwKd7DCeo/3ZS1wNT0uM2a28SvIsLgZsrBZxK1v0M5al1w5C/HC/dey8rXwOtJMNOFHAhvQBHE3PRr7FXucsQ3KqyW8ccScxFoLu9wZpaN4olpNwoxveHU3AyJR+OChHdQ92N4rA7NgcXU2+g/J8e82PPzk3s2bl7Mc7lk9NTAIUS0u/zgdNHAAB2P2/QnVMXQz64C0Lz+yAtnA1cioIw7k3d6tOyDIgicOYopG8/0V337ueo91APgzhL636HvNl4YrU4bxUEB4fSdpw6DGnedKB+KOw+KO3lkQ7thrzgK7O9/5qmFuyglI3ny+QqnLA44jF0TTmuS/ZukqFLAi+51cff9R7AjoD2AIC23iI67vsDndJPwV4q/zFszc05Tmc8IzCt9fMAgJExWzEwfjeWhfbGugbdAADhOfG44l7f4No1uybhnftexmV30xP0W/rY4/VQLTKcvPHOntIhuh8OzESMWxA+b/GsvuzLFkDovEmIcQ3E2+1eBwC0q2uPo6mlH5Srdr2DqB5PIy04Eo+cXo/fstyxtkF3AMB3/UMR5OEA7cWzePyo4RCdv6sSs7dMgqOkxqbATljUcKDB+dfOL8c3TXVPSj57ZTPS7uuBbdnG6wWZW0u7GzitrXg5iVv5F2QgxanqyzPcqmyP2Ny+IXh9S6zB+QbOQEpRxdu61AR/V6W+56yq3n2kMTrWFdmzQ0Q1r2QzVXHYc5Cup0F4ZFDpOaW9/tF28dUpQGIcENbY8MmNlu0hvv4h5OvpJp9qER4ZBPnMUQhtHoTYfxjkY/sBJyeDREd/n/e+APwN5zIJEU3LH7Ty9oE4aRbkjX9A3vdvaXnjSIgTP4Y09UUgvcxjuw3CIHTrC3nJvNtEpRye3rretKvlb0dhKtEBdCtov3rBxNN0dgpAq0GjG/FodDFen+yM+OsrhOZVvE+ZMiQCiNUlTi1V0fjs2LfIVzii9c11kfom7kOaoxfq56ViUPxupDp6Y22Dbtjt3xYPpumGMaeeXoR19buicc41g16e/7Wvg05fjNO9bQBNen2CC0W675mztgjtr5/T1124/2N457aGDN1+cB3TTsGvKBuj6tTBgoQsbA7qjHYZ52AHGa12lK7R5Bn0kP7rgG/fgzYzHbiRDddO05CrLF30c9IDvnDcpPsA9Ss07LW57/oFdLi53xwAaAQ7jNr4CR6K7A6fyEjMvCDhmmu9CuNYU6qa6ADAO1G/IvjBB/CM9gHkyXe+gnLZ3rBbEx0AiMsHampL0GaFyQjMvIbt9e6HdMsqX1VNdMJuJKBJdiy2BHVGUnYBUNfl9heZCXt2bmLPzt2LcbYMc8RZlmXIv8+HvOcvfZk4dxnksycghDeF4O0DuSAf8vYNkNcv07XjgW4Qx70JOS0J8r8bIfR6HEhPBvyDIHh661fJRnAExHdnQf51nm5Yr+CWvbb86gFpuuEu8dOfdU/HXb0EaeZbMMnbF8hMN3lKaNsJwvNvAQnXIJ84AHmTbo80YfjzQMJVyP/9AwBIcfRGuqOX0Ua14usfQlr6A5BRuhWJx6hXkHN4L+TzpyoXTOjmPe3yb4tOaafgqjGchJzu4Infw/qgU0NfdIj6C4i5qD+X6uiFFx94D+7FuVi4/xPYQcZx70YoEu3xYEaUbqJ5xi3bpAgCigQF/vNrhY7pp+GkNRwOKRKVWBTxGDpknEPbzNK5I5n27shw8MD3jYfAQVuMmRd/hXhzgr1K6YqX758EJ20hXrmwCq2zLkN4dBBG50Yi294N00/MR7PsWIPXUQt2WF+/C5aF9TGKx7xDn+OV+0vXogq9kYirboFogmxcgEeFsewfvxddU4/re7Ju1UzKxLutXfDqJWeDx+pL1C24jm8OfwmlrMXvob2xJrgHwnPiMTh+FwKDA/G1ZyfEaRwQlJeKWcfnYeRDn1TYHlMGx+3U956VdV+AC9wd7HAq6QbGX1iNutfj4OTmjJeaTNDXaawsQGK+hFylC3pmnsZ2b91SG30S9+G5y+tRENIUZ/uMw8Xd+7HG74FKt6lbiDte6OCPDScT0PXXKTjoG4nFEY/h4cZ+eK19Hav17DDZuYnJzt2LcbYMc8ZZn6BENIXdO7NM1pFPHYa0aQXEsW9ACKhvsk7ZewmPDob4pK43Q9aoIa/+RZd0FBXqFpL85Hvg5EEgpKHBRHA5LgbSJ2/o7jHmdeDaFQgt2kKIbAv59BH9sF5ZwnP/g3i/bn0f+fgBSN9/CgAQp3wN1K2n64VSlb++TsnQIgDIF04DcTGo98wEJMdehRx9HtI3t1mc0tffsJcL0K0crq38yr1JTj4QZQkBTz4F+TfjJxIBAPUa6Od+1YSSYb6yikQFFLKkf3JPfOFdXF80D4nOfmhezoriALAy+GH8GdwdarF0+5c/d03CipBHsDLkEQyK24VnY7YgxrUe/AqzsCmoM1aGPKKvu2D/J1jQcBAO+kYC0A0DAsDcJsOw27+t0est2D8d3sU5uOHghm3+7RF+IxHTW+p6yyZc/BM9Uo7qewKLRAX2+7ZC++vn4KopgNCxJ9KLgS2ZDuiTuB++RSr8Et4PR+o0Q6GLJ7Lk229h8+nxeQjOTcb4B99HXpmeMvfiXPxycjbEsRMh/bve4GGFpaG98GdwTwTkZ+C7w5+jSFQCkHHRPUQ/TLpsz/twlNS673VhAZCZjmi3IEyPHIsmObHwaN0WPTZ8hRPejQ3i5yDK+DRvD8IefxxwcoE0/U0gLQmHfJpjVotRaOrvhlkPBzHZsTYmO3cvxtkyzBln+cxRSNv+hDjqNQh+AdW7V1Ic5OMHIDwy0HClbABybg7knVsgPNgdgk9d09cXF0F6WbfGkjhtHoRAw/VzZI0a8pbVEJpEQlq/DLB3gPjKFN26TgDkGzm65CasMexe+0BXJmkBVaYuicq9YXA/ccpsCMGGT9PcGmt9AjdktG4zW0GAvHubrqzXYIhDxkA7bzpwqnQzWvGLxZDeNp6gbEQQ9esgCaNfh9CuM6RXnjRddfh4CGGNDXq/hC69Ie/ZZlzZxQ2o4wvExRifq0irDre8j18gvT1ad+DqZhS/siQIGNJNlyxPOb0Abds0hrR9I857hCA0N8mg90mCgCRnH/wb0AFNs2Nxf8ZZZDh44MNWz6PhjXi8cX45AOA/v1aY3Wxk6fuVJTyadAjPX15rkHhpBBFDu34GABgTvRGPJewt/z22aAvEXwWyTSfAj99c/6l9xln0SjqI0NxE7PdtiYUNB+nrrNr9LuxkCQV29lgW2guH6raGc1EuJp7/A8F5pldlliBgh3871M9PNdiqRgZwwrsxwm4kwFOdZ/LaW+UpHDGn6VPICm6G9kFuGPrDCxAACP2HQT5+QJ8Upzl4YntAB7R+9XVEekhMdqyNyc7di3G2jHspztrvPwVyc/RrHVWVfLNHRbAzvFbOybqZ9EzUFQRHwG7KbKPrb421fPYE5MRYCI8MgiAIkLb9CflP3arW4v+mQ2jSErJWC+mFwfp72P28wWACuPDc/yA4OUP6bgYg3VwjJ6C+yXWl9MnVyBchL/1eXy5+twqwUxi8jvjTegiCADn2MuQrFyH4BUD640eIo18HIppCXvMr5PQUiMOeA5xdIb06zOj1AABuHhCGPQeh9QOlyVbrB2D38uTS9ox5A0hPgbxpuel7AFgQMQBpjt54J3c/lM+9CWlKOXvXlcOgt8nNA3n5RXir3etIddLtxj76xjEMiNkOZBmv9TSzxWic9G6Ebw9/gbqFWbpJ/lLVF9T7I+RR/BX4ID47txD+WQn6dq2v3wWHfZpj1JXNumTF21fX+5Jvu9tmlAhadwApGRmcoExEZCvsXnyvWtffmuToy929AHcviK9MhbRhGcTRr5qsZ3Rd8zYQmrcpLSgqs05OeNPS1ywZznLSDWuI93eFpMoEbqggtH8IgihC/HIJcO0ypD9/hfjMyyZfT/xgLnA9DULr+yFBBq5c0PXq2N+ceH5ziEx47n/6Ce1CSEMIIbpFMu0ifypt+5BK9C4BujlW9xtu9SE00i32KE75GvLlKAgPdIUg2kFbQbLzXPTNIcEW9+njUK6SbWDKvmbZAzcPuH05F/O/n4WRyn4oVDigRX1v2L2wqHTotYz3+jVD7lcfwu3mfCnhidEQHhkI6fkyT5jdnLRekadi/8ZwIRZCTukkdgHAoPg9GBS/B4hsB+GBoRA7dIEcfxXSx6bnFVUoMBhINL35rjlo0pIB8fbDc+bCZIeIyMKEVu1h16r9nV//UC/IJw9DePgxCMrSDxDx5SmQ/vwF4mOlm9qKvQYbXuvmDrRoC7sWxvNQ9HXqhwL1dVutiN36At36GpwXP/oOcuxl3TIFVW37k2MgX7kIcczrug/+HBXkbX9CKJPoCF17657y69hDdxwcDiE4vPT135oB6cv3jW8eFAIkxOq+tlMATqVP/wgPDwDcvSCv+RXCo4MhtHkAQkgEXA/txA1HV90kdi8fwz3qPLwgiHawe+EdzJ80AemyPSIm6daWEgaO1C3mWYZdWEO439ce8uE9ujptOxrtSyW+NR3SltUQQiIgJ8YBx/dDGDIGwkOPQPp1HnBct4aT2OI+yLHRMEUc9SoEDy/dQWADIDgCuGaibh0/iK99AHnXFsg7twAOThAe6AqENobYqSfkHBXg6g550de6tbcqIDw5BkJQKKSvdUOz4ovvQVr4FVBcuXV6Co/tB9pXbt86c+Aw1k0cxrp7Mc6WwThbDmNd/gaW+vMnDkKaP1O32W+DMMgb/oD42oe69ZyyMiD0GQLx8WdLh8CeGAWx9xMG9zUaLszLhfTeeP2TeyXzoQBALioCcrN1GwffJO3bDly7mfTZO0BoEA5p/w7Ii+cADZvBbpJuDo/2k4lA3BXAyRl235T2SslFRbokJaIJBNEO8qWzkL7Q9SqK730BOSMV8s9fQhjxAuQVP+snnJed0F4SKxTk63qy4q9CmjcdwmPDId5cWkIuLIC8fSOEjj0heNUxjmVhPqSP3wDSUyA8MhDysX0GPV7iR/N0TzuKIqTNK4HMdAgjdcOD8pJ5kPdtB1rfr9ukuCxRhNB/OOQNyyB6ekOY8aNu0dIawjk7d4DJzt2LcbYMxtlyGOvKkdOSgDp1IdjZ6ZMY+UYO5DNHILS+H4Kzqz7ZESd/VboX3U2m4iwX5Ov2nTtxQDfx3Nm1am3SaoHoc0B4EwgK5c12JkNevwxCn8chVLA5sZyfC+nd8YBCAfHLX3QJUH4eBGcXyFcuQJo7DUKP/hAHPV2lNlW67arrgLsXkJUBacUC4MRBCA/2gDj2jcpdL8ulQ3YKJcTp3wNedSCvWIi6T45ChtKRE5StjcnO3YtxtgzG2XIY65ojJ17TzT9qaTxsaItxllXXATulbrjx1nOSpNt02FJtSbwG+AVAUBrvTF8eaf1SyMf26zYwdtW9B3PFmROUiYiIAAiBwbrJuHcJwdN4mEl/zoKJDnAzdlUkDhwJDBx5+4oWZtnIEREREVkYkx0iIiKq1ZjsEBERUa3GZIeIiIhqNSY7REREVKvZ5NNY27Ztw8aNG6FSqRAcHIyxY8ciIiKi3PoHDhzAihUrkJ6eDn9/f4wcORL33XefBVtMREREtsrmenb279+PJUuWYMiQIZg1axaCg4MxY8YMZGdnm6x/8eJFzJ07Fz169MCsWbPQvn17fPHFF4iLizNZn4iIiO4tNpfsbNq0CT179kT37t0RFBSE8ePHw97eHjt37jRZf8uWLWjdujUGDBiAoKAgDB8+HGFhYdi2bZuFW05ERES2yKaGsTQaDWJiYjBo0CB9mSiKiIyMxKVLl0xec+nSJfTv39+grFWrVjhy5IjJ+mq12mClZEEQ4OTkpP+6JpXdf4XMh3G2DMbZchhry2CcLcMW4mxTyU5OTg4kSYKnp6dBuaenJ5KSkkxeo1Kp4OHhYVDm4eEBlUplsv7atWuxevVq/XFoaChmzZpV6SWn74S/v7/Z7k2lGGfLYJwth7G2DMbZMqwZZ5tKdixh8ODBBj1BJZlmeno6NBpNjb6WIAjw9/dHSkqKzey7UhsxzpbBOFsOY20ZjLNlmCvOCoXi7twby93dHaIoGvXKqFQqo96eEp6enkaTl7Ozs8utr1QqoVQqTZ4z1w+7LMv8H8kCGGfLYJwth7G2DMbZMqwZZ5uaoKxQKBAWFoaoqCh9mSRJiIqKQqNGjUxe06hRI5w5c8ag7PTp02jYsKFZ20pERER3B5tKdgCgf//+2L59O3bt2oWEhAQsWLAARUVF6NatGwBg3rx5WLZsmb5+3759cerUKWzcuBGJiYlYuXIlrly5gt69e1vpHRAREZEtsalhLADo2LEjcnJysHLlSqhUKoSEhGDy5Mn6YamMjAyDGd2NGzfGa6+9huXLl+OPP/5AQEAA3n77bTRo0KBKr6tQmC8U5rw3lWKcLYNxthzG2jIYZ8uo6ThX5X6CzIFKIiIiqsVsbhirNikoKMA777yDgoICazelVmOcLYNxthzG2jIYZ8uwhTgz2TEjWZZx9epVzvI3M8bZMhhny2GsLYNxtgxbiDOTHSIiIqrVmOwQERFRrcZkx4yUSiWGDBlS7iKGVDMYZ8tgnC2HsbYMxtkybCHOfBqLiIiIajX27BAREVGtxmSHiIiIajUmO0RERFSrMdkhIiKiWo0bgpjJtm3bsHHjRqhUKgQHB2Ps2LGIiIiwdrPuGmvXrsXhw4eRmJgIe3t7NGrUCE8//TTq1aunr1NcXIwlS5Zg//79UKvVaNWqFZ577jn9PmqAbi+1n3/+GWfPnoWjoyO6du2KESNGwM7OzgrvyvatW7cOy5YtQ9++fTF69GgAjHNNyczMxO+//46TJ0+iqKgI/v7+eOmllxAeHg5At/DaypUrsX37duTl5aFJkyZ47rnnEBAQoL9Hbm4uFi1ahGPHjkEQBNx///0YM2YMHB0drfW2bI4kSVi5ciX27t0LlUoFb29vdO3aFU888YR+X0XGuurOnTuHDRs24OrVq8jKysJbb72FDh066M/XVEyvXbuGhQsX4sqVK3B3d0fv3r0xcODAarefPTtmsH//fixZsgRDhgzBrFmzEBwcjBkzZiA7O9vaTbtrnDt3Dr169cKMGTMwZcoUaLVaTJ8+HYWFhfo6v/76K44dO4Y333wTH330EbKysvDVV1/pz0uShE8//RQajQbTp0/Hyy+/jF27dmHFihXWeEs2Lzo6Gv/88w+Cg4MNyhnn6svNzcXUqVOhUCgwefJkfP3113j22Wfh4uKir7N+/Xps3boV48ePx8yZM+Hg4IAZM2aguLhYX+ebb75BfHw8pkyZgnfffRfnz5/Hjz/+aI23ZLPWrVuHf/75B+PGjcPXX3+NkSNHYsOGDdi6dau+DmNddUVFRQgJCcG4ceNMnq+JmObn52P69Onw8fHBZ599hqeffhqrVq3Cv//+W/03IFONe++99+QFCxboj7Varfz888/La9eutV6j7nLZ2dnyk08+KZ89e1aWZVnOy8uThw8fLh84cEBfJyEhQX7yySflixcvyrIsy8ePH5eHDh0qZ2Vl6ev89ddf8rPPPiur1WqLtt/WFRQUyK+99pp86tQp+cMPP5QXL14syzLjXFN+//13eerUqeWelyRJHj9+vLx+/Xp9WV5enjxixAj5v//+k2VZluPj4+Unn3xSjo6O1tc5ceKEPHToUPn69evma/xd5tNPP5Xnz59vUPbFF1/Ic+fOlWWZsa4JTz75pHzo0CH9cU3F9K+//pJHjx5t8Hvj999/l19//fVqt5k9OzVMo9EgJiYGkZGR+jJRFBEZGYlLly5ZsWV3t/z8fACAq6srACAmJgZardYgzoGBgfDx8dHH+dKlS2jQoIHBcEvr1q1RUFCA+Ph4yzX+LrBgwQK0adMGLVu2NChnnGvG0aNHERYWhtmzZ+O5557DpEmTDP5aTUtLg0qlMoi/s7MzIiIiDOLs4uKiH/YCgMjISAiCgOjoaMu9GRvXqFEjREVFISkpCQAQGxuLixcvok2bNgAYa3OoqZheunQJTZs2hUJROsOmVatWSEpKQm5ubrXayDk7NSwnJweSJBn84gcAT09P/f98VDWSJOGXX35B48aN0aBBAwCASqWCQqEwGAYAAA8PD6hUKn2dW78PHh4e+nOks2/fPly9ehWffvqp0TnGuWakpaXhn3/+Qb9+/TB48GBcuXIFixcvhkKhQLdu3fRxKolbiVvj7O7ubnDezs4Orq6ujHMZgwYNQkFBASZOnAhRFCFJEoYPH46HHnoIABhrM6ipmKpUKvj5+RnUKfndolKp9H/s3gkmO2TzFi5ciPj4eHz88cfWbkqtk5GRgV9++QVTpkyBvb29tZtTa0mShPDwcIwYMQIAEBoairi4OPzzzz/o1q2bdRtXyxw4cAD//fcfXnvtNdSvXx+xsbH45Zdf4OXlxVjfw5js1DB3d3eIomiU/Zv665dub+HChTh+/Dg++ugj1KlTR1/u6ekJjUaDvLw8g16H7OxsfZw9PT2NupxLJonze6ETExOD7OxsvPPOO/oySZJw/vx5bNu2De+//z7jXAO8vLwQFBRkUBYUFIRDhw4BKI1TdnY2vLy89HWys7MREhKir5OTk2NwD61Wi9zcXMa5jN9//x0DBw5Ep06dAAANGjRAeno61q1bh27dujHWZlBTMfX09DT52Vn2Ne4U5+zUMIVCgbCwMERFRenLJElCVFQUGjVqZMWW3V1kWcbChQtx+PBhfPDBB0Zdm2FhYbCzs8OZM2f0ZUlJScjIyNDHuVGjRoiLizN4Cu706dNwcnIy+uC5V0VGRuLLL7/E559/rv8XHh6Ozp07679mnKuvcePGRsPYSUlJ8PX1BQD4+fnB09PTIM75+fmIjo42iHNeXh5iYmL0daKioiDLMpe1KKOoqAiiaPjRJooi5JvbQDLWNa+mYtqoUSOcP38eGo1GX+f06dOoV69etYawAPbsmEX//v3x3XffISwsDBEREdiyZQuKiorYhVoFCxcuxH///YdJkybByclJn907OzvD3t4ezs7O6NGjB5YsWQJXV1c4Oztj0aJFaNSokf5/rlatWiEoKAjz5s3DyJEjoVKpsHz5cvTq1Yu7HN/k5OSknwdVwsHBAW5ubvpyxrn6+vXrh6lTp2LNmjXo2LEjoqOjsX37djz//PMAAEEQ0LdvX6xZswYBAQHw8/PD8uXL4eXlhfbt2wPQ9QS1bt0aP/74I8aPHw+NRoNFixahY8eO8Pb2tubbsylt27bFmjVr4OPjg6CgIMTGxmLTpk3o3r07AMb6ThUWFiIlJUV/nJaWhtjYWLi6usLHx6dGYtq5c2esWrUKP/zwAwYOHIj4+Hhs3boVo0aNqnb7ueu5mWzbtg0bNmyASqVCSEgIxowZg4YNG1q7WXeNoUOHmix/6aWX9EljyWJ3+/btg0ajMbnYXXp6OhYsWICzZ8/CwcEBXbt2xciRI7nYXQWmTZuGkJAQo0UFGefqOXbsGJYtW4aUlBT4+fmhX79+ePjhh/Xn5ZuLsv3777/Iz89HkyZNMG7cOIOFNHNzc7Fw4UKDRdnGjh17zy50Z0pBQQFWrFiBw4cPIzs7G97e3ujUqROGDBmif8qHsa66s2fP4qOPPjIq79q1K15++eUai2nZRQXd3NzQu3dvDBo0qNrtZ7JDREREtRrn7BAREVGtxmSHiIiIajUmO0RERFSrMdkhIiKiWo3JDhEREdVqTHaIiIioVmOyQ0RERLUakx0iuift2rULQ4cOxZUrV6zdFCIyM24XQURmsWvXLsyfP7/c89OnT69V+8UdOXIEX331FX755Rc4Ojpi8eLFuHbtGqZNm2btphHd85jsEJFZDR061GgjVwDw9/e3QmvM5/Lly2jQoIF+6ftLly6hRYsWVm4VEQFMdojIzNq0aYPw8HBrN8Psrly5ot//rri4GLGxsRg8eLCVW0VEAJMdIrKytLQ0vPLKK3j66achiiK2bNmC7OxsREREYNy4cUa7skdFRWHlypW4evUq7Ozs0KxZM4wYMQJBQUEG9TIzM7FixQqcPHkSN27cgJeXF1q3bo0xY8boN4QEALVajV9//RV79uxBcXExWrZsiQkTJsDd3f22bc/JydF/feXKFbRr1w45OTm4cuUKtFot6tati5ycHDg4OMDBwaGakSKiO8WNQInILErm7EydOhXBwcEG5wRBgJubG4DSZKdBgwYoKCjAo48+CrVajS1btkAURXz55Zf6HdZPnz6NTz/9FH5+fujZsyeKi4uxdetWSJKEWbNm6YfLMjMz8d577yE/Px89e/ZEYGAgMjMzcfDgQUyfPh0uLi769oWGhsLFxQUdOnRAWloatmzZgvvvvx8TJ0687XscOnRopWIxZMiQStcloprHnh0iMqtPPvnEqEypVGLp0qUGZSkpKfjmm2/g7e0NAGjdujUmT56M9evXY9SoUQCA33//Ha6urpgxYwZcXV0BAO3bt8ekSZOwcuVKvPLKKwCAZcuWQaVSYebMmQZDaMOGDcOtf9+5urpiypQpEAQBACDLMrZu3Yr8/Hw4OztX+N6mTJkCADh48CCOHDmCV199FQCwdOlSeHl5oW/fvgCAunXrViJSRGQuTHaIyKzGjRuHgIAAgzJRNF71on379vpEBwAiIiLQsGFDnDhxAqNGjUJWVhZiY2MxYMAAfaIDAMHBwWjZsiVOnDgBAJAkCUeOHEHbtm1NzhUqSWpKPPzwwwZlTZs2xebNm5Genm7UI3Wrli1bAgD+/vtvtGjRAi1btoQkSUhJSUGfPn3054nIupjsEJFZRUREVGqC8q0JUUnZgQMHAADp6ekAgHr16hnVCwwMxKlTp1BYWIjCwkIUFBQYzfUpj4+Pj8Gxi4sLACAvL6/C63JzcyFJEgDg3LlzePzxx5GTk4O4uDj96+fk5MDe3l7/hBYRWQeTHSK6p5nqZQJgNNx1q3feeUefgAHAkiVLsGTJEv3xu+++CwDo2rUrXn755RpoKRHdKSY7RGQTkpOTTZb5+voCgP6/SUlJRvWSkpLg5uYGR0dH2Nvbw8nJCXFxcWZt76uvvori4mIcOXIEBw4cwGuvvQYAWL58Odzc3NCvXz8AMBiaIyLr4HYRRGQTjhw5gszMTP1xdHQ0Ll++jNatWwMAvLy8EBISgt27dxsMMcXFxeHUqVNo06YNAF1PTfv27XHs2DGTW0HU1AOoTZo0QcuWLVFQUIBGjRqhZcuWaNmyJTIyMtC2bVv98a2PxBOR5bFnh4jM6sSJE0hMTDQqb9y4scFTSv7+/pg6darBo+dubm4YOHCgvs7TTz+NTz/9FFOmTEH37t1RXFyMbdu2wdnZ2eDR7hEjRuD06dOYNm0aevbsiaCgIGRlZeHgwYP4+OOP9fNyasLFixfx8MMPAwBSU1OhUqnQuHHjGrs/EVUfkx0iMquVK1eaLH/ppZcMkp0uXbpAFEVs3rwZOTk5iIiIwNixY+Hl5aWv07JlS0yePBkrV67EypUr9YsKjhw50mBLCm9vb8ycORPLly/Hf//9h4KCAnh7e6N169Y1urifSqVCamqqPrm5dOkSnJycUL9+/Rp7DSKqPi4qSERWVXYF5QEDBli7OURUC3HODhEREdVqTHaIiIioVmOyQ0RERLUa5+wQERFRrcaeHSIiIqrVmOwQERFRrcZkh4iIiGo1JjtERERUqzHZISIiolqNyQ4RERHVakx2iIiIqFZjskNERES1GpMdIiIiqtX+DxAUSwJc884fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8\n",
        "vnoise=1\n",
        "\n",
        "# Train/ Val sets \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=42)\n",
        "train_set = train_df.values\n",
        "val_set = val_df.values\n",
        "\n",
        "train_set = np.asarray(train_set).astype(np.float32)\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "datagen_val = ImageDataGenerator()\n",
        "\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "nn=256  # units 200 to 256\n",
        "\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-4))    # Learning rate changes 1e-5 to 1e-4\n",
        "\n",
        "# Training\n",
        "nb_epochs=1000;    # epochs change to 1000\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "# Plot the curves\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "SsLDEkmiQdeD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoJ_LtWsjMAB"
      },
      "source": [
        "The Training Loss is around 0.03,  the Validation Loss is around 0.04, which is far more below the naive prediction 0.11."
      ],
      "id": "xoJ_LtWsjMAB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrEgpgcmMEWp"
      },
      "source": [
        "#**-- ABANDONED --**: Predict probability distribution over the 3 classes#\n",
        "**This part is actually classification based on most selected result, which is not to predict the distribution**  \n",
        "  \n",
        "**3 classes:**   \n",
        "\"Cannot see any difference\" - 0  [1,0,0] (one-hot encoding).  \n",
        "\"Just noticeable difference(JND)\" - 1  [0,1,0].    \n",
        "\"Clearly see a difference\" - 2  [0,0,1].  \n",
        "\n",
        "\n",
        "1.Use the \"cross entropy\" loss function between the prediction and the ground truth.  \n",
        "$H(p, q) = -\\sum_{i=1}^{C} p_i \\log(q_i)$  \n",
        "(Input the Euclidean distance + the outputs of the twin networks to the last layers)  \n",
        "**--> Question: Ground truth <-> Average color difference assessment? Rounding?**  \n",
        "--> In this trail, the ground truth will be chosen as the most selected results by paticipants in the experiment.  **--> Question: Is it reasonable?**  \n",
        "\n",
        "**--> Reply: What we want to predict for each tested pair is the probability aver the three classes, so that we can say, for example : \"for this pair, 70% of\n",
        "the observers would say JND, 25% would say 'see nothing' and 5% would say 'clear difference'. we have to train with real ground truth values, not integers. We have to learn to predict the true values, which are float numbers. And there is no problem for a Deep network to predict float numbers, even\n",
        "for a classification problem.**\n",
        "\n",
        "2.Add a softmax activation function in the last layer. Try to add a \"Temperature\" in this softmax.\n",
        "\n"
      ],
      "id": "JrEgpgcmMEWp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c3b4e356",
        "outputId": "c62253b7-4e6f-451f-811b-9e99fa6cb04c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "80/80 [==============================] - 10s 83ms/step - loss: 0.8158 - val_loss: 0.6156\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.7167 - val_loss: 0.5977\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.6546 - val_loss: 0.5902\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.6266 - val_loss: 0.5187\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.6133 - val_loss: 0.4934\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5741 - val_loss: 0.5526\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5589 - val_loss: 0.4466\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.5387 - val_loss: 0.4624\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5078 - val_loss: 0.4515\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.5079 - val_loss: 0.4120\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.4956 - val_loss: 0.4444\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.4836 - val_loss: 0.4688\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.4830 - val_loss: 0.4648\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.4952 - val_loss: 0.4078\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.4613 - val_loss: 0.3997\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.4481 - val_loss: 0.3759\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.4484 - val_loss: 0.4183\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.4339 - val_loss: 0.3864\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 5s 69ms/step - loss: 0.4228 - val_loss: 0.3500\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.4208 - val_loss: 0.3619\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 5s 60ms/step - loss: 0.4056 - val_loss: 0.3500\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.4102 - val_loss: 0.3433\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.4045 - val_loss: 0.3553\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3990 - val_loss: 0.4026\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.4031 - val_loss: 0.3282\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3873 - val_loss: 0.3468\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.3924 - val_loss: 0.3513\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.3822 - val_loss: 0.3302\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3827 - val_loss: 0.3440\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.3694 - val_loss: 0.3405\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3709 - val_loss: 0.3451\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.3686 - val_loss: 0.2983\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.3574 - val_loss: 0.3065\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3609 - val_loss: 0.3311\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.3452 - val_loss: 0.2969\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.3630 - val_loss: 0.3129\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3497 - val_loss: 0.3383\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.3606 - val_loss: 0.3832\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3436 - val_loss: 0.2801\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.3340 - val_loss: 0.2856\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.3288 - val_loss: 0.2993\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3443 - val_loss: 0.2875\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.3296 - val_loss: 0.2908\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3269 - val_loss: 0.2694\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3237 - val_loss: 0.3151\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.3176 - val_loss: 0.2921\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3388 - val_loss: 0.3020\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.3230 - val_loss: 0.2708\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 0.3136 - val_loss: 0.2554\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.3143 - val_loss: 0.2576\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.3093 - val_loss: 0.2709\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3137 - val_loss: 0.2474\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.3104 - val_loss: 0.2480\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.3004 - val_loss: 0.2602\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3051 - val_loss: 0.2659\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.3048 - val_loss: 0.2464\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3094 - val_loss: 0.2833\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.2954 - val_loss: 0.2849\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.3081 - val_loss: 0.2423\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2984 - val_loss: 0.2375\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2947 - val_loss: 0.2605\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2946 - val_loss: 0.2662\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2944 - val_loss: 0.2454\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2917 - val_loss: 0.2371\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2891 - val_loss: 0.2406\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2867 - val_loss: 0.2868\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.2845 - val_loss: 0.2326\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2847 - val_loss: 0.2341\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.2798 - val_loss: 0.2254\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2820 - val_loss: 0.2279\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.2898 - val_loss: 0.2282\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.2721 - val_loss: 0.2438\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2706 - val_loss: 0.2250\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2797 - val_loss: 0.2472\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2774 - val_loss: 0.2239\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.2763 - val_loss: 0.2328\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.2682 - val_loss: 0.2188\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2629 - val_loss: 0.2367\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.2697 - val_loss: 0.2851\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2746 - val_loss: 0.2361\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2678 - val_loss: 0.2195\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2631 - val_loss: 0.2150\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2632 - val_loss: 0.2113\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.2662 - val_loss: 0.2213\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.2555 - val_loss: 0.2391\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2630 - val_loss: 0.2174\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2588 - val_loss: 0.2377\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2562 - val_loss: 0.2375\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2711 - val_loss: 0.2127\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2564 - val_loss: 0.2391\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2588 - val_loss: 0.2107\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2544 - val_loss: 0.2226\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2489 - val_loss: 0.2131\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2465 - val_loss: 0.1912\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2547 - val_loss: 0.2012\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2463 - val_loss: 0.2125\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.2492 - val_loss: 0.2028\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.2517 - val_loss: 0.2051\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2482 - val_loss: 0.1975\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2567 - val_loss: 0.2154\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2484 - val_loss: 0.2062\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.2436 - val_loss: 0.2055\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.2421 - val_loss: 0.1903\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2417 - val_loss: 0.2155\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.2442 - val_loss: 0.2030\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.2426 - val_loss: 0.2064\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2416 - val_loss: 0.1938\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2426 - val_loss: 0.2019\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2427 - val_loss: 0.1976\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.2377 - val_loss: 0.2140\n",
            "Epoch 111/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.2371 - val_loss: 0.2058\n",
            "Epoch 112/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2404 - val_loss: 0.1884\n",
            "Epoch 113/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2355 - val_loss: 0.1996\n",
            "Epoch 114/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2391 - val_loss: 0.1975\n",
            "Epoch 115/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2340 - val_loss: 0.1907\n",
            "Epoch 116/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.2325 - val_loss: 0.1885\n",
            "Epoch 117/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2348 - val_loss: 0.1959\n",
            "Epoch 118/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.2333 - val_loss: 0.1931\n",
            "Epoch 119/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.2316 - val_loss: 0.1924\n",
            "Epoch 120/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2336 - val_loss: 0.1923\n",
            "Epoch 121/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.2329 - val_loss: 0.1968\n",
            "Epoch 122/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2339 - val_loss: 0.2425\n",
            "Epoch 123/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.2312 - val_loss: 0.1870\n",
            "Epoch 124/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.2288 - val_loss: 0.1864\n",
            "Epoch 125/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2285 - val_loss: 0.2003\n",
            "Epoch 126/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.2362 - val_loss: 0.1910\n",
            "Epoch 127/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2276 - val_loss: 0.1910\n",
            "Epoch 128/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.2362 - val_loss: 0.1898\n",
            "Epoch 129/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.2327 - val_loss: 0.1905\n",
            "Epoch 130/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2271 - val_loss: 0.2037\n",
            "Epoch 131/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.2335 - val_loss: 0.2049\n",
            "Epoch 132/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2315 - val_loss: 0.1815\n",
            "Epoch 133/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2212 - val_loss: 0.1788\n",
            "Epoch 134/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2226 - val_loss: 0.1966\n",
            "Epoch 135/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2218 - val_loss: 0.1905\n",
            "Epoch 136/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.2156 - val_loss: 0.1858\n",
            "Epoch 137/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2219 - val_loss: 0.1937\n",
            "Epoch 138/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2163 - val_loss: 0.1783\n",
            "Epoch 139/500\n",
            "80/80 [==============================] - 6s 82ms/step - loss: 0.2112 - val_loss: 0.1883\n",
            "Epoch 140/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2181 - val_loss: 0.1788\n",
            "Epoch 141/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2141 - val_loss: 0.1860\n",
            "Epoch 142/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2180 - val_loss: 0.1856\n",
            "Epoch 143/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2315 - val_loss: 0.2087\n",
            "Epoch 144/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.2266 - val_loss: 0.2026\n",
            "Epoch 145/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.2216 - val_loss: 0.1861\n",
            "Epoch 146/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2165 - val_loss: 0.1763\n",
            "Epoch 147/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2186 - val_loss: 0.1808\n",
            "Epoch 148/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2186 - val_loss: 0.1847\n",
            "Epoch 149/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.2164 - val_loss: 0.1856\n",
            "Epoch 150/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2167 - val_loss: 0.1897\n",
            "Epoch 151/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2129 - val_loss: 0.1843\n",
            "Epoch 152/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2084 - val_loss: 0.1808\n",
            "Epoch 153/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2132 - val_loss: 0.2070\n",
            "Epoch 154/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2159 - val_loss: 0.2053\n",
            "Epoch 155/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2144 - val_loss: 0.1792\n",
            "Epoch 156/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2202 - val_loss: 0.1880\n",
            "Epoch 157/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.2118 - val_loss: 0.1948\n",
            "Epoch 158/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.2172 - val_loss: 0.1957\n",
            "Epoch 159/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2158 - val_loss: 0.1832\n",
            "Epoch 160/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2069 - val_loss: 0.1985\n",
            "Epoch 161/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2109 - val_loss: 0.1830\n",
            "Epoch 162/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.2119 - val_loss: 0.1807\n",
            "Epoch 163/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.2108 - val_loss: 0.1869\n",
            "Epoch 164/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2116 - val_loss: 0.1935\n",
            "Epoch 165/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.2088 - val_loss: 0.1814\n",
            "Epoch 166/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2064 - val_loss: 0.1807\n",
            "Epoch 167/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2018 - val_loss: 0.1840\n",
            "Epoch 168/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2063 - val_loss: 0.1879\n",
            "Epoch 169/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2012 - val_loss: 0.2007\n",
            "Epoch 170/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.2112 - val_loss: 0.1683\n",
            "Epoch 171/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.2020 - val_loss: 0.1767\n",
            "Epoch 172/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1990 - val_loss: 0.1687\n",
            "Epoch 173/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2015 - val_loss: 0.1750\n",
            "Epoch 174/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2009 - val_loss: 0.1704\n",
            "Epoch 175/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.2023 - val_loss: 0.1749\n",
            "Epoch 176/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1985 - val_loss: 0.1936\n",
            "Epoch 177/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1979 - val_loss: 0.2057\n",
            "Epoch 178/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2101 - val_loss: 0.1890\n",
            "Epoch 179/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1994 - val_loss: 0.1794\n",
            "Epoch 180/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2055 - val_loss: 0.1765\n",
            "Epoch 181/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1950 - val_loss: 0.1827\n",
            "Epoch 182/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1946 - val_loss: 0.1832\n",
            "Epoch 183/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.2018 - val_loss: 0.1829\n",
            "Epoch 184/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.2017 - val_loss: 0.1838\n",
            "Epoch 185/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2035 - val_loss: 0.1695\n",
            "Epoch 186/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1933 - val_loss: 0.1778\n",
            "Epoch 187/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1886 - val_loss: 0.1945\n",
            "Epoch 188/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1988 - val_loss: 0.1721\n",
            "Epoch 189/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1911 - val_loss: 0.1918\n",
            "Epoch 190/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2063 - val_loss: 0.2011\n",
            "Epoch 191/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2017 - val_loss: 0.1820\n",
            "Epoch 192/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1969 - val_loss: 0.1889\n",
            "Epoch 193/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1938 - val_loss: 0.1856\n",
            "Epoch 194/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1883 - val_loss: 0.1727\n",
            "Epoch 195/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1840 - val_loss: 0.1891\n",
            "Epoch 196/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2031 - val_loss: 0.1995\n",
            "Epoch 197/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1949 - val_loss: 0.1881\n",
            "Epoch 198/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1989 - val_loss: 0.1895\n",
            "Epoch 199/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1910 - val_loss: 0.1721\n",
            "Epoch 200/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1915 - val_loss: 0.1910\n",
            "Epoch 201/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1915 - val_loss: 0.1748\n",
            "Epoch 202/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1904 - val_loss: 0.1771\n",
            "Epoch 203/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1908 - val_loss: 0.2097\n",
            "Epoch 204/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1837 - val_loss: 0.1907\n",
            "Epoch 205/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1831 - val_loss: 0.2066\n",
            "Epoch 206/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.2084 - val_loss: 0.1881\n",
            "Epoch 207/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1907 - val_loss: 0.1874\n",
            "Epoch 208/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1829 - val_loss: 0.2126\n",
            "Epoch 209/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1880 - val_loss: 0.1750\n",
            "Epoch 210/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1848 - val_loss: 0.1808\n",
            "Epoch 211/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1890 - val_loss: 0.1938\n",
            "Epoch 212/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1910 - val_loss: 0.1838\n",
            "Epoch 213/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1852 - val_loss: 0.2084\n",
            "Epoch 214/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1829 - val_loss: 0.1900\n",
            "Epoch 215/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1872 - val_loss: 0.1913\n",
            "Epoch 216/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1841 - val_loss: 0.1863\n",
            "Epoch 217/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1752 - val_loss: 0.1957\n",
            "Epoch 218/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1871 - val_loss: 0.1806\n",
            "Epoch 219/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1809 - val_loss: 0.2046\n",
            "Epoch 220/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1816 - val_loss: 0.1963\n",
            "Epoch 221/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1844 - val_loss: 0.1875\n",
            "Epoch 222/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1832 - val_loss: 0.1918\n",
            "Epoch 223/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1874 - val_loss: 0.1858\n",
            "Epoch 224/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1808 - val_loss: 0.1822\n",
            "Epoch 225/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1786 - val_loss: 0.2313\n",
            "Epoch 226/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2066 - val_loss: 0.1862\n",
            "Epoch 227/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1823 - val_loss: 0.1993\n",
            "Epoch 228/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1772 - val_loss: 0.2021\n",
            "Epoch 229/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1748 - val_loss: 0.1866\n",
            "Epoch 230/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1809 - val_loss: 0.1988\n",
            "Epoch 231/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1792 - val_loss: 0.1816\n",
            "Epoch 232/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1806 - val_loss: 0.1872\n",
            "Epoch 233/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1788 - val_loss: 0.2065\n",
            "Epoch 234/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1896 - val_loss: 0.1864\n",
            "Epoch 235/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1839 - val_loss: 0.1996\n",
            "Epoch 236/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1795 - val_loss: 0.1860\n",
            "Epoch 237/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1791 - val_loss: 0.1904\n",
            "Epoch 238/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1771 - val_loss: 0.1991\n",
            "Epoch 239/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1730 - val_loss: 0.1804\n",
            "Epoch 240/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1813 - val_loss: 0.2007\n",
            "Epoch 241/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1810 - val_loss: 0.1796\n",
            "Epoch 242/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1722 - val_loss: 0.2080\n",
            "Epoch 243/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1787 - val_loss: 0.1718\n",
            "Epoch 244/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1703 - val_loss: 0.2015\n",
            "Epoch 245/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1758 - val_loss: 0.1743\n",
            "Epoch 246/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1702 - val_loss: 0.1947\n",
            "Epoch 247/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1733 - val_loss: 0.1861\n",
            "Epoch 248/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1732 - val_loss: 0.1961\n",
            "Epoch 249/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1730 - val_loss: 0.1746\n",
            "Epoch 250/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1683 - val_loss: 0.2031\n",
            "Epoch 251/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1671 - val_loss: 0.1861\n",
            "Epoch 252/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1709 - val_loss: 0.2267\n",
            "Epoch 253/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1672 - val_loss: 0.1966\n",
            "Epoch 254/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1694 - val_loss: 0.1971\n",
            "Epoch 255/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1799 - val_loss: 0.2009\n",
            "Epoch 256/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1733 - val_loss: 0.1817\n",
            "Epoch 257/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1778 - val_loss: 0.2025\n",
            "Epoch 258/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1645 - val_loss: 0.1837\n",
            "Epoch 259/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1704 - val_loss: 0.1964\n",
            "Epoch 260/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1639 - val_loss: 0.2000\n",
            "Epoch 261/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1608 - val_loss: 0.1927\n",
            "Epoch 262/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1624 - val_loss: 0.2002\n",
            "Epoch 263/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1691 - val_loss: 0.1722\n",
            "Epoch 264/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1714 - val_loss: 0.2054\n",
            "Epoch 265/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1761 - val_loss: 0.2385\n",
            "Epoch 266/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1713 - val_loss: 0.1835\n",
            "Epoch 267/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1671 - val_loss: 0.1946\n",
            "Epoch 268/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1581 - val_loss: 0.2508\n",
            "Epoch 269/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1716 - val_loss: 0.1739\n",
            "Epoch 270/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1743 - val_loss: 0.2388\n",
            "Epoch 271/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1705 - val_loss: 0.1920\n",
            "Epoch 272/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1781 - val_loss: 0.1812\n",
            "Epoch 273/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.1598 - val_loss: 0.2134\n",
            "Epoch 274/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1606 - val_loss: 0.2020\n",
            "Epoch 275/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1626 - val_loss: 0.2126\n",
            "Epoch 276/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1576 - val_loss: 0.2356\n",
            "Epoch 277/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1752 - val_loss: 0.1910\n",
            "Epoch 278/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1657 - val_loss: 0.2010\n",
            "Epoch 279/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1654 - val_loss: 0.2235\n",
            "Epoch 280/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1708 - val_loss: 0.2418\n",
            "Epoch 281/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1851 - val_loss: 0.2091\n",
            "Epoch 282/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1633 - val_loss: 0.2153\n",
            "Epoch 283/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1664 - val_loss: 0.2262\n",
            "Epoch 284/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1703 - val_loss: 0.2195\n",
            "Epoch 285/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1823 - val_loss: 0.1903\n",
            "Epoch 286/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1652 - val_loss: 0.2056\n",
            "Epoch 287/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1618 - val_loss: 0.1961\n",
            "Epoch 288/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1742 - val_loss: 0.2058\n",
            "Epoch 289/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1614 - val_loss: 0.1943\n",
            "Epoch 290/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1620 - val_loss: 0.1892\n",
            "Epoch 291/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1568 - val_loss: 0.2166\n",
            "Epoch 292/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1619 - val_loss: 0.1900\n",
            "Epoch 293/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1608 - val_loss: 0.1816\n",
            "Epoch 294/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1576 - val_loss: 0.1840\n",
            "Epoch 295/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1640 - val_loss: 0.2011\n",
            "Epoch 296/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1592 - val_loss: 0.2242\n",
            "Epoch 297/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1558 - val_loss: 0.2149\n",
            "Epoch 298/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1524 - val_loss: 0.2541\n",
            "Epoch 299/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1596 - val_loss: 0.2267\n",
            "Epoch 300/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1565 - val_loss: 0.2325\n",
            "Epoch 301/500\n",
            "80/80 [==============================] - 5s 69ms/step - loss: 0.1557 - val_loss: 0.1920\n",
            "Epoch 302/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1569 - val_loss: 0.2127\n",
            "Epoch 303/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1509 - val_loss: 0.2839\n",
            "Epoch 304/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1632 - val_loss: 0.2408\n",
            "Epoch 305/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1625 - val_loss: 0.2756\n",
            "Epoch 306/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1510 - val_loss: 0.2017\n",
            "Epoch 307/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1489 - val_loss: 0.1987\n",
            "Epoch 308/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1510 - val_loss: 0.2415\n",
            "Epoch 309/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1537 - val_loss: 0.2361\n",
            "Epoch 310/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1550 - val_loss: 0.2151\n",
            "Epoch 311/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1515 - val_loss: 0.2046\n",
            "Epoch 312/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1520 - val_loss: 0.2097\n",
            "Epoch 313/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1493 - val_loss: 0.2417\n",
            "Epoch 314/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.1589 - val_loss: 0.1961\n",
            "Epoch 315/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.1534 - val_loss: 0.2083\n",
            "Epoch 316/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1525 - val_loss: 0.2050\n",
            "Epoch 317/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1527 - val_loss: 0.1900\n",
            "Epoch 318/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1550 - val_loss: 0.2008\n",
            "Epoch 319/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1590 - val_loss: 0.1793\n",
            "Epoch 320/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1541 - val_loss: 0.2218\n",
            "Epoch 321/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1598 - val_loss: 0.2151\n",
            "Epoch 322/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1549 - val_loss: 0.1967\n",
            "Epoch 323/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1580 - val_loss: 0.2392\n",
            "Epoch 324/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1550 - val_loss: 0.2021\n",
            "Epoch 325/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1579 - val_loss: 0.2207\n",
            "Epoch 326/500\n",
            "80/80 [==============================] - 8s 97ms/step - loss: 0.1578 - val_loss: 0.2271\n",
            "Epoch 327/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1499 - val_loss: 0.2217\n",
            "Epoch 328/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1492 - val_loss: 0.2169\n",
            "Epoch 329/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1703 - val_loss: 0.2163\n",
            "Epoch 330/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1723 - val_loss: 0.2407\n",
            "Epoch 331/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1667 - val_loss: 0.2054\n",
            "Epoch 332/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1669 - val_loss: 0.2116\n",
            "Epoch 333/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1451 - val_loss: 0.2739\n",
            "Epoch 334/500\n",
            "80/80 [==============================] - 8s 101ms/step - loss: 0.1556 - val_loss: 0.2386\n",
            "Epoch 335/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1603 - val_loss: 0.1972\n",
            "Epoch 336/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1506 - val_loss: 0.2127\n",
            "Epoch 337/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1496 - val_loss: 0.2007\n",
            "Epoch 338/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1435 - val_loss: 0.2306\n",
            "Epoch 339/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1489 - val_loss: 0.2237\n",
            "Epoch 340/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1447 - val_loss: 0.2353\n",
            "Epoch 341/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1501 - val_loss: 0.1868\n",
            "Epoch 342/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1538 - val_loss: 0.2089\n",
            "Epoch 343/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1497 - val_loss: 0.2134\n",
            "Epoch 344/500\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 0.1503 - val_loss: 0.2007\n",
            "Epoch 345/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1561 - val_loss: 0.2731\n",
            "Epoch 346/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1555 - val_loss: 0.2031\n",
            "Epoch 347/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1555 - val_loss: 0.2048\n",
            "Epoch 348/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1591 - val_loss: 0.1787\n",
            "Epoch 349/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1462 - val_loss: 0.1910\n",
            "Epoch 350/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1485 - val_loss: 0.2170\n",
            "Epoch 351/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1572 - val_loss: 0.2091\n",
            "Epoch 352/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1481 - val_loss: 0.1951\n",
            "Epoch 353/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1454 - val_loss: 0.1940\n",
            "Epoch 354/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1507 - val_loss: 0.2202\n",
            "Epoch 355/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1507 - val_loss: 0.2537\n",
            "Epoch 356/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1492 - val_loss: 0.2055\n",
            "Epoch 357/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1449 - val_loss: 0.2136\n",
            "Epoch 358/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1515 - val_loss: 0.2277\n",
            "Epoch 359/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1561 - val_loss: 0.2184\n",
            "Epoch 360/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1487 - val_loss: 0.2076\n",
            "Epoch 361/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1481 - val_loss: 0.2321\n",
            "Epoch 362/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1470 - val_loss: 0.2461\n",
            "Epoch 363/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1444 - val_loss: 0.1949\n",
            "Epoch 364/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1454 - val_loss: 0.2109\n",
            "Epoch 365/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1446 - val_loss: 0.2695\n",
            "Epoch 366/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1441 - val_loss: 0.2488\n",
            "Epoch 367/500\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 0.1530 - val_loss: 0.1783\n",
            "Epoch 368/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1385 - val_loss: 0.2030\n",
            "Epoch 369/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1439 - val_loss: 0.2070\n",
            "Epoch 370/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1425 - val_loss: 0.2097\n",
            "Epoch 371/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1459 - val_loss: 0.2205\n",
            "Epoch 372/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1476 - val_loss: 0.2607\n",
            "Epoch 373/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1565 - val_loss: 0.2310\n",
            "Epoch 374/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1422 - val_loss: 0.2218\n",
            "Epoch 375/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1512 - val_loss: 0.2365\n",
            "Epoch 376/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1448 - val_loss: 0.1995\n",
            "Epoch 377/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1433 - val_loss: 0.2538\n",
            "Epoch 378/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1426 - val_loss: 0.2181\n",
            "Epoch 379/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1399 - val_loss: 0.2385\n",
            "Epoch 380/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1389 - val_loss: 0.2485\n",
            "Epoch 381/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1471 - val_loss: 0.2176\n",
            "Epoch 382/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1388 - val_loss: 0.2407\n",
            "Epoch 383/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1407 - val_loss: 0.1860\n",
            "Epoch 384/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1364 - val_loss: 0.2588\n",
            "Epoch 385/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1413 - val_loss: 0.2352\n",
            "Epoch 386/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1426 - val_loss: 0.2192\n",
            "Epoch 387/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1404 - val_loss: 0.2077\n",
            "Epoch 388/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1390 - val_loss: 0.2081\n",
            "Epoch 389/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1375 - val_loss: 0.2179\n",
            "Epoch 390/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1402 - val_loss: 0.2416\n",
            "Epoch 391/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1368 - val_loss: 0.2502\n",
            "Epoch 392/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1432 - val_loss: 0.2395\n",
            "Epoch 393/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1452 - val_loss: 0.2519\n",
            "Epoch 394/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1582 - val_loss: 0.2509\n",
            "Epoch 395/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1461 - val_loss: 0.2450\n",
            "Epoch 396/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1417 - val_loss: 0.2053\n",
            "Epoch 397/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1401 - val_loss: 0.2575\n",
            "Epoch 398/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1419 - val_loss: 0.2159\n",
            "Epoch 399/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1411 - val_loss: 0.2084\n",
            "Epoch 400/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1349 - val_loss: 0.2429\n",
            "Epoch 401/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1450 - val_loss: 0.2502\n",
            "Epoch 402/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1506 - val_loss: 0.2060\n",
            "Epoch 403/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1460 - val_loss: 0.2603\n",
            "Epoch 404/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1391 - val_loss: 0.2270\n",
            "Epoch 405/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1326 - val_loss: 0.2177\n",
            "Epoch 406/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1362 - val_loss: 0.2571\n",
            "Epoch 407/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1319 - val_loss: 0.3000\n",
            "Epoch 408/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1449 - val_loss: 0.2153\n",
            "Epoch 409/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.1374 - val_loss: 0.2501\n",
            "Epoch 410/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1334 - val_loss: 0.2011\n",
            "Epoch 411/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1370 - val_loss: 0.2145\n",
            "Epoch 412/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1317 - val_loss: 0.2196\n",
            "Epoch 413/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1380 - val_loss: 0.3266\n",
            "Epoch 414/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1428 - val_loss: 0.2270\n",
            "Epoch 415/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1319 - val_loss: 0.2572\n",
            "Epoch 416/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1379 - val_loss: 0.2274\n",
            "Epoch 417/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1319 - val_loss: 0.1960\n",
            "Epoch 418/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1359 - val_loss: 0.2444\n",
            "Epoch 419/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1360 - val_loss: 0.1998\n",
            "Epoch 420/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1391 - val_loss: 0.2491\n",
            "Epoch 421/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1345 - val_loss: 0.3121\n",
            "Epoch 422/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1340 - val_loss: 0.2189\n",
            "Epoch 423/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1364 - val_loss: 0.2289\n",
            "Epoch 424/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1376 - val_loss: 0.1961\n",
            "Epoch 425/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1350 - val_loss: 0.2738\n",
            "Epoch 426/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1301 - val_loss: 0.2477\n",
            "Epoch 427/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1350 - val_loss: 0.2681\n",
            "Epoch 428/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1317 - val_loss: 0.2648\n",
            "Epoch 429/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1311 - val_loss: 0.2119\n",
            "Epoch 430/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1342 - val_loss: 0.2608\n",
            "Epoch 431/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1307 - val_loss: 0.2532\n",
            "Epoch 432/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1384 - val_loss: 0.2247\n",
            "Epoch 433/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1396 - val_loss: 0.2130\n",
            "Epoch 434/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1311 - val_loss: 0.2440\n",
            "Epoch 435/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1424 - val_loss: 0.2398\n",
            "Epoch 436/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1330 - val_loss: 0.2471\n",
            "Epoch 437/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1365 - val_loss: 0.2253\n",
            "Epoch 438/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1298 - val_loss: 0.2204\n",
            "Epoch 439/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1425 - val_loss: 0.2832\n",
            "Epoch 440/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1410 - val_loss: 0.2008\n",
            "Epoch 441/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1403 - val_loss: 0.2305\n",
            "Epoch 442/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1366 - val_loss: 0.2161\n",
            "Epoch 443/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1419 - val_loss: 0.2443\n",
            "Epoch 444/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1341 - val_loss: 0.2519\n",
            "Epoch 445/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1345 - val_loss: 0.2941\n",
            "Epoch 446/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1351 - val_loss: 0.2637\n",
            "Epoch 447/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1297 - val_loss: 0.2075\n",
            "Epoch 448/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1331 - val_loss: 0.2215\n",
            "Epoch 449/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1421 - val_loss: 0.2230\n",
            "Epoch 450/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1313 - val_loss: 0.2227\n",
            "Epoch 451/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1289 - val_loss: 0.2532\n",
            "Epoch 452/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1284 - val_loss: 0.2545\n",
            "Epoch 453/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1368 - val_loss: 0.2403\n",
            "Epoch 454/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1327 - val_loss: 0.1928\n",
            "Epoch 455/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1363 - val_loss: 0.2384\n",
            "Epoch 456/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1366 - val_loss: 0.2798\n",
            "Epoch 457/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1288 - val_loss: 0.2702\n",
            "Epoch 458/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1422 - val_loss: 0.2271\n",
            "Epoch 459/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1346 - val_loss: 0.2517\n",
            "Epoch 460/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1387 - val_loss: 0.1860\n",
            "Epoch 461/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1385 - val_loss: 0.2272\n",
            "Epoch 462/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1277 - val_loss: 0.2101\n",
            "Epoch 463/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1277 - val_loss: 0.2438\n",
            "Epoch 464/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1241 - val_loss: 0.2867\n",
            "Epoch 465/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1275 - val_loss: 0.2269\n",
            "Epoch 466/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1310 - val_loss: 0.2274\n",
            "Epoch 467/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1272 - val_loss: 0.2972\n",
            "Epoch 468/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1326 - val_loss: 0.2337\n",
            "Epoch 469/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1330 - val_loss: 0.1982\n",
            "Epoch 470/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1325 - val_loss: 0.2719\n",
            "Epoch 471/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1360 - val_loss: 0.2217\n",
            "Epoch 472/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1291 - val_loss: 0.2480\n",
            "Epoch 473/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1345 - val_loss: 0.2382\n",
            "Epoch 474/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1294 - val_loss: 0.2364\n",
            "Epoch 475/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1229 - val_loss: 0.3287\n",
            "Epoch 476/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1324 - val_loss: 0.2134\n",
            "Epoch 477/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1299 - val_loss: 0.2393\n",
            "Epoch 478/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1265 - val_loss: 0.2690\n",
            "Epoch 479/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1301 - val_loss: 0.3002\n",
            "Epoch 480/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1296 - val_loss: 0.2220\n",
            "Epoch 481/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1622 - val_loss: 0.2185\n",
            "Epoch 482/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1441 - val_loss: 0.2303\n",
            "Epoch 483/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1312 - val_loss: 0.2117\n",
            "Epoch 484/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1267 - val_loss: 0.2667\n",
            "Epoch 485/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1303 - val_loss: 0.2424\n",
            "Epoch 486/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1247 - val_loss: 0.2393\n",
            "Epoch 487/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1252 - val_loss: 0.2330\n",
            "Epoch 488/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1269 - val_loss: 0.2697\n",
            "Epoch 489/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1324 - val_loss: 0.2618\n",
            "Epoch 490/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1278 - val_loss: 0.2494\n",
            "Epoch 491/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1252 - val_loss: 0.2463\n",
            "Epoch 492/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1246 - val_loss: 0.2801\n",
            "Epoch 493/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1279 - val_loss: 0.2497\n",
            "Epoch 494/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1270 - val_loss: 0.1989\n",
            "Epoch 495/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1240 - val_loss: 0.2153\n",
            "Epoch 496/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1269 - val_loss: 0.2278\n",
            "Epoch 497/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1293 - val_loss: 0.2567\n",
            "Epoch 498/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1251 - val_loss: 0.2238\n",
            "Epoch 499/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1276 - val_loss: 0.2430\n",
            "Epoch 500/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1220 - val_loss: 0.2428\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.2428\n",
            "5/5 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff0fdb9d5b0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "bs=8    # batch_size  \n",
        "vnoise=0    # the amount of noise to be added to training data\n",
        "nn=512    # training units number set as 256\n",
        "nb_epochs=500;    # training epochs change to 1000\n",
        "\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "subset_mode = results_subset.mode(axis=1)    # get mode along result subset - rows axis\n",
        "data_df['most_common'] = subset_mode.iloc[:, 0]    # extract most common element in each row\n",
        "\n",
        "# Data splitting(8:2)\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=42)    # split the data   \n",
        "train_set = train_df.values\n",
        "val_set = val_df.values\n",
        "\n",
        "# Get the Lab data (X) and most selected data (Y)\n",
        "train_set = np.asarray(train_set).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6].astype(int)    # convert the float type to int type\n",
        "Y_train = to_categorical(Y_train)    # one-hot encoding\n",
        "\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6].astype(int)\n",
        "Y_val = to_categorical(Y_val)\n",
        "\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "pred = Dense(3, activation='softmax')(dist)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 5e-5 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "c3b4e356"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqOqgMkNmUzr"
      },
      "source": [
        "#2. Predict Probability Distribution over the 3 Classes#  \n",
        "Use the \"cross entropy\" loss function between the prediction and the ground truth.  \n",
        "$H(p, q) = -\\sum_{i=1}^{C} p_i \\log(q_i)$  \n",
        "(Input the Euclidean distance + the outputs of the twin networks to the last layers)  \n",
        "\n",
        "- 2.1 Without Temperature in Softmax: Softmax only based the euclidean distance\n",
        "- 2.2 Without Temperature in Softmax: Softmax based the euclidean distance and the output of Siamese Neural Network\n",
        "- 2.3 Add the Temperature in Softmax\n",
        "- 2.4 Add Hidden Layers before Softmax Layer\n",
        "- 2.5 Use Normalization layer"
      ],
      "id": "gqOqgMkNmUzr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxm_PyRkFQCm"
      },
      "source": [
        "##Dataset Processment **--> Run this! <--**##"
      ],
      "id": "Yxm_PyRkFQCm"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0Xeog_E6pv",
        "outputId": "19d16257-807c-444f-9af2-6b25e6133fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.1  0.55 0.35]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.25 0.75]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.45 0.55]\n",
            " [0.   0.3  0.7 ]\n",
            " [0.05 0.4  0.55]\n",
            " [0.   0.   1.  ]\n",
            " [0.1  0.35 0.55]\n",
            " [0.   0.3  0.7 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.1  0.35 0.55]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.05 0.   0.95]\n",
            " [0.35 0.5  0.15]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.   0.95]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.2  0.75]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.1  0.35 0.55]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.2  0.75]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.05 0.   0.95]\n",
            " [0.2  0.75 0.05]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.   0.95]\n",
            " [0.05 0.   0.95]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.35 0.65]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.15 0.45 0.4 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.5  0.45]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.1  0.   0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.   0.95]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.1  0.45 0.45]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.1  0.3  0.6 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.65 0.3  0.05]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.3  0.25 0.45]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.4  0.55]\n",
            " [0.   0.3  0.7 ]\n",
            " [0.05 0.25 0.7 ]\n",
            " [0.15 0.45 0.4 ]\n",
            " [0.   0.4  0.6 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.   0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.4  0.6 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.4  0.55]\n",
            " [0.05 0.3  0.65]\n",
            " [0.1  0.65 0.25]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.1  0.85]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.4  0.55]\n",
            " [0.   0.05 0.95]\n",
            " [0.1  0.55 0.35]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.05 0.25 0.7 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.4  0.6 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.2  0.5  0.3 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.15 0.5  0.35]\n",
            " [0.   0.   1.  ]\n",
            " [0.15 0.5  0.35]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.35 0.55 0.1 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]]\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "\n",
        "# count occurrences in each row, and make new columns\n",
        "def count_occurrences(row,element):\n",
        "    count = 0\n",
        "    for value in row:\n",
        "        if value == element:\n",
        "            count += 1\n",
        "    return count/20\n",
        "\n",
        "data_df['0occurrences'] = results_subset.apply(count_occurrences, args=(0,), axis=1)\n",
        "data_df['1occurrences'] = results_subset.apply(count_occurrences, args=(1,), axis=1)\n",
        "data_df['2occurrences'] = results_subset.apply(count_occurrences, args=(2,), axis=1)\n",
        "\n",
        "# Data splitting(7:3)\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=42)    # split the data   \n",
        "train_set = train_df.values\n",
        "val_set = val_df.values\n",
        "\n",
        "# Get the Lab data (X) and probability distribution (Y)\n",
        "train_set = np.asarray(train_set).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6:9]    # probability distribution columns\n",
        "Y_train = Y_train.reshape(-1,3)\n",
        "\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6:9]\n",
        "Y_val = Y_val.reshape(-1,3)\n",
        "print(Y_val)  # example: output to see the possibility distribution for validation set"
      ],
      "id": "2d0Xeog_E6pv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZHdyLOfNdHT"
      },
      "source": [
        "##2.1 Without Temperature in Softmax: Softmax only based the euclidean distance##\n",
        "\n",
        "**Result**  \n",
        "Around Training_loss = 0.38, validation_loss = 0.38 (500 Epochs)"
      ],
      "id": "aZHdyLOfNdHT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uuVsht8xCISl",
        "outputId": "072e2118-6785-47e3-c622-6f584eb5f907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "70/70 [==============================] - 7s 14ms/step - loss: 1.0549 - val_loss: 1.0402\n",
            "Epoch 2/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 1.0334 - val_loss: 1.0203\n",
            "Epoch 3/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 1.0166 - val_loss: 1.0025\n",
            "Epoch 4/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.9990 - val_loss: 0.9854\n",
            "Epoch 5/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.9815 - val_loss: 0.9649\n",
            "Epoch 6/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.9590 - val_loss: 0.9478\n",
            "Epoch 7/500\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.9390 - val_loss: 0.9233\n",
            "Epoch 8/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9141 - val_loss: 0.8934\n",
            "Epoch 9/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.8864 - val_loss: 0.8638\n",
            "Epoch 10/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.8583 - val_loss: 0.8317\n",
            "Epoch 11/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.8222 - val_loss: 0.7926\n",
            "Epoch 12/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.7818 - val_loss: 0.7491\n",
            "Epoch 13/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.7167 - val_loss: 0.6457\n",
            "Epoch 14/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6077 - val_loss: 0.5659\n",
            "Epoch 15/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.5538 - val_loss: 0.5319\n",
            "Epoch 16/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.5216 - val_loss: 0.5015\n",
            "Epoch 17/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.5012 - val_loss: 0.5009\n",
            "Epoch 18/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4878 - val_loss: 0.4787\n",
            "Epoch 19/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4817 - val_loss: 0.4668\n",
            "Epoch 20/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4727 - val_loss: 0.4616\n",
            "Epoch 21/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4674 - val_loss: 0.4580\n",
            "Epoch 22/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4674 - val_loss: 0.4576\n",
            "Epoch 23/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4614 - val_loss: 0.4487\n",
            "Epoch 24/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4575 - val_loss: 0.4460\n",
            "Epoch 25/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4583 - val_loss: 0.4464\n",
            "Epoch 26/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4533 - val_loss: 0.4384\n",
            "Epoch 27/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4477 - val_loss: 0.4368\n",
            "Epoch 28/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4475 - val_loss: 0.4361\n",
            "Epoch 29/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4460 - val_loss: 0.4354\n",
            "Epoch 30/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4448 - val_loss: 0.4298\n",
            "Epoch 31/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4420 - val_loss: 0.4262\n",
            "Epoch 32/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4437 - val_loss: 0.4260\n",
            "Epoch 33/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4405 - val_loss: 0.4315\n",
            "Epoch 34/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4433 - val_loss: 0.4221\n",
            "Epoch 35/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4381 - val_loss: 0.4230\n",
            "Epoch 36/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4374 - val_loss: 0.4176\n",
            "Epoch 37/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4363 - val_loss: 0.4194\n",
            "Epoch 38/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4360 - val_loss: 0.4195\n",
            "Epoch 39/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4349 - val_loss: 0.4207\n",
            "Epoch 40/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4342 - val_loss: 0.4157\n",
            "Epoch 41/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4328 - val_loss: 0.4156\n",
            "Epoch 42/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4339 - val_loss: 0.4144\n",
            "Epoch 43/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4318 - val_loss: 0.4300\n",
            "Epoch 44/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4342 - val_loss: 0.4142\n",
            "Epoch 45/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4317 - val_loss: 0.4121\n",
            "Epoch 46/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4312 - val_loss: 0.4089\n",
            "Epoch 47/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4290 - val_loss: 0.4112\n",
            "Epoch 48/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4276 - val_loss: 0.4072\n",
            "Epoch 49/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4276 - val_loss: 0.4130\n",
            "Epoch 50/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4304 - val_loss: 0.4083\n",
            "Epoch 51/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4300 - val_loss: 0.4095\n",
            "Epoch 52/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4287 - val_loss: 0.4075\n",
            "Epoch 53/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4276 - val_loss: 0.4052\n",
            "Epoch 54/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4286 - val_loss: 0.4105\n",
            "Epoch 55/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4288 - val_loss: 0.4048\n",
            "Epoch 56/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4260 - val_loss: 0.4090\n",
            "Epoch 57/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4264 - val_loss: 0.4045\n",
            "Epoch 58/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4273 - val_loss: 0.4118\n",
            "Epoch 59/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4274 - val_loss: 0.4132\n",
            "Epoch 60/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4281 - val_loss: 0.4046\n",
            "Epoch 61/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4246 - val_loss: 0.4090\n",
            "Epoch 62/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4246 - val_loss: 0.4040\n",
            "Epoch 63/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4242 - val_loss: 0.4040\n",
            "Epoch 64/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4247 - val_loss: 0.4020\n",
            "Epoch 65/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4255 - val_loss: 0.4034\n",
            "Epoch 66/500\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4247 - val_loss: 0.4032\n",
            "Epoch 67/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4233 - val_loss: 0.4074\n",
            "Epoch 68/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4231 - val_loss: 0.4105\n",
            "Epoch 69/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4223 - val_loss: 0.4011\n",
            "Epoch 70/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4231 - val_loss: 0.4096\n",
            "Epoch 71/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4239 - val_loss: 0.4017\n",
            "Epoch 72/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4220 - val_loss: 0.4028\n",
            "Epoch 73/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4233 - val_loss: 0.4010\n",
            "Epoch 74/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4212 - val_loss: 0.4020\n",
            "Epoch 75/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4222 - val_loss: 0.4006\n",
            "Epoch 76/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4216 - val_loss: 0.4001\n",
            "Epoch 77/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4214 - val_loss: 0.4058\n",
            "Epoch 78/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4238 - val_loss: 0.4038\n",
            "Epoch 79/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4192 - val_loss: 0.3993\n",
            "Epoch 80/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4213 - val_loss: 0.4051\n",
            "Epoch 81/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4206 - val_loss: 0.4014\n",
            "Epoch 82/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4198 - val_loss: 0.3968\n",
            "Epoch 83/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4190 - val_loss: 0.3988\n",
            "Epoch 84/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4200 - val_loss: 0.3986\n",
            "Epoch 85/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4211 - val_loss: 0.4007\n",
            "Epoch 86/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4210 - val_loss: 0.3991\n",
            "Epoch 87/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4206 - val_loss: 0.3965\n",
            "Epoch 88/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4197 - val_loss: 0.3968\n",
            "Epoch 89/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4201 - val_loss: 0.3998\n",
            "Epoch 90/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4180 - val_loss: 0.4034\n",
            "Epoch 91/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4190 - val_loss: 0.4023\n",
            "Epoch 92/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4187 - val_loss: 0.3982\n",
            "Epoch 93/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4185 - val_loss: 0.4052\n",
            "Epoch 94/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4186 - val_loss: 0.4029\n",
            "Epoch 95/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4196 - val_loss: 0.3965\n",
            "Epoch 96/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4189 - val_loss: 0.4009\n",
            "Epoch 97/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4182 - val_loss: 0.3997\n",
            "Epoch 98/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4183 - val_loss: 0.3989\n",
            "Epoch 99/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4189 - val_loss: 0.4062\n",
            "Epoch 100/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4198 - val_loss: 0.4005\n",
            "Epoch 101/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4156 - val_loss: 0.3994\n",
            "Epoch 102/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4172 - val_loss: 0.4080\n",
            "Epoch 103/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4182 - val_loss: 0.3984\n",
            "Epoch 104/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4213 - val_loss: 0.3991\n",
            "Epoch 105/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4182 - val_loss: 0.4068\n",
            "Epoch 106/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4169 - val_loss: 0.3975\n",
            "Epoch 107/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4150 - val_loss: 0.3972\n",
            "Epoch 108/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4173 - val_loss: 0.4031\n",
            "Epoch 109/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4160 - val_loss: 0.4007\n",
            "Epoch 110/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4132 - val_loss: 0.3959\n",
            "Epoch 111/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4143 - val_loss: 0.3941\n",
            "Epoch 112/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4145 - val_loss: 0.3968\n",
            "Epoch 113/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4138 - val_loss: 0.3944\n",
            "Epoch 114/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4131 - val_loss: 0.3945\n",
            "Epoch 115/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4151 - val_loss: 0.3972\n",
            "Epoch 116/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4141 - val_loss: 0.3975\n",
            "Epoch 117/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4133 - val_loss: 0.3972\n",
            "Epoch 118/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4133 - val_loss: 0.4082\n",
            "Epoch 119/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4151 - val_loss: 0.3943\n",
            "Epoch 120/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4120 - val_loss: 0.3948\n",
            "Epoch 121/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4159 - val_loss: 0.3957\n",
            "Epoch 122/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4120 - val_loss: 0.3903\n",
            "Epoch 123/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4135 - val_loss: 0.3947\n",
            "Epoch 124/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4115 - val_loss: 0.3957\n",
            "Epoch 125/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4111 - val_loss: 0.4002\n",
            "Epoch 126/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4133 - val_loss: 0.3957\n",
            "Epoch 127/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4107 - val_loss: 0.3948\n",
            "Epoch 128/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4100 - val_loss: 0.3945\n",
            "Epoch 129/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4114 - val_loss: 0.3969\n",
            "Epoch 130/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4120 - val_loss: 0.3931\n",
            "Epoch 131/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4110 - val_loss: 0.4010\n",
            "Epoch 132/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4131 - val_loss: 0.3934\n",
            "Epoch 133/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4095 - val_loss: 0.3993\n",
            "Epoch 134/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4137 - val_loss: 0.3957\n",
            "Epoch 135/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4103 - val_loss: 0.3968\n",
            "Epoch 136/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4096 - val_loss: 0.3972\n",
            "Epoch 137/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4094 - val_loss: 0.3965\n",
            "Epoch 138/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4094 - val_loss: 0.3942\n",
            "Epoch 139/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4088 - val_loss: 0.3911\n",
            "Epoch 140/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4085 - val_loss: 0.3913\n",
            "Epoch 141/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4096 - val_loss: 0.3891\n",
            "Epoch 142/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4089 - val_loss: 0.3921\n",
            "Epoch 143/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4094 - val_loss: 0.3973\n",
            "Epoch 144/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4087 - val_loss: 0.3913\n",
            "Epoch 145/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4080 - val_loss: 0.3893\n",
            "Epoch 146/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4113 - val_loss: 0.3913\n",
            "Epoch 147/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4078 - val_loss: 0.3943\n",
            "Epoch 148/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4076 - val_loss: 0.3894\n",
            "Epoch 149/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4106 - val_loss: 0.3918\n",
            "Epoch 150/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4105 - val_loss: 0.3952\n",
            "Epoch 151/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4082 - val_loss: 0.3862\n",
            "Epoch 152/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4082 - val_loss: 0.3894\n",
            "Epoch 153/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4062 - val_loss: 0.3956\n",
            "Epoch 154/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4069 - val_loss: 0.3899\n",
            "Epoch 155/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4074 - val_loss: 0.3906\n",
            "Epoch 156/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4081 - val_loss: 0.3898\n",
            "Epoch 157/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4072 - val_loss: 0.4017\n",
            "Epoch 158/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4085 - val_loss: 0.4162\n",
            "Epoch 159/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4078 - val_loss: 0.3897\n",
            "Epoch 160/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4080 - val_loss: 0.3947\n",
            "Epoch 161/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4075 - val_loss: 0.3937\n",
            "Epoch 162/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4063 - val_loss: 0.3928\n",
            "Epoch 163/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4055 - val_loss: 0.3903\n",
            "Epoch 164/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4072 - val_loss: 0.3890\n",
            "Epoch 165/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4054 - val_loss: 0.3908\n",
            "Epoch 166/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4050 - val_loss: 0.3948\n",
            "Epoch 167/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4065 - val_loss: 0.3894\n",
            "Epoch 168/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4045 - val_loss: 0.3930\n",
            "Epoch 169/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4061 - val_loss: 0.3895\n",
            "Epoch 170/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4048 - val_loss: 0.3894\n",
            "Epoch 171/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4048 - val_loss: 0.3922\n",
            "Epoch 172/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4045 - val_loss: 0.3932\n",
            "Epoch 173/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4033 - val_loss: 0.3929\n",
            "Epoch 174/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4039 - val_loss: 0.3902\n",
            "Epoch 175/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4045 - val_loss: 0.3903\n",
            "Epoch 176/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4030 - val_loss: 0.3907\n",
            "Epoch 177/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4041 - val_loss: 0.3913\n",
            "Epoch 178/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4041 - val_loss: 0.3921\n",
            "Epoch 179/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4051 - val_loss: 0.3912\n",
            "Epoch 180/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4046 - val_loss: 0.3977\n",
            "Epoch 181/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4036 - val_loss: 0.3911\n",
            "Epoch 182/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4044 - val_loss: 0.3857\n",
            "Epoch 183/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4018 - val_loss: 0.3949\n",
            "Epoch 184/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4025 - val_loss: 0.3973\n",
            "Epoch 185/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4046 - val_loss: 0.3874\n",
            "Epoch 186/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4010 - val_loss: 0.3903\n",
            "Epoch 187/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4015 - val_loss: 0.3940\n",
            "Epoch 188/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4035 - val_loss: 0.3947\n",
            "Epoch 189/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4042 - val_loss: 0.3867\n",
            "Epoch 190/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4028 - val_loss: 0.3879\n",
            "Epoch 191/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4015 - val_loss: 0.3887\n",
            "Epoch 192/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4020 - val_loss: 0.3873\n",
            "Epoch 193/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4017 - val_loss: 0.3866\n",
            "Epoch 194/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4012 - val_loss: 0.3869\n",
            "Epoch 195/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4035 - val_loss: 0.3916\n",
            "Epoch 196/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4001 - val_loss: 0.3873\n",
            "Epoch 197/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4016 - val_loss: 0.3848\n",
            "Epoch 198/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3994 - val_loss: 0.3884\n",
            "Epoch 199/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4006 - val_loss: 0.3845\n",
            "Epoch 200/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3865\n",
            "Epoch 201/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4013 - val_loss: 0.3867\n",
            "Epoch 202/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4032 - val_loss: 0.3905\n",
            "Epoch 203/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3894\n",
            "Epoch 204/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4017 - val_loss: 0.3873\n",
            "Epoch 205/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4008 - val_loss: 0.3871\n",
            "Epoch 206/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4000 - val_loss: 0.3882\n",
            "Epoch 207/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4003 - val_loss: 0.3924\n",
            "Epoch 208/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3987 - val_loss: 0.3847\n",
            "Epoch 209/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4000 - val_loss: 0.3932\n",
            "Epoch 210/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4023 - val_loss: 0.3912\n",
            "Epoch 211/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4001 - val_loss: 0.3876\n",
            "Epoch 212/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4001 - val_loss: 0.3849\n",
            "Epoch 213/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4010 - val_loss: 0.3834\n",
            "Epoch 214/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4009 - val_loss: 0.3895\n",
            "Epoch 215/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4005 - val_loss: 0.3888\n",
            "Epoch 216/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4005 - val_loss: 0.3896\n",
            "Epoch 217/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3985 - val_loss: 0.3850\n",
            "Epoch 218/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3994 - val_loss: 0.3876\n",
            "Epoch 219/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3993 - val_loss: 0.3865\n",
            "Epoch 220/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3984 - val_loss: 0.3873\n",
            "Epoch 221/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3989 - val_loss: 0.3866\n",
            "Epoch 222/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4011 - val_loss: 0.3831\n",
            "Epoch 223/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3977 - val_loss: 0.3845\n",
            "Epoch 224/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4006 - val_loss: 0.3941\n",
            "Epoch 225/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4012 - val_loss: 0.3880\n",
            "Epoch 226/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3977 - val_loss: 0.3924\n",
            "Epoch 227/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3991 - val_loss: 0.3884\n",
            "Epoch 228/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3991 - val_loss: 0.3885\n",
            "Epoch 229/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3985 - val_loss: 0.3870\n",
            "Epoch 230/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3984 - val_loss: 0.3863\n",
            "Epoch 231/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3977 - val_loss: 0.3854\n",
            "Epoch 232/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3983 - val_loss: 0.3853\n",
            "Epoch 233/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3979 - val_loss: 0.3842\n",
            "Epoch 234/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3975 - val_loss: 0.3889\n",
            "Epoch 235/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.3857\n",
            "Epoch 236/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3968 - val_loss: 0.3866\n",
            "Epoch 237/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3986 - val_loss: 0.3842\n",
            "Epoch 238/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3978 - val_loss: 0.3820\n",
            "Epoch 239/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3975 - val_loss: 0.3878\n",
            "Epoch 240/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3977 - val_loss: 0.3833\n",
            "Epoch 241/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3980 - val_loss: 0.3903\n",
            "Epoch 242/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.3866\n",
            "Epoch 243/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3957 - val_loss: 0.3862\n",
            "Epoch 244/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3968 - val_loss: 0.3841\n",
            "Epoch 245/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3981 - val_loss: 0.3873\n",
            "Epoch 246/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3983 - val_loss: 0.3872\n",
            "Epoch 247/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3973 - val_loss: 0.3880\n",
            "Epoch 248/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3964 - val_loss: 0.3846\n",
            "Epoch 249/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3977 - val_loss: 0.3944\n",
            "Epoch 250/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3979 - val_loss: 0.3830\n",
            "Epoch 251/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3976 - val_loss: 0.3843\n",
            "Epoch 252/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3953 - val_loss: 0.3845\n",
            "Epoch 253/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3961 - val_loss: 0.3859\n",
            "Epoch 254/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3960 - val_loss: 0.3849\n",
            "Epoch 255/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3966 - val_loss: 0.3847\n",
            "Epoch 256/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3976 - val_loss: 0.3831\n",
            "Epoch 257/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3869\n",
            "Epoch 258/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3966 - val_loss: 0.3842\n",
            "Epoch 259/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3960 - val_loss: 0.3848\n",
            "Epoch 260/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3945 - val_loss: 0.3862\n",
            "Epoch 261/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3985 - val_loss: 0.3843\n",
            "Epoch 262/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3961 - val_loss: 0.3848\n",
            "Epoch 263/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3966 - val_loss: 0.3837\n",
            "Epoch 264/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3960 - val_loss: 0.3849\n",
            "Epoch 265/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3934 - val_loss: 0.3819\n",
            "Epoch 266/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3950 - val_loss: 0.3884\n",
            "Epoch 267/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3950 - val_loss: 0.3850\n",
            "Epoch 268/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3932 - val_loss: 0.3866\n",
            "Epoch 269/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3958 - val_loss: 0.3841\n",
            "Epoch 270/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3955 - val_loss: 0.3829\n",
            "Epoch 271/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3957 - val_loss: 0.3854\n",
            "Epoch 272/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3946 - val_loss: 0.3831\n",
            "Epoch 273/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3936 - val_loss: 0.3901\n",
            "Epoch 274/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3837\n",
            "Epoch 275/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3940 - val_loss: 0.3843\n",
            "Epoch 276/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3939 - val_loss: 0.3883\n",
            "Epoch 277/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3954 - val_loss: 0.3849\n",
            "Epoch 278/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3933 - val_loss: 0.3827\n",
            "Epoch 279/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3936 - val_loss: 0.3852\n",
            "Epoch 280/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3943 - val_loss: 0.3840\n",
            "Epoch 281/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3930 - val_loss: 0.3846\n",
            "Epoch 282/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3933 - val_loss: 0.3858\n",
            "Epoch 283/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3938 - val_loss: 0.3873\n",
            "Epoch 284/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3935 - val_loss: 0.3891\n",
            "Epoch 285/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3954 - val_loss: 0.3846\n",
            "Epoch 286/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3932 - val_loss: 0.3842\n",
            "Epoch 287/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3946 - val_loss: 0.3862\n",
            "Epoch 288/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3831\n",
            "Epoch 289/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3942 - val_loss: 0.3865\n",
            "Epoch 290/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3963 - val_loss: 0.3863\n",
            "Epoch 291/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3948 - val_loss: 0.3841\n",
            "Epoch 292/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3936 - val_loss: 0.3842\n",
            "Epoch 293/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3924 - val_loss: 0.3819\n",
            "Epoch 294/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3931 - val_loss: 0.3881\n",
            "Epoch 295/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3929 - val_loss: 0.3818\n",
            "Epoch 296/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3947 - val_loss: 0.3808\n",
            "Epoch 297/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3933 - val_loss: 0.3884\n",
            "Epoch 298/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3957 - val_loss: 0.3927\n",
            "Epoch 299/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3943 - val_loss: 0.3850\n",
            "Epoch 300/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3934 - val_loss: 0.3817\n",
            "Epoch 301/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3930 - val_loss: 0.3904\n",
            "Epoch 302/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3931 - val_loss: 0.3861\n",
            "Epoch 303/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3933 - val_loss: 0.3839\n",
            "Epoch 304/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3915 - val_loss: 0.3879\n",
            "Epoch 305/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3922 - val_loss: 0.3820\n",
            "Epoch 306/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3913 - val_loss: 0.3880\n",
            "Epoch 307/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.3816\n",
            "Epoch 308/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3919 - val_loss: 0.3806\n",
            "Epoch 309/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3931 - val_loss: 0.3811\n",
            "Epoch 310/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3904 - val_loss: 0.3853\n",
            "Epoch 311/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3918 - val_loss: 0.3864\n",
            "Epoch 312/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3909 - val_loss: 0.3880\n",
            "Epoch 313/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3900\n",
            "Epoch 314/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3912 - val_loss: 0.3816\n",
            "Epoch 315/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3907 - val_loss: 0.3799\n",
            "Epoch 316/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3922 - val_loss: 0.3836\n",
            "Epoch 317/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3902 - val_loss: 0.3793\n",
            "Epoch 318/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3913 - val_loss: 0.3813\n",
            "Epoch 319/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3914 - val_loss: 0.3827\n",
            "Epoch 320/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3919 - val_loss: 0.3807\n",
            "Epoch 321/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3921 - val_loss: 0.3823\n",
            "Epoch 322/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3914 - val_loss: 0.3831\n",
            "Epoch 323/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3919 - val_loss: 0.3786\n",
            "Epoch 324/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3917 - val_loss: 0.3863\n",
            "Epoch 325/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3925 - val_loss: 0.3825\n",
            "Epoch 326/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3919 - val_loss: 0.3827\n",
            "Epoch 327/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3905 - val_loss: 0.3810\n",
            "Epoch 328/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3907 - val_loss: 0.3841\n",
            "Epoch 329/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3945 - val_loss: 0.3803\n",
            "Epoch 330/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3895 - val_loss: 0.3875\n",
            "Epoch 331/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3898 - val_loss: 0.3797\n",
            "Epoch 332/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3885 - val_loss: 0.3854\n",
            "Epoch 333/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3895 - val_loss: 0.3850\n",
            "Epoch 334/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.3813\n",
            "Epoch 335/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3897 - val_loss: 0.3860\n",
            "Epoch 336/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3917 - val_loss: 0.3849\n",
            "Epoch 337/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3903 - val_loss: 0.3833\n",
            "Epoch 338/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3909 - val_loss: 0.3898\n",
            "Epoch 339/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3907 - val_loss: 0.3842\n",
            "Epoch 340/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3894\n",
            "Epoch 341/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3911 - val_loss: 0.3811\n",
            "Epoch 342/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3894 - val_loss: 0.3826\n",
            "Epoch 343/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3892 - val_loss: 0.3799\n",
            "Epoch 344/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3893 - val_loss: 0.3786\n",
            "Epoch 345/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3905 - val_loss: 0.3844\n",
            "Epoch 346/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3895 - val_loss: 0.3830\n",
            "Epoch 347/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3896 - val_loss: 0.3811\n",
            "Epoch 348/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3886 - val_loss: 0.3796\n",
            "Epoch 349/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3897 - val_loss: 0.3874\n",
            "Epoch 350/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3887 - val_loss: 0.3809\n",
            "Epoch 351/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3887 - val_loss: 0.3814\n",
            "Epoch 352/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3893 - val_loss: 0.3787\n",
            "Epoch 353/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3884 - val_loss: 0.3809\n",
            "Epoch 354/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3794\n",
            "Epoch 355/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3897 - val_loss: 0.3882\n",
            "Epoch 356/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3850\n",
            "Epoch 357/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3911 - val_loss: 0.3862\n",
            "Epoch 358/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3835\n",
            "Epoch 359/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3887 - val_loss: 0.3821\n",
            "Epoch 360/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3879 - val_loss: 0.3787\n",
            "Epoch 361/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3912 - val_loss: 0.3838\n",
            "Epoch 362/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3884 - val_loss: 0.3874\n",
            "Epoch 363/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3888 - val_loss: 0.3845\n",
            "Epoch 364/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3860\n",
            "Epoch 365/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3883 - val_loss: 0.3823\n",
            "Epoch 366/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3882 - val_loss: 0.3823\n",
            "Epoch 367/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3861\n",
            "Epoch 368/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3878 - val_loss: 0.3883\n",
            "Epoch 369/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3877 - val_loss: 0.3845\n",
            "Epoch 370/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3885 - val_loss: 0.3832\n",
            "Epoch 371/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3872 - val_loss: 0.3821\n",
            "Epoch 372/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3893 - val_loss: 0.3865\n",
            "Epoch 373/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3890 - val_loss: 0.3880\n",
            "Epoch 374/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3904\n",
            "Epoch 375/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3907 - val_loss: 0.3806\n",
            "Epoch 376/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3877 - val_loss: 0.3850\n",
            "Epoch 377/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3828\n",
            "Epoch 378/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3878 - val_loss: 0.3869\n",
            "Epoch 379/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3896 - val_loss: 0.3860\n",
            "Epoch 380/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3870 - val_loss: 0.3825\n",
            "Epoch 381/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3885\n",
            "Epoch 382/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3880 - val_loss: 0.3846\n",
            "Epoch 383/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3875 - val_loss: 0.3820\n",
            "Epoch 384/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3825\n",
            "Epoch 385/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3860 - val_loss: 0.3848\n",
            "Epoch 386/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3875 - val_loss: 0.3826\n",
            "Epoch 387/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3867 - val_loss: 0.3822\n",
            "Epoch 388/500\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3873 - val_loss: 0.3842\n",
            "Epoch 389/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3868 - val_loss: 0.3806\n",
            "Epoch 390/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3872 - val_loss: 0.3876\n",
            "Epoch 391/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3887 - val_loss: 0.3810\n",
            "Epoch 392/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3860 - val_loss: 0.3806\n",
            "Epoch 393/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3855 - val_loss: 0.3863\n",
            "Epoch 394/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3867 - val_loss: 0.3827\n",
            "Epoch 395/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3865 - val_loss: 0.3864\n",
            "Epoch 396/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3865 - val_loss: 0.3862\n",
            "Epoch 397/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3860\n",
            "Epoch 398/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3853\n",
            "Epoch 399/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.3830\n",
            "Epoch 400/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3876 - val_loss: 0.3850\n",
            "Epoch 401/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3835\n",
            "Epoch 402/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3873 - val_loss: 0.3841\n",
            "Epoch 403/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3844 - val_loss: 0.3829\n",
            "Epoch 404/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3862 - val_loss: 0.3845\n",
            "Epoch 405/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3859 - val_loss: 0.3827\n",
            "Epoch 406/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3857\n",
            "Epoch 407/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3859\n",
            "Epoch 408/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3857 - val_loss: 0.3827\n",
            "Epoch 409/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3861 - val_loss: 0.3861\n",
            "Epoch 410/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3885 - val_loss: 0.3828\n",
            "Epoch 411/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3865 - val_loss: 0.3859\n",
            "Epoch 412/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3865 - val_loss: 0.3817\n",
            "Epoch 413/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3861 - val_loss: 0.3853\n",
            "Epoch 414/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.3813\n",
            "Epoch 415/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.3832\n",
            "Epoch 416/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3864 - val_loss: 0.3816\n",
            "Epoch 417/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.3880\n",
            "Epoch 418/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3873 - val_loss: 0.3858\n",
            "Epoch 419/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3858 - val_loss: 0.3821\n",
            "Epoch 420/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3850 - val_loss: 0.3834\n",
            "Epoch 421/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3839\n",
            "Epoch 422/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3856 - val_loss: 0.3823\n",
            "Epoch 423/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3853\n",
            "Epoch 424/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3877 - val_loss: 0.3830\n",
            "Epoch 425/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3856 - val_loss: 0.3839\n",
            "Epoch 426/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3890\n",
            "Epoch 427/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3853 - val_loss: 0.3879\n",
            "Epoch 428/500\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3843 - val_loss: 0.3823\n",
            "Epoch 429/500\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3852 - val_loss: 0.3857\n",
            "Epoch 430/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3869 - val_loss: 0.3828\n",
            "Epoch 431/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3846 - val_loss: 0.3830\n",
            "Epoch 432/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3850 - val_loss: 0.3842\n",
            "Epoch 433/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3858\n",
            "Epoch 434/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3844\n",
            "Epoch 435/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.3839\n",
            "Epoch 436/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3853 - val_loss: 0.3842\n",
            "Epoch 437/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.3855\n",
            "Epoch 438/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3834 - val_loss: 0.3837\n",
            "Epoch 439/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3881\n",
            "Epoch 440/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3855 - val_loss: 0.3905\n",
            "Epoch 441/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3867 - val_loss: 0.3897\n",
            "Epoch 442/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3855\n",
            "Epoch 443/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3854 - val_loss: 0.3923\n",
            "Epoch 444/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3869 - val_loss: 0.3871\n",
            "Epoch 445/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3854\n",
            "Epoch 446/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3843 - val_loss: 0.3926\n",
            "Epoch 447/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3849 - val_loss: 0.3825\n",
            "Epoch 448/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3851 - val_loss: 0.3828\n",
            "Epoch 449/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3843 - val_loss: 0.3853\n",
            "Epoch 450/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3850 - val_loss: 0.3842\n",
            "Epoch 451/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3850 - val_loss: 0.3834\n",
            "Epoch 452/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3844 - val_loss: 0.3874\n",
            "Epoch 453/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3853 - val_loss: 0.3805\n",
            "Epoch 454/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3845 - val_loss: 0.3858\n",
            "Epoch 455/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.3829\n",
            "Epoch 456/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.3839\n",
            "Epoch 457/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3849 - val_loss: 0.3794\n",
            "Epoch 458/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3839 - val_loss: 0.3794\n",
            "Epoch 459/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3833\n",
            "Epoch 460/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3839 - val_loss: 0.3816\n",
            "Epoch 461/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3829 - val_loss: 0.3826\n",
            "Epoch 462/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3822 - val_loss: 0.3830\n",
            "Epoch 463/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3827 - val_loss: 0.3828\n",
            "Epoch 464/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3844 - val_loss: 0.3868\n",
            "Epoch 465/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.3887\n",
            "Epoch 466/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3833 - val_loss: 0.3856\n",
            "Epoch 467/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3844 - val_loss: 0.3913\n",
            "Epoch 468/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3831 - val_loss: 0.3844\n",
            "Epoch 469/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3846 - val_loss: 0.3846\n",
            "Epoch 470/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3857 - val_loss: 0.3870\n",
            "Epoch 471/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3841 - val_loss: 0.3834\n",
            "Epoch 472/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3827 - val_loss: 0.3881\n",
            "Epoch 473/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3829 - val_loss: 0.3837\n",
            "Epoch 474/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3822 - val_loss: 0.3830\n",
            "Epoch 475/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3851\n",
            "Epoch 476/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3842 - val_loss: 0.3826\n",
            "Epoch 477/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3823 - val_loss: 0.3859\n",
            "Epoch 478/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3833 - val_loss: 0.3865\n",
            "Epoch 479/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3837 - val_loss: 0.3840\n",
            "Epoch 480/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3836 - val_loss: 0.3886\n",
            "Epoch 481/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3840 - val_loss: 0.3876\n",
            "Epoch 482/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3831 - val_loss: 0.3892\n",
            "Epoch 483/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3802\n",
            "Epoch 484/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3845 - val_loss: 0.3861\n",
            "Epoch 485/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3842 - val_loss: 0.3889\n",
            "Epoch 486/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3820 - val_loss: 0.3899\n",
            "Epoch 487/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3818 - val_loss: 0.3860\n",
            "Epoch 488/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3855 - val_loss: 0.3848\n",
            "Epoch 489/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3828 - val_loss: 0.3829\n",
            "Epoch 490/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3837\n",
            "Epoch 491/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3828 - val_loss: 0.3843\n",
            "Epoch 492/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3830 - val_loss: 0.3811\n",
            "Epoch 493/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.3906\n",
            "Epoch 494/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3877\n",
            "Epoch 495/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3836 - val_loss: 0.3804\n",
            "Epoch 496/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3822 - val_loss: 0.3875\n",
            "Epoch 497/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3832 - val_loss: 0.3806\n",
            "Epoch 498/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3835 - val_loss: 0.3871\n",
            "Epoch 499/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3833 - val_loss: 0.3868\n",
            "Epoch 500/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3829 - val_loss: 0.3860\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3860\n",
            "8/8 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fce863eb100>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=500;    # training epochs\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!! softmax only based the euclidean distance\n",
        "pred = Dense(3, activation='softmax')(dist)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "uuVsht8xCISl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw3rR3HbKElf"
      },
      "source": [
        "##2.2 Without Temperature in Softmax: Softmax based the euclidean distance and the output of Siamese Neural Network##\n",
        "\n",
        "**Result**  \n",
        "Around Training_loss = 0.38, validation_loss = 0.40"
      ],
      "id": "Xw3rR3HbKElf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mJDUSNqnmVMa",
        "outputId": "7a096751-07c4-4f9f-baf3-49d2b849e8be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "70/70 [==============================] - 9s 14ms/step - loss: 1.0322 - val_loss: 0.9992\n",
            "Epoch 2/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.9072 - val_loss: 0.7348\n",
            "Epoch 3/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6831 - val_loss: 0.6107\n",
            "Epoch 4/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6404 - val_loss: 0.6026\n",
            "Epoch 5/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6309 - val_loss: 0.5979\n",
            "Epoch 6/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6244 - val_loss: 0.5931\n",
            "Epoch 7/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6212 - val_loss: 0.5877\n",
            "Epoch 8/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6134 - val_loss: 0.5828\n",
            "Epoch 9/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6035 - val_loss: 0.5774\n",
            "Epoch 10/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5953 - val_loss: 0.5724\n",
            "Epoch 11/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5865 - val_loss: 0.5651\n",
            "Epoch 12/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5817 - val_loss: 0.5605\n",
            "Epoch 13/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5770 - val_loss: 0.5562\n",
            "Epoch 14/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5740 - val_loss: 0.5557\n",
            "Epoch 15/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5697 - val_loss: 0.5530\n",
            "Epoch 16/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5680 - val_loss: 0.5500\n",
            "Epoch 17/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5667 - val_loss: 0.5468\n",
            "Epoch 18/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5634 - val_loss: 0.5462\n",
            "Epoch 19/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5612 - val_loss: 0.5444\n",
            "Epoch 20/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5594 - val_loss: 0.5412\n",
            "Epoch 21/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5580 - val_loss: 0.5424\n",
            "Epoch 22/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5562 - val_loss: 0.5377\n",
            "Epoch 23/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5541 - val_loss: 0.5376\n",
            "Epoch 24/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5519 - val_loss: 0.5355\n",
            "Epoch 25/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5502 - val_loss: 0.5340\n",
            "Epoch 26/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5489 - val_loss: 0.5346\n",
            "Epoch 27/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5478 - val_loss: 0.5321\n",
            "Epoch 28/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5448 - val_loss: 0.5287\n",
            "Epoch 29/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5454 - val_loss: 0.5267\n",
            "Epoch 30/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5418 - val_loss: 0.5266\n",
            "Epoch 31/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5405 - val_loss: 0.5254\n",
            "Epoch 32/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5397 - val_loss: 0.5233\n",
            "Epoch 33/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5381 - val_loss: 0.5232\n",
            "Epoch 34/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5354 - val_loss: 0.5211\n",
            "Epoch 35/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5344 - val_loss: 0.5206\n",
            "Epoch 36/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5335 - val_loss: 0.5185\n",
            "Epoch 37/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.5204\n",
            "Epoch 38/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5302 - val_loss: 0.5168\n",
            "Epoch 39/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5298 - val_loss: 0.5124\n",
            "Epoch 40/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5278 - val_loss: 0.5146\n",
            "Epoch 41/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5264 - val_loss: 0.5125\n",
            "Epoch 42/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5256 - val_loss: 0.5114\n",
            "Epoch 43/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5235 - val_loss: 0.5088\n",
            "Epoch 44/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5240 - val_loss: 0.5085\n",
            "Epoch 45/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5213 - val_loss: 0.5070\n",
            "Epoch 46/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5199 - val_loss: 0.5053\n",
            "Epoch 47/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5183 - val_loss: 0.5044\n",
            "Epoch 48/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5176 - val_loss: 0.5041\n",
            "Epoch 49/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5162 - val_loss: 0.5010\n",
            "Epoch 50/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5154 - val_loss: 0.4998\n",
            "Epoch 51/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5144 - val_loss: 0.5000\n",
            "Epoch 52/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5127 - val_loss: 0.4979\n",
            "Epoch 53/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5121 - val_loss: 0.4964\n",
            "Epoch 54/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5099 - val_loss: 0.4987\n",
            "Epoch 55/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5094 - val_loss: 0.4967\n",
            "Epoch 56/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5084 - val_loss: 0.4962\n",
            "Epoch 57/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5070 - val_loss: 0.4956\n",
            "Epoch 58/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5064 - val_loss: 0.4944\n",
            "Epoch 59/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5056 - val_loss: 0.4918\n",
            "Epoch 60/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5041 - val_loss: 0.4918\n",
            "Epoch 61/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5034 - val_loss: 0.4907\n",
            "Epoch 62/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5017 - val_loss: 0.4897\n",
            "Epoch 63/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5011 - val_loss: 0.4867\n",
            "Epoch 64/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5005 - val_loss: 0.4874\n",
            "Epoch 65/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4997 - val_loss: 0.4873\n",
            "Epoch 66/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4981 - val_loss: 0.4872\n",
            "Epoch 67/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4980 - val_loss: 0.4851\n",
            "Epoch 68/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4963 - val_loss: 0.4831\n",
            "Epoch 69/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4953 - val_loss: 0.4811\n",
            "Epoch 70/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4954 - val_loss: 0.4821\n",
            "Epoch 71/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4956 - val_loss: 0.4818\n",
            "Epoch 72/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4929 - val_loss: 0.4799\n",
            "Epoch 73/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4925 - val_loss: 0.4799\n",
            "Epoch 74/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4911 - val_loss: 0.4787\n",
            "Epoch 75/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4907 - val_loss: 0.4769\n",
            "Epoch 76/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4895 - val_loss: 0.4768\n",
            "Epoch 77/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4881 - val_loss: 0.4768\n",
            "Epoch 78/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4879 - val_loss: 0.4763\n",
            "Epoch 79/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4869 - val_loss: 0.4741\n",
            "Epoch 80/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4853 - val_loss: 0.4736\n",
            "Epoch 81/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4850 - val_loss: 0.4737\n",
            "Epoch 82/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4843 - val_loss: 0.4730\n",
            "Epoch 83/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4837 - val_loss: 0.4708\n",
            "Epoch 84/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4828 - val_loss: 0.4693\n",
            "Epoch 85/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4817 - val_loss: 0.4703\n",
            "Epoch 86/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4810 - val_loss: 0.4692\n",
            "Epoch 87/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4796 - val_loss: 0.4686\n",
            "Epoch 88/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4795 - val_loss: 0.4669\n",
            "Epoch 89/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4784 - val_loss: 0.4675\n",
            "Epoch 90/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4776 - val_loss: 0.4650\n",
            "Epoch 91/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4773 - val_loss: 0.4644\n",
            "Epoch 92/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4764 - val_loss: 0.4652\n",
            "Epoch 93/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4755 - val_loss: 0.4643\n",
            "Epoch 94/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4749 - val_loss: 0.4655\n",
            "Epoch 95/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4747 - val_loss: 0.4633\n",
            "Epoch 96/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4740 - val_loss: 0.4619\n",
            "Epoch 97/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4731 - val_loss: 0.4608\n",
            "Epoch 98/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4721 - val_loss: 0.4622\n",
            "Epoch 99/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4716 - val_loss: 0.4594\n",
            "Epoch 100/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4706 - val_loss: 0.4605\n",
            "Epoch 101/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4702 - val_loss: 0.4577\n",
            "Epoch 102/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4694 - val_loss: 0.4582\n",
            "Epoch 103/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4695 - val_loss: 0.4581\n",
            "Epoch 104/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4683 - val_loss: 0.4569\n",
            "Epoch 105/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4677 - val_loss: 0.4578\n",
            "Epoch 106/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4666 - val_loss: 0.4580\n",
            "Epoch 107/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4670 - val_loss: 0.4542\n",
            "Epoch 108/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4653 - val_loss: 0.4548\n",
            "Epoch 109/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4654 - val_loss: 0.4538\n",
            "Epoch 110/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4646 - val_loss: 0.4547\n",
            "Epoch 111/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4643 - val_loss: 0.4552\n",
            "Epoch 112/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4638 - val_loss: 0.4531\n",
            "Epoch 113/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4625 - val_loss: 0.4507\n",
            "Epoch 114/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4619 - val_loss: 0.4522\n",
            "Epoch 115/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4615 - val_loss: 0.4510\n",
            "Epoch 116/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4613 - val_loss: 0.4493\n",
            "Epoch 117/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4603 - val_loss: 0.4495\n",
            "Epoch 118/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4599 - val_loss: 0.4491\n",
            "Epoch 119/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4598 - val_loss: 0.4476\n",
            "Epoch 120/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4588 - val_loss: 0.4470\n",
            "Epoch 121/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4587 - val_loss: 0.4488\n",
            "Epoch 122/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4572 - val_loss: 0.4468\n",
            "Epoch 123/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4569 - val_loss: 0.4453\n",
            "Epoch 124/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4566 - val_loss: 0.4459\n",
            "Epoch 125/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4558 - val_loss: 0.4447\n",
            "Epoch 126/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4557 - val_loss: 0.4440\n",
            "Epoch 127/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4540 - val_loss: 0.4443\n",
            "Epoch 128/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4548 - val_loss: 0.4448\n",
            "Epoch 129/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4538 - val_loss: 0.4454\n",
            "Epoch 130/1000\n",
            "70/70 [==============================] - 2s 27ms/step - loss: 0.4539 - val_loss: 0.4439\n",
            "Epoch 131/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4530 - val_loss: 0.4422\n",
            "Epoch 132/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4526 - val_loss: 0.4416\n",
            "Epoch 133/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4520 - val_loss: 0.4400\n",
            "Epoch 134/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4508 - val_loss: 0.4439\n",
            "Epoch 135/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4514 - val_loss: 0.4395\n",
            "Epoch 136/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4501 - val_loss: 0.4426\n",
            "Epoch 137/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4499 - val_loss: 0.4416\n",
            "Epoch 138/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4495 - val_loss: 0.4397\n",
            "Epoch 139/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4492 - val_loss: 0.4383\n",
            "Epoch 140/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4482 - val_loss: 0.4390\n",
            "Epoch 141/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4478 - val_loss: 0.4393\n",
            "Epoch 142/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4480 - val_loss: 0.4375\n",
            "Epoch 143/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4472 - val_loss: 0.4381\n",
            "Epoch 144/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4469 - val_loss: 0.4351\n",
            "Epoch 145/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4468 - val_loss: 0.4356\n",
            "Epoch 146/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4452 - val_loss: 0.4367\n",
            "Epoch 147/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4455 - val_loss: 0.4361\n",
            "Epoch 148/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4446 - val_loss: 0.4344\n",
            "Epoch 149/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.4443 - val_loss: 0.4336\n",
            "Epoch 150/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4448 - val_loss: 0.4344\n",
            "Epoch 151/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4445 - val_loss: 0.4314\n",
            "Epoch 152/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4435 - val_loss: 0.4335\n",
            "Epoch 153/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4430 - val_loss: 0.4310\n",
            "Epoch 154/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4423 - val_loss: 0.4321\n",
            "Epoch 155/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4413 - val_loss: 0.4336\n",
            "Epoch 156/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4415 - val_loss: 0.4314\n",
            "Epoch 157/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4413 - val_loss: 0.4307\n",
            "Epoch 158/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4408 - val_loss: 0.4310\n",
            "Epoch 159/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4398 - val_loss: 0.4301\n",
            "Epoch 160/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4403 - val_loss: 0.4287\n",
            "Epoch 161/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4403 - val_loss: 0.4302\n",
            "Epoch 162/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4399 - val_loss: 0.4322\n",
            "Epoch 163/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4391 - val_loss: 0.4279\n",
            "Epoch 164/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4381 - val_loss: 0.4290\n",
            "Epoch 165/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4380 - val_loss: 0.4267\n",
            "Epoch 166/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4374 - val_loss: 0.4283\n",
            "Epoch 167/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4368 - val_loss: 0.4272\n",
            "Epoch 168/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4363 - val_loss: 0.4274\n",
            "Epoch 169/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4361 - val_loss: 0.4256\n",
            "Epoch 170/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4367 - val_loss: 0.4249\n",
            "Epoch 171/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4363 - val_loss: 0.4250\n",
            "Epoch 172/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4355 - val_loss: 0.4254\n",
            "Epoch 173/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4348 - val_loss: 0.4253\n",
            "Epoch 174/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4346 - val_loss: 0.4250\n",
            "Epoch 175/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4341 - val_loss: 0.4242\n",
            "Epoch 176/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4336 - val_loss: 0.4247\n",
            "Epoch 177/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4336 - val_loss: 0.4230\n",
            "Epoch 178/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4338 - val_loss: 0.4232\n",
            "Epoch 179/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4329 - val_loss: 0.4230\n",
            "Epoch 180/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4323 - val_loss: 0.4222\n",
            "Epoch 181/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4321 - val_loss: 0.4228\n",
            "Epoch 182/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4322 - val_loss: 0.4217\n",
            "Epoch 183/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4312 - val_loss: 0.4221\n",
            "Epoch 184/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4314 - val_loss: 0.4203\n",
            "Epoch 185/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4317 - val_loss: 0.4201\n",
            "Epoch 186/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4305 - val_loss: 0.4200\n",
            "Epoch 187/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4301 - val_loss: 0.4198\n",
            "Epoch 188/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4293 - val_loss: 0.4210\n",
            "Epoch 189/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4291 - val_loss: 0.4204\n",
            "Epoch 190/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4298 - val_loss: 0.4200\n",
            "Epoch 191/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4284 - val_loss: 0.4198\n",
            "Epoch 192/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4283 - val_loss: 0.4200\n",
            "Epoch 193/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4282 - val_loss: 0.4190\n",
            "Epoch 194/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4276 - val_loss: 0.4171\n",
            "Epoch 195/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4277 - val_loss: 0.4194\n",
            "Epoch 196/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4279 - val_loss: 0.4172\n",
            "Epoch 197/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4269 - val_loss: 0.4157\n",
            "Epoch 198/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4262 - val_loss: 0.4181\n",
            "Epoch 199/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4262 - val_loss: 0.4157\n",
            "Epoch 200/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4261 - val_loss: 0.4159\n",
            "Epoch 201/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4259 - val_loss: 0.4166\n",
            "Epoch 202/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4255 - val_loss: 0.4167\n",
            "Epoch 203/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4249 - val_loss: 0.4163\n",
            "Epoch 204/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4255 - val_loss: 0.4162\n",
            "Epoch 205/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4247 - val_loss: 0.4161\n",
            "Epoch 206/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4245 - val_loss: 0.4156\n",
            "Epoch 207/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4245 - val_loss: 0.4156\n",
            "Epoch 208/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4237 - val_loss: 0.4145\n",
            "Epoch 209/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4236 - val_loss: 0.4146\n",
            "Epoch 210/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4232 - val_loss: 0.4142\n",
            "Epoch 211/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4233 - val_loss: 0.4137\n",
            "Epoch 212/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4229 - val_loss: 0.4144\n",
            "Epoch 213/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4229 - val_loss: 0.4125\n",
            "Epoch 214/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4219 - val_loss: 0.4139\n",
            "Epoch 215/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4214 - val_loss: 0.4138\n",
            "Epoch 216/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4217 - val_loss: 0.4137\n",
            "Epoch 217/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4216 - val_loss: 0.4143\n",
            "Epoch 218/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4211 - val_loss: 0.4124\n",
            "Epoch 219/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4211 - val_loss: 0.4118\n",
            "Epoch 220/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4203 - val_loss: 0.4120\n",
            "Epoch 221/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4203 - val_loss: 0.4109\n",
            "Epoch 222/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4202 - val_loss: 0.4116\n",
            "Epoch 223/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4195 - val_loss: 0.4128\n",
            "Epoch 224/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4196 - val_loss: 0.4106\n",
            "Epoch 225/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4193 - val_loss: 0.4129\n",
            "Epoch 226/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4197 - val_loss: 0.4132\n",
            "Epoch 227/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4200 - val_loss: 0.4105\n",
            "Epoch 228/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4183 - val_loss: 0.4114\n",
            "Epoch 229/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4188 - val_loss: 0.4092\n",
            "Epoch 230/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4183 - val_loss: 0.4099\n",
            "Epoch 231/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4186 - val_loss: 0.4108\n",
            "Epoch 232/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4183 - val_loss: 0.4092\n",
            "Epoch 233/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4183 - val_loss: 0.4075\n",
            "Epoch 234/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4186 - val_loss: 0.4073\n",
            "Epoch 235/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4180 - val_loss: 0.4077\n",
            "Epoch 236/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4167 - val_loss: 0.4102\n",
            "Epoch 237/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4179 - val_loss: 0.4093\n",
            "Epoch 238/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4165 - val_loss: 0.4104\n",
            "Epoch 239/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4164 - val_loss: 0.4065\n",
            "Epoch 240/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4160 - val_loss: 0.4078\n",
            "Epoch 241/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4166 - val_loss: 0.4067\n",
            "Epoch 242/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4164 - val_loss: 0.4064\n",
            "Epoch 243/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4161 - val_loss: 0.4079\n",
            "Epoch 244/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4161 - val_loss: 0.4086\n",
            "Epoch 245/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4159 - val_loss: 0.4065\n",
            "Epoch 246/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4156 - val_loss: 0.4063\n",
            "Epoch 247/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4149 - val_loss: 0.4061\n",
            "Epoch 248/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4147 - val_loss: 0.4058\n",
            "Epoch 249/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4144 - val_loss: 0.4050\n",
            "Epoch 250/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4149 - val_loss: 0.4053\n",
            "Epoch 251/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4140 - val_loss: 0.4049\n",
            "Epoch 252/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4144 - val_loss: 0.4068\n",
            "Epoch 253/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4141 - val_loss: 0.4054\n",
            "Epoch 254/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4132 - val_loss: 0.4057\n",
            "Epoch 255/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4133 - val_loss: 0.4051\n",
            "Epoch 256/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4139 - val_loss: 0.4051\n",
            "Epoch 257/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4131 - val_loss: 0.4057\n",
            "Epoch 258/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4129 - val_loss: 0.4029\n",
            "Epoch 259/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4127 - val_loss: 0.4042\n",
            "Epoch 260/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4128 - val_loss: 0.4045\n",
            "Epoch 261/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4122 - val_loss: 0.4049\n",
            "Epoch 262/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4128 - val_loss: 0.4048\n",
            "Epoch 263/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4124 - val_loss: 0.4044\n",
            "Epoch 264/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4122 - val_loss: 0.4033\n",
            "Epoch 265/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4112 - val_loss: 0.4026\n",
            "Epoch 266/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4119 - val_loss: 0.4048\n",
            "Epoch 267/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4114 - val_loss: 0.4026\n",
            "Epoch 268/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4111 - val_loss: 0.4036\n",
            "Epoch 269/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4109 - val_loss: 0.4023\n",
            "Epoch 270/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4113 - val_loss: 0.4007\n",
            "Epoch 271/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4107 - val_loss: 0.4023\n",
            "Epoch 272/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4108 - val_loss: 0.4014\n",
            "Epoch 273/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4099 - val_loss: 0.4024\n",
            "Epoch 274/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4104 - val_loss: 0.4010\n",
            "Epoch 275/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4104 - val_loss: 0.4018\n",
            "Epoch 276/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4103 - val_loss: 0.3998\n",
            "Epoch 277/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4105 - val_loss: 0.4010\n",
            "Epoch 278/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4097 - val_loss: 0.4003\n",
            "Epoch 279/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4098 - val_loss: 0.4015\n",
            "Epoch 280/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4096 - val_loss: 0.4020\n",
            "Epoch 281/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4091 - val_loss: 0.4007\n",
            "Epoch 282/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4093 - val_loss: 0.4014\n",
            "Epoch 283/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4086 - val_loss: 0.4005\n",
            "Epoch 284/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4088 - val_loss: 0.4009\n",
            "Epoch 285/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4084 - val_loss: 0.4000\n",
            "Epoch 286/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4084 - val_loss: 0.3999\n",
            "Epoch 287/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4087 - val_loss: 0.4005\n",
            "Epoch 288/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4083 - val_loss: 0.4018\n",
            "Epoch 289/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4084 - val_loss: 0.3993\n",
            "Epoch 290/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4081 - val_loss: 0.3999\n",
            "Epoch 291/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4078 - val_loss: 0.4002\n",
            "Epoch 292/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4081 - val_loss: 0.3993\n",
            "Epoch 293/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4075 - val_loss: 0.4010\n",
            "Epoch 294/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4077 - val_loss: 0.3996\n",
            "Epoch 295/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4074 - val_loss: 0.4015\n",
            "Epoch 296/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4074 - val_loss: 0.3988\n",
            "Epoch 297/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4073 - val_loss: 0.3988\n",
            "Epoch 298/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4071 - val_loss: 0.3999\n",
            "Epoch 299/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4068 - val_loss: 0.4006\n",
            "Epoch 300/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4073 - val_loss: 0.4004\n",
            "Epoch 301/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4065 - val_loss: 0.4022\n",
            "Epoch 302/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4069 - val_loss: 0.4001\n",
            "Epoch 303/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4067 - val_loss: 0.3993\n",
            "Epoch 304/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4061 - val_loss: 0.3974\n",
            "Epoch 305/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4070 - val_loss: 0.3968\n",
            "Epoch 306/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4059 - val_loss: 0.3972\n",
            "Epoch 307/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4061 - val_loss: 0.3966\n",
            "Epoch 308/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4056 - val_loss: 0.3980\n",
            "Epoch 309/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4056 - val_loss: 0.3982\n",
            "Epoch 310/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4056 - val_loss: 0.3975\n",
            "Epoch 311/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4055 - val_loss: 0.3989\n",
            "Epoch 312/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4056 - val_loss: 0.3982\n",
            "Epoch 313/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4050 - val_loss: 0.3981\n",
            "Epoch 314/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4048 - val_loss: 0.3967\n",
            "Epoch 315/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4051 - val_loss: 0.4000\n",
            "Epoch 316/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4048 - val_loss: 0.3958\n",
            "Epoch 317/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4058 - val_loss: 0.3993\n",
            "Epoch 318/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.4049 - val_loss: 0.3951\n",
            "Epoch 319/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4040 - val_loss: 0.3967\n",
            "Epoch 320/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4047 - val_loss: 0.3976\n",
            "Epoch 321/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4041 - val_loss: 0.3958\n",
            "Epoch 322/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4044 - val_loss: 0.3958\n",
            "Epoch 323/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4041 - val_loss: 0.3960\n",
            "Epoch 324/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4039 - val_loss: 0.3970\n",
            "Epoch 325/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4037 - val_loss: 0.3974\n",
            "Epoch 326/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4040 - val_loss: 0.3952\n",
            "Epoch 327/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4040 - val_loss: 0.3960\n",
            "Epoch 328/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4036 - val_loss: 0.3959\n",
            "Epoch 329/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4035 - val_loss: 0.3968\n",
            "Epoch 330/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4032 - val_loss: 0.3985\n",
            "Epoch 331/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4039 - val_loss: 0.3958\n",
            "Epoch 332/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4032 - val_loss: 0.3963\n",
            "Epoch 333/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4033 - val_loss: 0.3965\n",
            "Epoch 334/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4029 - val_loss: 0.3973\n",
            "Epoch 335/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4026 - val_loss: 0.3964\n",
            "Epoch 336/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4023 - val_loss: 0.3975\n",
            "Epoch 337/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4025 - val_loss: 0.3996\n",
            "Epoch 338/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4030 - val_loss: 0.3961\n",
            "Epoch 339/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4030 - val_loss: 0.3950\n",
            "Epoch 340/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4025 - val_loss: 0.3952\n",
            "Epoch 341/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4024 - val_loss: 0.3974\n",
            "Epoch 342/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4024 - val_loss: 0.3959\n",
            "Epoch 343/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4023 - val_loss: 0.3947\n",
            "Epoch 344/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4020 - val_loss: 0.3951\n",
            "Epoch 345/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4021 - val_loss: 0.3942\n",
            "Epoch 346/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4014 - val_loss: 0.3952\n",
            "Epoch 347/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4020 - val_loss: 0.3965\n",
            "Epoch 348/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4015 - val_loss: 0.3960\n",
            "Epoch 349/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4012 - val_loss: 0.3964\n",
            "Epoch 350/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4013 - val_loss: 0.3958\n",
            "Epoch 351/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4020 - val_loss: 0.3937\n",
            "Epoch 352/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3953\n",
            "Epoch 353/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4012 - val_loss: 0.3958\n",
            "Epoch 354/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4011 - val_loss: 0.3938\n",
            "Epoch 355/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3945\n",
            "Epoch 356/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4012 - val_loss: 0.3930\n",
            "Epoch 357/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4011 - val_loss: 0.3943\n",
            "Epoch 358/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4009 - val_loss: 0.3940\n",
            "Epoch 359/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4006 - val_loss: 0.3948\n",
            "Epoch 360/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4004 - val_loss: 0.3942\n",
            "Epoch 361/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3998 - val_loss: 0.3959\n",
            "Epoch 362/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4007 - val_loss: 0.3952\n",
            "Epoch 363/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3964\n",
            "Epoch 364/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4009 - val_loss: 0.3935\n",
            "Epoch 365/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4012 - val_loss: 0.3959\n",
            "Epoch 366/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4002 - val_loss: 0.3934\n",
            "Epoch 367/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3997 - val_loss: 0.3939\n",
            "Epoch 368/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3996 - val_loss: 0.3925\n",
            "Epoch 369/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4007 - val_loss: 0.3935\n",
            "Epoch 370/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4000 - val_loss: 0.3930\n",
            "Epoch 371/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3998 - val_loss: 0.3920\n",
            "Epoch 372/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4004 - val_loss: 0.3943\n",
            "Epoch 373/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4005 - val_loss: 0.3915\n",
            "Epoch 374/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3997 - val_loss: 0.3948\n",
            "Epoch 375/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3998 - val_loss: 0.3939\n",
            "Epoch 376/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3993 - val_loss: 0.3931\n",
            "Epoch 377/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3990 - val_loss: 0.3952\n",
            "Epoch 378/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3994 - val_loss: 0.3939\n",
            "Epoch 379/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3992 - val_loss: 0.3930\n",
            "Epoch 380/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3999 - val_loss: 0.3945\n",
            "Epoch 381/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3987 - val_loss: 0.3931\n",
            "Epoch 382/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3983 - val_loss: 0.3925\n",
            "Epoch 383/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3996 - val_loss: 0.3918\n",
            "Epoch 384/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3987 - val_loss: 0.3938\n",
            "Epoch 385/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3995 - val_loss: 0.3940\n",
            "Epoch 386/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3986 - val_loss: 0.3907\n",
            "Epoch 387/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3994 - val_loss: 0.3912\n",
            "Epoch 388/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3992 - val_loss: 0.3909\n",
            "Epoch 389/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3992 - val_loss: 0.3939\n",
            "Epoch 390/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3985 - val_loss: 0.3918\n",
            "Epoch 391/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3981 - val_loss: 0.3924\n",
            "Epoch 392/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3981 - val_loss: 0.3925\n",
            "Epoch 393/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3981 - val_loss: 0.3923\n",
            "Epoch 394/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3974 - val_loss: 0.3916\n",
            "Epoch 395/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3978 - val_loss: 0.3925\n",
            "Epoch 396/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3980 - val_loss: 0.3942\n",
            "Epoch 397/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3984 - val_loss: 0.3941\n",
            "Epoch 398/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3979 - val_loss: 0.3922\n",
            "Epoch 399/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3977 - val_loss: 0.3926\n",
            "Epoch 400/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3972 - val_loss: 0.3909\n",
            "Epoch 401/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3980 - val_loss: 0.3928\n",
            "Epoch 402/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3976 - val_loss: 0.3913\n",
            "Epoch 403/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3978 - val_loss: 0.3920\n",
            "Epoch 404/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3975 - val_loss: 0.3904\n",
            "Epoch 405/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3974 - val_loss: 0.3928\n",
            "Epoch 406/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3975 - val_loss: 0.3920\n",
            "Epoch 407/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.3926\n",
            "Epoch 408/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3975 - val_loss: 0.3918\n",
            "Epoch 409/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.3913\n",
            "Epoch 410/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3975 - val_loss: 0.3921\n",
            "Epoch 411/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3975 - val_loss: 0.3923\n",
            "Epoch 412/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3966 - val_loss: 0.3944\n",
            "Epoch 413/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3970 - val_loss: 0.3948\n",
            "Epoch 414/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3969 - val_loss: 0.3918\n",
            "Epoch 415/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3964 - val_loss: 0.3923\n",
            "Epoch 416/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3967 - val_loss: 0.3913\n",
            "Epoch 417/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3968 - val_loss: 0.3930\n",
            "Epoch 418/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3933\n",
            "Epoch 419/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3970 - val_loss: 0.3917\n",
            "Epoch 420/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3964 - val_loss: 0.3935\n",
            "Epoch 421/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3965 - val_loss: 0.3938\n",
            "Epoch 422/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3966 - val_loss: 0.3928\n",
            "Epoch 423/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3963 - val_loss: 0.3939\n",
            "Epoch 424/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3961 - val_loss: 0.3900\n",
            "Epoch 425/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3912\n",
            "Epoch 426/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3915\n",
            "Epoch 427/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3964 - val_loss: 0.3900\n",
            "Epoch 428/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3962 - val_loss: 0.3915\n",
            "Epoch 429/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3959 - val_loss: 0.3925\n",
            "Epoch 430/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3958 - val_loss: 0.3927\n",
            "Epoch 431/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3956 - val_loss: 0.3910\n",
            "Epoch 432/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3957 - val_loss: 0.3937\n",
            "Epoch 433/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3957 - val_loss: 0.3922\n",
            "Epoch 434/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3959 - val_loss: 0.3901\n",
            "Epoch 435/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3953 - val_loss: 0.3937\n",
            "Epoch 436/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3954 - val_loss: 0.3904\n",
            "Epoch 437/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3953 - val_loss: 0.3898\n",
            "Epoch 438/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3958 - val_loss: 0.3931\n",
            "Epoch 439/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3924\n",
            "Epoch 440/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3953 - val_loss: 0.3932\n",
            "Epoch 441/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3957 - val_loss: 0.3915\n",
            "Epoch 442/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3952 - val_loss: 0.3904\n",
            "Epoch 443/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3955 - val_loss: 0.3932\n",
            "Epoch 444/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3958 - val_loss: 0.3941\n",
            "Epoch 445/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3949 - val_loss: 0.3916\n",
            "Epoch 446/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3955 - val_loss: 0.3937\n",
            "Epoch 447/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3950 - val_loss: 0.3908\n",
            "Epoch 448/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3946 - val_loss: 0.3911\n",
            "Epoch 449/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3944 - val_loss: 0.3910\n",
            "Epoch 450/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3949 - val_loss: 0.3907\n",
            "Epoch 451/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3950 - val_loss: 0.3925\n",
            "Epoch 452/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3946 - val_loss: 0.3927\n",
            "Epoch 453/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3949 - val_loss: 0.3918\n",
            "Epoch 454/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3951 - val_loss: 0.3923\n",
            "Epoch 455/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3945 - val_loss: 0.3922\n",
            "Epoch 456/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3950 - val_loss: 0.3917\n",
            "Epoch 457/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3947 - val_loss: 0.3910\n",
            "Epoch 458/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3942 - val_loss: 0.3929\n",
            "Epoch 459/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3942 - val_loss: 0.3910\n",
            "Epoch 460/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3952 - val_loss: 0.3931\n",
            "Epoch 461/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3946 - val_loss: 0.3928\n",
            "Epoch 462/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3905\n",
            "Epoch 463/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3936 - val_loss: 0.3962\n",
            "Epoch 464/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3940 - val_loss: 0.3906\n",
            "Epoch 465/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3926\n",
            "Epoch 466/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3937 - val_loss: 0.3903\n",
            "Epoch 467/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3940 - val_loss: 0.3906\n",
            "Epoch 468/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3947 - val_loss: 0.3931\n",
            "Epoch 469/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3936 - val_loss: 0.3900\n",
            "Epoch 470/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3938 - val_loss: 0.3917\n",
            "Epoch 471/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3941 - val_loss: 0.3936\n",
            "Epoch 472/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3937 - val_loss: 0.3933\n",
            "Epoch 473/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3935 - val_loss: 0.3910\n",
            "Epoch 474/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3942 - val_loss: 0.3917\n",
            "Epoch 475/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3946 - val_loss: 0.3912\n",
            "Epoch 476/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3935 - val_loss: 0.3922\n",
            "Epoch 477/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3938 - val_loss: 0.3933\n",
            "Epoch 478/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3923\n",
            "Epoch 479/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3934 - val_loss: 0.3948\n",
            "Epoch 480/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3933 - val_loss: 0.3912\n",
            "Epoch 481/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3936 - val_loss: 0.3907\n",
            "Epoch 482/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3936 - val_loss: 0.3905\n",
            "Epoch 483/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3929 - val_loss: 0.3922\n",
            "Epoch 484/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3931 - val_loss: 0.3924\n",
            "Epoch 485/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3935 - val_loss: 0.3914\n",
            "Epoch 486/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3931 - val_loss: 0.3918\n",
            "Epoch 487/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3933 - val_loss: 0.3906\n",
            "Epoch 488/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3937 - val_loss: 0.3940\n",
            "Epoch 489/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3938 - val_loss: 0.3943\n",
            "Epoch 490/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3927 - val_loss: 0.3910\n",
            "Epoch 491/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3929 - val_loss: 0.3898\n",
            "Epoch 492/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3928 - val_loss: 0.3903\n",
            "Epoch 493/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3936 - val_loss: 0.3930\n",
            "Epoch 494/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3927 - val_loss: 0.3916\n",
            "Epoch 495/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3929 - val_loss: 0.3917\n",
            "Epoch 496/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3930 - val_loss: 0.3904\n",
            "Epoch 497/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3926 - val_loss: 0.3913\n",
            "Epoch 498/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3945\n",
            "Epoch 499/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3925 - val_loss: 0.3919\n",
            "Epoch 500/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3928 - val_loss: 0.3898\n",
            "Epoch 501/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3929 - val_loss: 0.3925\n",
            "Epoch 502/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3924 - val_loss: 0.3940\n",
            "Epoch 503/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3925 - val_loss: 0.3938\n",
            "Epoch 504/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3926 - val_loss: 0.3912\n",
            "Epoch 505/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3926\n",
            "Epoch 506/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3920 - val_loss: 0.3942\n",
            "Epoch 507/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3925 - val_loss: 0.3925\n",
            "Epoch 508/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3923 - val_loss: 0.3958\n",
            "Epoch 509/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3925 - val_loss: 0.3944\n",
            "Epoch 510/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3921 - val_loss: 0.3944\n",
            "Epoch 511/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3922 - val_loss: 0.3907\n",
            "Epoch 512/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3927\n",
            "Epoch 513/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3922 - val_loss: 0.3944\n",
            "Epoch 514/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3918 - val_loss: 0.3921\n",
            "Epoch 515/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3918 - val_loss: 0.3927\n",
            "Epoch 516/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3921 - val_loss: 0.3927\n",
            "Epoch 517/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3916 - val_loss: 0.3909\n",
            "Epoch 518/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3918 - val_loss: 0.3908\n",
            "Epoch 519/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3915 - val_loss: 0.3934\n",
            "Epoch 520/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3913 - val_loss: 0.3916\n",
            "Epoch 521/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3920 - val_loss: 0.3907\n",
            "Epoch 522/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3919 - val_loss: 0.3935\n",
            "Epoch 523/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3915 - val_loss: 0.3921\n",
            "Epoch 524/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3921 - val_loss: 0.3920\n",
            "Epoch 525/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3924 - val_loss: 0.3929\n",
            "Epoch 526/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3911 - val_loss: 0.3914\n",
            "Epoch 527/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3917 - val_loss: 0.3918\n",
            "Epoch 528/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3915 - val_loss: 0.3942\n",
            "Epoch 529/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3918 - val_loss: 0.3934\n",
            "Epoch 530/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3920 - val_loss: 0.3918\n",
            "Epoch 531/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3914 - val_loss: 0.3935\n",
            "Epoch 532/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3915 - val_loss: 0.3931\n",
            "Epoch 533/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3914 - val_loss: 0.3910\n",
            "Epoch 534/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3909 - val_loss: 0.3911\n",
            "Epoch 535/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3911 - val_loss: 0.3937\n",
            "Epoch 536/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3908 - val_loss: 0.3940\n",
            "Epoch 537/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3908 - val_loss: 0.3901\n",
            "Epoch 538/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3909 - val_loss: 0.3944\n",
            "Epoch 539/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3909 - val_loss: 0.3945\n",
            "Epoch 540/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3906 - val_loss: 0.3929\n",
            "Epoch 541/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3910 - val_loss: 0.3942\n",
            "Epoch 542/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3907 - val_loss: 0.3930\n",
            "Epoch 543/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3910 - val_loss: 0.3909\n",
            "Epoch 544/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3912 - val_loss: 0.3952\n",
            "Epoch 545/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3904 - val_loss: 0.3932\n",
            "Epoch 546/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3911 - val_loss: 0.3928\n",
            "Epoch 547/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3909 - val_loss: 0.3950\n",
            "Epoch 548/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3907 - val_loss: 0.3900\n",
            "Epoch 549/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3913 - val_loss: 0.3925\n",
            "Epoch 550/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3904 - val_loss: 0.3948\n",
            "Epoch 551/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3909 - val_loss: 0.3905\n",
            "Epoch 552/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3907 - val_loss: 0.3936\n",
            "Epoch 553/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3906 - val_loss: 0.3924\n",
            "Epoch 554/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3899 - val_loss: 0.3936\n",
            "Epoch 555/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.3932\n",
            "Epoch 556/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.3940\n",
            "Epoch 557/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3904 - val_loss: 0.3942\n",
            "Epoch 558/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3895 - val_loss: 0.3931\n",
            "Epoch 559/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3904 - val_loss: 0.3951\n",
            "Epoch 560/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3900 - val_loss: 0.3897\n",
            "Epoch 561/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3899 - val_loss: 0.3937\n",
            "Epoch 562/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3966\n",
            "Epoch 563/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3904 - val_loss: 0.3938\n",
            "Epoch 564/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3899 - val_loss: 0.3903\n",
            "Epoch 565/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3901 - val_loss: 0.3945\n",
            "Epoch 566/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3902 - val_loss: 0.3948\n",
            "Epoch 567/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3904 - val_loss: 0.3939\n",
            "Epoch 568/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3912 - val_loss: 0.3912\n",
            "Epoch 569/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3900 - val_loss: 0.3930\n",
            "Epoch 570/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3894 - val_loss: 0.3932\n",
            "Epoch 571/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3901 - val_loss: 0.3943\n",
            "Epoch 572/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3908 - val_loss: 0.3954\n",
            "Epoch 573/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3900 - val_loss: 0.3937\n",
            "Epoch 574/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3896 - val_loss: 0.3921\n",
            "Epoch 575/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3902 - val_loss: 0.3940\n",
            "Epoch 576/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3895 - val_loss: 0.3936\n",
            "Epoch 577/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.3955\n",
            "Epoch 578/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3925\n",
            "Epoch 579/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3946\n",
            "Epoch 580/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3901 - val_loss: 0.3940\n",
            "Epoch 581/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3894 - val_loss: 0.3935\n",
            "Epoch 582/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3946\n",
            "Epoch 583/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3896 - val_loss: 0.3940\n",
            "Epoch 584/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3892 - val_loss: 0.3937\n",
            "Epoch 585/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3895 - val_loss: 0.3933\n",
            "Epoch 586/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3897 - val_loss: 0.3938\n",
            "Epoch 587/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3889 - val_loss: 0.3970\n",
            "Epoch 588/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3898 - val_loss: 0.3939\n",
            "Epoch 589/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3897 - val_loss: 0.3960\n",
            "Epoch 590/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3951\n",
            "Epoch 591/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3895 - val_loss: 0.3909\n",
            "Epoch 592/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3894 - val_loss: 0.3958\n",
            "Epoch 593/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3938\n",
            "Epoch 594/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3888 - val_loss: 0.3966\n",
            "Epoch 595/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3896 - val_loss: 0.3964\n",
            "Epoch 596/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3901 - val_loss: 0.3948\n",
            "Epoch 597/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3966\n",
            "Epoch 598/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3891 - val_loss: 0.3954\n",
            "Epoch 599/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3885 - val_loss: 0.3947\n",
            "Epoch 600/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3886 - val_loss: 0.3923\n",
            "Epoch 601/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.3970\n",
            "Epoch 602/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3890 - val_loss: 0.3973\n",
            "Epoch 603/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3887 - val_loss: 0.3964\n",
            "Epoch 604/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3889 - val_loss: 0.3954\n",
            "Epoch 605/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3891 - val_loss: 0.3988\n",
            "Epoch 606/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3888 - val_loss: 0.3965\n",
            "Epoch 607/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3890 - val_loss: 0.3961\n",
            "Epoch 608/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3887 - val_loss: 0.3963\n",
            "Epoch 609/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3891 - val_loss: 0.3948\n",
            "Epoch 610/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.3961\n",
            "Epoch 611/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3893 - val_loss: 0.3926\n",
            "Epoch 612/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3953\n",
            "Epoch 613/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3983\n",
            "Epoch 614/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3956\n",
            "Epoch 615/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3944\n",
            "Epoch 616/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3879 - val_loss: 0.3949\n",
            "Epoch 617/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3984\n",
            "Epoch 618/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3940\n",
            "Epoch 619/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3972\n",
            "Epoch 620/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3890 - val_loss: 0.3906\n",
            "Epoch 621/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3964\n",
            "Epoch 622/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3885 - val_loss: 0.3954\n",
            "Epoch 623/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3886 - val_loss: 0.3949\n",
            "Epoch 624/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3875 - val_loss: 0.3942\n",
            "Epoch 625/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3884 - val_loss: 0.3954\n",
            "Epoch 626/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3884 - val_loss: 0.3986\n",
            "Epoch 627/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3887 - val_loss: 0.3961\n",
            "Epoch 628/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3877 - val_loss: 0.3963\n",
            "Epoch 629/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3955\n",
            "Epoch 630/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3881 - val_loss: 0.3975\n",
            "Epoch 631/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3957\n",
            "Epoch 632/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3958\n",
            "Epoch 633/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3877 - val_loss: 0.3973\n",
            "Epoch 634/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3972\n",
            "Epoch 635/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3949\n",
            "Epoch 636/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3962\n",
            "Epoch 637/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3970\n",
            "Epoch 638/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3974\n",
            "Epoch 639/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3946\n",
            "Epoch 640/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3877 - val_loss: 0.3966\n",
            "Epoch 641/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3880 - val_loss: 0.3980\n",
            "Epoch 642/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3878 - val_loss: 0.3944\n",
            "Epoch 643/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3886 - val_loss: 0.3968\n",
            "Epoch 644/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3876 - val_loss: 0.3949\n",
            "Epoch 645/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3880 - val_loss: 0.3947\n",
            "Epoch 646/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3876 - val_loss: 0.3969\n",
            "Epoch 647/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3947\n",
            "Epoch 648/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3873 - val_loss: 0.3975\n",
            "Epoch 649/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3958\n",
            "Epoch 650/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3953\n",
            "Epoch 651/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3871 - val_loss: 0.3954\n",
            "Epoch 652/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3873 - val_loss: 0.3979\n",
            "Epoch 653/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3875 - val_loss: 0.3947\n",
            "Epoch 654/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3875 - val_loss: 0.3990\n",
            "Epoch 655/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3972\n",
            "Epoch 656/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3979\n",
            "Epoch 657/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3936\n",
            "Epoch 658/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3876 - val_loss: 0.3970\n",
            "Epoch 659/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3958\n",
            "Epoch 660/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3872 - val_loss: 0.3960\n",
            "Epoch 661/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3870 - val_loss: 0.4016\n",
            "Epoch 662/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3879 - val_loss: 0.3944\n",
            "Epoch 663/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3871 - val_loss: 0.3962\n",
            "Epoch 664/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3880 - val_loss: 0.3962\n",
            "Epoch 665/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3873 - val_loss: 0.3982\n",
            "Epoch 666/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3974\n",
            "Epoch 667/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3877 - val_loss: 0.3970\n",
            "Epoch 668/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3869 - val_loss: 0.3999\n",
            "Epoch 669/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3988\n",
            "Epoch 670/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3943\n",
            "Epoch 671/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3985\n",
            "Epoch 672/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3873 - val_loss: 0.4009\n",
            "Epoch 673/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3964\n",
            "Epoch 674/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3989\n",
            "Epoch 675/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3980\n",
            "Epoch 676/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3971\n",
            "Epoch 677/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3867 - val_loss: 0.3992\n",
            "Epoch 678/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3869 - val_loss: 0.3978\n",
            "Epoch 679/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3864 - val_loss: 0.3967\n",
            "Epoch 680/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3874 - val_loss: 0.3969\n",
            "Epoch 681/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3862 - val_loss: 0.3979\n",
            "Epoch 682/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3869 - val_loss: 0.3997\n",
            "Epoch 683/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3867 - val_loss: 0.3985\n",
            "Epoch 684/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3867 - val_loss: 0.3987\n",
            "Epoch 685/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.4019\n",
            "Epoch 686/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3998\n",
            "Epoch 687/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3981\n",
            "Epoch 688/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3865 - val_loss: 0.3988\n",
            "Epoch 689/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3988\n",
            "Epoch 690/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3866 - val_loss: 0.3983\n",
            "Epoch 691/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3978\n",
            "Epoch 692/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3986\n",
            "Epoch 693/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3859 - val_loss: 0.4002\n",
            "Epoch 694/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.3975\n",
            "Epoch 695/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3988\n",
            "Epoch 696/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3862 - val_loss: 0.4000\n",
            "Epoch 697/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3977\n",
            "Epoch 698/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3866 - val_loss: 0.3974\n",
            "Epoch 699/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3863 - val_loss: 0.3960\n",
            "Epoch 700/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3857 - val_loss: 0.3999\n",
            "Epoch 701/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3863 - val_loss: 0.3996\n",
            "Epoch 702/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3861 - val_loss: 0.3976\n",
            "Epoch 703/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3859 - val_loss: 0.3958\n",
            "Epoch 704/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3862 - val_loss: 0.3982\n",
            "Epoch 705/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3862 - val_loss: 0.3966\n",
            "Epoch 706/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3858 - val_loss: 0.3991\n",
            "Epoch 707/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3996\n",
            "Epoch 708/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.4002\n",
            "Epoch 709/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3998\n",
            "Epoch 710/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3947\n",
            "Epoch 711/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3865 - val_loss: 0.3959\n",
            "Epoch 712/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3969\n",
            "Epoch 713/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3981\n",
            "Epoch 714/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.3990\n",
            "Epoch 715/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.3999\n",
            "Epoch 716/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.4010\n",
            "Epoch 717/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3857 - val_loss: 0.3957\n",
            "Epoch 718/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3866 - val_loss: 0.3966\n",
            "Epoch 719/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3861 - val_loss: 0.3987\n",
            "Epoch 720/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3863 - val_loss: 0.3964\n",
            "Epoch 721/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3855 - val_loss: 0.3984\n",
            "Epoch 722/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3854 - val_loss: 0.3989\n",
            "Epoch 723/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.3976\n",
            "Epoch 724/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.3993\n",
            "Epoch 725/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3851 - val_loss: 0.4013\n",
            "Epoch 726/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3860 - val_loss: 0.3988\n",
            "Epoch 727/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3974\n",
            "Epoch 728/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.4010\n",
            "Epoch 729/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.4009\n",
            "Epoch 730/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.4002\n",
            "Epoch 731/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3850 - val_loss: 0.3985\n",
            "Epoch 732/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.4005\n",
            "Epoch 733/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.3974\n",
            "Epoch 734/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.3989\n",
            "Epoch 735/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.3977\n",
            "Epoch 736/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3855 - val_loss: 0.4007\n",
            "Epoch 737/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3857 - val_loss: 0.3976\n",
            "Epoch 738/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3860 - val_loss: 0.4032\n",
            "Epoch 739/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3850 - val_loss: 0.3970\n",
            "Epoch 740/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3855 - val_loss: 0.3994\n",
            "Epoch 741/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3852 - val_loss: 0.4007\n",
            "Epoch 742/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.3995\n",
            "Epoch 743/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3979\n",
            "Epoch 744/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.3987\n",
            "Epoch 745/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.4009\n",
            "Epoch 746/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3846 - val_loss: 0.4021\n",
            "Epoch 747/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.4027\n",
            "Epoch 748/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.4010\n",
            "Epoch 749/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3844 - val_loss: 0.3998\n",
            "Epoch 750/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3991\n",
            "Epoch 751/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3844 - val_loss: 0.3990\n",
            "Epoch 752/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3842 - val_loss: 0.4001\n",
            "Epoch 753/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3847 - val_loss: 0.4027\n",
            "Epoch 754/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3853 - val_loss: 0.3991\n",
            "Epoch 755/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3852 - val_loss: 0.3987\n",
            "Epoch 756/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3848 - val_loss: 0.4018\n",
            "Epoch 757/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3853 - val_loss: 0.4006\n",
            "Epoch 758/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3848 - val_loss: 0.4013\n",
            "Epoch 759/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3842 - val_loss: 0.3985\n",
            "Epoch 760/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3843 - val_loss: 0.4027\n",
            "Epoch 761/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3844 - val_loss: 0.4015\n",
            "Epoch 762/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.3982\n",
            "Epoch 763/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3852 - val_loss: 0.3992\n",
            "Epoch 764/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.4015\n",
            "Epoch 765/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.4027\n",
            "Epoch 766/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.4012\n",
            "Epoch 767/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3849 - val_loss: 0.3991\n",
            "Epoch 768/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4030\n",
            "Epoch 769/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.4020\n",
            "Epoch 770/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.4014\n",
            "Epoch 771/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3833 - val_loss: 0.4042\n",
            "Epoch 772/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.4055\n",
            "Epoch 773/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4034\n",
            "Epoch 774/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3841 - val_loss: 0.4038\n",
            "Epoch 775/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3842 - val_loss: 0.3996\n",
            "Epoch 776/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3841 - val_loss: 0.4054\n",
            "Epoch 777/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3847 - val_loss: 0.4019\n",
            "Epoch 778/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3838 - val_loss: 0.4019\n",
            "Epoch 779/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4028\n",
            "Epoch 780/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3845 - val_loss: 0.4014\n",
            "Epoch 781/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4026\n",
            "Epoch 782/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3841 - val_loss: 0.4012\n",
            "Epoch 783/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.4035\n",
            "Epoch 784/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.4027\n",
            "Epoch 785/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.4032\n",
            "Epoch 786/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3836 - val_loss: 0.4021\n",
            "Epoch 787/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3839 - val_loss: 0.3994\n",
            "Epoch 788/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.4040\n",
            "Epoch 789/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3839 - val_loss: 0.4016\n",
            "Epoch 790/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.4051\n",
            "Epoch 791/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3841 - val_loss: 0.4040\n",
            "Epoch 792/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3839 - val_loss: 0.4035\n",
            "Epoch 793/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3840 - val_loss: 0.4013\n",
            "Epoch 794/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3834 - val_loss: 0.4027\n",
            "Epoch 795/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3834 - val_loss: 0.4067\n",
            "Epoch 796/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3847 - val_loss: 0.4012\n",
            "Epoch 797/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3838 - val_loss: 0.4018\n",
            "Epoch 798/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.4013\n",
            "Epoch 799/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4004\n",
            "Epoch 800/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.4024\n",
            "Epoch 801/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.4038\n",
            "Epoch 802/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3834 - val_loss: 0.3995\n",
            "Epoch 803/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3835 - val_loss: 0.4023\n",
            "Epoch 804/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3835 - val_loss: 0.4019\n",
            "Epoch 805/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.4021\n",
            "Epoch 806/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4004\n",
            "Epoch 807/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.4056\n",
            "Epoch 808/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.4026\n",
            "Epoch 809/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.4034\n",
            "Epoch 810/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.4040\n",
            "Epoch 811/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3840 - val_loss: 0.4030\n",
            "Epoch 812/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3834 - val_loss: 0.4021\n",
            "Epoch 813/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3833 - val_loss: 0.4025\n",
            "Epoch 814/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3832 - val_loss: 0.4002\n",
            "Epoch 815/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3836 - val_loss: 0.4022\n",
            "Epoch 816/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3830 - val_loss: 0.4018\n",
            "Epoch 817/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3834 - val_loss: 0.4023\n",
            "Epoch 818/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.4012\n",
            "Epoch 819/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.4072\n",
            "Epoch 820/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.4049\n",
            "Epoch 821/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3827 - val_loss: 0.4065\n",
            "Epoch 822/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.3998\n",
            "Epoch 823/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3830 - val_loss: 0.4039\n",
            "Epoch 824/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.4028\n",
            "Epoch 825/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.4065\n",
            "Epoch 826/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3829 - val_loss: 0.4064\n",
            "Epoch 827/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.4017\n",
            "Epoch 828/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.4034\n",
            "Epoch 829/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.4023\n",
            "Epoch 830/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3828 - val_loss: 0.4032\n",
            "Epoch 831/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3835 - val_loss: 0.4054\n",
            "Epoch 832/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3834 - val_loss: 0.4037\n",
            "Epoch 833/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3829 - val_loss: 0.4013\n",
            "Epoch 834/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3832 - val_loss: 0.4027\n",
            "Epoch 835/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3828 - val_loss: 0.4019\n",
            "Epoch 836/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4034\n",
            "Epoch 837/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3828 - val_loss: 0.4026\n",
            "Epoch 838/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.4043\n",
            "Epoch 839/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.4032\n",
            "Epoch 840/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3831 - val_loss: 0.4037\n",
            "Epoch 841/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.4039\n",
            "Epoch 842/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.4067\n",
            "Epoch 843/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4038\n",
            "Epoch 844/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4030\n",
            "Epoch 845/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3819 - val_loss: 0.4053\n",
            "Epoch 846/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3822 - val_loss: 0.4019\n",
            "Epoch 847/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.4029\n",
            "Epoch 848/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.4032\n",
            "Epoch 849/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3823 - val_loss: 0.4048\n",
            "Epoch 850/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3826 - val_loss: 0.3999\n",
            "Epoch 851/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3822 - val_loss: 0.4001\n",
            "Epoch 852/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3820 - val_loss: 0.4027\n",
            "Epoch 853/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3818 - val_loss: 0.4046\n",
            "Epoch 854/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3821 - val_loss: 0.4056\n",
            "Epoch 855/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.4029\n",
            "Epoch 856/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4038\n",
            "Epoch 857/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.4055\n",
            "Epoch 858/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.4041\n",
            "Epoch 859/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4067\n",
            "Epoch 860/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.4050\n",
            "Epoch 861/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.4063\n",
            "Epoch 862/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.4048\n",
            "Epoch 863/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3817 - val_loss: 0.4062\n",
            "Epoch 864/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4071\n",
            "Epoch 865/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3821 - val_loss: 0.4036\n",
            "Epoch 866/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4037\n",
            "Epoch 867/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3824 - val_loss: 0.4074\n",
            "Epoch 868/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.4035\n",
            "Epoch 869/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3822 - val_loss: 0.4047\n",
            "Epoch 870/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3817 - val_loss: 0.4026\n",
            "Epoch 871/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3818 - val_loss: 0.4041\n",
            "Epoch 872/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3825 - val_loss: 0.4060\n",
            "Epoch 873/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3816 - val_loss: 0.4026\n",
            "Epoch 874/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.4040\n",
            "Epoch 875/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3821 - val_loss: 0.4050\n",
            "Epoch 876/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3816 - val_loss: 0.4011\n",
            "Epoch 877/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.4040\n",
            "Epoch 878/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.4070\n",
            "Epoch 879/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.4033\n",
            "Epoch 880/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4048\n",
            "Epoch 881/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4064\n",
            "Epoch 882/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4073\n",
            "Epoch 883/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.4081\n",
            "Epoch 884/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3812 - val_loss: 0.4034\n",
            "Epoch 885/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4043\n",
            "Epoch 886/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3819 - val_loss: 0.4019\n",
            "Epoch 887/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4022\n",
            "Epoch 888/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3816 - val_loss: 0.4038\n",
            "Epoch 889/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3810 - val_loss: 0.4042\n",
            "Epoch 890/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3813 - val_loss: 0.4054\n",
            "Epoch 891/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3812 - val_loss: 0.4065\n",
            "Epoch 892/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3805 - val_loss: 0.4019\n",
            "Epoch 893/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3815 - val_loss: 0.4025\n",
            "Epoch 894/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.4061\n",
            "Epoch 895/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.4058\n",
            "Epoch 896/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.4074\n",
            "Epoch 897/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.4059\n",
            "Epoch 898/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4062\n",
            "Epoch 899/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.4080\n",
            "Epoch 900/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4055\n",
            "Epoch 901/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4029\n",
            "Epoch 902/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3818 - val_loss: 0.4057\n",
            "Epoch 903/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3812 - val_loss: 0.4062\n",
            "Epoch 904/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4053\n",
            "Epoch 905/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.4053\n",
            "Epoch 906/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3806 - val_loss: 0.4065\n",
            "Epoch 907/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3815 - val_loss: 0.4043\n",
            "Epoch 908/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3813 - val_loss: 0.4069\n",
            "Epoch 909/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3819 - val_loss: 0.4024\n",
            "Epoch 910/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3812 - val_loss: 0.4051\n",
            "Epoch 911/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3809 - val_loss: 0.4055\n",
            "Epoch 912/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3807 - val_loss: 0.4039\n",
            "Epoch 913/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3815 - val_loss: 0.4109\n",
            "Epoch 914/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4070\n",
            "Epoch 915/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3815 - val_loss: 0.4087\n",
            "Epoch 916/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3813 - val_loss: 0.4066\n",
            "Epoch 917/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4030\n",
            "Epoch 918/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3806 - val_loss: 0.4066\n",
            "Epoch 919/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3810 - val_loss: 0.4089\n",
            "Epoch 920/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.4073\n",
            "Epoch 921/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.4045\n",
            "Epoch 922/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3805 - val_loss: 0.4071\n",
            "Epoch 923/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3803 - val_loss: 0.4064\n",
            "Epoch 924/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3806 - val_loss: 0.4081\n",
            "Epoch 925/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3808 - val_loss: 0.4066\n",
            "Epoch 926/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3808 - val_loss: 0.4069\n",
            "Epoch 927/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3805 - val_loss: 0.4063\n",
            "Epoch 928/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3809 - val_loss: 0.4065\n",
            "Epoch 929/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3807 - val_loss: 0.4074\n",
            "Epoch 930/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3806 - val_loss: 0.4103\n",
            "Epoch 931/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.4046\n",
            "Epoch 932/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3816 - val_loss: 0.4103\n",
            "Epoch 933/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4075\n",
            "Epoch 934/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4078\n",
            "Epoch 935/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4093\n",
            "Epoch 936/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4108\n",
            "Epoch 937/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3809 - val_loss: 0.4104\n",
            "Epoch 938/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4136\n",
            "Epoch 939/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4075\n",
            "Epoch 940/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.4084\n",
            "Epoch 941/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.4086\n",
            "Epoch 942/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3800 - val_loss: 0.4110\n",
            "Epoch 943/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.4131\n",
            "Epoch 944/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3801 - val_loss: 0.4112\n",
            "Epoch 945/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3802 - val_loss: 0.4088\n",
            "Epoch 946/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3804 - val_loss: 0.4111\n",
            "Epoch 947/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3806 - val_loss: 0.4091\n",
            "Epoch 948/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3806 - val_loss: 0.4070\n",
            "Epoch 949/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3805 - val_loss: 0.4107\n",
            "Epoch 950/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4070\n",
            "Epoch 951/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4057\n",
            "Epoch 952/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4071\n",
            "Epoch 953/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4080\n",
            "Epoch 954/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.4089\n",
            "Epoch 955/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.4080\n",
            "Epoch 956/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4084\n",
            "Epoch 957/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.4066\n",
            "Epoch 958/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3800 - val_loss: 0.4094\n",
            "Epoch 959/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4094\n",
            "Epoch 960/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.4067\n",
            "Epoch 961/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.4114\n",
            "Epoch 962/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4091\n",
            "Epoch 963/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.4091\n",
            "Epoch 964/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3801 - val_loss: 0.4090\n",
            "Epoch 965/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3795 - val_loss: 0.4092\n",
            "Epoch 966/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3794 - val_loss: 0.4129\n",
            "Epoch 967/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3801 - val_loss: 0.4098\n",
            "Epoch 968/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3788 - val_loss: 0.4099\n",
            "Epoch 969/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3798 - val_loss: 0.4074\n",
            "Epoch 970/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.4104\n",
            "Epoch 971/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.4117\n",
            "Epoch 972/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.4091\n",
            "Epoch 973/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4070\n",
            "Epoch 974/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.4112\n",
            "Epoch 975/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4072\n",
            "Epoch 976/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.4067\n",
            "Epoch 977/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4056\n",
            "Epoch 978/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3798 - val_loss: 0.4108\n",
            "Epoch 979/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3797 - val_loss: 0.4074\n",
            "Epoch 980/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4093\n",
            "Epoch 981/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.4061\n",
            "Epoch 982/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3795 - val_loss: 0.4089\n",
            "Epoch 983/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3795 - val_loss: 0.4094\n",
            "Epoch 984/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3803 - val_loss: 0.4087\n",
            "Epoch 985/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3801 - val_loss: 0.4093\n",
            "Epoch 986/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3796 - val_loss: 0.4095\n",
            "Epoch 987/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3797 - val_loss: 0.4089\n",
            "Epoch 988/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3792 - val_loss: 0.4056\n",
            "Epoch 989/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.4084\n",
            "Epoch 990/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.4107\n",
            "Epoch 991/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3798 - val_loss: 0.4090\n",
            "Epoch 992/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.4088\n",
            "Epoch 993/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.4066\n",
            "Epoch 994/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.4078\n",
            "Epoch 995/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.4054\n",
            "Epoch 996/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.4081\n",
            "Epoch 997/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.4051\n",
            "Epoch 998/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.4090\n",
            "Epoch 999/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3789 - val_loss: 0.4154\n",
            "Epoch 1000/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3797 - val_loss: 0.4065\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4065\n",
            "8/8 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb0a0124730>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000;    # training epochs\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "pred = Dense(3, activation='softmax')(con_value)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-5))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n"
      ],
      "id": "mJDUSNqnmVMa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl0GYtnNXFJC"
      },
      "source": [
        "##2.3 Add the Temperature in Softmax （3000 Epochs）##\n",
        "The formula of softmax is:  \n",
        "$\\large P_i=\\frac{e^{y_i}}{\\sum_{k=1}^n e^{y_k}}$  \n",
        "\n",
        "After add temperature is:  \n",
        "$\\large P_i=\\frac{e^{\\frac{y_i}T}}{\\sum_{k=1}^n e^{\\frac{y_k}T}}$  \n",
        "A higher temperature (above 1) makes the model less confident, meaning that the probability distribution becomes more flat.\n",
        " "
      ],
      "id": "Pl0GYtnNXFJC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yn1Ny_xiXDfL",
        "outputId": "3bd083ae-1d66-4fb1-9ce1-0d6fbfdc388d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3869 - val_loss: 0.3993\n",
            "Epoch 503/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3870 - val_loss: 0.3927\n",
            "Epoch 504/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3881 - val_loss: 0.3937\n",
            "Epoch 505/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3887 - val_loss: 0.3987\n",
            "Epoch 506/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3892 - val_loss: 0.3913\n",
            "Epoch 507/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3866 - val_loss: 0.3886\n",
            "Epoch 508/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3857 - val_loss: 0.3895\n",
            "Epoch 509/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3857 - val_loss: 0.3930\n",
            "Epoch 510/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3948\n",
            "Epoch 511/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3861 - val_loss: 0.3941\n",
            "Epoch 512/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3876 - val_loss: 0.3909\n",
            "Epoch 513/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3866 - val_loss: 0.3903\n",
            "Epoch 514/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3884 - val_loss: 0.3894\n",
            "Epoch 515/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.3937\n",
            "Epoch 516/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3954\n",
            "Epoch 517/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.3858\n",
            "Epoch 518/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3911 - val_loss: 0.3923\n",
            "Epoch 519/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3887 - val_loss: 0.3895\n",
            "Epoch 520/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3866 - val_loss: 0.3889\n",
            "Epoch 521/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3859 - val_loss: 0.3894\n",
            "Epoch 522/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3860 - val_loss: 0.3920\n",
            "Epoch 523/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3884 - val_loss: 0.3936\n",
            "Epoch 524/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3860 - val_loss: 0.3907\n",
            "Epoch 525/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3861 - val_loss: 0.3905\n",
            "Epoch 526/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3941\n",
            "Epoch 527/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3907\n",
            "Epoch 528/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.3912\n",
            "Epoch 529/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3852 - val_loss: 0.3942\n",
            "Epoch 530/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3870 - val_loss: 0.3889\n",
            "Epoch 531/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3866 - val_loss: 0.3918\n",
            "Epoch 532/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3854 - val_loss: 0.3928\n",
            "Epoch 533/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3845 - val_loss: 0.3939\n",
            "Epoch 534/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3856 - val_loss: 0.3938\n",
            "Epoch 535/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3875 - val_loss: 0.3921\n",
            "Epoch 536/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3902 - val_loss: 0.3976\n",
            "Epoch 537/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3924\n",
            "Epoch 538/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3954\n",
            "Epoch 539/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3849 - val_loss: 0.3939\n",
            "Epoch 540/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3845 - val_loss: 0.3921\n",
            "Epoch 541/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.4001\n",
            "Epoch 542/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.3890\n",
            "Epoch 543/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3902\n",
            "Epoch 544/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3934\n",
            "Epoch 545/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3834 - val_loss: 0.3898\n",
            "Epoch 546/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3866 - val_loss: 0.3907\n",
            "Epoch 547/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3845 - val_loss: 0.3935\n",
            "Epoch 548/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3850 - val_loss: 0.3970\n",
            "Epoch 549/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3848 - val_loss: 0.3957\n",
            "Epoch 550/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3854 - val_loss: 0.3875\n",
            "Epoch 551/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.4038\n",
            "Epoch 552/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3889 - val_loss: 0.3884\n",
            "Epoch 553/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3928\n",
            "Epoch 554/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.3886\n",
            "Epoch 555/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3883\n",
            "Epoch 556/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3904\n",
            "Epoch 557/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3868 - val_loss: 0.3909\n",
            "Epoch 558/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3845 - val_loss: 0.3964\n",
            "Epoch 559/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3851\n",
            "Epoch 560/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3834 - val_loss: 0.3970\n",
            "Epoch 561/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3860 - val_loss: 0.3919\n",
            "Epoch 562/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3827 - val_loss: 0.3893\n",
            "Epoch 563/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3860 - val_loss: 0.3887\n",
            "Epoch 564/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3864\n",
            "Epoch 565/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3851 - val_loss: 0.3903\n",
            "Epoch 566/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3843 - val_loss: 0.3897\n",
            "Epoch 567/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3845 - val_loss: 0.3870\n",
            "Epoch 568/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3854 - val_loss: 0.3928\n",
            "Epoch 569/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3877\n",
            "Epoch 570/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3961\n",
            "Epoch 571/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3841 - val_loss: 0.3879\n",
            "Epoch 572/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3899\n",
            "Epoch 573/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.3886\n",
            "Epoch 574/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.3912\n",
            "Epoch 575/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3831 - val_loss: 0.3921\n",
            "Epoch 576/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3844 - val_loss: 0.3931\n",
            "Epoch 577/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.3904\n",
            "Epoch 578/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3833 - val_loss: 0.3885\n",
            "Epoch 579/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3851 - val_loss: 0.3872\n",
            "Epoch 580/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3893\n",
            "Epoch 581/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3848 - val_loss: 0.3935\n",
            "Epoch 582/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3825 - val_loss: 0.3923\n",
            "Epoch 583/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3828 - val_loss: 0.3939\n",
            "Epoch 584/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3842 - val_loss: 0.3924\n",
            "Epoch 585/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3837 - val_loss: 0.3899\n",
            "Epoch 586/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3835 - val_loss: 0.3873\n",
            "Epoch 587/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3837 - val_loss: 0.3845\n",
            "Epoch 588/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3910\n",
            "Epoch 589/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3892\n",
            "Epoch 590/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3817 - val_loss: 0.3892\n",
            "Epoch 591/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.3901\n",
            "Epoch 592/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3826 - val_loss: 0.3940\n",
            "Epoch 593/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3915\n",
            "Epoch 594/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.3923\n",
            "Epoch 595/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3982\n",
            "Epoch 596/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3852 - val_loss: 0.4021\n",
            "Epoch 597/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3846 - val_loss: 0.3882\n",
            "Epoch 598/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3967\n",
            "Epoch 599/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3854 - val_loss: 0.3907\n",
            "Epoch 600/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3906\n",
            "Epoch 601/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3849 - val_loss: 0.3911\n",
            "Epoch 602/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3843 - val_loss: 0.3883\n",
            "Epoch 603/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3835 - val_loss: 0.3944\n",
            "Epoch 604/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3858 - val_loss: 0.3939\n",
            "Epoch 605/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3930\n",
            "Epoch 606/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.3930\n",
            "Epoch 607/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3816 - val_loss: 0.3935\n",
            "Epoch 608/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3834 - val_loss: 0.3869\n",
            "Epoch 609/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3881\n",
            "Epoch 610/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3903\n",
            "Epoch 611/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3857 - val_loss: 0.3896\n",
            "Epoch 612/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3820 - val_loss: 0.3883\n",
            "Epoch 613/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3817 - val_loss: 0.3983\n",
            "Epoch 614/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3890\n",
            "Epoch 615/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3821 - val_loss: 0.3898\n",
            "Epoch 616/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3900\n",
            "Epoch 617/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3932\n",
            "Epoch 618/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3930\n",
            "Epoch 619/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3829 - val_loss: 0.3924\n",
            "Epoch 620/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3824 - val_loss: 0.3956\n",
            "Epoch 621/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3845 - val_loss: 0.3993\n",
            "Epoch 622/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3824 - val_loss: 0.3904\n",
            "Epoch 623/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.3930\n",
            "Epoch 624/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3823 - val_loss: 0.3893\n",
            "Epoch 625/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3826 - val_loss: 0.3908\n",
            "Epoch 626/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.3888\n",
            "Epoch 627/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3905\n",
            "Epoch 628/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.3868\n",
            "Epoch 629/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3911\n",
            "Epoch 630/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3811 - val_loss: 0.3903\n",
            "Epoch 631/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3950\n",
            "Epoch 632/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.4025\n",
            "Epoch 633/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3912\n",
            "Epoch 634/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3896\n",
            "Epoch 635/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3913\n",
            "Epoch 636/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3910\n",
            "Epoch 637/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3838 - val_loss: 0.3951\n",
            "Epoch 638/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3823 - val_loss: 0.4005\n",
            "Epoch 639/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3838 - val_loss: 0.4005\n",
            "Epoch 640/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3842 - val_loss: 0.3912\n",
            "Epoch 641/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3820 - val_loss: 0.3876\n",
            "Epoch 642/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.3993\n",
            "Epoch 643/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.3944\n",
            "Epoch 644/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3929\n",
            "Epoch 645/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.3902\n",
            "Epoch 646/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3919\n",
            "Epoch 647/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3823 - val_loss: 0.3986\n",
            "Epoch 648/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3862\n",
            "Epoch 649/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3871\n",
            "Epoch 650/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3886\n",
            "Epoch 651/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.3905\n",
            "Epoch 652/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3911\n",
            "Epoch 653/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.3919\n",
            "Epoch 654/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3836 - val_loss: 0.3912\n",
            "Epoch 655/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3810 - val_loss: 0.3874\n",
            "Epoch 656/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3823 - val_loss: 0.3942\n",
            "Epoch 657/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3821 - val_loss: 0.3952\n",
            "Epoch 658/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3834 - val_loss: 0.3891\n",
            "Epoch 659/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3823 - val_loss: 0.3889\n",
            "Epoch 660/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.3916\n",
            "Epoch 661/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3893\n",
            "Epoch 662/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3811 - val_loss: 0.3937\n",
            "Epoch 663/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3916\n",
            "Epoch 664/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3905\n",
            "Epoch 665/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3817 - val_loss: 0.3912\n",
            "Epoch 666/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3850 - val_loss: 0.3913\n",
            "Epoch 667/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.3904\n",
            "Epoch 668/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3889\n",
            "Epoch 669/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.3912\n",
            "Epoch 670/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3898\n",
            "Epoch 671/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3832 - val_loss: 0.3915\n",
            "Epoch 672/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3834 - val_loss: 0.3978\n",
            "Epoch 673/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3826 - val_loss: 0.3898\n",
            "Epoch 674/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3827 - val_loss: 0.3891\n",
            "Epoch 675/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3824 - val_loss: 0.3895\n",
            "Epoch 676/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3819 - val_loss: 0.3893\n",
            "Epoch 677/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3812 - val_loss: 0.3919\n",
            "Epoch 678/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3952\n",
            "Epoch 679/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3810 - val_loss: 0.3898\n",
            "Epoch 680/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3912\n",
            "Epoch 681/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3938\n",
            "Epoch 682/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3824 - val_loss: 0.3955\n",
            "Epoch 683/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3805 - val_loss: 0.3861\n",
            "Epoch 684/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.3905\n",
            "Epoch 685/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3812 - val_loss: 0.3911\n",
            "Epoch 686/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3887\n",
            "Epoch 687/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3918\n",
            "Epoch 688/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3935\n",
            "Epoch 689/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3808 - val_loss: 0.3963\n",
            "Epoch 690/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.4024\n",
            "Epoch 691/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3824 - val_loss: 0.3901\n",
            "Epoch 692/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3835 - val_loss: 0.3897\n",
            "Epoch 693/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3825 - val_loss: 0.3910\n",
            "Epoch 694/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3819 - val_loss: 0.3945\n",
            "Epoch 695/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3912\n",
            "Epoch 696/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3920\n",
            "Epoch 697/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.3915\n",
            "Epoch 698/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3909\n",
            "Epoch 699/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3913\n",
            "Epoch 700/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3796 - val_loss: 0.3951\n",
            "Epoch 701/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3818 - val_loss: 0.3899\n",
            "Epoch 702/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3945\n",
            "Epoch 703/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3808 - val_loss: 0.3915\n",
            "Epoch 704/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3912\n",
            "Epoch 705/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3949\n",
            "Epoch 706/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.3949\n",
            "Epoch 707/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3796 - val_loss: 0.3949\n",
            "Epoch 708/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3916\n",
            "Epoch 709/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3803 - val_loss: 0.3919\n",
            "Epoch 710/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3799 - val_loss: 0.3943\n",
            "Epoch 711/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3803 - val_loss: 0.3921\n",
            "Epoch 712/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3797 - val_loss: 0.3888\n",
            "Epoch 713/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3809 - val_loss: 0.3906\n",
            "Epoch 714/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3802 - val_loss: 0.3872\n",
            "Epoch 715/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3919\n",
            "Epoch 716/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3800 - val_loss: 0.3924\n",
            "Epoch 717/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3956\n",
            "Epoch 718/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3846 - val_loss: 0.3911\n",
            "Epoch 719/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3940\n",
            "Epoch 720/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3842 - val_loss: 0.3901\n",
            "Epoch 721/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3895\n",
            "Epoch 722/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3800 - val_loss: 0.3909\n",
            "Epoch 723/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3812 - val_loss: 0.3894\n",
            "Epoch 724/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3807 - val_loss: 0.3917\n",
            "Epoch 725/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3885\n",
            "Epoch 726/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3944\n",
            "Epoch 727/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3897\n",
            "Epoch 728/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3791 - val_loss: 0.3917\n",
            "Epoch 729/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3806 - val_loss: 0.3900\n",
            "Epoch 730/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3790 - val_loss: 0.3897\n",
            "Epoch 731/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3835 - val_loss: 0.3942\n",
            "Epoch 732/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3826 - val_loss: 0.3961\n",
            "Epoch 733/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3909\n",
            "Epoch 734/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3904\n",
            "Epoch 735/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3799 - val_loss: 0.3919\n",
            "Epoch 736/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3793 - val_loss: 0.3884\n",
            "Epoch 737/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.3912\n",
            "Epoch 738/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3920\n",
            "Epoch 739/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.3895\n",
            "Epoch 740/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.3902\n",
            "Epoch 741/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3906\n",
            "Epoch 742/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3894\n",
            "Epoch 743/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3911\n",
            "Epoch 744/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3903\n",
            "Epoch 745/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.3888\n",
            "Epoch 746/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3792 - val_loss: 0.3949\n",
            "Epoch 747/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3811 - val_loss: 0.3888\n",
            "Epoch 748/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3798 - val_loss: 0.3867\n",
            "Epoch 749/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3786 - val_loss: 0.3935\n",
            "Epoch 750/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3899\n",
            "Epoch 751/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.3984\n",
            "Epoch 752/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3802 - val_loss: 0.3902\n",
            "Epoch 753/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.3905\n",
            "Epoch 754/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3947\n",
            "Epoch 755/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3815 - val_loss: 0.3921\n",
            "Epoch 756/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3919\n",
            "Epoch 757/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3942\n",
            "Epoch 758/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3808 - val_loss: 0.3910\n",
            "Epoch 759/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3895\n",
            "Epoch 760/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3955\n",
            "Epoch 761/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3928\n",
            "Epoch 762/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3943\n",
            "Epoch 763/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3896\n",
            "Epoch 764/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3795 - val_loss: 0.3945\n",
            "Epoch 765/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3787 - val_loss: 0.3923\n",
            "Epoch 766/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3797 - val_loss: 0.3925\n",
            "Epoch 767/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3809 - val_loss: 0.3913\n",
            "Epoch 768/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3951\n",
            "Epoch 769/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3815 - val_loss: 0.3933\n",
            "Epoch 770/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.3918\n",
            "Epoch 771/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3797 - val_loss: 0.3873\n",
            "Epoch 772/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3947\n",
            "Epoch 773/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3970\n",
            "Epoch 774/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3904\n",
            "Epoch 775/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3900\n",
            "Epoch 776/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3894\n",
            "Epoch 777/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3790 - val_loss: 0.3903\n",
            "Epoch 778/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3802 - val_loss: 0.3929\n",
            "Epoch 779/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.3898\n",
            "Epoch 780/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3878\n",
            "Epoch 781/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3883\n",
            "Epoch 782/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3792 - val_loss: 0.3923\n",
            "Epoch 783/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3790 - val_loss: 0.3895\n",
            "Epoch 784/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3801 - val_loss: 0.3908\n",
            "Epoch 785/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3782 - val_loss: 0.3897\n",
            "Epoch 786/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3800 - val_loss: 0.3916\n",
            "Epoch 787/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3950\n",
            "Epoch 788/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3911\n",
            "Epoch 789/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3910\n",
            "Epoch 790/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3937\n",
            "Epoch 791/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3903\n",
            "Epoch 792/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3907\n",
            "Epoch 793/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3971\n",
            "Epoch 794/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3837 - val_loss: 0.3931\n",
            "Epoch 795/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3922\n",
            "Epoch 796/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3859\n",
            "Epoch 797/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3889\n",
            "Epoch 798/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3909\n",
            "Epoch 799/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3907\n",
            "Epoch 800/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3812 - val_loss: 0.3917\n",
            "Epoch 801/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3801 - val_loss: 0.3916\n",
            "Epoch 802/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3796 - val_loss: 0.3899\n",
            "Epoch 803/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3811 - val_loss: 0.3938\n",
            "Epoch 804/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3788 - val_loss: 0.3891\n",
            "Epoch 805/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.3926\n",
            "Epoch 806/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3795 - val_loss: 0.3909\n",
            "Epoch 807/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3894\n",
            "Epoch 808/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3912\n",
            "Epoch 809/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3790 - val_loss: 0.3935\n",
            "Epoch 810/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3901\n",
            "Epoch 811/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3887\n",
            "Epoch 812/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3927\n",
            "Epoch 813/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3895\n",
            "Epoch 814/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3896\n",
            "Epoch 815/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3935\n",
            "Epoch 816/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3787 - val_loss: 0.3919\n",
            "Epoch 817/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.3901\n",
            "Epoch 818/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3795 - val_loss: 0.3915\n",
            "Epoch 819/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3793 - val_loss: 0.3890\n",
            "Epoch 820/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3783 - val_loss: 0.3906\n",
            "Epoch 821/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3787 - val_loss: 0.3945\n",
            "Epoch 822/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3913\n",
            "Epoch 823/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3898\n",
            "Epoch 824/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3950\n",
            "Epoch 825/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3804 - val_loss: 0.3972\n",
            "Epoch 826/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3898\n",
            "Epoch 827/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3889\n",
            "Epoch 828/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3917\n",
            "Epoch 829/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3779 - val_loss: 0.3889\n",
            "Epoch 830/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3919\n",
            "Epoch 831/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3789 - val_loss: 0.3928\n",
            "Epoch 832/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.3976\n",
            "Epoch 833/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3896\n",
            "Epoch 834/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.3922\n",
            "Epoch 835/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3926\n",
            "Epoch 836/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3779 - val_loss: 0.3909\n",
            "Epoch 837/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3776 - val_loss: 0.3905\n",
            "Epoch 838/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3774 - val_loss: 0.3956\n",
            "Epoch 839/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3786 - val_loss: 0.3924\n",
            "Epoch 840/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3786 - val_loss: 0.3918\n",
            "Epoch 841/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.3916\n",
            "Epoch 842/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.3908\n",
            "Epoch 843/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3943\n",
            "Epoch 844/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.3915\n",
            "Epoch 845/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3947\n",
            "Epoch 846/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3925\n",
            "Epoch 847/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.3929\n",
            "Epoch 848/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3920\n",
            "Epoch 849/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3943\n",
            "Epoch 850/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3900\n",
            "Epoch 851/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3892\n",
            "Epoch 852/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3899\n",
            "Epoch 853/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3790 - val_loss: 0.3892\n",
            "Epoch 854/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3787 - val_loss: 0.3939\n",
            "Epoch 855/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3791 - val_loss: 0.3947\n",
            "Epoch 856/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3786 - val_loss: 0.3911\n",
            "Epoch 857/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3781 - val_loss: 0.3914\n",
            "Epoch 858/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3920\n",
            "Epoch 859/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.3910\n",
            "Epoch 860/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.3926\n",
            "Epoch 861/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3924\n",
            "Epoch 862/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3777 - val_loss: 0.3902\n",
            "Epoch 863/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3905\n",
            "Epoch 864/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3976\n",
            "Epoch 865/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3787 - val_loss: 0.3942\n",
            "Epoch 866/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3941\n",
            "Epoch 867/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3777 - val_loss: 0.3953\n",
            "Epoch 868/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3937\n",
            "Epoch 869/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3800 - val_loss: 0.3888\n",
            "Epoch 870/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3906\n",
            "Epoch 871/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3942\n",
            "Epoch 872/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3771 - val_loss: 0.3952\n",
            "Epoch 873/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3773 - val_loss: 0.3919\n",
            "Epoch 874/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3788 - val_loss: 0.3944\n",
            "Epoch 875/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3773 - val_loss: 0.3941\n",
            "Epoch 876/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3942\n",
            "Epoch 877/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3887\n",
            "Epoch 878/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3943\n",
            "Epoch 879/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3773 - val_loss: 0.3955\n",
            "Epoch 880/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3775 - val_loss: 0.3916\n",
            "Epoch 881/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3992\n",
            "Epoch 882/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3810 - val_loss: 0.3910\n",
            "Epoch 883/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3776 - val_loss: 0.3947\n",
            "Epoch 884/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3786 - val_loss: 0.3973\n",
            "Epoch 885/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3895\n",
            "Epoch 886/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3907\n",
            "Epoch 887/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3771 - val_loss: 0.3911\n",
            "Epoch 888/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3899\n",
            "Epoch 889/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3777 - val_loss: 0.3945\n",
            "Epoch 890/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3774 - val_loss: 0.3926\n",
            "Epoch 891/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3763 - val_loss: 0.3940\n",
            "Epoch 892/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3768 - val_loss: 0.3927\n",
            "Epoch 893/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3790 - val_loss: 0.3944\n",
            "Epoch 894/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3961\n",
            "Epoch 895/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3793 - val_loss: 0.4098\n",
            "Epoch 896/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3806 - val_loss: 0.3908\n",
            "Epoch 897/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3983\n",
            "Epoch 898/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3765 - val_loss: 0.3931\n",
            "Epoch 899/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3916\n",
            "Epoch 900/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3964\n",
            "Epoch 901/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3975\n",
            "Epoch 902/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.4002\n",
            "Epoch 903/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3939\n",
            "Epoch 904/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3947\n",
            "Epoch 905/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3917\n",
            "Epoch 906/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3764 - val_loss: 0.3904\n",
            "Epoch 907/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3758 - val_loss: 0.3909\n",
            "Epoch 908/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3779 - val_loss: 0.3933\n",
            "Epoch 909/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3771 - val_loss: 0.3926\n",
            "Epoch 910/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3770 - val_loss: 0.3913\n",
            "Epoch 911/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3772 - val_loss: 0.3934\n",
            "Epoch 912/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3957\n",
            "Epoch 913/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3771 - val_loss: 0.3919\n",
            "Epoch 914/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3958\n",
            "Epoch 915/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3775 - val_loss: 0.3919\n",
            "Epoch 916/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3910\n",
            "Epoch 917/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3782 - val_loss: 0.3912\n",
            "Epoch 918/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3786 - val_loss: 0.3986\n",
            "Epoch 919/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3938\n",
            "Epoch 920/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3772 - val_loss: 0.3965\n",
            "Epoch 921/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3778 - val_loss: 0.3985\n",
            "Epoch 922/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3941\n",
            "Epoch 923/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3758 - val_loss: 0.3948\n",
            "Epoch 924/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3945\n",
            "Epoch 925/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3907\n",
            "Epoch 926/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3781 - val_loss: 0.3971\n",
            "Epoch 927/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3798 - val_loss: 0.3964\n",
            "Epoch 928/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3772 - val_loss: 0.3927\n",
            "Epoch 929/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3766 - val_loss: 0.3951\n",
            "Epoch 930/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3766 - val_loss: 0.3934\n",
            "Epoch 931/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3790 - val_loss: 0.3969\n",
            "Epoch 932/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3802 - val_loss: 0.3961\n",
            "Epoch 933/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3961\n",
            "Epoch 934/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3940\n",
            "Epoch 935/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3969\n",
            "Epoch 936/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3789 - val_loss: 0.3962\n",
            "Epoch 937/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3783 - val_loss: 0.3973\n",
            "Epoch 938/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3921\n",
            "Epoch 939/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3915\n",
            "Epoch 940/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3948\n",
            "Epoch 941/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3951\n",
            "Epoch 942/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3778 - val_loss: 0.3923\n",
            "Epoch 943/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3967\n",
            "Epoch 944/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3773 - val_loss: 0.3957\n",
            "Epoch 945/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3773 - val_loss: 0.3907\n",
            "Epoch 946/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3802 - val_loss: 0.3947\n",
            "Epoch 947/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3803 - val_loss: 0.3939\n",
            "Epoch 948/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3929\n",
            "Epoch 949/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3923\n",
            "Epoch 950/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3955\n",
            "Epoch 951/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3986\n",
            "Epoch 952/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3982\n",
            "Epoch 953/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3907\n",
            "Epoch 954/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3960\n",
            "Epoch 955/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3909\n",
            "Epoch 956/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3957\n",
            "Epoch 957/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3763 - val_loss: 0.3902\n",
            "Epoch 958/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3948\n",
            "Epoch 959/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3929\n",
            "Epoch 960/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3913\n",
            "Epoch 961/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3987\n",
            "Epoch 962/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3766 - val_loss: 0.3937\n",
            "Epoch 963/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3769 - val_loss: 0.3917\n",
            "Epoch 964/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3765 - val_loss: 0.3973\n",
            "Epoch 965/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3767 - val_loss: 0.3914\n",
            "Epoch 966/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3909\n",
            "Epoch 967/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3770 - val_loss: 0.3928\n",
            "Epoch 968/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3934\n",
            "Epoch 969/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3969\n",
            "Epoch 970/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3764 - val_loss: 0.3933\n",
            "Epoch 971/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3941\n",
            "Epoch 972/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3991\n",
            "Epoch 973/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3966\n",
            "Epoch 974/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3943\n",
            "Epoch 975/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3775 - val_loss: 0.3930\n",
            "Epoch 976/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3756 - val_loss: 0.3939\n",
            "Epoch 977/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3907\n",
            "Epoch 978/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3944\n",
            "Epoch 979/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3945\n",
            "Epoch 980/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3784 - val_loss: 0.3966\n",
            "Epoch 981/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3779 - val_loss: 0.3928\n",
            "Epoch 982/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3782 - val_loss: 0.3905\n",
            "Epoch 983/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3953\n",
            "Epoch 984/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3776 - val_loss: 0.3929\n",
            "Epoch 985/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3964\n",
            "Epoch 986/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.3972\n",
            "Epoch 987/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3932\n",
            "Epoch 988/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3925\n",
            "Epoch 989/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3917\n",
            "Epoch 990/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3931\n",
            "Epoch 991/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3955\n",
            "Epoch 992/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3988\n",
            "Epoch 993/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3777 - val_loss: 0.3933\n",
            "Epoch 994/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3939\n",
            "Epoch 995/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3936\n",
            "Epoch 996/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3949\n",
            "Epoch 997/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3789 - val_loss: 0.3938\n",
            "Epoch 998/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3767 - val_loss: 0.3938\n",
            "Epoch 999/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3780 - val_loss: 0.3947\n",
            "Epoch 1000/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3766 - val_loss: 0.3991\n",
            "Epoch 1001/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3767 - val_loss: 0.3945\n",
            "Epoch 1002/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3940\n",
            "Epoch 1003/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3953\n",
            "Epoch 1004/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3939\n",
            "Epoch 1005/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3760 - val_loss: 0.3989\n",
            "Epoch 1006/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3953\n",
            "Epoch 1007/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3937\n",
            "Epoch 1008/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3934\n",
            "Epoch 1009/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3764 - val_loss: 0.3956\n",
            "Epoch 1010/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3939\n",
            "Epoch 1011/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3931\n",
            "Epoch 1012/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3771 - val_loss: 0.3910\n",
            "Epoch 1013/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.3949\n",
            "Epoch 1014/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3766 - val_loss: 0.3926\n",
            "Epoch 1015/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3939\n",
            "Epoch 1016/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3744 - val_loss: 0.3966\n",
            "Epoch 1017/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3755 - val_loss: 0.3940\n",
            "Epoch 1018/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3763 - val_loss: 0.3971\n",
            "Epoch 1019/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3781 - val_loss: 0.3931\n",
            "Epoch 1020/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3770 - val_loss: 0.3916\n",
            "Epoch 1021/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3941\n",
            "Epoch 1022/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3987\n",
            "Epoch 1023/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3956\n",
            "Epoch 1024/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3934\n",
            "Epoch 1025/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3960\n",
            "Epoch 1026/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3968\n",
            "Epoch 1027/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3777 - val_loss: 0.3928\n",
            "Epoch 1028/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3939\n",
            "Epoch 1029/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3966\n",
            "Epoch 1030/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.4011\n",
            "Epoch 1031/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3790 - val_loss: 0.3910\n",
            "Epoch 1032/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3970\n",
            "Epoch 1033/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3930\n",
            "Epoch 1034/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3941\n",
            "Epoch 1035/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3754 - val_loss: 0.3953\n",
            "Epoch 1036/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3755 - val_loss: 0.3954\n",
            "Epoch 1037/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3750 - val_loss: 0.4021\n",
            "Epoch 1038/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3980\n",
            "Epoch 1039/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3758 - val_loss: 0.3905\n",
            "Epoch 1040/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3922\n",
            "Epoch 1041/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3936\n",
            "Epoch 1042/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.3954\n",
            "Epoch 1043/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3978\n",
            "Epoch 1044/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3960\n",
            "Epoch 1045/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3937\n",
            "Epoch 1046/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.4004\n",
            "Epoch 1047/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3921\n",
            "Epoch 1048/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3966\n",
            "Epoch 1049/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3988\n",
            "Epoch 1050/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3908\n",
            "Epoch 1051/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3944\n",
            "Epoch 1052/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3937\n",
            "Epoch 1053/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3756 - val_loss: 0.3935\n",
            "Epoch 1054/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3756 - val_loss: 0.3966\n",
            "Epoch 1055/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3754 - val_loss: 0.3982\n",
            "Epoch 1056/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3757 - val_loss: 0.3945\n",
            "Epoch 1057/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3940\n",
            "Epoch 1058/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3758 - val_loss: 0.4000\n",
            "Epoch 1059/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3757 - val_loss: 0.3934\n",
            "Epoch 1060/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3970\n",
            "Epoch 1061/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3769 - val_loss: 0.3941\n",
            "Epoch 1062/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3930\n",
            "Epoch 1063/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3968\n",
            "Epoch 1064/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3956\n",
            "Epoch 1065/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3973\n",
            "Epoch 1066/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3936\n",
            "Epoch 1067/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3925\n",
            "Epoch 1068/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3968\n",
            "Epoch 1069/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3948\n",
            "Epoch 1070/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3768 - val_loss: 0.4032\n",
            "Epoch 1071/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3791 - val_loss: 0.3938\n",
            "Epoch 1072/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3768 - val_loss: 0.3935\n",
            "Epoch 1073/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3764 - val_loss: 0.3947\n",
            "Epoch 1074/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3749 - val_loss: 0.3973\n",
            "Epoch 1075/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3961\n",
            "Epoch 1076/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3929\n",
            "Epoch 1077/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3967\n",
            "Epoch 1078/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3995\n",
            "Epoch 1079/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3990\n",
            "Epoch 1080/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3933\n",
            "Epoch 1081/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3938\n",
            "Epoch 1082/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3736 - val_loss: 0.3940\n",
            "Epoch 1083/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3745 - val_loss: 0.3950\n",
            "Epoch 1084/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3966\n",
            "Epoch 1085/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3945\n",
            "Epoch 1086/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3944\n",
            "Epoch 1087/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3954\n",
            "Epoch 1088/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3956\n",
            "Epoch 1089/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3986\n",
            "Epoch 1090/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3779 - val_loss: 0.3925\n",
            "Epoch 1091/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3970\n",
            "Epoch 1092/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3753 - val_loss: 0.3980\n",
            "Epoch 1093/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3977\n",
            "Epoch 1094/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3995\n",
            "Epoch 1095/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3955\n",
            "Epoch 1096/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3991\n",
            "Epoch 1097/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3947\n",
            "Epoch 1098/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3956\n",
            "Epoch 1099/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.3921\n",
            "Epoch 1100/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3951\n",
            "Epoch 1101/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3768 - val_loss: 0.3961\n",
            "Epoch 1102/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3966\n",
            "Epoch 1103/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.4025\n",
            "Epoch 1104/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3974\n",
            "Epoch 1105/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3988\n",
            "Epoch 1106/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3933\n",
            "Epoch 1107/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3764 - val_loss: 0.3995\n",
            "Epoch 1108/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3758 - val_loss: 0.3951\n",
            "Epoch 1109/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3762 - val_loss: 0.3981\n",
            "Epoch 1110/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3803 - val_loss: 0.3954\n",
            "Epoch 1111/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3775 - val_loss: 0.3930\n",
            "Epoch 1112/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3936\n",
            "Epoch 1113/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3930\n",
            "Epoch 1114/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3963\n",
            "Epoch 1115/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.3925\n",
            "Epoch 1116/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3760 - val_loss: 0.3953\n",
            "Epoch 1117/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3936\n",
            "Epoch 1118/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3949\n",
            "Epoch 1119/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3772 - val_loss: 0.3940\n",
            "Epoch 1120/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3921\n",
            "Epoch 1121/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.4011\n",
            "Epoch 1122/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3952\n",
            "Epoch 1123/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3935\n",
            "Epoch 1124/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3955\n",
            "Epoch 1125/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3956\n",
            "Epoch 1126/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3743 - val_loss: 0.3978\n",
            "Epoch 1127/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3753 - val_loss: 0.3947\n",
            "Epoch 1128/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3753 - val_loss: 0.3945\n",
            "Epoch 1129/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3919\n",
            "Epoch 1130/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3949\n",
            "Epoch 1131/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3938\n",
            "Epoch 1132/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.3963\n",
            "Epoch 1133/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3973\n",
            "Epoch 1134/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3952\n",
            "Epoch 1135/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3971\n",
            "Epoch 1136/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3919\n",
            "Epoch 1137/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3968\n",
            "Epoch 1138/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3923\n",
            "Epoch 1139/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3974\n",
            "Epoch 1140/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3995\n",
            "Epoch 1141/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3966\n",
            "Epoch 1142/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3945\n",
            "Epoch 1143/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3937\n",
            "Epoch 1144/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3750 - val_loss: 0.3965\n",
            "Epoch 1145/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3752 - val_loss: 0.3983\n",
            "Epoch 1146/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3753 - val_loss: 0.4025\n",
            "Epoch 1147/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3767 - val_loss: 0.3949\n",
            "Epoch 1148/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3947\n",
            "Epoch 1149/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3956\n",
            "Epoch 1150/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3956\n",
            "Epoch 1151/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3903\n",
            "Epoch 1152/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3937\n",
            "Epoch 1153/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3939\n",
            "Epoch 1154/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3948\n",
            "Epoch 1155/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3929\n",
            "Epoch 1156/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3926\n",
            "Epoch 1157/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3932\n",
            "Epoch 1158/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3947\n",
            "Epoch 1159/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3935\n",
            "Epoch 1160/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3972\n",
            "Epoch 1161/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3974\n",
            "Epoch 1162/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3750 - val_loss: 0.3972\n",
            "Epoch 1163/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3745 - val_loss: 0.3951\n",
            "Epoch 1164/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3750 - val_loss: 0.3965\n",
            "Epoch 1165/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3736 - val_loss: 0.3970\n",
            "Epoch 1166/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3979\n",
            "Epoch 1167/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3955\n",
            "Epoch 1168/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3954\n",
            "Epoch 1169/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3946\n",
            "Epoch 1170/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3729 - val_loss: 0.3966\n",
            "Epoch 1171/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3941\n",
            "Epoch 1172/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.4057\n",
            "Epoch 1173/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3805 - val_loss: 0.3962\n",
            "Epoch 1174/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.4011\n",
            "Epoch 1175/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3784 - val_loss: 0.3952\n",
            "Epoch 1176/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3923\n",
            "Epoch 1177/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3948\n",
            "Epoch 1178/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3946\n",
            "Epoch 1179/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3937\n",
            "Epoch 1180/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3744 - val_loss: 0.3955\n",
            "Epoch 1181/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3743 - val_loss: 0.3940\n",
            "Epoch 1182/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3740 - val_loss: 0.3939\n",
            "Epoch 1183/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3740 - val_loss: 0.3938\n",
            "Epoch 1184/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3745 - val_loss: 0.3924\n",
            "Epoch 1185/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3945\n",
            "Epoch 1186/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3960\n",
            "Epoch 1187/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3756 - val_loss: 0.3945\n",
            "Epoch 1188/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3963\n",
            "Epoch 1189/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.4017\n",
            "Epoch 1190/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3751 - val_loss: 0.3971\n",
            "Epoch 1191/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3964\n",
            "Epoch 1192/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3973\n",
            "Epoch 1193/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3963\n",
            "Epoch 1194/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3984\n",
            "Epoch 1195/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3992\n",
            "Epoch 1196/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3958\n",
            "Epoch 1197/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3771 - val_loss: 0.3959\n",
            "Epoch 1198/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3976\n",
            "Epoch 1199/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3754 - val_loss: 0.3988\n",
            "Epoch 1200/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3740 - val_loss: 0.3949\n",
            "Epoch 1201/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3749 - val_loss: 0.3958\n",
            "Epoch 1202/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3748 - val_loss: 0.3952\n",
            "Epoch 1203/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3751 - val_loss: 0.3993\n",
            "Epoch 1204/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3948\n",
            "Epoch 1205/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.3954\n",
            "Epoch 1206/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3956\n",
            "Epoch 1207/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3748 - val_loss: 0.3954\n",
            "Epoch 1208/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3769 - val_loss: 0.3943\n",
            "Epoch 1209/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3998\n",
            "Epoch 1210/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3974\n",
            "Epoch 1211/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3981\n",
            "Epoch 1212/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3755 - val_loss: 0.3948\n",
            "Epoch 1213/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3977\n",
            "Epoch 1214/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3972\n",
            "Epoch 1215/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3949\n",
            "Epoch 1216/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3954\n",
            "Epoch 1217/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3970\n",
            "Epoch 1218/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3744 - val_loss: 0.4000\n",
            "Epoch 1219/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3748 - val_loss: 0.3992\n",
            "Epoch 1220/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3755 - val_loss: 0.3989\n",
            "Epoch 1221/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.4008\n",
            "Epoch 1222/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3964\n",
            "Epoch 1223/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3986\n",
            "Epoch 1224/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3958\n",
            "Epoch 1225/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3958\n",
            "Epoch 1226/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3970\n",
            "Epoch 1227/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3928\n",
            "Epoch 1228/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3769 - val_loss: 0.3940\n",
            "Epoch 1229/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3970\n",
            "Epoch 1230/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3965\n",
            "Epoch 1231/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3934\n",
            "Epoch 1232/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3958\n",
            "Epoch 1233/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3944\n",
            "Epoch 1234/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3937\n",
            "Epoch 1235/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3960\n",
            "Epoch 1236/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3764 - val_loss: 0.3953\n",
            "Epoch 1237/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3751 - val_loss: 0.3948\n",
            "Epoch 1238/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3740 - val_loss: 0.3936\n",
            "Epoch 1239/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3986\n",
            "Epoch 1240/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3972\n",
            "Epoch 1241/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3991\n",
            "Epoch 1242/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3946\n",
            "Epoch 1243/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.4013\n",
            "Epoch 1244/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3949\n",
            "Epoch 1245/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3960\n",
            "Epoch 1246/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3963\n",
            "Epoch 1247/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3963\n",
            "Epoch 1248/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3754 - val_loss: 0.3958\n",
            "Epoch 1249/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3969\n",
            "Epoch 1250/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3967\n",
            "Epoch 1251/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3982\n",
            "Epoch 1252/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3981\n",
            "Epoch 1253/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3962\n",
            "Epoch 1254/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3738 - val_loss: 0.3983\n",
            "Epoch 1255/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3746 - val_loss: 0.3973\n",
            "Epoch 1256/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3770 - val_loss: 0.3967\n",
            "Epoch 1257/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3740 - val_loss: 0.3937\n",
            "Epoch 1258/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3960\n",
            "Epoch 1259/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3972\n",
            "Epoch 1260/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3980\n",
            "Epoch 1261/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3971\n",
            "Epoch 1262/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3987\n",
            "Epoch 1263/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3972\n",
            "Epoch 1264/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3940\n",
            "Epoch 1265/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3993\n",
            "Epoch 1266/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3961\n",
            "Epoch 1267/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.4054\n",
            "Epoch 1268/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3965\n",
            "Epoch 1269/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3777 - val_loss: 0.3959\n",
            "Epoch 1270/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3961\n",
            "Epoch 1271/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3969\n",
            "Epoch 1272/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3755 - val_loss: 0.3984\n",
            "Epoch 1273/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3753 - val_loss: 0.3981\n",
            "Epoch 1274/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3745 - val_loss: 0.3986\n",
            "Epoch 1275/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3737 - val_loss: 0.3959\n",
            "Epoch 1276/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3744 - val_loss: 0.3969\n",
            "Epoch 1277/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3753 - val_loss: 0.3955\n",
            "Epoch 1278/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3948\n",
            "Epoch 1279/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3933\n",
            "Epoch 1280/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3965\n",
            "Epoch 1281/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3970\n",
            "Epoch 1282/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3973\n",
            "Epoch 1283/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3933\n",
            "Epoch 1284/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3937\n",
            "Epoch 1285/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3930\n",
            "Epoch 1286/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3973\n",
            "Epoch 1287/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3971\n",
            "Epoch 1288/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3988\n",
            "Epoch 1289/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.4006\n",
            "Epoch 1290/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3739 - val_loss: 0.3989\n",
            "Epoch 1291/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3727 - val_loss: 0.3988\n",
            "Epoch 1292/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3740 - val_loss: 0.4013\n",
            "Epoch 1293/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3744 - val_loss: 0.3957\n",
            "Epoch 1294/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3728 - val_loss: 0.3945\n",
            "Epoch 1295/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3972\n",
            "Epoch 1296/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.4004\n",
            "Epoch 1297/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.4003\n",
            "Epoch 1298/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3766 - val_loss: 0.3961\n",
            "Epoch 1299/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3938\n",
            "Epoch 1300/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3976\n",
            "Epoch 1301/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3973\n",
            "Epoch 1302/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3962\n",
            "Epoch 1303/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3952\n",
            "Epoch 1304/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3945\n",
            "Epoch 1305/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3975\n",
            "Epoch 1306/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3993\n",
            "Epoch 1307/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3968\n",
            "Epoch 1308/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3970\n",
            "Epoch 1309/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3739 - val_loss: 0.3942\n",
            "Epoch 1310/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3734 - val_loss: 0.3963\n",
            "Epoch 1311/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3733 - val_loss: 0.3960\n",
            "Epoch 1312/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3731 - val_loss: 0.3975\n",
            "Epoch 1313/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3981\n",
            "Epoch 1314/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3972\n",
            "Epoch 1315/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3947\n",
            "Epoch 1316/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3942\n",
            "Epoch 1317/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3963\n",
            "Epoch 1318/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3763 - val_loss: 0.3988\n",
            "Epoch 1319/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3959\n",
            "Epoch 1320/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3959\n",
            "Epoch 1321/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3944\n",
            "Epoch 1322/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3963\n",
            "Epoch 1323/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.3930\n",
            "Epoch 1324/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3946\n",
            "Epoch 1325/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3962\n",
            "Epoch 1326/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3989\n",
            "Epoch 1327/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3737 - val_loss: 0.3962\n",
            "Epoch 1328/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3734 - val_loss: 0.3990\n",
            "Epoch 1329/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3750 - val_loss: 0.3971\n",
            "Epoch 1330/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3735 - val_loss: 0.3979\n",
            "Epoch 1331/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3726 - val_loss: 0.3982\n",
            "Epoch 1332/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.4005\n",
            "Epoch 1333/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3757 - val_loss: 0.3992\n",
            "Epoch 1334/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3967\n",
            "Epoch 1335/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3968\n",
            "Epoch 1336/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3983\n",
            "Epoch 1337/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.4027\n",
            "Epoch 1338/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3807 - val_loss: 0.4012\n",
            "Epoch 1339/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3990\n",
            "Epoch 1340/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3961\n",
            "Epoch 1341/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3960\n",
            "Epoch 1342/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3966\n",
            "Epoch 1343/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3957\n",
            "Epoch 1344/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3982\n",
            "Epoch 1345/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3722 - val_loss: 0.3978\n",
            "Epoch 1346/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3723 - val_loss: 0.3992\n",
            "Epoch 1347/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3736 - val_loss: 0.3994\n",
            "Epoch 1348/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3745 - val_loss: 0.3984\n",
            "Epoch 1349/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3728 - val_loss: 0.4006\n",
            "Epoch 1350/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3948\n",
            "Epoch 1351/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3983\n",
            "Epoch 1352/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3979\n",
            "Epoch 1353/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3971\n",
            "Epoch 1354/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3963\n",
            "Epoch 1355/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3968\n",
            "Epoch 1356/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3956\n",
            "Epoch 1357/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3987\n",
            "Epoch 1358/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.4004\n",
            "Epoch 1359/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.3983\n",
            "Epoch 1360/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3988\n",
            "Epoch 1361/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3980\n",
            "Epoch 1362/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.4011\n",
            "Epoch 1363/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3735 - val_loss: 0.4009\n",
            "Epoch 1364/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3737 - val_loss: 0.3972\n",
            "Epoch 1365/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.3978\n",
            "Epoch 1366/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3982\n",
            "Epoch 1367/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3727 - val_loss: 0.3970\n",
            "Epoch 1368/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4008\n",
            "Epoch 1369/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.4035\n",
            "Epoch 1370/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.4027\n",
            "Epoch 1371/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3986\n",
            "Epoch 1372/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3995\n",
            "Epoch 1373/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3739 - val_loss: 0.3976\n",
            "Epoch 1374/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3996\n",
            "Epoch 1375/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.4011\n",
            "Epoch 1376/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3982\n",
            "Epoch 1377/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.4001\n",
            "Epoch 1378/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3981\n",
            "Epoch 1379/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.4016\n",
            "Epoch 1380/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.4010\n",
            "Epoch 1381/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3954\n",
            "Epoch 1382/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3743 - val_loss: 0.4034\n",
            "Epoch 1383/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3746 - val_loss: 0.3989\n",
            "Epoch 1384/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3723 - val_loss: 0.3993\n",
            "Epoch 1385/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3726 - val_loss: 0.3990\n",
            "Epoch 1386/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3753 - val_loss: 0.3983\n",
            "Epoch 1387/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3765 - val_loss: 0.3951\n",
            "Epoch 1388/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3972\n",
            "Epoch 1389/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3959\n",
            "Epoch 1390/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3972\n",
            "Epoch 1391/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.3956\n",
            "Epoch 1392/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3978\n",
            "Epoch 1393/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3984\n",
            "Epoch 1394/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3984\n",
            "Epoch 1395/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3978\n",
            "Epoch 1396/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3969\n",
            "Epoch 1397/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.3972\n",
            "Epoch 1398/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3997\n",
            "Epoch 1399/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3725 - val_loss: 0.3986\n",
            "Epoch 1400/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3721 - val_loss: 0.3987\n",
            "Epoch 1401/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3736 - val_loss: 0.3986\n",
            "Epoch 1402/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3728 - val_loss: 0.3966\n",
            "Epoch 1403/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3724 - val_loss: 0.4043\n",
            "Epoch 1404/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.4008\n",
            "Epoch 1405/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3971\n",
            "Epoch 1406/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3961\n",
            "Epoch 1407/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3975\n",
            "Epoch 1408/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.3985\n",
            "Epoch 1409/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.4006\n",
            "Epoch 1410/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3984\n",
            "Epoch 1411/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3731 - val_loss: 0.4001\n",
            "Epoch 1412/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3993\n",
            "Epoch 1413/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3953\n",
            "Epoch 1414/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3982\n",
            "Epoch 1415/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3948\n",
            "Epoch 1416/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3971\n",
            "Epoch 1417/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3988\n",
            "Epoch 1418/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3734 - val_loss: 0.3993\n",
            "Epoch 1419/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3728 - val_loss: 0.3977\n",
            "Epoch 1420/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3986\n",
            "Epoch 1421/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3727 - val_loss: 0.3978\n",
            "Epoch 1422/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3733 - val_loss: 0.3966\n",
            "Epoch 1423/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.3981\n",
            "Epoch 1424/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.4000\n",
            "Epoch 1425/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3992\n",
            "Epoch 1426/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3976\n",
            "Epoch 1427/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3975\n",
            "Epoch 1428/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3986\n",
            "Epoch 1429/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3981\n",
            "Epoch 1430/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3976\n",
            "Epoch 1431/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3989\n",
            "Epoch 1432/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3964\n",
            "Epoch 1433/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3954\n",
            "Epoch 1434/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3949\n",
            "Epoch 1435/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3986\n",
            "Epoch 1436/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.4015\n",
            "Epoch 1437/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3727 - val_loss: 0.4005\n",
            "Epoch 1438/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3726 - val_loss: 0.3960\n",
            "Epoch 1439/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3729 - val_loss: 0.3988\n",
            "Epoch 1440/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3745 - val_loss: 0.3989\n",
            "Epoch 1441/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3977\n",
            "Epoch 1442/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3999\n",
            "Epoch 1443/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3985\n",
            "Epoch 1444/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.3948\n",
            "Epoch 1445/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3956\n",
            "Epoch 1446/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3948\n",
            "Epoch 1447/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.3959\n",
            "Epoch 1448/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3990\n",
            "Epoch 1449/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3988\n",
            "Epoch 1450/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3963\n",
            "Epoch 1451/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.4009\n",
            "Epoch 1452/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3985\n",
            "Epoch 1453/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3731 - val_loss: 0.3998\n",
            "Epoch 1454/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.4004\n",
            "Epoch 1455/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3727 - val_loss: 0.3996\n",
            "Epoch 1456/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3729 - val_loss: 0.3998\n",
            "Epoch 1457/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3732 - val_loss: 0.3994\n",
            "Epoch 1458/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3728 - val_loss: 0.3959\n",
            "Epoch 1459/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3727 - val_loss: 0.3991\n",
            "Epoch 1460/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3749 - val_loss: 0.3981\n",
            "Epoch 1461/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3968\n",
            "Epoch 1462/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3952\n",
            "Epoch 1463/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3972\n",
            "Epoch 1464/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.3975\n",
            "Epoch 1465/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3993\n",
            "Epoch 1466/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3980\n",
            "Epoch 1467/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3726 - val_loss: 0.3982\n",
            "Epoch 1468/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.4010\n",
            "Epoch 1469/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.4028\n",
            "Epoch 1470/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3996\n",
            "Epoch 1471/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3739 - val_loss: 0.4013\n",
            "Epoch 1472/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.4011\n",
            "Epoch 1473/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3719 - val_loss: 0.3987\n",
            "Epoch 1474/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3721 - val_loss: 0.4002\n",
            "Epoch 1475/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3724 - val_loss: 0.4019\n",
            "Epoch 1476/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3719 - val_loss: 0.4002\n",
            "Epoch 1477/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3718 - val_loss: 0.4023\n",
            "Epoch 1478/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3998\n",
            "Epoch 1479/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3994\n",
            "Epoch 1480/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.4024\n",
            "Epoch 1481/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.4035\n",
            "Epoch 1482/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3979\n",
            "Epoch 1483/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3972\n",
            "Epoch 1484/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.3996\n",
            "Epoch 1485/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.4012\n",
            "Epoch 1486/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.4044\n",
            "Epoch 1487/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.4028\n",
            "Epoch 1488/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.4059\n",
            "Epoch 1489/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.4016\n",
            "Epoch 1490/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.4001\n",
            "Epoch 1491/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3716 - val_loss: 0.3997\n",
            "Epoch 1492/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3723 - val_loss: 0.4022\n",
            "Epoch 1493/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3765 - val_loss: 0.3988\n",
            "Epoch 1494/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3782 - val_loss: 0.3980\n",
            "Epoch 1495/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3805 - val_loss: 0.3964\n",
            "Epoch 1496/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3774 - val_loss: 0.3955\n",
            "Epoch 1497/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.4020\n",
            "Epoch 1498/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3952\n",
            "Epoch 1499/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3942\n",
            "Epoch 1500/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3957\n",
            "Epoch 1501/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3971\n",
            "Epoch 1502/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3985\n",
            "Epoch 1503/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.4032\n",
            "Epoch 1504/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3735 - val_loss: 0.3992\n",
            "Epoch 1505/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.4028\n",
            "Epoch 1506/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3995\n",
            "Epoch 1507/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3983\n",
            "Epoch 1508/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4002\n",
            "Epoch 1509/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3988\n",
            "Epoch 1510/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3736 - val_loss: 0.4037\n",
            "Epoch 1511/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3731 - val_loss: 0.3965\n",
            "Epoch 1512/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3725 - val_loss: 0.3990\n",
            "Epoch 1513/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3723 - val_loss: 0.4007\n",
            "Epoch 1514/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.4002\n",
            "Epoch 1515/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3974\n",
            "Epoch 1516/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.4001\n",
            "Epoch 1517/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4012\n",
            "Epoch 1518/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3985\n",
            "Epoch 1519/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.4032\n",
            "Epoch 1520/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.4034\n",
            "Epoch 1521/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.4017\n",
            "Epoch 1522/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4017\n",
            "Epoch 1523/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4039\n",
            "Epoch 1524/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3997\n",
            "Epoch 1525/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4008\n",
            "Epoch 1526/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3726 - val_loss: 0.3961\n",
            "Epoch 1527/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.4021\n",
            "Epoch 1528/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3721 - val_loss: 0.4026\n",
            "Epoch 1529/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3730 - val_loss: 0.4045\n",
            "Epoch 1530/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3733 - val_loss: 0.4004\n",
            "Epoch 1531/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3738 - val_loss: 0.4005\n",
            "Epoch 1532/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.4022\n",
            "Epoch 1533/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3739 - val_loss: 0.4003\n",
            "Epoch 1534/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.4004\n",
            "Epoch 1535/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.4036\n",
            "Epoch 1536/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3971\n",
            "Epoch 1537/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3988\n",
            "Epoch 1538/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3946\n",
            "Epoch 1539/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4005\n",
            "Epoch 1540/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4019\n",
            "Epoch 1541/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3983\n",
            "Epoch 1542/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3985\n",
            "Epoch 1543/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3980\n",
            "Epoch 1544/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3958\n",
            "Epoch 1545/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3720 - val_loss: 0.3941\n",
            "Epoch 1546/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3719 - val_loss: 0.3976\n",
            "Epoch 1547/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.4002\n",
            "Epoch 1548/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3716 - val_loss: 0.3984\n",
            "Epoch 1549/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3728 - val_loss: 0.4035\n",
            "Epoch 1550/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.4052\n",
            "Epoch 1551/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.4012\n",
            "Epoch 1552/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3983\n",
            "Epoch 1553/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3955\n",
            "Epoch 1554/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3969\n",
            "Epoch 1555/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4005\n",
            "Epoch 1556/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3984\n",
            "Epoch 1557/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.4004\n",
            "Epoch 1558/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3960\n",
            "Epoch 1559/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3987\n",
            "Epoch 1560/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.4001\n",
            "Epoch 1561/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4003\n",
            "Epoch 1562/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.4032\n",
            "Epoch 1563/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3743 - val_loss: 0.4013\n",
            "Epoch 1564/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3720 - val_loss: 0.3997\n",
            "Epoch 1565/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3717 - val_loss: 0.4009\n",
            "Epoch 1566/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3727 - val_loss: 0.3972\n",
            "Epoch 1567/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3717 - val_loss: 0.3978\n",
            "Epoch 1568/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.4067\n",
            "Epoch 1569/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.4021\n",
            "Epoch 1570/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4004\n",
            "Epoch 1571/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.3987\n",
            "Epoch 1572/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3985\n",
            "Epoch 1573/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.4019\n",
            "Epoch 1574/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.3964\n",
            "Epoch 1575/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.4011\n",
            "Epoch 1576/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3998\n",
            "Epoch 1577/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4007\n",
            "Epoch 1578/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.4016\n",
            "Epoch 1579/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3964\n",
            "Epoch 1580/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3994\n",
            "Epoch 1581/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3721 - val_loss: 0.4021\n",
            "Epoch 1582/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3720 - val_loss: 0.4018\n",
            "Epoch 1583/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3730 - val_loss: 0.3987\n",
            "Epoch 1584/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3741 - val_loss: 0.4003\n",
            "Epoch 1585/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3725 - val_loss: 0.4004\n",
            "Epoch 1586/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.4024\n",
            "Epoch 1587/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4029\n",
            "Epoch 1588/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4024\n",
            "Epoch 1589/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4010\n",
            "Epoch 1590/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3969\n",
            "Epoch 1591/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3983\n",
            "Epoch 1592/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4014\n",
            "Epoch 1593/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3964\n",
            "Epoch 1594/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3958\n",
            "Epoch 1595/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4032\n",
            "Epoch 1596/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3994\n",
            "Epoch 1597/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.4019\n",
            "Epoch 1598/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.4005\n",
            "Epoch 1599/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3988\n",
            "Epoch 1600/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3715 - val_loss: 0.3974\n",
            "Epoch 1601/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3724 - val_loss: 0.3976\n",
            "Epoch 1602/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3723 - val_loss: 0.3972\n",
            "Epoch 1603/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3722 - val_loss: 0.3983\n",
            "Epoch 1604/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3994\n",
            "Epoch 1605/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4058\n",
            "Epoch 1606/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.4016\n",
            "Epoch 1607/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3991\n",
            "Epoch 1608/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4021\n",
            "Epoch 1609/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4039\n",
            "Epoch 1610/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4058\n",
            "Epoch 1611/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.4019\n",
            "Epoch 1612/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4027\n",
            "Epoch 1613/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.4005\n",
            "Epoch 1614/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4001\n",
            "Epoch 1615/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4008\n",
            "Epoch 1616/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4001\n",
            "Epoch 1617/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.4016\n",
            "Epoch 1618/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3711 - val_loss: 0.4008\n",
            "Epoch 1619/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3712 - val_loss: 0.3992\n",
            "Epoch 1620/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3715 - val_loss: 0.3986\n",
            "Epoch 1621/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3715 - val_loss: 0.4027\n",
            "Epoch 1622/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4021\n",
            "Epoch 1623/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4015\n",
            "Epoch 1624/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.3993\n",
            "Epoch 1625/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4042\n",
            "Epoch 1626/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.4004\n",
            "Epoch 1627/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.4018\n",
            "Epoch 1628/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.4010\n",
            "Epoch 1629/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3993\n",
            "Epoch 1630/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4041\n",
            "Epoch 1631/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3998\n",
            "Epoch 1632/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.4023\n",
            "Epoch 1633/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3997\n",
            "Epoch 1634/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.3992\n",
            "Epoch 1635/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3717 - val_loss: 0.3985\n",
            "Epoch 1636/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3722 - val_loss: 0.4001\n",
            "Epoch 1637/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.4001\n",
            "Epoch 1638/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.3981\n",
            "Epoch 1639/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4019\n",
            "Epoch 1640/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4036\n",
            "Epoch 1641/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3995\n",
            "Epoch 1642/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.3991\n",
            "Epoch 1643/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4025\n",
            "Epoch 1644/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.4014\n",
            "Epoch 1645/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.4034\n",
            "Epoch 1646/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3739 - val_loss: 0.3986\n",
            "Epoch 1647/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3972\n",
            "Epoch 1648/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4014\n",
            "Epoch 1649/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3978\n",
            "Epoch 1650/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.4008\n",
            "Epoch 1651/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4034\n",
            "Epoch 1652/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3720 - val_loss: 0.3982\n",
            "Epoch 1653/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3728 - val_loss: 0.4031\n",
            "Epoch 1654/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3721 - val_loss: 0.4042\n",
            "Epoch 1655/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3732 - val_loss: 0.3986\n",
            "Epoch 1656/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.4004\n",
            "Epoch 1657/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.4072\n",
            "Epoch 1658/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3729 - val_loss: 0.4027\n",
            "Epoch 1659/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4028\n",
            "Epoch 1660/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4028\n",
            "Epoch 1661/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.4050\n",
            "Epoch 1662/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.4010\n",
            "Epoch 1663/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4026\n",
            "Epoch 1664/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4066\n",
            "Epoch 1665/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3984\n",
            "Epoch 1666/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.4016\n",
            "Epoch 1667/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3998\n",
            "Epoch 1668/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3989\n",
            "Epoch 1669/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3720 - val_loss: 0.4037\n",
            "Epoch 1670/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3715 - val_loss: 0.4013\n",
            "Epoch 1671/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3730 - val_loss: 0.4021\n",
            "Epoch 1672/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3718 - val_loss: 0.4000\n",
            "Epoch 1673/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.4009\n",
            "Epoch 1674/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4030\n",
            "Epoch 1675/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.4046\n",
            "Epoch 1676/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.4030\n",
            "Epoch 1677/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.3998\n",
            "Epoch 1678/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4004\n",
            "Epoch 1679/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.4012\n",
            "Epoch 1680/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4020\n",
            "Epoch 1681/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.4000\n",
            "Epoch 1682/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.3969\n",
            "Epoch 1683/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.4006\n",
            "Epoch 1684/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4026\n",
            "Epoch 1685/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.3990\n",
            "Epoch 1686/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3718 - val_loss: 0.4053\n",
            "Epoch 1687/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3724 - val_loss: 0.4033\n",
            "Epoch 1688/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3709 - val_loss: 0.4001\n",
            "Epoch 1689/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3727 - val_loss: 0.4006\n",
            "Epoch 1690/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3976\n",
            "Epoch 1691/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.4019\n",
            "Epoch 1692/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3971\n",
            "Epoch 1693/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3976\n",
            "Epoch 1694/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4022\n",
            "Epoch 1695/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4021\n",
            "Epoch 1696/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3969\n",
            "Epoch 1697/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4035\n",
            "Epoch 1698/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.3995\n",
            "Epoch 1699/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3959\n",
            "Epoch 1700/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4003\n",
            "Epoch 1701/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3698 - val_loss: 0.4044\n",
            "Epoch 1702/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3995\n",
            "Epoch 1703/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3970\n",
            "Epoch 1704/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.4021\n",
            "Epoch 1705/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3714 - val_loss: 0.4006\n",
            "Epoch 1706/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3707 - val_loss: 0.4014\n",
            "Epoch 1707/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3710 - val_loss: 0.3972\n",
            "Epoch 1708/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.4020\n",
            "Epoch 1709/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3700 - val_loss: 0.3977\n",
            "Epoch 1710/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.3995\n",
            "Epoch 1711/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4039\n",
            "Epoch 1712/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4056\n",
            "Epoch 1713/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3995\n",
            "Epoch 1714/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3965\n",
            "Epoch 1715/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.3979\n",
            "Epoch 1716/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4013\n",
            "Epoch 1717/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4026\n",
            "Epoch 1718/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3999\n",
            "Epoch 1719/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3993\n",
            "Epoch 1720/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4010\n",
            "Epoch 1721/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.4008\n",
            "Epoch 1722/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3706 - val_loss: 0.4024\n",
            "Epoch 1723/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3699 - val_loss: 0.4017\n",
            "Epoch 1724/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3726 - val_loss: 0.4007\n",
            "Epoch 1725/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3709 - val_loss: 0.4007\n",
            "Epoch 1726/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.4016\n",
            "Epoch 1727/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.3987\n",
            "Epoch 1728/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3966\n",
            "Epoch 1729/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3996\n",
            "Epoch 1730/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4012\n",
            "Epoch 1731/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4031\n",
            "Epoch 1732/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4049\n",
            "Epoch 1733/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4053\n",
            "Epoch 1734/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4029\n",
            "Epoch 1735/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.4057\n",
            "Epoch 1736/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.4043\n",
            "Epoch 1737/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4032\n",
            "Epoch 1738/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4009\n",
            "Epoch 1739/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3716 - val_loss: 0.4072\n",
            "Epoch 1740/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3701 - val_loss: 0.4052\n",
            "Epoch 1741/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3697 - val_loss: 0.4018\n",
            "Epoch 1742/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3700 - val_loss: 0.4028\n",
            "Epoch 1743/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4008\n",
            "Epoch 1744/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4032\n",
            "Epoch 1745/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.4027\n",
            "Epoch 1746/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3997\n",
            "Epoch 1747/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.4066\n",
            "Epoch 1748/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4050\n",
            "Epoch 1749/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3988\n",
            "Epoch 1750/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.4005\n",
            "Epoch 1751/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.4029\n",
            "Epoch 1752/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4027\n",
            "Epoch 1753/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4007\n",
            "Epoch 1754/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4054\n",
            "Epoch 1755/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4051\n",
            "Epoch 1756/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3709 - val_loss: 0.4014\n",
            "Epoch 1757/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.3980\n",
            "Epoch 1758/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3714 - val_loss: 0.3985\n",
            "Epoch 1759/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3703 - val_loss: 0.4049\n",
            "Epoch 1760/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3702 - val_loss: 0.4017\n",
            "Epoch 1761/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3995\n",
            "Epoch 1762/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4011\n",
            "Epoch 1763/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4030\n",
            "Epoch 1764/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.4008\n",
            "Epoch 1765/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.4004\n",
            "Epoch 1766/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4051\n",
            "Epoch 1767/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4041\n",
            "Epoch 1768/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4000\n",
            "Epoch 1769/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4026\n",
            "Epoch 1770/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4015\n",
            "Epoch 1771/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4037\n",
            "Epoch 1772/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3987\n",
            "Epoch 1773/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4040\n",
            "Epoch 1774/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4032\n",
            "Epoch 1775/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3713 - val_loss: 0.4046\n",
            "Epoch 1776/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3736 - val_loss: 0.3947\n",
            "Epoch 1777/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3714 - val_loss: 0.3975\n",
            "Epoch 1778/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3699 - val_loss: 0.3989\n",
            "Epoch 1779/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3695 - val_loss: 0.4027\n",
            "Epoch 1780/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.4014\n",
            "Epoch 1781/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.4009\n",
            "Epoch 1782/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4025\n",
            "Epoch 1783/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4021\n",
            "Epoch 1784/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.4002\n",
            "Epoch 1785/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.4002\n",
            "Epoch 1786/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4040\n",
            "Epoch 1787/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4000\n",
            "Epoch 1788/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.3972\n",
            "Epoch 1789/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4038\n",
            "Epoch 1790/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.3989\n",
            "Epoch 1791/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4090\n",
            "Epoch 1792/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3996\n",
            "Epoch 1793/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.4015\n",
            "Epoch 1794/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3697 - val_loss: 0.4010\n",
            "Epoch 1795/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3708 - val_loss: 0.4063\n",
            "Epoch 1796/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.4027\n",
            "Epoch 1797/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3717 - val_loss: 0.4056\n",
            "Epoch 1798/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4004\n",
            "Epoch 1799/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4028\n",
            "Epoch 1800/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.4023\n",
            "Epoch 1801/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4014\n",
            "Epoch 1802/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.4041\n",
            "Epoch 1803/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4028\n",
            "Epoch 1804/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4019\n",
            "Epoch 1805/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4051\n",
            "Epoch 1806/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3988\n",
            "Epoch 1807/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4016\n",
            "Epoch 1808/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4069\n",
            "Epoch 1809/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4087\n",
            "Epoch 1810/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.4054\n",
            "Epoch 1811/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3727 - val_loss: 0.4016\n",
            "Epoch 1812/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.4051\n",
            "Epoch 1813/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3714 - val_loss: 0.4004\n",
            "Epoch 1814/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3714 - val_loss: 0.4018\n",
            "Epoch 1815/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3701 - val_loss: 0.4015\n",
            "Epoch 1816/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4044\n",
            "Epoch 1817/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.4002\n",
            "Epoch 1818/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4043\n",
            "Epoch 1819/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4032\n",
            "Epoch 1820/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4014\n",
            "Epoch 1821/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4022\n",
            "Epoch 1822/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4070\n",
            "Epoch 1823/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4030\n",
            "Epoch 1824/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.4063\n",
            "Epoch 1825/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4014\n",
            "Epoch 1826/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4017\n",
            "Epoch 1827/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4006\n",
            "Epoch 1828/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4030\n",
            "Epoch 1829/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3703 - val_loss: 0.4008\n",
            "Epoch 1830/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3698 - val_loss: 0.4030\n",
            "Epoch 1831/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3695 - val_loss: 0.4012\n",
            "Epoch 1832/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3693 - val_loss: 0.4038\n",
            "Epoch 1833/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.3999\n",
            "Epoch 1834/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4042\n",
            "Epoch 1835/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4009\n",
            "Epoch 1836/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4001\n",
            "Epoch 1837/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4048\n",
            "Epoch 1838/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4049\n",
            "Epoch 1839/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4032\n",
            "Epoch 1840/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4053\n",
            "Epoch 1841/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4005\n",
            "Epoch 1842/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4002\n",
            "Epoch 1843/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4073\n",
            "Epoch 1844/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3743 - val_loss: 0.4009\n",
            "Epoch 1845/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3985\n",
            "Epoch 1846/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3980\n",
            "Epoch 1847/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3708 - val_loss: 0.4005\n",
            "Epoch 1848/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3713 - val_loss: 0.3973\n",
            "Epoch 1849/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3704 - val_loss: 0.4011\n",
            "Epoch 1850/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3707 - val_loss: 0.4012\n",
            "Epoch 1851/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4000\n",
            "Epoch 1852/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3698 - val_loss: 0.4001\n",
            "Epoch 1853/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4001\n",
            "Epoch 1854/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4043\n",
            "Epoch 1855/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.3980\n",
            "Epoch 1856/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.3982\n",
            "Epoch 1857/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4015\n",
            "Epoch 1858/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4024\n",
            "Epoch 1859/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4011\n",
            "Epoch 1860/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4003\n",
            "Epoch 1861/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4031\n",
            "Epoch 1862/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4000\n",
            "Epoch 1863/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4029\n",
            "Epoch 1864/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3721 - val_loss: 0.4022\n",
            "Epoch 1865/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.4003\n",
            "Epoch 1866/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3712 - val_loss: 0.3967\n",
            "Epoch 1867/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3698 - val_loss: 0.3977\n",
            "Epoch 1868/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4025\n",
            "Epoch 1869/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3926\n",
            "Epoch 1870/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3997\n",
            "Epoch 1871/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.4021\n",
            "Epoch 1872/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4009\n",
            "Epoch 1873/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4004\n",
            "Epoch 1874/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4026\n",
            "Epoch 1875/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4014\n",
            "Epoch 1876/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4036\n",
            "Epoch 1877/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.3979\n",
            "Epoch 1878/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4030\n",
            "Epoch 1879/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4053\n",
            "Epoch 1880/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4071\n",
            "Epoch 1881/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3695 - val_loss: 0.4026\n",
            "Epoch 1882/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3695 - val_loss: 0.4031\n",
            "Epoch 1883/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3698 - val_loss: 0.4035\n",
            "Epoch 1884/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.4026\n",
            "Epoch 1885/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3692 - val_loss: 0.4032\n",
            "Epoch 1886/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3706 - val_loss: 0.4045\n",
            "Epoch 1887/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4028\n",
            "Epoch 1888/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3975\n",
            "Epoch 1889/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4041\n",
            "Epoch 1890/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.4040\n",
            "Epoch 1891/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.4011\n",
            "Epoch 1892/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4018\n",
            "Epoch 1893/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4039\n",
            "Epoch 1894/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4030\n",
            "Epoch 1895/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4028\n",
            "Epoch 1896/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4012\n",
            "Epoch 1897/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.3988\n",
            "Epoch 1898/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4046\n",
            "Epoch 1899/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.4063\n",
            "Epoch 1900/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3707 - val_loss: 0.4079\n",
            "Epoch 1901/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3697 - val_loss: 0.4020\n",
            "Epoch 1902/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4057\n",
            "Epoch 1903/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3703 - val_loss: 0.4009\n",
            "Epoch 1904/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4043\n",
            "Epoch 1905/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.4009\n",
            "Epoch 1906/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3773 - val_loss: 0.4013\n",
            "Epoch 1907/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4036\n",
            "Epoch 1908/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3997\n",
            "Epoch 1909/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3960\n",
            "Epoch 1910/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4054\n",
            "Epoch 1911/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.3996\n",
            "Epoch 1912/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4018\n",
            "Epoch 1913/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3959\n",
            "Epoch 1914/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.4020\n",
            "Epoch 1915/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4021\n",
            "Epoch 1916/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4003\n",
            "Epoch 1917/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3953\n",
            "Epoch 1918/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3693 - val_loss: 0.3988\n",
            "Epoch 1919/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3710 - val_loss: 0.3995\n",
            "Epoch 1920/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.4013\n",
            "Epoch 1921/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.4037\n",
            "Epoch 1922/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4034\n",
            "Epoch 1923/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4016\n",
            "Epoch 1924/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4011\n",
            "Epoch 1925/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.4026\n",
            "Epoch 1926/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3990\n",
            "Epoch 1927/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3993\n",
            "Epoch 1928/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4036\n",
            "Epoch 1929/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4015\n",
            "Epoch 1930/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4006\n",
            "Epoch 1931/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4017\n",
            "Epoch 1932/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4056\n",
            "Epoch 1933/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.4021\n",
            "Epoch 1934/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3689 - val_loss: 0.4069\n",
            "Epoch 1935/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3687 - val_loss: 0.4053\n",
            "Epoch 1936/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3685 - val_loss: 0.4048\n",
            "Epoch 1937/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3691 - val_loss: 0.4024\n",
            "Epoch 1938/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3693 - val_loss: 0.4035\n",
            "Epoch 1939/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4043\n",
            "Epoch 1940/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4025\n",
            "Epoch 1941/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.4052\n",
            "Epoch 1942/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4043\n",
            "Epoch 1943/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.4008\n",
            "Epoch 1944/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4039\n",
            "Epoch 1945/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4009\n",
            "Epoch 1946/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4023\n",
            "Epoch 1947/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4003\n",
            "Epoch 1948/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3977\n",
            "Epoch 1949/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4005\n",
            "Epoch 1950/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.3980\n",
            "Epoch 1951/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4034\n",
            "Epoch 1952/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3688 - val_loss: 0.4016\n",
            "Epoch 1953/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.4049\n",
            "Epoch 1954/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3685 - val_loss: 0.4021\n",
            "Epoch 1955/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3694 - val_loss: 0.4030\n",
            "Epoch 1956/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4052\n",
            "Epoch 1957/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4090\n",
            "Epoch 1958/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4036\n",
            "Epoch 1959/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4078\n",
            "Epoch 1960/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4012\n",
            "Epoch 1961/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4052\n",
            "Epoch 1962/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4018\n",
            "Epoch 1963/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3691 - val_loss: 0.4047\n",
            "Epoch 1964/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3996\n",
            "Epoch 1965/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4059\n",
            "Epoch 1966/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4049\n",
            "Epoch 1967/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.4095\n",
            "Epoch 1968/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4069\n",
            "Epoch 1969/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3699 - val_loss: 0.4022\n",
            "Epoch 1970/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.4027\n",
            "Epoch 1971/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3709 - val_loss: 0.4033\n",
            "Epoch 1972/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.4027\n",
            "Epoch 1973/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3695 - val_loss: 0.4063\n",
            "Epoch 1974/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4003\n",
            "Epoch 1975/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4034\n",
            "Epoch 1976/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4002\n",
            "Epoch 1977/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4004\n",
            "Epoch 1978/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4042\n",
            "Epoch 1979/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4042\n",
            "Epoch 1980/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4054\n",
            "Epoch 1981/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4011\n",
            "Epoch 1982/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.4057\n",
            "Epoch 1983/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4008\n",
            "Epoch 1984/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4026\n",
            "Epoch 1985/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3998\n",
            "Epoch 1986/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4029\n",
            "Epoch 1987/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3693 - val_loss: 0.4021\n",
            "Epoch 1988/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.4055\n",
            "Epoch 1989/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3685 - val_loss: 0.4014\n",
            "Epoch 1990/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3687 - val_loss: 0.4043\n",
            "Epoch 1991/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4052\n",
            "Epoch 1992/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.4040\n",
            "Epoch 1993/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4046\n",
            "Epoch 1994/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4037\n",
            "Epoch 1995/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4077\n",
            "Epoch 1996/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4068\n",
            "Epoch 1997/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4034\n",
            "Epoch 1998/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4028\n",
            "Epoch 1999/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4002\n",
            "Epoch 2000/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4026\n",
            "Epoch 2001/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4008\n",
            "Epoch 2002/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4067\n",
            "Epoch 2003/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4028\n",
            "Epoch 2004/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3690 - val_loss: 0.3995\n",
            "Epoch 2005/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3692 - val_loss: 0.4026\n",
            "Epoch 2006/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3682 - val_loss: 0.4026\n",
            "Epoch 2007/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3695 - val_loss: 0.4009\n",
            "Epoch 2008/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4020\n",
            "Epoch 2009/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4029\n",
            "Epoch 2010/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4008\n",
            "Epoch 2011/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.3983\n",
            "Epoch 2012/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4003\n",
            "Epoch 2013/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.3979\n",
            "Epoch 2014/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4039\n",
            "Epoch 2015/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4027\n",
            "Epoch 2016/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4078\n",
            "Epoch 2017/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.4061\n",
            "Epoch 2018/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.4003\n",
            "Epoch 2019/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.4080\n",
            "Epoch 2020/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4074\n",
            "Epoch 2021/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4051\n",
            "Epoch 2022/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3709 - val_loss: 0.4083\n",
            "Epoch 2023/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3692 - val_loss: 0.4071\n",
            "Epoch 2024/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3685 - val_loss: 0.4037\n",
            "Epoch 2025/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3684 - val_loss: 0.4030\n",
            "Epoch 2026/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.4026\n",
            "Epoch 2027/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4052\n",
            "Epoch 2028/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3681 - val_loss: 0.4043\n",
            "Epoch 2029/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4056\n",
            "Epoch 2030/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4031\n",
            "Epoch 2031/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3983\n",
            "Epoch 2032/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4048\n",
            "Epoch 2033/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4030\n",
            "Epoch 2034/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4052\n",
            "Epoch 2035/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4061\n",
            "Epoch 2036/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4062\n",
            "Epoch 2037/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4057\n",
            "Epoch 2038/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4040\n",
            "Epoch 2039/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3686 - val_loss: 0.4049\n",
            "Epoch 2040/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3688 - val_loss: 0.3997\n",
            "Epoch 2041/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.4021\n",
            "Epoch 2042/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3683 - val_loss: 0.4045\n",
            "Epoch 2043/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4086\n",
            "Epoch 2044/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4024\n",
            "Epoch 2045/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4058\n",
            "Epoch 2046/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4067\n",
            "Epoch 2047/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4070\n",
            "Epoch 2048/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4068\n",
            "Epoch 2049/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4069\n",
            "Epoch 2050/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4053\n",
            "Epoch 2051/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4024\n",
            "Epoch 2052/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4018\n",
            "Epoch 2053/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4014\n",
            "Epoch 2054/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4032\n",
            "Epoch 2055/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4044\n",
            "Epoch 2056/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4049\n",
            "Epoch 2057/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.4022\n",
            "Epoch 2058/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3688 - val_loss: 0.4085\n",
            "Epoch 2059/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.4044\n",
            "Epoch 2060/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3700 - val_loss: 0.4035\n",
            "Epoch 2061/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3690 - val_loss: 0.4091\n",
            "Epoch 2062/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.4051\n",
            "Epoch 2063/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3998\n",
            "Epoch 2064/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.4027\n",
            "Epoch 2065/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3997\n",
            "Epoch 2066/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4032\n",
            "Epoch 2067/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4005\n",
            "Epoch 2068/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4060\n",
            "Epoch 2069/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4034\n",
            "Epoch 2070/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4032\n",
            "Epoch 2071/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4031\n",
            "Epoch 2072/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.4047\n",
            "Epoch 2073/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.4044\n",
            "Epoch 2074/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4064\n",
            "Epoch 2075/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4030\n",
            "Epoch 2076/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3703 - val_loss: 0.4029\n",
            "Epoch 2077/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4000\n",
            "Epoch 2078/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3689 - val_loss: 0.4025\n",
            "Epoch 2079/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4020\n",
            "Epoch 2080/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3698 - val_loss: 0.4081\n",
            "Epoch 2081/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4034\n",
            "Epoch 2082/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4059\n",
            "Epoch 2083/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.4037\n",
            "Epoch 2084/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4042\n",
            "Epoch 2085/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4039\n",
            "Epoch 2086/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4058\n",
            "Epoch 2087/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4029\n",
            "Epoch 2088/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3678 - val_loss: 0.4053\n",
            "Epoch 2089/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4044\n",
            "Epoch 2090/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4057\n",
            "Epoch 2091/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.4006\n",
            "Epoch 2092/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4053\n",
            "Epoch 2093/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4044\n",
            "Epoch 2094/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3704 - val_loss: 0.4051\n",
            "Epoch 2095/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3705 - val_loss: 0.4076\n",
            "Epoch 2096/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.4029\n",
            "Epoch 2097/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3701 - val_loss: 0.4026\n",
            "Epoch 2098/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4031\n",
            "Epoch 2099/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4021\n",
            "Epoch 2100/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4036\n",
            "Epoch 2101/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4003\n",
            "Epoch 2102/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.3995\n",
            "Epoch 2103/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4005\n",
            "Epoch 2104/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4016\n",
            "Epoch 2105/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.3994\n",
            "Epoch 2106/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4063\n",
            "Epoch 2107/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4074\n",
            "Epoch 2108/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4049\n",
            "Epoch 2109/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4011\n",
            "Epoch 2110/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4054\n",
            "Epoch 2111/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4005\n",
            "Epoch 2112/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3684 - val_loss: 0.4065\n",
            "Epoch 2113/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3687 - val_loss: 0.4030\n",
            "Epoch 2114/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3692 - val_loss: 0.4024\n",
            "Epoch 2115/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3698 - val_loss: 0.4051\n",
            "Epoch 2116/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4076\n",
            "Epoch 2117/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4097\n",
            "Epoch 2118/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3680 - val_loss: 0.4060\n",
            "Epoch 2119/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4034\n",
            "Epoch 2120/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4071\n",
            "Epoch 2121/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4072\n",
            "Epoch 2122/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4077\n",
            "Epoch 2123/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4040\n",
            "Epoch 2124/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4048\n",
            "Epoch 2125/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4044\n",
            "Epoch 2126/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4038\n",
            "Epoch 2127/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4058\n",
            "Epoch 2128/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4059\n",
            "Epoch 2129/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3683 - val_loss: 0.4054\n",
            "Epoch 2130/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3683 - val_loss: 0.4118\n",
            "Epoch 2131/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3691 - val_loss: 0.4014\n",
            "Epoch 2132/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3685 - val_loss: 0.4077\n",
            "Epoch 2133/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3683 - val_loss: 0.4085\n",
            "Epoch 2134/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.4081\n",
            "Epoch 2135/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4075\n",
            "Epoch 2136/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4051\n",
            "Epoch 2137/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.4036\n",
            "Epoch 2138/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4034\n",
            "Epoch 2139/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4069\n",
            "Epoch 2140/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4072\n",
            "Epoch 2141/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4049\n",
            "Epoch 2142/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4066\n",
            "Epoch 2143/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4052\n",
            "Epoch 2144/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.4015\n",
            "Epoch 2145/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4012\n",
            "Epoch 2146/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.4078\n",
            "Epoch 2147/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3706 - val_loss: 0.4061\n",
            "Epoch 2148/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3713 - val_loss: 0.4014\n",
            "Epoch 2149/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.4032\n",
            "Epoch 2150/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3692 - val_loss: 0.4027\n",
            "Epoch 2151/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4010\n",
            "Epoch 2152/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4033\n",
            "Epoch 2153/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4037\n",
            "Epoch 2154/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4052\n",
            "Epoch 2155/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4059\n",
            "Epoch 2156/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4055\n",
            "Epoch 2157/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4044\n",
            "Epoch 2158/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4075\n",
            "Epoch 2159/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4029\n",
            "Epoch 2160/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4009\n",
            "Epoch 2161/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4037\n",
            "Epoch 2162/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4010\n",
            "Epoch 2163/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3680 - val_loss: 0.4074\n",
            "Epoch 2164/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3688 - val_loss: 0.4044\n",
            "Epoch 2165/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3687 - val_loss: 0.4056\n",
            "Epoch 2166/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3685 - val_loss: 0.4063\n",
            "Epoch 2167/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3680 - val_loss: 0.4058\n",
            "Epoch 2168/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4062\n",
            "Epoch 2169/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4030\n",
            "Epoch 2170/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4043\n",
            "Epoch 2171/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4021\n",
            "Epoch 2172/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4045\n",
            "Epoch 2173/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.4084\n",
            "Epoch 2174/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4040\n",
            "Epoch 2175/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4069\n",
            "Epoch 2176/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4016\n",
            "Epoch 2177/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4028\n",
            "Epoch 2178/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4086\n",
            "Epoch 2179/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4051\n",
            "Epoch 2180/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4070\n",
            "Epoch 2181/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3682 - val_loss: 0.4061\n",
            "Epoch 2182/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.4055\n",
            "Epoch 2183/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3681 - val_loss: 0.4013\n",
            "Epoch 2184/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3682 - val_loss: 0.4033\n",
            "Epoch 2185/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4029\n",
            "Epoch 2186/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4056\n",
            "Epoch 2187/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4052\n",
            "Epoch 2188/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4095\n",
            "Epoch 2189/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4101\n",
            "Epoch 2190/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4102\n",
            "Epoch 2191/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4046\n",
            "Epoch 2192/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4053\n",
            "Epoch 2193/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4077\n",
            "Epoch 2194/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4068\n",
            "Epoch 2195/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4049\n",
            "Epoch 2196/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4057\n",
            "Epoch 2197/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4019\n",
            "Epoch 2198/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3681 - val_loss: 0.4050\n",
            "Epoch 2199/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4066\n",
            "Epoch 2200/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3680 - val_loss: 0.4096\n",
            "Epoch 2201/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3677 - val_loss: 0.4092\n",
            "Epoch 2202/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3689 - val_loss: 0.4056\n",
            "Epoch 2203/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3683 - val_loss: 0.4039\n",
            "Epoch 2204/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4044\n",
            "Epoch 2205/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4140\n",
            "Epoch 2206/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4074\n",
            "Epoch 2207/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4061\n",
            "Epoch 2208/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4048\n",
            "Epoch 2209/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4047\n",
            "Epoch 2210/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4068\n",
            "Epoch 2211/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4070\n",
            "Epoch 2212/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3675 - val_loss: 0.4066\n",
            "Epoch 2213/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3676 - val_loss: 0.4101\n",
            "Epoch 2214/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4026\n",
            "Epoch 2215/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4079\n",
            "Epoch 2216/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3680 - val_loss: 0.4074\n",
            "Epoch 2217/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3722 - val_loss: 0.4109\n",
            "Epoch 2218/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3716 - val_loss: 0.4078\n",
            "Epoch 2219/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3704 - val_loss: 0.4088\n",
            "Epoch 2220/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.4021\n",
            "Epoch 2221/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.4043\n",
            "Epoch 2222/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.4025\n",
            "Epoch 2223/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4033\n",
            "Epoch 2224/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4057\n",
            "Epoch 2225/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3675 - val_loss: 0.4078\n",
            "Epoch 2226/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4086\n",
            "Epoch 2227/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4069\n",
            "Epoch 2228/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.4112\n",
            "Epoch 2229/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4097\n",
            "Epoch 2230/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4102\n",
            "Epoch 2231/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4069\n",
            "Epoch 2232/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4059\n",
            "Epoch 2233/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3681 - val_loss: 0.4061\n",
            "Epoch 2234/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3693 - val_loss: 0.4051\n",
            "Epoch 2235/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3689 - val_loss: 0.4085\n",
            "Epoch 2236/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3700 - val_loss: 0.4058\n",
            "Epoch 2237/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3729 - val_loss: 0.4059\n",
            "Epoch 2238/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.4070\n",
            "Epoch 2239/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.4048\n",
            "Epoch 2240/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3997\n",
            "Epoch 2241/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4040\n",
            "Epoch 2242/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4000\n",
            "Epoch 2243/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4020\n",
            "Epoch 2244/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4070\n",
            "Epoch 2245/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3671 - val_loss: 0.4064\n",
            "Epoch 2246/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3671 - val_loss: 0.4051\n",
            "Epoch 2247/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4079\n",
            "Epoch 2248/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3680 - val_loss: 0.4052\n",
            "Epoch 2249/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4057\n",
            "Epoch 2250/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4065\n",
            "Epoch 2251/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3686 - val_loss: 0.4059\n",
            "Epoch 2252/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3676 - val_loss: 0.4062\n",
            "Epoch 2253/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3674 - val_loss: 0.4045\n",
            "Epoch 2254/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3681 - val_loss: 0.4058\n",
            "Epoch 2255/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4041\n",
            "Epoch 2256/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4044\n",
            "Epoch 2257/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4030\n",
            "Epoch 2258/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4068\n",
            "Epoch 2259/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3676 - val_loss: 0.4080\n",
            "Epoch 2260/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4038\n",
            "Epoch 2261/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4047\n",
            "Epoch 2262/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4076\n",
            "Epoch 2263/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4053\n",
            "Epoch 2264/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4033\n",
            "Epoch 2265/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3669 - val_loss: 0.4051\n",
            "Epoch 2266/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4109\n",
            "Epoch 2267/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4028\n",
            "Epoch 2268/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3676 - val_loss: 0.4020\n",
            "Epoch 2269/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3681 - val_loss: 0.4060\n",
            "Epoch 2270/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3752 - val_loss: 0.4017\n",
            "Epoch 2271/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3729 - val_loss: 0.4048\n",
            "Epoch 2272/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.4060\n",
            "Epoch 2273/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.4021\n",
            "Epoch 2274/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3984\n",
            "Epoch 2275/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4037\n",
            "Epoch 2276/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4013\n",
            "Epoch 2277/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.3976\n",
            "Epoch 2278/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4005\n",
            "Epoch 2279/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4022\n",
            "Epoch 2280/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3675 - val_loss: 0.4005\n",
            "Epoch 2281/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4046\n",
            "Epoch 2282/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.4082\n",
            "Epoch 2283/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4037\n",
            "Epoch 2284/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3673 - val_loss: 0.4006\n",
            "Epoch 2285/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3668 - val_loss: 0.4023\n",
            "Epoch 2286/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3674 - val_loss: 0.4056\n",
            "Epoch 2287/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3670 - val_loss: 0.4057\n",
            "Epoch 2288/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3681 - val_loss: 0.4037\n",
            "Epoch 2289/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4039\n",
            "Epoch 2290/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4014\n",
            "Epoch 2291/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4050\n",
            "Epoch 2292/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4076\n",
            "Epoch 2293/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4065\n",
            "Epoch 2294/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4009\n",
            "Epoch 2295/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4069\n",
            "Epoch 2296/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3675 - val_loss: 0.4027\n",
            "Epoch 2297/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4081\n",
            "Epoch 2298/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4017\n",
            "Epoch 2299/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4036\n",
            "Epoch 2300/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3678 - val_loss: 0.4017\n",
            "Epoch 2301/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4017\n",
            "Epoch 2302/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3675 - val_loss: 0.4059\n",
            "Epoch 2303/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3681 - val_loss: 0.4047\n",
            "Epoch 2304/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3675 - val_loss: 0.4034\n",
            "Epoch 2305/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3681 - val_loss: 0.4051\n",
            "Epoch 2306/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3677 - val_loss: 0.4045\n",
            "Epoch 2307/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4053\n",
            "Epoch 2308/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4063\n",
            "Epoch 2309/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4062\n",
            "Epoch 2310/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4096\n",
            "Epoch 2311/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4051\n",
            "Epoch 2312/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4078\n",
            "Epoch 2313/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4042\n",
            "Epoch 2314/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4033\n",
            "Epoch 2315/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4052\n",
            "Epoch 2316/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4032\n",
            "Epoch 2317/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4041\n",
            "Epoch 2318/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4066\n",
            "Epoch 2319/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4037\n",
            "Epoch 2320/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3671 - val_loss: 0.4063\n",
            "Epoch 2321/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3665 - val_loss: 0.4083\n",
            "Epoch 2322/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3687 - val_loss: 0.4041\n",
            "Epoch 2323/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3677 - val_loss: 0.4040\n",
            "Epoch 2324/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3673 - val_loss: 0.4052\n",
            "Epoch 2325/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4077\n",
            "Epoch 2326/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4063\n",
            "Epoch 2327/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4083\n",
            "Epoch 2328/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4062\n",
            "Epoch 2329/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4058\n",
            "Epoch 2330/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3676 - val_loss: 0.4039\n",
            "Epoch 2331/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4044\n",
            "Epoch 2332/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4027\n",
            "Epoch 2333/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4081\n",
            "Epoch 2334/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4086\n",
            "Epoch 2335/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.4018\n",
            "Epoch 2336/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4047\n",
            "Epoch 2337/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3685 - val_loss: 0.4056\n",
            "Epoch 2338/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3687 - val_loss: 0.4052\n",
            "Epoch 2339/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3692 - val_loss: 0.4026\n",
            "Epoch 2340/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3716 - val_loss: 0.4001\n",
            "Epoch 2341/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3684 - val_loss: 0.4051\n",
            "Epoch 2342/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4045\n",
            "Epoch 2343/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4039\n",
            "Epoch 2344/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4060\n",
            "Epoch 2345/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4057\n",
            "Epoch 2346/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3669 - val_loss: 0.4064\n",
            "Epoch 2347/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4025\n",
            "Epoch 2348/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4035\n",
            "Epoch 2349/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4024\n",
            "Epoch 2350/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3668 - val_loss: 0.4043\n",
            "Epoch 2351/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4063\n",
            "Epoch 2352/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4072\n",
            "Epoch 2353/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4085\n",
            "Epoch 2354/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3974\n",
            "Epoch 2355/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3678 - val_loss: 0.4038\n",
            "Epoch 2356/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3668 - val_loss: 0.3988\n",
            "Epoch 2357/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3667 - val_loss: 0.4024\n",
            "Epoch 2358/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3678 - val_loss: 0.4111\n",
            "Epoch 2359/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3679 - val_loss: 0.4039\n",
            "Epoch 2360/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4059\n",
            "Epoch 2361/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4017\n",
            "Epoch 2362/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4083\n",
            "Epoch 2363/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4063\n",
            "Epoch 2364/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4053\n",
            "Epoch 2365/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4053\n",
            "Epoch 2366/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4070\n",
            "Epoch 2367/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4031\n",
            "Epoch 2368/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4062\n",
            "Epoch 2369/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4060\n",
            "Epoch 2370/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4044\n",
            "Epoch 2371/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4046\n",
            "Epoch 2372/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3669 - val_loss: 0.4045\n",
            "Epoch 2373/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3667 - val_loss: 0.4073\n",
            "Epoch 2374/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3689 - val_loss: 0.4062\n",
            "Epoch 2375/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3674 - val_loss: 0.4014\n",
            "Epoch 2376/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4038\n",
            "Epoch 2377/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4036\n",
            "Epoch 2378/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4017\n",
            "Epoch 2379/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3670 - val_loss: 0.4041\n",
            "Epoch 2380/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4047\n",
            "Epoch 2381/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4049\n",
            "Epoch 2382/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.4072\n",
            "Epoch 2383/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4022\n",
            "Epoch 2384/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4052\n",
            "Epoch 2385/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4016\n",
            "Epoch 2386/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4029\n",
            "Epoch 2387/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4051\n",
            "Epoch 2388/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4021\n",
            "Epoch 2389/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3682 - val_loss: 0.4043\n",
            "Epoch 2390/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3676 - val_loss: 0.4018\n",
            "Epoch 2391/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3680 - val_loss: 0.4056\n",
            "Epoch 2392/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3684 - val_loss: 0.4059\n",
            "Epoch 2393/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3674 - val_loss: 0.4081\n",
            "Epoch 2394/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4065\n",
            "Epoch 2395/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4022\n",
            "Epoch 2396/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4055\n",
            "Epoch 2397/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4057\n",
            "Epoch 2398/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4076\n",
            "Epoch 2399/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4049\n",
            "Epoch 2400/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4080\n",
            "Epoch 2401/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4056\n",
            "Epoch 2402/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4086\n",
            "Epoch 2403/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4033\n",
            "Epoch 2404/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4057\n",
            "Epoch 2405/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3675 - val_loss: 0.4066\n",
            "Epoch 2406/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3668 - val_loss: 0.4069\n",
            "Epoch 2407/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3677 - val_loss: 0.4086\n",
            "Epoch 2408/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3682 - val_loss: 0.4076\n",
            "Epoch 2409/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3691 - val_loss: 0.4026\n",
            "Epoch 2410/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.4033\n",
            "Epoch 2411/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4042\n",
            "Epoch 2412/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4031\n",
            "Epoch 2413/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4044\n",
            "Epoch 2414/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4047\n",
            "Epoch 2415/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4068\n",
            "Epoch 2416/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4070\n",
            "Epoch 2417/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4088\n",
            "Epoch 2418/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3668 - val_loss: 0.4045\n",
            "Epoch 2419/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4074\n",
            "Epoch 2420/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4041\n",
            "Epoch 2421/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4052\n",
            "Epoch 2422/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3681 - val_loss: 0.4051\n",
            "Epoch 2423/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3686 - val_loss: 0.4073\n",
            "Epoch 2424/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3688 - val_loss: 0.4024\n",
            "Epoch 2425/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3679 - val_loss: 0.4027\n",
            "Epoch 2426/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3680 - val_loss: 0.4008\n",
            "Epoch 2427/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4101\n",
            "Epoch 2428/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3668 - val_loss: 0.4066\n",
            "Epoch 2429/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4040\n",
            "Epoch 2430/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4055\n",
            "Epoch 2431/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4038\n",
            "Epoch 2432/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4087\n",
            "Epoch 2433/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4105\n",
            "Epoch 2434/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4097\n",
            "Epoch 2435/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4073\n",
            "Epoch 2436/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4085\n",
            "Epoch 2437/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4088\n",
            "Epoch 2438/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4050\n",
            "Epoch 2439/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4032\n",
            "Epoch 2440/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3672 - val_loss: 0.4065\n",
            "Epoch 2441/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3661 - val_loss: 0.4059\n",
            "Epoch 2442/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4043\n",
            "Epoch 2443/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.4036\n",
            "Epoch 2444/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3683 - val_loss: 0.4086\n",
            "Epoch 2445/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3685 - val_loss: 0.4070\n",
            "Epoch 2446/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4105\n",
            "Epoch 2447/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4107\n",
            "Epoch 2448/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4067\n",
            "Epoch 2449/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4078\n",
            "Epoch 2450/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4117\n",
            "Epoch 2451/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3669 - val_loss: 0.4056\n",
            "Epoch 2452/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4055\n",
            "Epoch 2453/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4080\n",
            "Epoch 2454/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4036\n",
            "Epoch 2455/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4041\n",
            "Epoch 2456/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3662 - val_loss: 0.4099\n",
            "Epoch 2457/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3674 - val_loss: 0.4075\n",
            "Epoch 2458/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3684 - val_loss: 0.4050\n",
            "Epoch 2459/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3684 - val_loss: 0.4059\n",
            "Epoch 2460/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3668 - val_loss: 0.4093\n",
            "Epoch 2461/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3673 - val_loss: 0.4063\n",
            "Epoch 2462/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3670 - val_loss: 0.4031\n",
            "Epoch 2463/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4063\n",
            "Epoch 2464/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4076\n",
            "Epoch 2465/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4128\n",
            "Epoch 2466/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4126\n",
            "Epoch 2467/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4081\n",
            "Epoch 2468/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4134\n",
            "Epoch 2469/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4075\n",
            "Epoch 2470/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4074\n",
            "Epoch 2471/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4106\n",
            "Epoch 2472/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4095\n",
            "Epoch 2473/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4095\n",
            "Epoch 2474/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3665 - val_loss: 0.4057\n",
            "Epoch 2475/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3680 - val_loss: 0.4074\n",
            "Epoch 2476/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3673 - val_loss: 0.4097\n",
            "Epoch 2477/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.4047\n",
            "Epoch 2478/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3670 - val_loss: 0.4086\n",
            "Epoch 2479/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4044\n",
            "Epoch 2480/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4087\n",
            "Epoch 2481/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.4147\n",
            "Epoch 2482/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4054\n",
            "Epoch 2483/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4107\n",
            "Epoch 2484/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3685 - val_loss: 0.4129\n",
            "Epoch 2485/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4064\n",
            "Epoch 2486/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4082\n",
            "Epoch 2487/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4104\n",
            "Epoch 2488/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4105\n",
            "Epoch 2489/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4094\n",
            "Epoch 2490/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4081\n",
            "Epoch 2491/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3673 - val_loss: 0.4047\n",
            "Epoch 2492/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3672 - val_loss: 0.4099\n",
            "Epoch 2493/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3670 - val_loss: 0.4097\n",
            "Epoch 2494/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3676 - val_loss: 0.4029\n",
            "Epoch 2495/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4021\n",
            "Epoch 2496/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4093\n",
            "Epoch 2497/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4079\n",
            "Epoch 2498/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4084\n",
            "Epoch 2499/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4097\n",
            "Epoch 2500/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4077\n",
            "Epoch 2501/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4085\n",
            "Epoch 2502/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3675 - val_loss: 0.4004\n",
            "Epoch 2503/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4084\n",
            "Epoch 2504/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4080\n",
            "Epoch 2505/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4068\n",
            "Epoch 2506/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4063\n",
            "Epoch 2507/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3673 - val_loss: 0.4049\n",
            "Epoch 2508/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3667 - val_loss: 0.4094\n",
            "Epoch 2509/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3661 - val_loss: 0.4073\n",
            "Epoch 2510/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3661 - val_loss: 0.4069\n",
            "Epoch 2511/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3661 - val_loss: 0.4092\n",
            "Epoch 2512/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4067\n",
            "Epoch 2513/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4049\n",
            "Epoch 2514/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4097\n",
            "Epoch 2515/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4125\n",
            "Epoch 2516/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4123\n",
            "Epoch 2517/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4074\n",
            "Epoch 2518/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4061\n",
            "Epoch 2519/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4081\n",
            "Epoch 2520/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4084\n",
            "Epoch 2521/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4056\n",
            "Epoch 2522/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4088\n",
            "Epoch 2523/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4054\n",
            "Epoch 2524/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4086\n",
            "Epoch 2525/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3663 - val_loss: 0.4061\n",
            "Epoch 2526/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3679 - val_loss: 0.4098\n",
            "Epoch 2527/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3692 - val_loss: 0.4030\n",
            "Epoch 2528/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3693 - val_loss: 0.4066\n",
            "Epoch 2529/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4062\n",
            "Epoch 2530/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4040\n",
            "Epoch 2531/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.4054\n",
            "Epoch 2532/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4119\n",
            "Epoch 2533/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4041\n",
            "Epoch 2534/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4100\n",
            "Epoch 2535/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3661 - val_loss: 0.4071\n",
            "Epoch 2536/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4111\n",
            "Epoch 2537/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3670 - val_loss: 0.4110\n",
            "Epoch 2538/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3669 - val_loss: 0.4109\n",
            "Epoch 2539/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4081\n",
            "Epoch 2540/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3667 - val_loss: 0.4101\n",
            "Epoch 2541/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3678 - val_loss: 0.4080\n",
            "Epoch 2542/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3663 - val_loss: 0.4105\n",
            "Epoch 2543/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3667 - val_loss: 0.4107\n",
            "Epoch 2544/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4111\n",
            "Epoch 2545/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3679 - val_loss: 0.4066\n",
            "Epoch 2546/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3661 - val_loss: 0.4037\n",
            "Epoch 2547/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4164\n",
            "Epoch 2548/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4052\n",
            "Epoch 2549/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4142\n",
            "Epoch 2550/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4103\n",
            "Epoch 2551/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3675 - val_loss: 0.4106\n",
            "Epoch 2552/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4127\n",
            "Epoch 2553/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4065\n",
            "Epoch 2554/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4110\n",
            "Epoch 2555/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4092\n",
            "Epoch 2556/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4071\n",
            "Epoch 2557/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4082\n",
            "Epoch 2558/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4060\n",
            "Epoch 2559/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3667 - val_loss: 0.4068\n",
            "Epoch 2560/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3668 - val_loss: 0.4061\n",
            "Epoch 2561/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3662 - val_loss: 0.4124\n",
            "Epoch 2562/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3670 - val_loss: 0.4091\n",
            "Epoch 2563/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4084\n",
            "Epoch 2564/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4086\n",
            "Epoch 2565/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3671 - val_loss: 0.4103\n",
            "Epoch 2566/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4064\n",
            "Epoch 2567/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4050\n",
            "Epoch 2568/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4114\n",
            "Epoch 2569/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4079\n",
            "Epoch 2570/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3668 - val_loss: 0.4065\n",
            "Epoch 2571/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4085\n",
            "Epoch 2572/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4016\n",
            "Epoch 2573/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4117\n",
            "Epoch 2574/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4079\n",
            "Epoch 2575/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4118\n",
            "Epoch 2576/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3665 - val_loss: 0.4082\n",
            "Epoch 2577/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3663 - val_loss: 0.4065\n",
            "Epoch 2578/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3655 - val_loss: 0.4027\n",
            "Epoch 2579/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3659 - val_loss: 0.4065\n",
            "Epoch 2580/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3659 - val_loss: 0.4038\n",
            "Epoch 2581/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4082\n",
            "Epoch 2582/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3712 - val_loss: 0.4076\n",
            "Epoch 2583/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4076\n",
            "Epoch 2584/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3678 - val_loss: 0.4077\n",
            "Epoch 2585/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4088\n",
            "Epoch 2586/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.3996\n",
            "Epoch 2587/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4017\n",
            "Epoch 2588/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4100\n",
            "Epoch 2589/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4057\n",
            "Epoch 2590/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4099\n",
            "Epoch 2591/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4045\n",
            "Epoch 2592/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4077\n",
            "Epoch 2593/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.4048\n",
            "Epoch 2594/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.4070\n",
            "Epoch 2595/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4083\n",
            "Epoch 2596/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3667 - val_loss: 0.4086\n",
            "Epoch 2597/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.4052\n",
            "Epoch 2598/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3695 - val_loss: 0.4106\n",
            "Epoch 2599/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3676 - val_loss: 0.4078\n",
            "Epoch 2600/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4059\n",
            "Epoch 2601/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4105\n",
            "Epoch 2602/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3669 - val_loss: 0.4115\n",
            "Epoch 2603/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4051\n",
            "Epoch 2604/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4066\n",
            "Epoch 2605/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4046\n",
            "Epoch 2606/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4076\n",
            "Epoch 2607/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4101\n",
            "Epoch 2608/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4047\n",
            "Epoch 2609/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4086\n",
            "Epoch 2610/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3663 - val_loss: 0.4104\n",
            "Epoch 2611/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4086\n",
            "Epoch 2612/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3668 - val_loss: 0.4064\n",
            "Epoch 2613/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3669 - val_loss: 0.4099\n",
            "Epoch 2614/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3675 - val_loss: 0.4077\n",
            "Epoch 2615/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3665 - val_loss: 0.4082\n",
            "Epoch 2616/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4084\n",
            "Epoch 2617/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3680 - val_loss: 0.4041\n",
            "Epoch 2618/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4066\n",
            "Epoch 2619/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3664 - val_loss: 0.4085\n",
            "Epoch 2620/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4071\n",
            "Epoch 2621/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4097\n",
            "Epoch 2622/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4036\n",
            "Epoch 2623/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4077\n",
            "Epoch 2624/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3662 - val_loss: 0.4126\n",
            "Epoch 2625/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4137\n",
            "Epoch 2626/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4098\n",
            "Epoch 2627/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4095\n",
            "Epoch 2628/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4090\n",
            "Epoch 2629/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3666 - val_loss: 0.4126\n",
            "Epoch 2630/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3676 - val_loss: 0.4094\n",
            "Epoch 2631/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3670 - val_loss: 0.4114\n",
            "Epoch 2632/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3669 - val_loss: 0.4099\n",
            "Epoch 2633/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4100\n",
            "Epoch 2634/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4109\n",
            "Epoch 2635/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4113\n",
            "Epoch 2636/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4093\n",
            "Epoch 2637/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4112\n",
            "Epoch 2638/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4139\n",
            "Epoch 2639/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4116\n",
            "Epoch 2640/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4105\n",
            "Epoch 2641/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4059\n",
            "Epoch 2642/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4085\n",
            "Epoch 2643/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4074\n",
            "Epoch 2644/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4069\n",
            "Epoch 2645/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3672 - val_loss: 0.4117\n",
            "Epoch 2646/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3691 - val_loss: 0.4118\n",
            "Epoch 2647/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.4104\n",
            "Epoch 2648/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4083\n",
            "Epoch 2649/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3684 - val_loss: 0.4062\n",
            "Epoch 2650/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3679 - val_loss: 0.4075\n",
            "Epoch 2651/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4079\n",
            "Epoch 2652/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4099\n",
            "Epoch 2653/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3664 - val_loss: 0.4099\n",
            "Epoch 2654/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4088\n",
            "Epoch 2655/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3663 - val_loss: 0.4114\n",
            "Epoch 2656/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4058\n",
            "Epoch 2657/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4125\n",
            "Epoch 2658/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4114\n",
            "Epoch 2659/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4107\n",
            "Epoch 2660/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4095\n",
            "Epoch 2661/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4075\n",
            "Epoch 2662/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4042\n",
            "Epoch 2663/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3667 - val_loss: 0.4068\n",
            "Epoch 2664/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3661 - val_loss: 0.4121\n",
            "Epoch 2665/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3669 - val_loss: 0.4096\n",
            "Epoch 2666/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3666 - val_loss: 0.4078\n",
            "Epoch 2667/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4132\n",
            "Epoch 2668/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4075\n",
            "Epoch 2669/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4079\n",
            "Epoch 2670/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4079\n",
            "Epoch 2671/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4096\n",
            "Epoch 2672/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4063\n",
            "Epoch 2673/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4053\n",
            "Epoch 2674/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4091\n",
            "Epoch 2675/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4065\n",
            "Epoch 2676/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3661 - val_loss: 0.4102\n",
            "Epoch 2677/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4087\n",
            "Epoch 2678/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4078\n",
            "Epoch 2679/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3657 - val_loss: 0.4129\n",
            "Epoch 2680/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3659 - val_loss: 0.4138\n",
            "Epoch 2681/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3653 - val_loss: 0.4132\n",
            "Epoch 2682/3000\n",
            "70/70 [==============================] - 1s 19ms/step - loss: 0.3663 - val_loss: 0.4114\n",
            "Epoch 2683/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3675 - val_loss: 0.4106\n",
            "Epoch 2684/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4108\n",
            "Epoch 2685/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4143\n",
            "Epoch 2686/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4098\n",
            "Epoch 2687/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4110\n",
            "Epoch 2688/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4100\n",
            "Epoch 2689/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4124\n",
            "Epoch 2690/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4118\n",
            "Epoch 2691/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4107\n",
            "Epoch 2692/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4140\n",
            "Epoch 2693/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3671 - val_loss: 0.4053\n",
            "Epoch 2694/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4130\n",
            "Epoch 2695/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4101\n",
            "Epoch 2696/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3657 - val_loss: 0.4094\n",
            "Epoch 2697/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3649 - val_loss: 0.4112\n",
            "Epoch 2698/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3657 - val_loss: 0.4125\n",
            "Epoch 2699/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3664 - val_loss: 0.4107\n",
            "Epoch 2700/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3662 - val_loss: 0.4133\n",
            "Epoch 2701/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4123\n",
            "Epoch 2702/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4081\n",
            "Epoch 2703/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4128\n",
            "Epoch 2704/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4106\n",
            "Epoch 2705/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3667 - val_loss: 0.4093\n",
            "Epoch 2706/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4073\n",
            "Epoch 2707/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4109\n",
            "Epoch 2708/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4147\n",
            "Epoch 2709/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3698 - val_loss: 0.4092\n",
            "Epoch 2710/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4107\n",
            "Epoch 2711/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4170\n",
            "Epoch 2712/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4081\n",
            "Epoch 2713/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3722 - val_loss: 0.4127\n",
            "Epoch 2714/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3708 - val_loss: 0.4106\n",
            "Epoch 2715/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3683 - val_loss: 0.4104\n",
            "Epoch 2716/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3669 - val_loss: 0.4086\n",
            "Epoch 2717/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4077\n",
            "Epoch 2718/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3663 - val_loss: 0.4111\n",
            "Epoch 2719/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4099\n",
            "Epoch 2720/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4099\n",
            "Epoch 2721/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4092\n",
            "Epoch 2722/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4103\n",
            "Epoch 2723/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4069\n",
            "Epoch 2724/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4084\n",
            "Epoch 2725/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3654 - val_loss: 0.4082\n",
            "Epoch 2726/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4074\n",
            "Epoch 2727/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4045\n",
            "Epoch 2728/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4084\n",
            "Epoch 2729/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4074\n",
            "Epoch 2730/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3660 - val_loss: 0.4087\n",
            "Epoch 2731/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3665 - val_loss: 0.4071\n",
            "Epoch 2732/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3689 - val_loss: 0.4073\n",
            "Epoch 2733/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3698 - val_loss: 0.4112\n",
            "Epoch 2734/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4074\n",
            "Epoch 2735/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4101\n",
            "Epoch 2736/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4063\n",
            "Epoch 2737/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4065\n",
            "Epoch 2738/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4096\n",
            "Epoch 2739/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4066\n",
            "Epoch 2740/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4043\n",
            "Epoch 2741/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3664 - val_loss: 0.4058\n",
            "Epoch 2742/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4123\n",
            "Epoch 2743/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4091\n",
            "Epoch 2744/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3664 - val_loss: 0.4120\n",
            "Epoch 2745/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4072\n",
            "Epoch 2746/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3664 - val_loss: 0.4107\n",
            "Epoch 2747/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3658 - val_loss: 0.4105\n",
            "Epoch 2748/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3664 - val_loss: 0.4079\n",
            "Epoch 2749/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3657 - val_loss: 0.4094\n",
            "Epoch 2750/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4121\n",
            "Epoch 2751/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3650 - val_loss: 0.4061\n",
            "Epoch 2752/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4140\n",
            "Epoch 2753/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4138\n",
            "Epoch 2754/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4162\n",
            "Epoch 2755/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.4023\n",
            "Epoch 2756/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4065\n",
            "Epoch 2757/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4035\n",
            "Epoch 2758/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4067\n",
            "Epoch 2759/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4077\n",
            "Epoch 2760/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4040\n",
            "Epoch 2761/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4054\n",
            "Epoch 2762/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4066\n",
            "Epoch 2763/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3659 - val_loss: 0.4079\n",
            "Epoch 2764/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3650 - val_loss: 0.4116\n",
            "Epoch 2765/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3649 - val_loss: 0.4070\n",
            "Epoch 2766/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3654 - val_loss: 0.4089\n",
            "Epoch 2767/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4108\n",
            "Epoch 2768/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3651 - val_loss: 0.4084\n",
            "Epoch 2769/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4135\n",
            "Epoch 2770/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4092\n",
            "Epoch 2771/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4111\n",
            "Epoch 2772/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4078\n",
            "Epoch 2773/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4079\n",
            "Epoch 2774/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3662 - val_loss: 0.4079\n",
            "Epoch 2775/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4117\n",
            "Epoch 2776/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4064\n",
            "Epoch 2777/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4086\n",
            "Epoch 2778/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3653 - val_loss: 0.4093\n",
            "Epoch 2779/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4054\n",
            "Epoch 2780/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4113\n",
            "Epoch 2781/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3661 - val_loss: 0.4116\n",
            "Epoch 2782/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4102\n",
            "Epoch 2783/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3676 - val_loss: 0.4046\n",
            "Epoch 2784/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3660 - val_loss: 0.4098\n",
            "Epoch 2785/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4089\n",
            "Epoch 2786/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4068\n",
            "Epoch 2787/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4119\n",
            "Epoch 2788/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4098\n",
            "Epoch 2789/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3652 - val_loss: 0.4101\n",
            "Epoch 2790/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3649 - val_loss: 0.4101\n",
            "Epoch 2791/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3664 - val_loss: 0.4090\n",
            "Epoch 2792/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4115\n",
            "Epoch 2793/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4129\n",
            "Epoch 2794/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4101\n",
            "Epoch 2795/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4059\n",
            "Epoch 2796/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4110\n",
            "Epoch 2797/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3656 - val_loss: 0.4126\n",
            "Epoch 2798/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3673 - val_loss: 0.4149\n",
            "Epoch 2799/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3662 - val_loss: 0.4102\n",
            "Epoch 2800/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3663 - val_loss: 0.4174\n",
            "Epoch 2801/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3677 - val_loss: 0.4112\n",
            "Epoch 2802/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4110\n",
            "Epoch 2803/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4127\n",
            "Epoch 2804/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4102\n",
            "Epoch 2805/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4109\n",
            "Epoch 2806/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4093\n",
            "Epoch 2807/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4098\n",
            "Epoch 2808/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4128\n",
            "Epoch 2809/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4138\n",
            "Epoch 2810/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4163\n",
            "Epoch 2811/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4101\n",
            "Epoch 2812/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4135\n",
            "Epoch 2813/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4054\n",
            "Epoch 2814/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3653 - val_loss: 0.4137\n",
            "Epoch 2815/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3655 - val_loss: 0.4069\n",
            "Epoch 2816/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3659 - val_loss: 0.4086\n",
            "Epoch 2817/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3660 - val_loss: 0.4130\n",
            "Epoch 2818/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4120\n",
            "Epoch 2819/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3661 - val_loss: 0.4069\n",
            "Epoch 2820/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3660 - val_loss: 0.4063\n",
            "Epoch 2821/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4123\n",
            "Epoch 2822/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4123\n",
            "Epoch 2823/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3662 - val_loss: 0.4123\n",
            "Epoch 2824/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4089\n",
            "Epoch 2825/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3667 - val_loss: 0.4114\n",
            "Epoch 2826/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3653 - val_loss: 0.4075\n",
            "Epoch 2827/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4102\n",
            "Epoch 2828/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4075\n",
            "Epoch 2829/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4112\n",
            "Epoch 2830/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3659 - val_loss: 0.4102\n",
            "Epoch 2831/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3656 - val_loss: 0.4132\n",
            "Epoch 2832/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3670 - val_loss: 0.4112\n",
            "Epoch 2833/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3668 - val_loss: 0.4116\n",
            "Epoch 2834/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3669 - val_loss: 0.4138\n",
            "Epoch 2835/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3674 - val_loss: 0.4115\n",
            "Epoch 2836/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3679 - val_loss: 0.4081\n",
            "Epoch 2837/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4104\n",
            "Epoch 2838/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4047\n",
            "Epoch 2839/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4097\n",
            "Epoch 2840/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4116\n",
            "Epoch 2841/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4107\n",
            "Epoch 2842/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4087\n",
            "Epoch 2843/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3660 - val_loss: 0.4098\n",
            "Epoch 2844/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4131\n",
            "Epoch 2845/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4131\n",
            "Epoch 2846/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3675 - val_loss: 0.4157\n",
            "Epoch 2847/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3664 - val_loss: 0.4140\n",
            "Epoch 2848/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3655 - val_loss: 0.4118\n",
            "Epoch 2849/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3650 - val_loss: 0.4104\n",
            "Epoch 2850/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3660 - val_loss: 0.4122\n",
            "Epoch 2851/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4114\n",
            "Epoch 2852/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4121\n",
            "Epoch 2853/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4118\n",
            "Epoch 2854/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4097\n",
            "Epoch 2855/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4133\n",
            "Epoch 2856/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4093\n",
            "Epoch 2857/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4102\n",
            "Epoch 2858/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4114\n",
            "Epoch 2859/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3662 - val_loss: 0.4142\n",
            "Epoch 2860/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4104\n",
            "Epoch 2861/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3657 - val_loss: 0.4109\n",
            "Epoch 2862/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4066\n",
            "Epoch 2863/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4086\n",
            "Epoch 2864/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3673 - val_loss: 0.4105\n",
            "Epoch 2865/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3681 - val_loss: 0.4083\n",
            "Epoch 2866/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3667 - val_loss: 0.4073\n",
            "Epoch 2867/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3656 - val_loss: 0.4146\n",
            "Epoch 2868/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3669 - val_loss: 0.4118\n",
            "Epoch 2869/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4142\n",
            "Epoch 2870/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4117\n",
            "Epoch 2871/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4134\n",
            "Epoch 2872/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3652 - val_loss: 0.4087\n",
            "Epoch 2873/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4144\n",
            "Epoch 2874/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4126\n",
            "Epoch 2875/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4100\n",
            "Epoch 2876/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4062\n",
            "Epoch 2877/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4100\n",
            "Epoch 2878/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4121\n",
            "Epoch 2879/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4120\n",
            "Epoch 2880/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3667 - val_loss: 0.4121\n",
            "Epoch 2881/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4134\n",
            "Epoch 2882/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3666 - val_loss: 0.4093\n",
            "Epoch 2883/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3663 - val_loss: 0.4122\n",
            "Epoch 2884/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3662 - val_loss: 0.4129\n",
            "Epoch 2885/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3657 - val_loss: 0.4119\n",
            "Epoch 2886/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4079\n",
            "Epoch 2887/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4095\n",
            "Epoch 2888/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4070\n",
            "Epoch 2889/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4064\n",
            "Epoch 2890/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4067\n",
            "Epoch 2891/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4087\n",
            "Epoch 2892/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4114\n",
            "Epoch 2893/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4088\n",
            "Epoch 2894/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4079\n",
            "Epoch 2895/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4082\n",
            "Epoch 2896/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4136\n",
            "Epoch 2897/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3663 - val_loss: 0.4132\n",
            "Epoch 2898/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3663 - val_loss: 0.4098\n",
            "Epoch 2899/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3664 - val_loss: 0.4138\n",
            "Epoch 2900/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3662 - val_loss: 0.4112\n",
            "Epoch 2901/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3652 - val_loss: 0.4100\n",
            "Epoch 2902/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4103\n",
            "Epoch 2903/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3650 - val_loss: 0.4098\n",
            "Epoch 2904/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4142\n",
            "Epoch 2905/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4103\n",
            "Epoch 2906/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4121\n",
            "Epoch 2907/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4060\n",
            "Epoch 2908/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4123\n",
            "Epoch 2909/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4124\n",
            "Epoch 2910/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4123\n",
            "Epoch 2911/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4115\n",
            "Epoch 2912/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4131\n",
            "Epoch 2913/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4103\n",
            "Epoch 2914/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3656 - val_loss: 0.4094\n",
            "Epoch 2915/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4111\n",
            "Epoch 2916/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3656 - val_loss: 0.4153\n",
            "Epoch 2917/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3656 - val_loss: 0.4065\n",
            "Epoch 2918/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3654 - val_loss: 0.4146\n",
            "Epoch 2919/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3650 - val_loss: 0.4166\n",
            "Epoch 2920/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4115\n",
            "Epoch 2921/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3661 - val_loss: 0.4124\n",
            "Epoch 2922/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4126\n",
            "Epoch 2923/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4104\n",
            "Epoch 2924/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3655 - val_loss: 0.4161\n",
            "Epoch 2925/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4115\n",
            "Epoch 2926/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4138\n",
            "Epoch 2927/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4106\n",
            "Epoch 2928/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3655 - val_loss: 0.4106\n",
            "Epoch 2929/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4130\n",
            "Epoch 2930/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4131\n",
            "Epoch 2931/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3657 - val_loss: 0.4141\n",
            "Epoch 2932/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3660 - val_loss: 0.4122\n",
            "Epoch 2933/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3657 - val_loss: 0.4138\n",
            "Epoch 2934/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3662 - val_loss: 0.4129\n",
            "Epoch 2935/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3653 - val_loss: 0.4115\n",
            "Epoch 2936/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3653 - val_loss: 0.4155\n",
            "Epoch 2937/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4138\n",
            "Epoch 2938/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4091\n",
            "Epoch 2939/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4099\n",
            "Epoch 2940/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4158\n",
            "Epoch 2941/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4112\n",
            "Epoch 2942/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3665 - val_loss: 0.4071\n",
            "Epoch 2943/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4120\n",
            "Epoch 2944/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4119\n",
            "Epoch 2945/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4071\n",
            "Epoch 2946/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4119\n",
            "Epoch 2947/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4091\n",
            "Epoch 2948/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3661 - val_loss: 0.4120\n",
            "Epoch 2949/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3649 - val_loss: 0.4093\n",
            "Epoch 2950/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3654 - val_loss: 0.4097\n",
            "Epoch 2951/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3653 - val_loss: 0.4124\n",
            "Epoch 2952/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3665 - val_loss: 0.4102\n",
            "Epoch 2953/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4082\n",
            "Epoch 2954/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3674 - val_loss: 0.4108\n",
            "Epoch 2955/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4108\n",
            "Epoch 2956/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4085\n",
            "Epoch 2957/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4170\n",
            "Epoch 2958/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3671 - val_loss: 0.4132\n",
            "Epoch 2959/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4135\n",
            "Epoch 2960/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4115\n",
            "Epoch 2961/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3653 - val_loss: 0.4129\n",
            "Epoch 2962/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3669 - val_loss: 0.4100\n",
            "Epoch 2963/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4113\n",
            "Epoch 2964/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3654 - val_loss: 0.4110\n",
            "Epoch 2965/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3666 - val_loss: 0.4117\n",
            "Epoch 2966/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3669 - val_loss: 0.4069\n",
            "Epoch 2967/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3660 - val_loss: 0.4155\n",
            "Epoch 2968/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3659 - val_loss: 0.4098\n",
            "Epoch 2969/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3647 - val_loss: 0.4125\n",
            "Epoch 2970/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3652 - val_loss: 0.4120\n",
            "Epoch 2971/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4137\n",
            "Epoch 2972/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4102\n",
            "Epoch 2973/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3652 - val_loss: 0.4141\n",
            "Epoch 2974/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4111\n",
            "Epoch 2975/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4101\n",
            "Epoch 2976/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3651 - val_loss: 0.4107\n",
            "Epoch 2977/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3657 - val_loss: 0.4160\n",
            "Epoch 2978/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4139\n",
            "Epoch 2979/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3653 - val_loss: 0.4172\n",
            "Epoch 2980/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4062\n",
            "Epoch 2981/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3674 - val_loss: 0.4158\n",
            "Epoch 2982/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3678 - val_loss: 0.4140\n",
            "Epoch 2983/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3668 - val_loss: 0.4128\n",
            "Epoch 2984/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3659 - val_loss: 0.4103\n",
            "Epoch 2985/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3654 - val_loss: 0.4088\n",
            "Epoch 2986/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4154\n",
            "Epoch 2987/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3652 - val_loss: 0.4120\n",
            "Epoch 2988/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3674 - val_loss: 0.4107\n",
            "Epoch 2989/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3651 - val_loss: 0.4086\n",
            "Epoch 2990/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3650 - val_loss: 0.4124\n",
            "Epoch 2991/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3650 - val_loss: 0.4114\n",
            "Epoch 2992/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3649 - val_loss: 0.4129\n",
            "Epoch 2993/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4136\n",
            "Epoch 2994/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4117\n",
            "Epoch 2995/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4079\n",
            "Epoch 2996/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4116\n",
            "Epoch 2997/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4175\n",
            "Epoch 2998/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3677 - val_loss: 0.4173\n",
            "Epoch 2999/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3723 - val_loss: 0.4158\n",
            "Epoch 3000/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.4071\n",
            "8/8 [==============================] - 1s 8ms/step - loss: 0.4071\n",
            "8/8 [==============================] - 1s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb090577b20>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=3000;    # training epochs\n",
        "temperature = 0.1    # temprature control the smooth of the probabilities\n",
        "\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(con_value)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "yn1Ny_xiXDfL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZQ1q1t2hfPU"
      },
      "outputs": [],
      "source": [],
      "id": "vZQ1q1t2hfPU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uYwG8d50qQf"
      },
      "source": [
        "## 2.4 Add Hidden Layers before Softmax Layer ##"
      ],
      "id": "2uYwG8d50qQf"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gJxJD_M41H_v",
        "outputId": "24bccbd3-c3ac-435d-d2c2-982759d3f48c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "80/80 [==============================] - 9s 14ms/step - loss: 0.9713 - val_loss: 0.6996\n",
            "Epoch 2/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.5084 - val_loss: 0.4249\n",
            "Epoch 3/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4414 - val_loss: 0.4128\n",
            "Epoch 4/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4298 - val_loss: 0.3983\n",
            "Epoch 5/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4284 - val_loss: 0.3936\n",
            "Epoch 6/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4292 - val_loss: 0.3970\n",
            "Epoch 7/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4253 - val_loss: 0.3895\n",
            "Epoch 8/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4254 - val_loss: 0.3965\n",
            "Epoch 9/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4248 - val_loss: 0.3897\n",
            "Epoch 10/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.4238 - val_loss: 0.3878\n",
            "Epoch 11/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.4226 - val_loss: 0.3986\n",
            "Epoch 12/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4251 - val_loss: 0.3900\n",
            "Epoch 13/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4248 - val_loss: 0.3950\n",
            "Epoch 14/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4218 - val_loss: 0.3866\n",
            "Epoch 15/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4215 - val_loss: 0.3900\n",
            "Epoch 16/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4196 - val_loss: 0.3869\n",
            "Epoch 17/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4192 - val_loss: 0.3886\n",
            "Epoch 18/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4229 - val_loss: 0.3833\n",
            "Epoch 19/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4154 - val_loss: 0.3861\n",
            "Epoch 20/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4190 - val_loss: 0.3948\n",
            "Epoch 21/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4197 - val_loss: 0.3871\n",
            "Epoch 22/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4159 - val_loss: 0.3915\n",
            "Epoch 23/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4158 - val_loss: 0.3848\n",
            "Epoch 24/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4127 - val_loss: 0.3840\n",
            "Epoch 25/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4119 - val_loss: 0.3849\n",
            "Epoch 26/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4108 - val_loss: 0.3799\n",
            "Epoch 27/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4145 - val_loss: 0.3803\n",
            "Epoch 28/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4119 - val_loss: 0.3815\n",
            "Epoch 29/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4087 - val_loss: 0.3805\n",
            "Epoch 30/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4110 - val_loss: 0.3785\n",
            "Epoch 31/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4099 - val_loss: 0.3773\n",
            "Epoch 32/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4079 - val_loss: 0.3858\n",
            "Epoch 33/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4098 - val_loss: 0.3774\n",
            "Epoch 34/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4086 - val_loss: 0.3829\n",
            "Epoch 35/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4066 - val_loss: 0.3810\n",
            "Epoch 36/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4095 - val_loss: 0.3770\n",
            "Epoch 37/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4080 - val_loss: 0.3735\n",
            "Epoch 38/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4066 - val_loss: 0.3768\n",
            "Epoch 39/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4060 - val_loss: 0.3761\n",
            "Epoch 40/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.4057 - val_loss: 0.3760\n",
            "Epoch 41/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4069 - val_loss: 0.3792\n",
            "Epoch 42/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4082 - val_loss: 0.4070\n",
            "Epoch 43/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4075 - val_loss: 0.3756\n",
            "Epoch 44/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.4051 - val_loss: 0.3775\n",
            "Epoch 45/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4046 - val_loss: 0.3781\n",
            "Epoch 46/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4048 - val_loss: 0.3851\n",
            "Epoch 47/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4088 - val_loss: 0.3883\n",
            "Epoch 48/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4077 - val_loss: 0.3758\n",
            "Epoch 49/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4042 - val_loss: 0.3804\n",
            "Epoch 50/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4056 - val_loss: 0.3793\n",
            "Epoch 51/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4085 - val_loss: 0.3780\n",
            "Epoch 52/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4014 - val_loss: 0.3795\n",
            "Epoch 53/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4032 - val_loss: 0.3862\n",
            "Epoch 54/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4023 - val_loss: 0.3821\n",
            "Epoch 55/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4029 - val_loss: 0.3780\n",
            "Epoch 56/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.4032 - val_loss: 0.3761\n",
            "Epoch 57/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4027 - val_loss: 0.3874\n",
            "Epoch 58/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4020 - val_loss: 0.3845\n",
            "Epoch 59/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.4018 - val_loss: 0.3717\n",
            "Epoch 60/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4004 - val_loss: 0.3735\n",
            "Epoch 61/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4014 - val_loss: 0.3734\n",
            "Epoch 62/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4023 - val_loss: 0.3823\n",
            "Epoch 63/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.4002 - val_loss: 0.3847\n",
            "Epoch 64/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3996 - val_loss: 0.3752\n",
            "Epoch 65/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4017 - val_loss: 0.3779\n",
            "Epoch 66/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.4018 - val_loss: 0.3766\n",
            "Epoch 67/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4048 - val_loss: 0.3752\n",
            "Epoch 68/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3995 - val_loss: 0.3741\n",
            "Epoch 69/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4010 - val_loss: 0.3735\n",
            "Epoch 70/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.4015 - val_loss: 0.3837\n",
            "Epoch 71/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4048 - val_loss: 0.3733\n",
            "Epoch 72/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4022 - val_loss: 0.3720\n",
            "Epoch 73/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3988 - val_loss: 0.3715\n",
            "Epoch 74/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4032 - val_loss: 0.3719\n",
            "Epoch 75/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3992 - val_loss: 0.3755\n",
            "Epoch 76/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3977 - val_loss: 0.3736\n",
            "Epoch 77/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3997 - val_loss: 0.3697\n",
            "Epoch 78/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3979 - val_loss: 0.3788\n",
            "Epoch 79/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3980 - val_loss: 0.3735\n",
            "Epoch 80/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3976 - val_loss: 0.3779\n",
            "Epoch 81/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3993 - val_loss: 0.3720\n",
            "Epoch 82/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3978 - val_loss: 0.3715\n",
            "Epoch 83/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3964 - val_loss: 0.3734\n",
            "Epoch 84/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4011 - val_loss: 0.3721\n",
            "Epoch 85/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4014 - val_loss: 0.3776\n",
            "Epoch 86/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.4012 - val_loss: 0.3754\n",
            "Epoch 87/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.4016 - val_loss: 0.3871\n",
            "Epoch 88/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3984 - val_loss: 0.3757\n",
            "Epoch 89/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3997 - val_loss: 0.3802\n",
            "Epoch 90/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3977 - val_loss: 0.3720\n",
            "Epoch 91/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3988 - val_loss: 0.3750\n",
            "Epoch 92/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3991 - val_loss: 0.3763\n",
            "Epoch 93/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3983 - val_loss: 0.3746\n",
            "Epoch 94/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4008 - val_loss: 0.3790\n",
            "Epoch 95/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3974 - val_loss: 0.3719\n",
            "Epoch 96/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3964 - val_loss: 0.3747\n",
            "Epoch 97/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3963 - val_loss: 0.3751\n",
            "Epoch 98/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3977 - val_loss: 0.3795\n",
            "Epoch 99/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3977 - val_loss: 0.3777\n",
            "Epoch 100/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4005 - val_loss: 0.3729\n",
            "Epoch 101/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4019 - val_loss: 0.3763\n",
            "Epoch 102/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3977 - val_loss: 0.3725\n",
            "Epoch 103/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3983 - val_loss: 0.3832\n",
            "Epoch 104/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.4018 - val_loss: 0.3771\n",
            "Epoch 105/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4000 - val_loss: 0.3741\n",
            "Epoch 106/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3987 - val_loss: 0.3736\n",
            "Epoch 107/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3968 - val_loss: 0.3723\n",
            "Epoch 108/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3965 - val_loss: 0.3787\n",
            "Epoch 109/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3969 - val_loss: 0.3720\n",
            "Epoch 110/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3968 - val_loss: 0.3726\n",
            "Epoch 111/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3964 - val_loss: 0.3769\n",
            "Epoch 112/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3951 - val_loss: 0.3707\n",
            "Epoch 113/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3984 - val_loss: 0.3704\n",
            "Epoch 114/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3995 - val_loss: 0.3693\n",
            "Epoch 115/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3953 - val_loss: 0.3723\n",
            "Epoch 116/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.3719\n",
            "Epoch 117/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3949 - val_loss: 0.3739\n",
            "Epoch 118/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3969 - val_loss: 0.3816\n",
            "Epoch 119/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3983 - val_loss: 0.3722\n",
            "Epoch 120/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3972 - val_loss: 0.3732\n",
            "Epoch 121/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3963 - val_loss: 0.3702\n",
            "Epoch 122/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3952 - val_loss: 0.3680\n",
            "Epoch 123/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3958 - val_loss: 0.3808\n",
            "Epoch 124/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3976 - val_loss: 0.3693\n",
            "Epoch 125/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3967 - val_loss: 0.3712\n",
            "Epoch 126/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3718\n",
            "Epoch 127/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3982 - val_loss: 0.3708\n",
            "Epoch 128/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3980 - val_loss: 0.3730\n",
            "Epoch 129/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3950 - val_loss: 0.3753\n",
            "Epoch 130/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3959 - val_loss: 0.3905\n",
            "Epoch 131/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3965 - val_loss: 0.3692\n",
            "Epoch 132/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3935 - val_loss: 0.3720\n",
            "Epoch 133/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3958 - val_loss: 0.3695\n",
            "Epoch 134/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3945 - val_loss: 0.3701\n",
            "Epoch 135/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3926 - val_loss: 0.3707\n",
            "Epoch 136/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3974 - val_loss: 0.3803\n",
            "Epoch 137/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.4008 - val_loss: 0.3798\n",
            "Epoch 138/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3968 - val_loss: 0.3781\n",
            "Epoch 139/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3954 - val_loss: 0.3783\n",
            "Epoch 140/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3956 - val_loss: 0.3713\n",
            "Epoch 141/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3697\n",
            "Epoch 142/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3972 - val_loss: 0.3739\n",
            "Epoch 143/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3972 - val_loss: 0.3713\n",
            "Epoch 144/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3946 - val_loss: 0.3708\n",
            "Epoch 145/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3946 - val_loss: 0.3688\n",
            "Epoch 146/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3943 - val_loss: 0.3714\n",
            "Epoch 147/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3946 - val_loss: 0.3677\n",
            "Epoch 148/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3952 - val_loss: 0.3692\n",
            "Epoch 149/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3936 - val_loss: 0.3663\n",
            "Epoch 150/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3928 - val_loss: 0.3748\n",
            "Epoch 151/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3932 - val_loss: 0.3693\n",
            "Epoch 152/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3949 - val_loss: 0.3704\n",
            "Epoch 153/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3943 - val_loss: 0.3695\n",
            "Epoch 154/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3974 - val_loss: 0.3731\n",
            "Epoch 155/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3967 - val_loss: 0.3693\n",
            "Epoch 156/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3731\n",
            "Epoch 157/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3950 - val_loss: 0.3733\n",
            "Epoch 158/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3957 - val_loss: 0.3870\n",
            "Epoch 159/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3983 - val_loss: 0.3797\n",
            "Epoch 160/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3931 - val_loss: 0.3707\n",
            "Epoch 161/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3934 - val_loss: 0.3722\n",
            "Epoch 162/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3958 - val_loss: 0.3742\n",
            "Epoch 163/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3711\n",
            "Epoch 164/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3936 - val_loss: 0.3673\n",
            "Epoch 165/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3936 - val_loss: 0.3742\n",
            "Epoch 166/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3942 - val_loss: 0.3681\n",
            "Epoch 167/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3943 - val_loss: 0.3715\n",
            "Epoch 168/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3972 - val_loss: 0.3800\n",
            "Epoch 169/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3961 - val_loss: 0.3721\n",
            "Epoch 170/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3954 - val_loss: 0.3730\n",
            "Epoch 171/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3919 - val_loss: 0.3752\n",
            "Epoch 172/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3925 - val_loss: 0.3760\n",
            "Epoch 173/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3931 - val_loss: 0.3676\n",
            "Epoch 174/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3934 - val_loss: 0.3773\n",
            "Epoch 175/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3947 - val_loss: 0.3685\n",
            "Epoch 176/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3921 - val_loss: 0.3721\n",
            "Epoch 177/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3927 - val_loss: 0.3698\n",
            "Epoch 178/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3919 - val_loss: 0.3666\n",
            "Epoch 179/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3914 - val_loss: 0.3687\n",
            "Epoch 180/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3940 - val_loss: 0.3716\n",
            "Epoch 181/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3924 - val_loss: 0.3691\n",
            "Epoch 182/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3926 - val_loss: 0.3692\n",
            "Epoch 183/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3937 - val_loss: 0.3780\n",
            "Epoch 184/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3924 - val_loss: 0.3741\n",
            "Epoch 185/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3925 - val_loss: 0.3687\n",
            "Epoch 186/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3917 - val_loss: 0.3718\n",
            "Epoch 187/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3949 - val_loss: 0.3746\n",
            "Epoch 188/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3735\n",
            "Epoch 189/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3904 - val_loss: 0.3715\n",
            "Epoch 190/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3966 - val_loss: 0.3751\n",
            "Epoch 191/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3936 - val_loss: 0.3752\n",
            "Epoch 192/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3925 - val_loss: 0.3692\n",
            "Epoch 193/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3936 - val_loss: 0.3677\n",
            "Epoch 194/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3936 - val_loss: 0.3701\n",
            "Epoch 195/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3923 - val_loss: 0.3672\n",
            "Epoch 196/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3927 - val_loss: 0.3665\n",
            "Epoch 197/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3931 - val_loss: 0.3661\n",
            "Epoch 198/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3898 - val_loss: 0.3697\n",
            "Epoch 199/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3921 - val_loss: 0.3695\n",
            "Epoch 200/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3906 - val_loss: 0.3689\n",
            "Epoch 201/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.3719\n",
            "Epoch 202/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3917 - val_loss: 0.3673\n",
            "Epoch 203/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3929 - val_loss: 0.3711\n",
            "Epoch 204/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3923 - val_loss: 0.3704\n",
            "Epoch 205/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3917 - val_loss: 0.3650\n",
            "Epoch 206/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3921 - val_loss: 0.3703\n",
            "Epoch 207/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3919 - val_loss: 0.3666\n",
            "Epoch 208/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3918 - val_loss: 0.3703\n",
            "Epoch 209/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3908 - val_loss: 0.3672\n",
            "Epoch 210/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3902 - val_loss: 0.3699\n",
            "Epoch 211/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3912 - val_loss: 0.3678\n",
            "Epoch 212/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3931 - val_loss: 0.3687\n",
            "Epoch 213/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3910 - val_loss: 0.3688\n",
            "Epoch 214/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3667\n",
            "Epoch 215/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.3660\n",
            "Epoch 216/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3909 - val_loss: 0.3670\n",
            "Epoch 217/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3899 - val_loss: 0.3686\n",
            "Epoch 218/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3901 - val_loss: 0.3684\n",
            "Epoch 219/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3945 - val_loss: 0.3649\n",
            "Epoch 220/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3956 - val_loss: 0.3740\n",
            "Epoch 221/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.3687\n",
            "Epoch 222/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3951 - val_loss: 0.3680\n",
            "Epoch 223/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3919 - val_loss: 0.3665\n",
            "Epoch 224/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3927 - val_loss: 0.3713\n",
            "Epoch 225/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3890 - val_loss: 0.3703\n",
            "Epoch 226/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3906 - val_loss: 0.3688\n",
            "Epoch 227/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3886 - val_loss: 0.3717\n",
            "Epoch 228/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3917 - val_loss: 0.3674\n",
            "Epoch 229/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3917 - val_loss: 0.3760\n",
            "Epoch 230/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3897 - val_loss: 0.3678\n",
            "Epoch 231/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3896 - val_loss: 0.3725\n",
            "Epoch 232/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3890 - val_loss: 0.3665\n",
            "Epoch 233/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3699\n",
            "Epoch 234/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3898 - val_loss: 0.3694\n",
            "Epoch 235/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3667\n",
            "Epoch 236/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3891 - val_loss: 0.3665\n",
            "Epoch 237/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3675\n",
            "Epoch 238/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3889 - val_loss: 0.3679\n",
            "Epoch 239/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3966 - val_loss: 0.3695\n",
            "Epoch 240/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3918 - val_loss: 0.3734\n",
            "Epoch 241/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3911 - val_loss: 0.3699\n",
            "Epoch 242/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3897 - val_loss: 0.3701\n",
            "Epoch 243/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3892 - val_loss: 0.3746\n",
            "Epoch 244/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3898 - val_loss: 0.3687\n",
            "Epoch 245/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.3684\n",
            "Epoch 246/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3785\n",
            "Epoch 247/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3732\n",
            "Epoch 248/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3651\n",
            "Epoch 249/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3899 - val_loss: 0.3673\n",
            "Epoch 250/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3922 - val_loss: 0.3663\n",
            "Epoch 251/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3685\n",
            "Epoch 252/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3893 - val_loss: 0.3692\n",
            "Epoch 253/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3702\n",
            "Epoch 254/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3890 - val_loss: 0.3750\n",
            "Epoch 255/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3724\n",
            "Epoch 256/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3900 - val_loss: 0.3702\n",
            "Epoch 257/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3879 - val_loss: 0.3663\n",
            "Epoch 258/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3892 - val_loss: 0.3678\n",
            "Epoch 259/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3880 - val_loss: 0.3668\n",
            "Epoch 260/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3911 - val_loss: 0.3696\n",
            "Epoch 261/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3868 - val_loss: 0.3669\n",
            "Epoch 262/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3882 - val_loss: 0.3691\n",
            "Epoch 263/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3875 - val_loss: 0.3728\n",
            "Epoch 264/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3734\n",
            "Epoch 265/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3880 - val_loss: 0.3698\n",
            "Epoch 266/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3876 - val_loss: 0.3675\n",
            "Epoch 267/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3678\n",
            "Epoch 268/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.3752\n",
            "Epoch 269/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3906 - val_loss: 0.3665\n",
            "Epoch 270/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3883 - val_loss: 0.3658\n",
            "Epoch 271/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3894 - val_loss: 0.3716\n",
            "Epoch 272/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3880 - val_loss: 0.3739\n",
            "Epoch 273/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3890 - val_loss: 0.3654\n",
            "Epoch 274/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3884 - val_loss: 0.3653\n",
            "Epoch 275/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3648\n",
            "Epoch 276/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3654\n",
            "Epoch 277/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3684\n",
            "Epoch 278/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3696\n",
            "Epoch 279/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3880 - val_loss: 0.3702\n",
            "Epoch 280/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3888 - val_loss: 0.3678\n",
            "Epoch 281/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.3653\n",
            "Epoch 282/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3873 - val_loss: 0.3694\n",
            "Epoch 283/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3913 - val_loss: 0.3663\n",
            "Epoch 284/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3894 - val_loss: 0.3695\n",
            "Epoch 285/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3682\n",
            "Epoch 286/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3669\n",
            "Epoch 287/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3853 - val_loss: 0.3688\n",
            "Epoch 288/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3848 - val_loss: 0.3663\n",
            "Epoch 289/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3867 - val_loss: 0.3694\n",
            "Epoch 290/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3867 - val_loss: 0.3648\n",
            "Epoch 291/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3668\n",
            "Epoch 292/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3698\n",
            "Epoch 293/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.3693\n",
            "Epoch 294/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3728\n",
            "Epoch 295/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3642\n",
            "Epoch 296/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.3695\n",
            "Epoch 297/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3877 - val_loss: 0.3674\n",
            "Epoch 298/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3870 - val_loss: 0.3673\n",
            "Epoch 299/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.3681\n",
            "Epoch 300/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3664\n",
            "Epoch 301/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3685\n",
            "Epoch 302/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3851 - val_loss: 0.3685\n",
            "Epoch 303/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3866 - val_loss: 0.3677\n",
            "Epoch 304/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3867 - val_loss: 0.3638\n",
            "Epoch 305/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3843 - val_loss: 0.3660\n",
            "Epoch 306/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3851 - val_loss: 0.3659\n",
            "Epoch 307/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3852 - val_loss: 0.3660\n",
            "Epoch 308/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3877 - val_loss: 0.3660\n",
            "Epoch 309/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3664\n",
            "Epoch 310/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3660\n",
            "Epoch 311/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3671\n",
            "Epoch 312/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3852 - val_loss: 0.3627\n",
            "Epoch 313/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3857 - val_loss: 0.3686\n",
            "Epoch 314/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.3680\n",
            "Epoch 315/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3869 - val_loss: 0.3663\n",
            "Epoch 316/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.3713\n",
            "Epoch 317/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3715\n",
            "Epoch 318/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3861 - val_loss: 0.3665\n",
            "Epoch 319/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3886 - val_loss: 0.3713\n",
            "Epoch 320/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3877 - val_loss: 0.3713\n",
            "Epoch 321/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3873 - val_loss: 0.3653\n",
            "Epoch 322/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3657\n",
            "Epoch 323/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.3714\n",
            "Epoch 324/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.3642\n",
            "Epoch 325/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3837 - val_loss: 0.3683\n",
            "Epoch 326/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.3669\n",
            "Epoch 327/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3837 - val_loss: 0.3711\n",
            "Epoch 328/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3733\n",
            "Epoch 329/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.3674\n",
            "Epoch 330/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3672\n",
            "Epoch 331/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3882 - val_loss: 0.3650\n",
            "Epoch 332/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3880 - val_loss: 0.3651\n",
            "Epoch 333/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3847 - val_loss: 0.3680\n",
            "Epoch 334/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3840 - val_loss: 0.3662\n",
            "Epoch 335/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3884 - val_loss: 0.3670\n",
            "Epoch 336/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3837 - val_loss: 0.3639\n",
            "Epoch 337/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3843 - val_loss: 0.3673\n",
            "Epoch 338/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.3675\n",
            "Epoch 339/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.3641\n",
            "Epoch 340/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3875 - val_loss: 0.3616\n",
            "Epoch 341/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.3695\n",
            "Epoch 342/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3727\n",
            "Epoch 343/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3685\n",
            "Epoch 344/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.3645\n",
            "Epoch 345/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3837 - val_loss: 0.3715\n",
            "Epoch 346/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3659\n",
            "Epoch 347/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.3673\n",
            "Epoch 348/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.3655\n",
            "Epoch 349/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3839 - val_loss: 0.3638\n",
            "Epoch 350/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3853 - val_loss: 0.3664\n",
            "Epoch 351/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3845 - val_loss: 0.3661\n",
            "Epoch 352/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3847 - val_loss: 0.3650\n",
            "Epoch 353/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.3675\n",
            "Epoch 354/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3867 - val_loss: 0.3702\n",
            "Epoch 355/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.3678\n",
            "Epoch 356/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.3658\n",
            "Epoch 357/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.3673\n",
            "Epoch 358/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.3716\n",
            "Epoch 359/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.3672\n",
            "Epoch 360/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.3670\n",
            "Epoch 361/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.3651\n",
            "Epoch 362/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3871 - val_loss: 0.3655\n",
            "Epoch 363/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3851 - val_loss: 0.3622\n",
            "Epoch 364/2000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.3826 - val_loss: 0.3664\n",
            "Epoch 365/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.3629\n",
            "Epoch 366/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3848 - val_loss: 0.3673\n",
            "Epoch 367/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3830 - val_loss: 0.3684\n",
            "Epoch 368/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3851 - val_loss: 0.3649\n",
            "Epoch 369/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.3698\n",
            "Epoch 370/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.3663\n",
            "Epoch 371/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.3680\n",
            "Epoch 372/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3855 - val_loss: 0.3626\n",
            "Epoch 373/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.3729\n",
            "Epoch 374/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3832 - val_loss: 0.3639\n",
            "Epoch 375/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3688\n",
            "Epoch 376/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.3632\n",
            "Epoch 377/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.3652\n",
            "Epoch 378/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3643\n",
            "Epoch 379/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.3635\n",
            "Epoch 380/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3852 - val_loss: 0.3644\n",
            "Epoch 381/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3821 - val_loss: 0.3626\n",
            "Epoch 382/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3842 - val_loss: 0.3685\n",
            "Epoch 383/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3842 - val_loss: 0.3696\n",
            "Epoch 384/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3845 - val_loss: 0.3688\n",
            "Epoch 385/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3857 - val_loss: 0.3642\n",
            "Epoch 386/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3644\n",
            "Epoch 387/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3849 - val_loss: 0.3718\n",
            "Epoch 388/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3836 - val_loss: 0.3647\n",
            "Epoch 389/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3643\n",
            "Epoch 390/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3636\n",
            "Epoch 391/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3661\n",
            "Epoch 392/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3625\n",
            "Epoch 393/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.3663\n",
            "Epoch 394/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3677\n",
            "Epoch 395/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.3640\n",
            "Epoch 396/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.3664\n",
            "Epoch 397/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3850 - val_loss: 0.3682\n",
            "Epoch 398/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3841 - val_loss: 0.3653\n",
            "Epoch 399/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3838 - val_loss: 0.3653\n",
            "Epoch 400/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.3670\n",
            "Epoch 401/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.3670\n",
            "Epoch 402/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.3678\n",
            "Epoch 403/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3829 - val_loss: 0.3644\n",
            "Epoch 404/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3706\n",
            "Epoch 405/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3745\n",
            "Epoch 406/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3875 - val_loss: 0.3636\n",
            "Epoch 407/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.3690\n",
            "Epoch 408/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3845 - val_loss: 0.3639\n",
            "Epoch 409/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3639\n",
            "Epoch 410/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3648\n",
            "Epoch 411/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3622\n",
            "Epoch 412/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3830 - val_loss: 0.3642\n",
            "Epoch 413/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3823 - val_loss: 0.3639\n",
            "Epoch 414/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3821 - val_loss: 0.3652\n",
            "Epoch 415/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3802 - val_loss: 0.3640\n",
            "Epoch 416/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3666\n",
            "Epoch 417/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.3670\n",
            "Epoch 418/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.3673\n",
            "Epoch 419/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.3633\n",
            "Epoch 420/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.3651\n",
            "Epoch 421/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.3644\n",
            "Epoch 422/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3648\n",
            "Epoch 423/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.3612\n",
            "Epoch 424/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3841 - val_loss: 0.3630\n",
            "Epoch 425/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3622\n",
            "Epoch 426/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3824 - val_loss: 0.3651\n",
            "Epoch 427/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3615\n",
            "Epoch 428/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3845 - val_loss: 0.3658\n",
            "Epoch 429/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3818 - val_loss: 0.3676\n",
            "Epoch 430/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3815 - val_loss: 0.3615\n",
            "Epoch 431/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3815 - val_loss: 0.3659\n",
            "Epoch 432/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.3665\n",
            "Epoch 433/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3820 - val_loss: 0.3646\n",
            "Epoch 434/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.3634\n",
            "Epoch 435/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3669\n",
            "Epoch 436/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3846 - val_loss: 0.3692\n",
            "Epoch 437/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3661\n",
            "Epoch 438/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.3657\n",
            "Epoch 439/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.3616\n",
            "Epoch 440/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3731\n",
            "Epoch 441/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.3651\n",
            "Epoch 442/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3627\n",
            "Epoch 443/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.3667\n",
            "Epoch 444/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3837 - val_loss: 0.3628\n",
            "Epoch 445/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3800 - val_loss: 0.3654\n",
            "Epoch 446/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3816 - val_loss: 0.3650\n",
            "Epoch 447/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3859 - val_loss: 0.3655\n",
            "Epoch 448/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3657\n",
            "Epoch 449/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.3716\n",
            "Epoch 450/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3650\n",
            "Epoch 451/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3660\n",
            "Epoch 452/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.3650\n",
            "Epoch 453/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3745\n",
            "Epoch 454/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3729\n",
            "Epoch 455/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3671\n",
            "Epoch 456/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.3664\n",
            "Epoch 457/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3708\n",
            "Epoch 458/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3662\n",
            "Epoch 459/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3677\n",
            "Epoch 460/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3813 - val_loss: 0.3627\n",
            "Epoch 461/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3819 - val_loss: 0.3638\n",
            "Epoch 462/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3807 - val_loss: 0.3681\n",
            "Epoch 463/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3822 - val_loss: 0.3636\n",
            "Epoch 464/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.3674\n",
            "Epoch 465/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.3644\n",
            "Epoch 466/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3643\n",
            "Epoch 467/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3808 - val_loss: 0.3646\n",
            "Epoch 468/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3828 - val_loss: 0.3670\n",
            "Epoch 469/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3828 - val_loss: 0.3617\n",
            "Epoch 470/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3800 - val_loss: 0.3641\n",
            "Epoch 471/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3810 - val_loss: 0.3669\n",
            "Epoch 472/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3844 - val_loss: 0.3705\n",
            "Epoch 473/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.3628\n",
            "Epoch 474/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3821 - val_loss: 0.3759\n",
            "Epoch 475/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3814 - val_loss: 0.3637\n",
            "Epoch 476/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3818 - val_loss: 0.3611\n",
            "Epoch 477/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3806 - val_loss: 0.3644\n",
            "Epoch 478/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3798 - val_loss: 0.3687\n",
            "Epoch 479/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3631\n",
            "Epoch 480/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.3827\n",
            "Epoch 481/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3653\n",
            "Epoch 482/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.3687\n",
            "Epoch 483/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3663\n",
            "Epoch 484/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3646\n",
            "Epoch 485/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3815 - val_loss: 0.3658\n",
            "Epoch 486/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3652\n",
            "Epoch 487/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3640\n",
            "Epoch 488/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.3647\n",
            "Epoch 489/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.3653\n",
            "Epoch 490/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3818 - val_loss: 0.3694\n",
            "Epoch 491/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3816 - val_loss: 0.3642\n",
            "Epoch 492/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3802 - val_loss: 0.3636\n",
            "Epoch 493/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3804 - val_loss: 0.3645\n",
            "Epoch 494/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.3750\n",
            "Epoch 495/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.3626\n",
            "Epoch 496/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3819 - val_loss: 0.3658\n",
            "Epoch 497/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.3661\n",
            "Epoch 498/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3619\n",
            "Epoch 499/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.3655\n",
            "Epoch 500/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.3612\n",
            "Epoch 501/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3684\n",
            "Epoch 502/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.3723\n",
            "Epoch 503/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3635\n",
            "Epoch 504/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.3668\n",
            "Epoch 505/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3661\n",
            "Epoch 506/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3789 - val_loss: 0.3656\n",
            "Epoch 507/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3803 - val_loss: 0.3635\n",
            "Epoch 508/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3800 - val_loss: 0.3639\n",
            "Epoch 509/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3795 - val_loss: 0.3662\n",
            "Epoch 510/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.3636\n",
            "Epoch 511/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3634\n",
            "Epoch 512/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3646\n",
            "Epoch 513/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3845 - val_loss: 0.3667\n",
            "Epoch 514/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.3672\n",
            "Epoch 515/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3830 - val_loss: 0.3630\n",
            "Epoch 516/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3752\n",
            "Epoch 517/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3668\n",
            "Epoch 518/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3815 - val_loss: 0.3656\n",
            "Epoch 519/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.3600\n",
            "Epoch 520/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3674\n",
            "Epoch 521/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3641\n",
            "Epoch 522/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3784 - val_loss: 0.3660\n",
            "Epoch 523/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3794 - val_loss: 0.3610\n",
            "Epoch 524/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3793 - val_loss: 0.3656\n",
            "Epoch 525/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3812 - val_loss: 0.3720\n",
            "Epoch 526/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3715\n",
            "Epoch 527/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3683\n",
            "Epoch 528/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3661\n",
            "Epoch 529/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3665\n",
            "Epoch 530/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3632\n",
            "Epoch 531/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3636\n",
            "Epoch 532/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.3677\n",
            "Epoch 533/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3676\n",
            "Epoch 534/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3786 - val_loss: 0.3624\n",
            "Epoch 535/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3624\n",
            "Epoch 536/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3633\n",
            "Epoch 537/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3788 - val_loss: 0.3633\n",
            "Epoch 538/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3789 - val_loss: 0.3634\n",
            "Epoch 539/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3795 - val_loss: 0.3652\n",
            "Epoch 540/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3796 - val_loss: 0.3700\n",
            "Epoch 541/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3791 - val_loss: 0.3673\n",
            "Epoch 542/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3706\n",
            "Epoch 543/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3651\n",
            "Epoch 544/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3799 - val_loss: 0.3653\n",
            "Epoch 545/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3635\n",
            "Epoch 546/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3621\n",
            "Epoch 547/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3631\n",
            "Epoch 548/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3791 - val_loss: 0.3679\n",
            "Epoch 549/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.3623\n",
            "Epoch 550/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3681\n",
            "Epoch 551/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.3663\n",
            "Epoch 552/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.3628\n",
            "Epoch 553/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3792 - val_loss: 0.3710\n",
            "Epoch 554/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3782 - val_loss: 0.3614\n",
            "Epoch 555/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3785 - val_loss: 0.3676\n",
            "Epoch 556/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3809 - val_loss: 0.3638\n",
            "Epoch 557/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3651\n",
            "Epoch 558/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3688\n",
            "Epoch 559/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3639\n",
            "Epoch 560/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3679\n",
            "Epoch 561/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3627\n",
            "Epoch 562/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.3629\n",
            "Epoch 563/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3630\n",
            "Epoch 564/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3786 - val_loss: 0.3643\n",
            "Epoch 565/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.3640\n",
            "Epoch 566/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3686\n",
            "Epoch 567/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3676\n",
            "Epoch 568/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3622\n",
            "Epoch 569/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3794 - val_loss: 0.3676\n",
            "Epoch 570/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3800 - val_loss: 0.3640\n",
            "Epoch 571/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3791 - val_loss: 0.3654\n",
            "Epoch 572/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3794 - val_loss: 0.3667\n",
            "Epoch 573/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3658\n",
            "Epoch 574/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.3637\n",
            "Epoch 575/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3638\n",
            "Epoch 576/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3788 - val_loss: 0.3629\n",
            "Epoch 577/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.3633\n",
            "Epoch 578/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3714\n",
            "Epoch 579/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3666\n",
            "Epoch 580/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3664\n",
            "Epoch 581/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3659\n",
            "Epoch 582/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3653\n",
            "Epoch 583/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3828 - val_loss: 0.3674\n",
            "Epoch 584/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3652\n",
            "Epoch 585/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3784 - val_loss: 0.3651\n",
            "Epoch 586/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3796 - val_loss: 0.3672\n",
            "Epoch 587/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3783 - val_loss: 0.3650\n",
            "Epoch 588/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3787 - val_loss: 0.3624\n",
            "Epoch 589/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3658\n",
            "Epoch 590/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3637\n",
            "Epoch 591/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3671\n",
            "Epoch 592/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3633\n",
            "Epoch 593/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.3649\n",
            "Epoch 594/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3781 - val_loss: 0.3653\n",
            "Epoch 595/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.3803\n",
            "Epoch 596/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3657\n",
            "Epoch 597/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3788 - val_loss: 0.3675\n",
            "Epoch 598/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3615\n",
            "Epoch 599/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3633\n",
            "Epoch 600/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3793 - val_loss: 0.3672\n",
            "Epoch 601/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3792 - val_loss: 0.3631\n",
            "Epoch 602/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3817 - val_loss: 0.3663\n",
            "Epoch 603/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3783 - val_loss: 0.3619\n",
            "Epoch 604/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3801 - val_loss: 0.3645\n",
            "Epoch 605/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3630\n",
            "Epoch 606/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3677\n",
            "Epoch 607/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3693\n",
            "Epoch 608/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3774 - val_loss: 0.3649\n",
            "Epoch 609/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3687\n",
            "Epoch 610/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3780 - val_loss: 0.3655\n",
            "Epoch 611/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3692\n",
            "Epoch 612/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3656\n",
            "Epoch 613/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3810 - val_loss: 0.3691\n",
            "Epoch 614/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.3660\n",
            "Epoch 615/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3790 - val_loss: 0.3662\n",
            "Epoch 616/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3826 - val_loss: 0.3632\n",
            "Epoch 617/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3780 - val_loss: 0.3614\n",
            "Epoch 618/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3772 - val_loss: 0.3679\n",
            "Epoch 619/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3770 - val_loss: 0.3654\n",
            "Epoch 620/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3647\n",
            "Epoch 621/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3621\n",
            "Epoch 622/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3639\n",
            "Epoch 623/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3652\n",
            "Epoch 624/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3629\n",
            "Epoch 625/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3773 - val_loss: 0.3627\n",
            "Epoch 626/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3616\n",
            "Epoch 627/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3697\n",
            "Epoch 628/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3690\n",
            "Epoch 629/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.3675\n",
            "Epoch 630/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3658\n",
            "Epoch 631/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3781 - val_loss: 0.3694\n",
            "Epoch 632/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3796 - val_loss: 0.3653\n",
            "Epoch 633/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3773 - val_loss: 0.3692\n",
            "Epoch 634/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3771 - val_loss: 0.3641\n",
            "Epoch 635/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3784 - val_loss: 0.3645\n",
            "Epoch 636/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3641\n",
            "Epoch 637/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3772 - val_loss: 0.3638\n",
            "Epoch 638/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3660\n",
            "Epoch 639/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.3717\n",
            "Epoch 640/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3642\n",
            "Epoch 641/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3656\n",
            "Epoch 642/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3690\n",
            "Epoch 643/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3728\n",
            "Epoch 644/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3641\n",
            "Epoch 645/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.3650\n",
            "Epoch 646/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3776 - val_loss: 0.3642\n",
            "Epoch 647/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3768 - val_loss: 0.3631\n",
            "Epoch 648/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3784 - val_loss: 0.3734\n",
            "Epoch 649/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3800 - val_loss: 0.3658\n",
            "Epoch 650/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3773 - val_loss: 0.3650\n",
            "Epoch 651/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.3708\n",
            "Epoch 652/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3669\n",
            "Epoch 653/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3659\n",
            "Epoch 654/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3680\n",
            "Epoch 655/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3627\n",
            "Epoch 656/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3638\n",
            "Epoch 657/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3673\n",
            "Epoch 658/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3657\n",
            "Epoch 659/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3681\n",
            "Epoch 660/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3675\n",
            "Epoch 661/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3671\n",
            "Epoch 662/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3805 - val_loss: 0.3648\n",
            "Epoch 663/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3768 - val_loss: 0.3669\n",
            "Epoch 664/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3788 - val_loss: 0.3631\n",
            "Epoch 665/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3773 - val_loss: 0.3692\n",
            "Epoch 666/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3766 - val_loss: 0.3681\n",
            "Epoch 667/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3665\n",
            "Epoch 668/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3674\n",
            "Epoch 669/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3685\n",
            "Epoch 670/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3642\n",
            "Epoch 671/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3714\n",
            "Epoch 672/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3651\n",
            "Epoch 673/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3775 - val_loss: 0.3659\n",
            "Epoch 674/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.3678\n",
            "Epoch 675/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3699\n",
            "Epoch 676/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3645\n",
            "Epoch 677/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3667\n",
            "Epoch 678/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3772 - val_loss: 0.3667\n",
            "Epoch 679/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3788 - val_loss: 0.3653\n",
            "Epoch 680/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3768 - val_loss: 0.3644\n",
            "Epoch 681/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3775 - val_loss: 0.3689\n",
            "Epoch 682/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3779 - val_loss: 0.3695\n",
            "Epoch 683/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3677\n",
            "Epoch 684/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3682\n",
            "Epoch 685/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3771 - val_loss: 0.3664\n",
            "Epoch 686/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3776 - val_loss: 0.3669\n",
            "Epoch 687/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3765 - val_loss: 0.3657\n",
            "Epoch 688/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3810 - val_loss: 0.3697\n",
            "Epoch 689/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3810 - val_loss: 0.3688\n",
            "Epoch 690/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3769 - val_loss: 0.3637\n",
            "Epoch 691/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3659\n",
            "Epoch 692/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3669\n",
            "Epoch 693/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3763 - val_loss: 0.3694\n",
            "Epoch 694/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3769 - val_loss: 0.3676\n",
            "Epoch 695/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3777 - val_loss: 0.3696\n",
            "Epoch 696/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3777 - val_loss: 0.3736\n",
            "Epoch 697/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3772 - val_loss: 0.3644\n",
            "Epoch 698/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3715\n",
            "Epoch 699/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3649\n",
            "Epoch 700/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3711\n",
            "Epoch 701/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3709\n",
            "Epoch 702/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3765 - val_loss: 0.3667\n",
            "Epoch 703/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3658\n",
            "Epoch 704/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3674\n",
            "Epoch 705/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3630\n",
            "Epoch 706/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3759 - val_loss: 0.3653\n",
            "Epoch 707/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3775 - val_loss: 0.3638\n",
            "Epoch 708/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3629\n",
            "Epoch 709/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3776 - val_loss: 0.3902\n",
            "Epoch 710/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3807 - val_loss: 0.3681\n",
            "Epoch 711/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3798 - val_loss: 0.3658\n",
            "Epoch 712/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3768 - val_loss: 0.3691\n",
            "Epoch 713/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3766 - val_loss: 0.3677\n",
            "Epoch 714/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3677\n",
            "Epoch 715/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3652\n",
            "Epoch 716/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3633\n",
            "Epoch 717/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3763 - val_loss: 0.3647\n",
            "Epoch 718/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3621\n",
            "Epoch 719/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3706\n",
            "Epoch 720/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3674\n",
            "Epoch 721/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3681\n",
            "Epoch 722/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3627\n",
            "Epoch 723/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3669\n",
            "Epoch 724/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3697\n",
            "Epoch 725/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3780 - val_loss: 0.3650\n",
            "Epoch 726/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3762 - val_loss: 0.3651\n",
            "Epoch 727/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3767 - val_loss: 0.3665\n",
            "Epoch 728/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3782 - val_loss: 0.3664\n",
            "Epoch 729/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3637\n",
            "Epoch 730/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3643\n",
            "Epoch 731/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3681\n",
            "Epoch 732/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3694\n",
            "Epoch 733/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3777 - val_loss: 0.3661\n",
            "Epoch 734/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3768 - val_loss: 0.3641\n",
            "Epoch 735/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3652\n",
            "Epoch 736/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3718\n",
            "Epoch 737/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3789 - val_loss: 0.3713\n",
            "Epoch 738/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3665\n",
            "Epoch 739/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3667\n",
            "Epoch 740/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3709\n",
            "Epoch 741/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3785 - val_loss: 0.3690\n",
            "Epoch 742/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3815 - val_loss: 0.3630\n",
            "Epoch 743/2000\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.3766 - val_loss: 0.3676\n",
            "Epoch 744/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3766 - val_loss: 0.3661\n",
            "Epoch 745/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3758 - val_loss: 0.3681\n",
            "Epoch 746/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3769 - val_loss: 0.3680\n",
            "Epoch 747/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3686\n",
            "Epoch 748/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3635\n",
            "Epoch 749/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3668\n",
            "Epoch 750/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3633\n",
            "Epoch 751/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3696\n",
            "Epoch 752/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3659\n",
            "Epoch 753/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.3717\n",
            "Epoch 754/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3754 - val_loss: 0.3679\n",
            "Epoch 755/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3763 - val_loss: 0.3641\n",
            "Epoch 756/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.3663\n",
            "Epoch 757/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3756 - val_loss: 0.3704\n",
            "Epoch 758/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3771 - val_loss: 0.3659\n",
            "Epoch 759/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3761 - val_loss: 0.3633\n",
            "Epoch 760/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3752 - val_loss: 0.3636\n",
            "Epoch 761/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3763 - val_loss: 0.3658\n",
            "Epoch 762/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3768 - val_loss: 0.3668\n",
            "Epoch 763/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3637\n",
            "Epoch 764/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3680\n",
            "Epoch 765/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3669\n",
            "Epoch 766/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3754 - val_loss: 0.3626\n",
            "Epoch 767/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.3640\n",
            "Epoch 768/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3635\n",
            "Epoch 769/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3749 - val_loss: 0.3672\n",
            "Epoch 770/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.3641\n",
            "Epoch 771/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3773 - val_loss: 0.3645\n",
            "Epoch 772/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3768 - val_loss: 0.3683\n",
            "Epoch 773/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3756 - val_loss: 0.3659\n",
            "Epoch 774/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3792 - val_loss: 0.3692\n",
            "Epoch 775/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3772 - val_loss: 0.3690\n",
            "Epoch 776/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3675\n",
            "Epoch 777/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3759 - val_loss: 0.3669\n",
            "Epoch 778/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3700\n",
            "Epoch 779/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3678\n",
            "Epoch 780/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3773 - val_loss: 0.3705\n",
            "Epoch 781/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3748 - val_loss: 0.3658\n",
            "Epoch 782/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3662\n",
            "Epoch 783/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3668\n",
            "Epoch 784/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3664\n",
            "Epoch 785/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3673\n",
            "Epoch 786/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3752 - val_loss: 0.3710\n",
            "Epoch 787/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3759 - val_loss: 0.3649\n",
            "Epoch 788/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3765 - val_loss: 0.3710\n",
            "Epoch 789/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3757 - val_loss: 0.3654\n",
            "Epoch 790/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3766 - val_loss: 0.3626\n",
            "Epoch 791/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3764 - val_loss: 0.3628\n",
            "Epoch 792/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3762 - val_loss: 0.3703\n",
            "Epoch 793/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3748 - val_loss: 0.3642\n",
            "Epoch 794/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3751 - val_loss: 0.3682\n",
            "Epoch 795/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3638\n",
            "Epoch 796/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3654\n",
            "Epoch 797/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3759 - val_loss: 0.3685\n",
            "Epoch 798/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3686\n",
            "Epoch 799/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3751 - val_loss: 0.3663\n",
            "Epoch 800/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3702\n",
            "Epoch 801/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3670\n",
            "Epoch 802/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3756 - val_loss: 0.3624\n",
            "Epoch 803/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3752 - val_loss: 0.3650\n",
            "Epoch 804/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3750 - val_loss: 0.3634\n",
            "Epoch 805/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3773 - val_loss: 0.3659\n",
            "Epoch 806/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3751 - val_loss: 0.3712\n",
            "Epoch 807/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3765 - val_loss: 0.3666\n",
            "Epoch 808/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3629\n",
            "Epoch 809/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3743 - val_loss: 0.3635\n",
            "Epoch 810/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3640\n",
            "Epoch 811/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3655\n",
            "Epoch 812/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3757 - val_loss: 0.3668\n",
            "Epoch 813/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3649\n",
            "Epoch 814/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3658\n",
            "Epoch 815/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3653\n",
            "Epoch 816/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3748 - val_loss: 0.3642\n",
            "Epoch 817/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3764 - val_loss: 0.3630\n",
            "Epoch 818/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3754 - val_loss: 0.3678\n",
            "Epoch 819/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3759 - val_loss: 0.3655\n",
            "Epoch 820/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3769 - val_loss: 0.3640\n",
            "Epoch 821/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3675\n",
            "Epoch 822/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.3663\n",
            "Epoch 823/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3696\n",
            "Epoch 824/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3751 - val_loss: 0.3674\n",
            "Epoch 825/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3765 - val_loss: 0.3662\n",
            "Epoch 826/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3663\n",
            "Epoch 827/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3807 - val_loss: 0.3620\n",
            "Epoch 828/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3775 - val_loss: 0.3662\n",
            "Epoch 829/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3748 - val_loss: 0.3669\n",
            "Epoch 830/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3753 - val_loss: 0.3645\n",
            "Epoch 831/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3681\n",
            "Epoch 832/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3749 - val_loss: 0.3661\n",
            "Epoch 833/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3757 - val_loss: 0.3625\n",
            "Epoch 834/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3756 - val_loss: 0.3656\n",
            "Epoch 835/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3754 - val_loss: 0.3666\n",
            "Epoch 836/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3644\n",
            "Epoch 837/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3628\n",
            "Epoch 838/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3707\n",
            "Epoch 839/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3689\n",
            "Epoch 840/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3771 - val_loss: 0.3640\n",
            "Epoch 841/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3799 - val_loss: 0.3793\n",
            "Epoch 842/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3781 - val_loss: 0.3638\n",
            "Epoch 843/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3754 - val_loss: 0.3661\n",
            "Epoch 844/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3665\n",
            "Epoch 845/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3759 - val_loss: 0.3627\n",
            "Epoch 846/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3616\n",
            "Epoch 847/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3749 - val_loss: 0.3669\n",
            "Epoch 848/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3770 - val_loss: 0.3620\n",
            "Epoch 849/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3766 - val_loss: 0.3662\n",
            "Epoch 850/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3759 - val_loss: 0.3667\n",
            "Epoch 851/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3749 - val_loss: 0.3641\n",
            "Epoch 852/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3679\n",
            "Epoch 853/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3631\n",
            "Epoch 854/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3659\n",
            "Epoch 855/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3696\n",
            "Epoch 856/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3754 - val_loss: 0.3656\n",
            "Epoch 857/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3714\n",
            "Epoch 858/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3743 - val_loss: 0.3709\n",
            "Epoch 859/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3686\n",
            "Epoch 860/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3752 - val_loss: 0.3706\n",
            "Epoch 861/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3775 - val_loss: 0.3665\n",
            "Epoch 862/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3677\n",
            "Epoch 863/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3746 - val_loss: 0.3642\n",
            "Epoch 864/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3761 - val_loss: 0.3664\n",
            "Epoch 865/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3782 - val_loss: 0.3660\n",
            "Epoch 866/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3768 - val_loss: 0.3707\n",
            "Epoch 867/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3757 - val_loss: 0.3694\n",
            "Epoch 868/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3651\n",
            "Epoch 869/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3664\n",
            "Epoch 870/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3641\n",
            "Epoch 871/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3748 - val_loss: 0.3740\n",
            "Epoch 872/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3741\n",
            "Epoch 873/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3692\n",
            "Epoch 874/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3694\n",
            "Epoch 875/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3752 - val_loss: 0.3720\n",
            "Epoch 876/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3667\n",
            "Epoch 877/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3745 - val_loss: 0.3651\n",
            "Epoch 878/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3748 - val_loss: 0.3660\n",
            "Epoch 879/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3752 - val_loss: 0.3652\n",
            "Epoch 880/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3744 - val_loss: 0.3652\n",
            "Epoch 881/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3744 - val_loss: 0.3718\n",
            "Epoch 882/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3758 - val_loss: 0.3647\n",
            "Epoch 883/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3742 - val_loss: 0.3679\n",
            "Epoch 884/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3751 - val_loss: 0.3620\n",
            "Epoch 885/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3671\n",
            "Epoch 886/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.3634\n",
            "Epoch 887/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3698\n",
            "Epoch 888/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3753 - val_loss: 0.3657\n",
            "Epoch 889/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3658\n",
            "Epoch 890/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3695\n",
            "Epoch 891/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3668\n",
            "Epoch 892/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3752 - val_loss: 0.3743\n",
            "Epoch 893/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3735 - val_loss: 0.3667\n",
            "Epoch 894/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3757 - val_loss: 0.3660\n",
            "Epoch 895/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3751 - val_loss: 0.3679\n",
            "Epoch 896/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3735 - val_loss: 0.3681\n",
            "Epoch 897/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3754 - val_loss: 0.3686\n",
            "Epoch 898/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3781 - val_loss: 0.3661\n",
            "Epoch 899/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3683\n",
            "Epoch 900/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3753 - val_loss: 0.3629\n",
            "Epoch 901/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3644\n",
            "Epoch 902/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3764 - val_loss: 0.3685\n",
            "Epoch 903/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3770 - val_loss: 0.3657\n",
            "Epoch 904/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3733 - val_loss: 0.3725\n",
            "Epoch 905/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3763 - val_loss: 0.3667\n",
            "Epoch 906/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3741 - val_loss: 0.3644\n",
            "Epoch 907/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3757 - val_loss: 0.3770\n",
            "Epoch 908/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3747 - val_loss: 0.3676\n",
            "Epoch 909/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3747 - val_loss: 0.3649\n",
            "Epoch 910/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3736 - val_loss: 0.3694\n",
            "Epoch 911/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3731 - val_loss: 0.3697\n",
            "Epoch 912/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3753 - val_loss: 0.3706\n",
            "Epoch 913/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3662\n",
            "Epoch 914/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3664\n",
            "Epoch 915/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3748 - val_loss: 0.3634\n",
            "Epoch 916/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3666\n",
            "Epoch 917/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3666\n",
            "Epoch 918/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3673\n",
            "Epoch 919/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3652\n",
            "Epoch 920/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3700\n",
            "Epoch 921/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3742 - val_loss: 0.3718\n",
            "Epoch 922/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3751 - val_loss: 0.3700\n",
            "Epoch 923/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3747 - val_loss: 0.3726\n",
            "Epoch 924/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3738 - val_loss: 0.3699\n",
            "Epoch 925/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3743 - val_loss: 0.3709\n",
            "Epoch 926/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3730 - val_loss: 0.3692\n",
            "Epoch 927/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3741 - val_loss: 0.3708\n",
            "Epoch 928/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3743 - val_loss: 0.3667\n",
            "Epoch 929/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3765 - val_loss: 0.3739\n",
            "Epoch 930/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3745 - val_loss: 0.3723\n",
            "Epoch 931/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3715\n",
            "Epoch 932/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3774 - val_loss: 0.3636\n",
            "Epoch 933/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3656\n",
            "Epoch 934/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3747 - val_loss: 0.3628\n",
            "Epoch 935/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3701\n",
            "Epoch 936/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3736 - val_loss: 0.3708\n",
            "Epoch 937/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3766 - val_loss: 0.3661\n",
            "Epoch 938/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3743 - val_loss: 0.3668\n",
            "Epoch 939/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3743 - val_loss: 0.3713\n",
            "Epoch 940/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3752 - val_loss: 0.3668\n",
            "Epoch 941/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3749 - val_loss: 0.3717\n",
            "Epoch 942/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3676\n",
            "Epoch 943/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3737 - val_loss: 0.3669\n",
            "Epoch 944/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3713\n",
            "Epoch 945/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3739 - val_loss: 0.3709\n",
            "Epoch 946/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3753 - val_loss: 0.3666\n",
            "Epoch 947/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3756 - val_loss: 0.3720\n",
            "Epoch 948/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3754 - val_loss: 0.3676\n",
            "Epoch 949/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3647\n",
            "Epoch 950/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3672\n",
            "Epoch 951/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3754 - val_loss: 0.3664\n",
            "Epoch 952/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3737 - val_loss: 0.3665\n",
            "Epoch 953/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3770 - val_loss: 0.3707\n",
            "Epoch 954/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3748 - val_loss: 0.3716\n",
            "Epoch 955/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3749\n",
            "Epoch 956/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3744 - val_loss: 0.3709\n",
            "Epoch 957/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.3659\n",
            "Epoch 958/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3668\n",
            "Epoch 959/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3733\n",
            "Epoch 960/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3642\n",
            "Epoch 961/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3733 - val_loss: 0.3699\n",
            "Epoch 962/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3737 - val_loss: 0.3664\n",
            "Epoch 963/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3755 - val_loss: 0.3695\n",
            "Epoch 964/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.3707\n",
            "Epoch 965/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3741 - val_loss: 0.3660\n",
            "Epoch 966/2000\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.3736 - val_loss: 0.3641\n",
            "Epoch 967/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3749 - val_loss: 0.3680\n",
            "Epoch 968/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3741 - val_loss: 0.3658\n",
            "Epoch 969/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3648\n",
            "Epoch 970/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3744 - val_loss: 0.3670\n",
            "Epoch 971/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3756 - val_loss: 0.3760\n",
            "Epoch 972/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3747 - val_loss: 0.3623\n",
            "Epoch 973/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3733\n",
            "Epoch 974/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3766 - val_loss: 0.3646\n",
            "Epoch 975/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3746 - val_loss: 0.3659\n",
            "Epoch 976/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3739 - val_loss: 0.3654\n",
            "Epoch 977/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3756 - val_loss: 0.3736\n",
            "Epoch 978/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3760 - val_loss: 0.3655\n",
            "Epoch 979/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3730 - val_loss: 0.3653\n",
            "Epoch 980/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3740 - val_loss: 0.3693\n",
            "Epoch 981/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3746 - val_loss: 0.3683\n",
            "Epoch 982/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3730 - val_loss: 0.3670\n",
            "Epoch 983/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3652\n",
            "Epoch 984/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3728\n",
            "Epoch 985/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3689\n",
            "Epoch 986/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3668\n",
            "Epoch 987/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.3655\n",
            "Epoch 988/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3729\n",
            "Epoch 989/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3667\n",
            "Epoch 990/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3641\n",
            "Epoch 991/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3635\n",
            "Epoch 992/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3764 - val_loss: 0.3757\n",
            "Epoch 993/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3767 - val_loss: 0.3709\n",
            "Epoch 994/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3757 - val_loss: 0.3702\n",
            "Epoch 995/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3748 - val_loss: 0.3657\n",
            "Epoch 996/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3737 - val_loss: 0.3668\n",
            "Epoch 997/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3738 - val_loss: 0.3675\n",
            "Epoch 998/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3675\n",
            "Epoch 999/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3628\n",
            "Epoch 1000/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3696\n",
            "Epoch 1001/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3736 - val_loss: 0.3645\n",
            "Epoch 1002/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3740 - val_loss: 0.3716\n",
            "Epoch 1003/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3731 - val_loss: 0.3689\n",
            "Epoch 1004/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3737 - val_loss: 0.3691\n",
            "Epoch 1005/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3747 - val_loss: 0.3687\n",
            "Epoch 1006/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3750 - val_loss: 0.3675\n",
            "Epoch 1007/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3743 - val_loss: 0.3675\n",
            "Epoch 1008/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3720 - val_loss: 0.3668\n",
            "Epoch 1009/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3751 - val_loss: 0.3665\n",
            "Epoch 1010/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3739 - val_loss: 0.3682\n",
            "Epoch 1011/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3735 - val_loss: 0.3654\n",
            "Epoch 1012/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3655\n",
            "Epoch 1013/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3673\n",
            "Epoch 1014/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3657\n",
            "Epoch 1015/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3667\n",
            "Epoch 1016/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3630\n",
            "Epoch 1017/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3718\n",
            "Epoch 1018/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3738\n",
            "Epoch 1019/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3743 - val_loss: 0.3688\n",
            "Epoch 1020/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3664\n",
            "Epoch 1021/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3744 - val_loss: 0.3729\n",
            "Epoch 1022/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3754 - val_loss: 0.3664\n",
            "Epoch 1023/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3727 - val_loss: 0.3658\n",
            "Epoch 1024/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3725 - val_loss: 0.3656\n",
            "Epoch 1025/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3734 - val_loss: 0.3675\n",
            "Epoch 1026/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3733 - val_loss: 0.3693\n",
            "Epoch 1027/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3660\n",
            "Epoch 1028/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3749 - val_loss: 0.3671\n",
            "Epoch 1029/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3742\n",
            "Epoch 1030/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3715\n",
            "Epoch 1031/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3750\n",
            "Epoch 1032/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3690\n",
            "Epoch 1033/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3698\n",
            "Epoch 1034/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3665\n",
            "Epoch 1035/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3748\n",
            "Epoch 1036/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.3659\n",
            "Epoch 1037/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3731 - val_loss: 0.3659\n",
            "Epoch 1038/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3705\n",
            "Epoch 1039/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3739 - val_loss: 0.3701\n",
            "Epoch 1040/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3732 - val_loss: 0.3710\n",
            "Epoch 1041/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3740 - val_loss: 0.3699\n",
            "Epoch 1042/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3736 - val_loss: 0.3683\n",
            "Epoch 1043/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3680\n",
            "Epoch 1044/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3671\n",
            "Epoch 1045/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3661\n",
            "Epoch 1046/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3720\n",
            "Epoch 1047/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3629\n",
            "Epoch 1048/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3675\n",
            "Epoch 1049/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3632\n",
            "Epoch 1050/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3644\n",
            "Epoch 1051/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3715\n",
            "Epoch 1052/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3671\n",
            "Epoch 1053/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.3679\n",
            "Epoch 1054/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3736 - val_loss: 0.3677\n",
            "Epoch 1055/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3739 - val_loss: 0.3718\n",
            "Epoch 1056/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3730 - val_loss: 0.3721\n",
            "Epoch 1057/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3727 - val_loss: 0.3678\n",
            "Epoch 1058/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3731 - val_loss: 0.3696\n",
            "Epoch 1059/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3689\n",
            "Epoch 1060/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3651\n",
            "Epoch 1061/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3669\n",
            "Epoch 1062/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3649\n",
            "Epoch 1063/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3624\n",
            "Epoch 1064/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3674\n",
            "Epoch 1065/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3668\n",
            "Epoch 1066/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3657\n",
            "Epoch 1067/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3648\n",
            "Epoch 1068/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3696\n",
            "Epoch 1069/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3684\n",
            "Epoch 1070/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3726 - val_loss: 0.3696\n",
            "Epoch 1071/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3750 - val_loss: 0.3705\n",
            "Epoch 1072/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3740 - val_loss: 0.3646\n",
            "Epoch 1073/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3743 - val_loss: 0.3673\n",
            "Epoch 1074/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3737 - val_loss: 0.3730\n",
            "Epoch 1075/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3744 - val_loss: 0.3672\n",
            "Epoch 1076/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.3650\n",
            "Epoch 1077/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.3661\n",
            "Epoch 1078/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3660\n",
            "Epoch 1079/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3643\n",
            "Epoch 1080/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3665\n",
            "Epoch 1081/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3722\n",
            "Epoch 1082/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3705\n",
            "Epoch 1083/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3732\n",
            "Epoch 1084/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3655\n",
            "Epoch 1085/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3722\n",
            "Epoch 1086/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3649\n",
            "Epoch 1087/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3731 - val_loss: 0.3674\n",
            "Epoch 1088/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3724 - val_loss: 0.3673\n",
            "Epoch 1089/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3716 - val_loss: 0.3667\n",
            "Epoch 1090/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3719 - val_loss: 0.3687\n",
            "Epoch 1091/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3684\n",
            "Epoch 1092/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3635\n",
            "Epoch 1093/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.3705\n",
            "Epoch 1094/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3713\n",
            "Epoch 1095/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3725\n",
            "Epoch 1096/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3712\n",
            "Epoch 1097/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3699\n",
            "Epoch 1098/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3667\n",
            "Epoch 1099/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3664\n",
            "Epoch 1100/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3637\n",
            "Epoch 1101/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3706\n",
            "Epoch 1102/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3694\n",
            "Epoch 1103/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3732 - val_loss: 0.3644\n",
            "Epoch 1104/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3719 - val_loss: 0.3687\n",
            "Epoch 1105/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3727 - val_loss: 0.3636\n",
            "Epoch 1106/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3714 - val_loss: 0.3696\n",
            "Epoch 1107/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3737 - val_loss: 0.3649\n",
            "Epoch 1108/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3665\n",
            "Epoch 1109/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3683\n",
            "Epoch 1110/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3718\n",
            "Epoch 1111/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.3738\n",
            "Epoch 1112/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.3753\n",
            "Epoch 1113/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3704\n",
            "Epoch 1114/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3717\n",
            "Epoch 1115/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3663\n",
            "Epoch 1116/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3748 - val_loss: 0.3678\n",
            "Epoch 1117/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3671\n",
            "Epoch 1118/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3655\n",
            "Epoch 1119/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3738 - val_loss: 0.3652\n",
            "Epoch 1120/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3722 - val_loss: 0.3676\n",
            "Epoch 1121/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3722 - val_loss: 0.3651\n",
            "Epoch 1122/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3735 - val_loss: 0.3807\n",
            "Epoch 1123/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3772 - val_loss: 0.3647\n",
            "Epoch 1124/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3685\n",
            "Epoch 1125/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3661\n",
            "Epoch 1126/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3718 - val_loss: 0.3707\n",
            "Epoch 1127/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3683\n",
            "Epoch 1128/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3660\n",
            "Epoch 1129/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3694\n",
            "Epoch 1130/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3653\n",
            "Epoch 1131/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3682\n",
            "Epoch 1132/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3687\n",
            "Epoch 1133/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3741\n",
            "Epoch 1134/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3730 - val_loss: 0.3650\n",
            "Epoch 1135/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3726 - val_loss: 0.3670\n",
            "Epoch 1136/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3718 - val_loss: 0.3697\n",
            "Epoch 1137/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3720 - val_loss: 0.3669\n",
            "Epoch 1138/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3727 - val_loss: 0.3652\n",
            "Epoch 1139/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.3670\n",
            "Epoch 1140/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3655\n",
            "Epoch 1141/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3751\n",
            "Epoch 1142/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3705\n",
            "Epoch 1143/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3663\n",
            "Epoch 1144/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3674\n",
            "Epoch 1145/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3701\n",
            "Epoch 1146/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3708\n",
            "Epoch 1147/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3678\n",
            "Epoch 1148/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3664\n",
            "Epoch 1149/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3669\n",
            "Epoch 1150/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3726 - val_loss: 0.3631\n",
            "Epoch 1151/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3738 - val_loss: 0.3673\n",
            "Epoch 1152/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3729 - val_loss: 0.3676\n",
            "Epoch 1153/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3721 - val_loss: 0.3636\n",
            "Epoch 1154/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3730 - val_loss: 0.3651\n",
            "Epoch 1155/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3660\n",
            "Epoch 1156/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3674\n",
            "Epoch 1157/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.3622\n",
            "Epoch 1158/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3689\n",
            "Epoch 1159/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3672\n",
            "Epoch 1160/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3681\n",
            "Epoch 1161/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3627\n",
            "Epoch 1162/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3652\n",
            "Epoch 1163/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3696\n",
            "Epoch 1164/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3656\n",
            "Epoch 1165/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3741\n",
            "Epoch 1166/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3742 - val_loss: 0.3694\n",
            "Epoch 1167/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3727 - val_loss: 0.3696\n",
            "Epoch 1168/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3721 - val_loss: 0.3658\n",
            "Epoch 1169/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3730 - val_loss: 0.3687\n",
            "Epoch 1170/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3737 - val_loss: 0.3667\n",
            "Epoch 1171/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3647\n",
            "Epoch 1172/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3707\n",
            "Epoch 1173/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3660\n",
            "Epoch 1174/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3669\n",
            "Epoch 1175/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.3671\n",
            "Epoch 1176/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3660\n",
            "Epoch 1177/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3633\n",
            "Epoch 1178/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3637\n",
            "Epoch 1179/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3651\n",
            "Epoch 1180/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3670\n",
            "Epoch 1181/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3719 - val_loss: 0.3730\n",
            "Epoch 1182/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3733 - val_loss: 0.3728\n",
            "Epoch 1183/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3729 - val_loss: 0.3727\n",
            "Epoch 1184/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3722 - val_loss: 0.3699\n",
            "Epoch 1185/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3739 - val_loss: 0.3703\n",
            "Epoch 1186/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3708\n",
            "Epoch 1187/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3641\n",
            "Epoch 1188/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.3705\n",
            "Epoch 1189/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3687\n",
            "Epoch 1190/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3722\n",
            "Epoch 1191/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3684\n",
            "Epoch 1192/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.3666\n",
            "Epoch 1193/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3695\n",
            "Epoch 1194/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.3662\n",
            "Epoch 1195/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3639\n",
            "Epoch 1196/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3705\n",
            "Epoch 1197/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3719 - val_loss: 0.3634\n",
            "Epoch 1198/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3710 - val_loss: 0.3710\n",
            "Epoch 1199/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3705 - val_loss: 0.3635\n",
            "Epoch 1200/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.3637\n",
            "Epoch 1201/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3737 - val_loss: 0.3678\n",
            "Epoch 1202/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.3701\n",
            "Epoch 1203/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3712 - val_loss: 0.3677\n",
            "Epoch 1204/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3650\n",
            "Epoch 1205/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.3706\n",
            "Epoch 1206/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3698\n",
            "Epoch 1207/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3639\n",
            "Epoch 1208/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.3696\n",
            "Epoch 1209/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.3687\n",
            "Epoch 1210/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3685\n",
            "Epoch 1211/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3677\n",
            "Epoch 1212/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3736 - val_loss: 0.3686\n",
            "Epoch 1213/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3716 - val_loss: 0.3633\n",
            "Epoch 1214/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3725 - val_loss: 0.3660\n",
            "Epoch 1215/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3721 - val_loss: 0.3652\n",
            "Epoch 1216/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3742 - val_loss: 0.3666\n",
            "Epoch 1217/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3659\n",
            "Epoch 1218/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3650\n",
            "Epoch 1219/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3651\n",
            "Epoch 1220/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3689\n",
            "Epoch 1221/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3703\n",
            "Epoch 1222/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3703\n",
            "Epoch 1223/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3650\n",
            "Epoch 1224/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3677\n",
            "Epoch 1225/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.3678\n",
            "Epoch 1226/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3677\n",
            "Epoch 1227/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3673\n",
            "Epoch 1228/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3712 - val_loss: 0.3682\n",
            "Epoch 1229/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3723 - val_loss: 0.3696\n",
            "Epoch 1230/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3723 - val_loss: 0.3680\n",
            "Epoch 1231/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3717 - val_loss: 0.3670\n",
            "Epoch 1232/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3715 - val_loss: 0.3744\n",
            "Epoch 1233/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3622\n",
            "Epoch 1234/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.3757\n",
            "Epoch 1235/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.3709\n",
            "Epoch 1236/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3736 - val_loss: 0.3679\n",
            "Epoch 1237/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3717 - val_loss: 0.3676\n",
            "Epoch 1238/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3733\n",
            "Epoch 1239/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.3663\n",
            "Epoch 1240/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3735\n",
            "Epoch 1241/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3664\n",
            "Epoch 1242/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.3719\n",
            "Epoch 1243/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3729 - val_loss: 0.3649\n",
            "Epoch 1244/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3724 - val_loss: 0.3675\n",
            "Epoch 1245/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3728 - val_loss: 0.3644\n",
            "Epoch 1246/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3729 - val_loss: 0.3733\n",
            "Epoch 1247/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3718 - val_loss: 0.3676\n",
            "Epoch 1248/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3724\n",
            "Epoch 1249/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3695\n",
            "Epoch 1250/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3641\n",
            "Epoch 1251/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3717\n",
            "Epoch 1252/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3644\n",
            "Epoch 1253/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3645\n",
            "Epoch 1254/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3745\n",
            "Epoch 1255/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3631\n",
            "Epoch 1256/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3721 - val_loss: 0.3638\n",
            "Epoch 1257/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3711 - val_loss: 0.3696\n",
            "Epoch 1258/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3714 - val_loss: 0.3763\n",
            "Epoch 1259/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3720 - val_loss: 0.3722\n",
            "Epoch 1260/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3715 - val_loss: 0.3665\n",
            "Epoch 1261/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3725 - val_loss: 0.3702\n",
            "Epoch 1262/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3729 - val_loss: 0.3662\n",
            "Epoch 1263/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3706\n",
            "Epoch 1264/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.3662\n",
            "Epoch 1265/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3673\n",
            "Epoch 1266/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3645\n",
            "Epoch 1267/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3711 - val_loss: 0.3636\n",
            "Epoch 1268/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3715 - val_loss: 0.3680\n",
            "Epoch 1269/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3726 - val_loss: 0.3645\n",
            "Epoch 1270/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3712 - val_loss: 0.3662\n",
            "Epoch 1271/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3712 - val_loss: 0.3663\n",
            "Epoch 1272/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3725 - val_loss: 0.3685\n",
            "Epoch 1273/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3719 - val_loss: 0.3716\n",
            "Epoch 1274/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3715 - val_loss: 0.3645\n",
            "Epoch 1275/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3729 - val_loss: 0.3674\n",
            "Epoch 1276/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3722 - val_loss: 0.3641\n",
            "Epoch 1277/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3678\n",
            "Epoch 1278/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3698\n",
            "Epoch 1279/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3695\n",
            "Epoch 1280/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3690\n",
            "Epoch 1281/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3647\n",
            "Epoch 1282/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3634\n",
            "Epoch 1283/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3784\n",
            "Epoch 1284/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3698\n",
            "Epoch 1285/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3740\n",
            "Epoch 1286/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3708\n",
            "Epoch 1287/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3712 - val_loss: 0.3646\n",
            "Epoch 1288/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3710 - val_loss: 0.3678\n",
            "Epoch 1289/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3716 - val_loss: 0.3673\n",
            "Epoch 1290/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3720 - val_loss: 0.3654\n",
            "Epoch 1291/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3714 - val_loss: 0.3684\n",
            "Epoch 1292/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3716\n",
            "Epoch 1293/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.3661\n",
            "Epoch 1294/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3725\n",
            "Epoch 1295/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3701\n",
            "Epoch 1296/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3673\n",
            "Epoch 1297/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3648\n",
            "Epoch 1298/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3620\n",
            "Epoch 1299/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.3666\n",
            "Epoch 1300/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3637\n",
            "Epoch 1301/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3675\n",
            "Epoch 1302/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3714 - val_loss: 0.3734\n",
            "Epoch 1303/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3719 - val_loss: 0.3679\n",
            "Epoch 1304/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3718 - val_loss: 0.3653\n",
            "Epoch 1305/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3721 - val_loss: 0.3672\n",
            "Epoch 1306/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3712 - val_loss: 0.3680\n",
            "Epoch 1307/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3646\n",
            "Epoch 1308/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3663\n",
            "Epoch 1309/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3701\n",
            "Epoch 1310/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.3687\n",
            "Epoch 1311/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.3676\n",
            "Epoch 1312/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3698\n",
            "Epoch 1313/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.3694\n",
            "Epoch 1314/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3668\n",
            "Epoch 1315/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3624\n",
            "Epoch 1316/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3636\n",
            "Epoch 1317/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3729\n",
            "Epoch 1318/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3715\n",
            "Epoch 1319/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3722 - val_loss: 0.3642\n",
            "Epoch 1320/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3719 - val_loss: 0.3728\n",
            "Epoch 1321/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3723 - val_loss: 0.3688\n",
            "Epoch 1322/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.3688\n",
            "Epoch 1323/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3653\n",
            "Epoch 1324/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.3677\n",
            "Epoch 1325/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3684\n",
            "Epoch 1326/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.3708\n",
            "Epoch 1327/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3658\n",
            "Epoch 1328/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3711\n",
            "Epoch 1329/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3739 - val_loss: 0.3687\n",
            "Epoch 1330/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3726 - val_loss: 0.3659\n",
            "Epoch 1331/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3689\n",
            "Epoch 1332/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3703 - val_loss: 0.3696\n",
            "Epoch 1333/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3680\n",
            "Epoch 1334/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3716 - val_loss: 0.3633\n",
            "Epoch 1335/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3708 - val_loss: 0.3656\n",
            "Epoch 1336/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3710 - val_loss: 0.3641\n",
            "Epoch 1337/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3711 - val_loss: 0.3655\n",
            "Epoch 1338/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3708 - val_loss: 0.3671\n",
            "Epoch 1339/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3704\n",
            "Epoch 1340/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3682\n",
            "Epoch 1341/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3728 - val_loss: 0.3667\n",
            "Epoch 1342/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3675\n",
            "Epoch 1343/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.3650\n",
            "Epoch 1344/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3634\n",
            "Epoch 1345/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3636\n",
            "Epoch 1346/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3699\n",
            "Epoch 1347/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3697\n",
            "Epoch 1348/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3735\n",
            "Epoch 1349/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3654\n",
            "Epoch 1350/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3713 - val_loss: 0.3681\n",
            "Epoch 1351/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3706 - val_loss: 0.3658\n",
            "Epoch 1352/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.3673\n",
            "Epoch 1353/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3704 - val_loss: 0.3681\n",
            "Epoch 1354/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3722 - val_loss: 0.3650\n",
            "Epoch 1355/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3768\n",
            "Epoch 1356/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3718 - val_loss: 0.3660\n",
            "Epoch 1357/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3655\n",
            "Epoch 1358/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3666\n",
            "Epoch 1359/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.3676\n",
            "Epoch 1360/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3635\n",
            "Epoch 1361/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3628\n",
            "Epoch 1362/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3711 - val_loss: 0.3726\n",
            "Epoch 1363/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.3677\n",
            "Epoch 1364/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.3709\n",
            "Epoch 1365/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3719 - val_loss: 0.3728\n",
            "Epoch 1366/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3738 - val_loss: 0.3683\n",
            "Epoch 1367/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3739 - val_loss: 0.3673\n",
            "Epoch 1368/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3708 - val_loss: 0.3714\n",
            "Epoch 1369/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3706 - val_loss: 0.3676\n",
            "Epoch 1370/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3626\n",
            "Epoch 1371/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3690\n",
            "Epoch 1372/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3674\n",
            "Epoch 1373/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3659\n",
            "Epoch 1374/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3669\n",
            "Epoch 1375/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3642\n",
            "Epoch 1376/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3715\n",
            "Epoch 1377/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.3671\n",
            "Epoch 1378/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3666\n",
            "Epoch 1379/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3664\n",
            "Epoch 1380/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3717 - val_loss: 0.3744\n",
            "Epoch 1381/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3710 - val_loss: 0.3685\n",
            "Epoch 1382/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3716 - val_loss: 0.3619\n",
            "Epoch 1383/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3712 - val_loss: 0.3691\n",
            "Epoch 1384/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3717 - val_loss: 0.3648\n",
            "Epoch 1385/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3697\n",
            "Epoch 1386/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3661\n",
            "Epoch 1387/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3714 - val_loss: 0.3651\n",
            "Epoch 1388/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3690\n",
            "Epoch 1389/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3726\n",
            "Epoch 1390/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3677\n",
            "Epoch 1391/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3694\n",
            "Epoch 1392/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3708\n",
            "Epoch 1393/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.3659\n",
            "Epoch 1394/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3651\n",
            "Epoch 1395/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3715 - val_loss: 0.3652\n",
            "Epoch 1396/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3701 - val_loss: 0.3638\n",
            "Epoch 1397/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.3694\n",
            "Epoch 1398/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3701 - val_loss: 0.3643\n",
            "Epoch 1399/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3707 - val_loss: 0.3639\n",
            "Epoch 1400/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3854\n",
            "Epoch 1401/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3720\n",
            "Epoch 1402/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.3681\n",
            "Epoch 1403/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3679\n",
            "Epoch 1404/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3651\n",
            "Epoch 1405/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3674\n",
            "Epoch 1406/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3623\n",
            "Epoch 1407/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3692\n",
            "Epoch 1408/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.3726\n",
            "Epoch 1409/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3701\n",
            "Epoch 1410/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3655\n",
            "Epoch 1411/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3648\n",
            "Epoch 1412/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3707 - val_loss: 0.3658\n",
            "Epoch 1413/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3708 - val_loss: 0.3682\n",
            "Epoch 1414/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3708 - val_loss: 0.3702\n",
            "Epoch 1415/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3716 - val_loss: 0.3758\n",
            "Epoch 1416/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3716 - val_loss: 0.3653\n",
            "Epoch 1417/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.3619\n",
            "Epoch 1418/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3640\n",
            "Epoch 1419/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.3679\n",
            "Epoch 1420/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.3713\n",
            "Epoch 1421/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.3692\n",
            "Epoch 1422/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.3714\n",
            "Epoch 1423/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3683\n",
            "Epoch 1424/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3674\n",
            "Epoch 1425/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.3740\n",
            "Epoch 1426/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3681\n",
            "Epoch 1427/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3638\n",
            "Epoch 1428/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3702 - val_loss: 0.3672\n",
            "Epoch 1429/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3711 - val_loss: 0.3651\n",
            "Epoch 1430/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3707 - val_loss: 0.3650\n",
            "Epoch 1431/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3722 - val_loss: 0.3681\n",
            "Epoch 1432/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3674\n",
            "Epoch 1433/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3676\n",
            "Epoch 1434/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.3681\n",
            "Epoch 1435/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3680\n",
            "Epoch 1436/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.3633\n",
            "Epoch 1437/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3660\n",
            "Epoch 1438/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3675\n",
            "Epoch 1439/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3717\n",
            "Epoch 1440/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3656\n",
            "Epoch 1441/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3669\n",
            "Epoch 1442/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3679\n",
            "Epoch 1443/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3655\n",
            "Epoch 1444/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3698 - val_loss: 0.3659\n",
            "Epoch 1445/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3702 - val_loss: 0.3673\n",
            "Epoch 1446/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3698 - val_loss: 0.3718\n",
            "Epoch 1447/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.3668\n",
            "Epoch 1448/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3702 - val_loss: 0.3669\n",
            "Epoch 1449/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3689\n",
            "Epoch 1450/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3678\n",
            "Epoch 1451/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3627\n",
            "Epoch 1452/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3638\n",
            "Epoch 1453/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3716\n",
            "Epoch 1454/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3660\n",
            "Epoch 1455/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3645\n",
            "Epoch 1456/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3637\n",
            "Epoch 1457/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3655\n",
            "Epoch 1458/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3705 - val_loss: 0.3693\n",
            "Epoch 1459/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3671\n",
            "Epoch 1460/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3703 - val_loss: 0.3697\n",
            "Epoch 1461/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3702 - val_loss: 0.3676\n",
            "Epoch 1462/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3722 - val_loss: 0.3674\n",
            "Epoch 1463/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3715 - val_loss: 0.3639\n",
            "Epoch 1464/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3715 - val_loss: 0.3745\n",
            "Epoch 1465/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3712\n",
            "Epoch 1466/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3631\n",
            "Epoch 1467/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.3674\n",
            "Epoch 1468/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3696\n",
            "Epoch 1469/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3701\n",
            "Epoch 1470/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3720\n",
            "Epoch 1471/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.3687\n",
            "Epoch 1472/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3689\n",
            "Epoch 1473/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3643\n",
            "Epoch 1474/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3703 - val_loss: 0.3667\n",
            "Epoch 1475/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3651\n",
            "Epoch 1476/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3709 - val_loss: 0.3664\n",
            "Epoch 1477/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3706 - val_loss: 0.3626\n",
            "Epoch 1478/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3709 - val_loss: 0.3741\n",
            "Epoch 1479/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3696\n",
            "Epoch 1480/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3707 - val_loss: 0.3706\n",
            "Epoch 1481/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.3691\n",
            "Epoch 1482/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3655\n",
            "Epoch 1483/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.3705\n",
            "Epoch 1484/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3621\n",
            "Epoch 1485/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3630\n",
            "Epoch 1486/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.3672\n",
            "Epoch 1487/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3708\n",
            "Epoch 1488/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3716 - val_loss: 0.3675\n",
            "Epoch 1489/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3647\n",
            "Epoch 1490/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3684\n",
            "Epoch 1491/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3651\n",
            "Epoch 1492/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3716 - val_loss: 0.3742\n",
            "Epoch 1493/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3701 - val_loss: 0.3716\n",
            "Epoch 1494/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3698 - val_loss: 0.3651\n",
            "Epoch 1495/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3711 - val_loss: 0.3686\n",
            "Epoch 1496/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3631\n",
            "Epoch 1497/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3664\n",
            "Epoch 1498/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3691\n",
            "Epoch 1499/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3636\n",
            "Epoch 1500/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3647\n",
            "Epoch 1501/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3668\n",
            "Epoch 1502/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.3685\n",
            "Epoch 1503/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3660\n",
            "Epoch 1504/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.3678\n",
            "Epoch 1505/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3676\n",
            "Epoch 1506/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3598\n",
            "Epoch 1507/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3701 - val_loss: 0.3671\n",
            "Epoch 1508/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.3658\n",
            "Epoch 1509/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3706 - val_loss: 0.3691\n",
            "Epoch 1510/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3718 - val_loss: 0.3642\n",
            "Epoch 1511/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3693 - val_loss: 0.3675\n",
            "Epoch 1512/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3705 - val_loss: 0.3700\n",
            "Epoch 1513/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3679\n",
            "Epoch 1514/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3705 - val_loss: 0.3648\n",
            "Epoch 1515/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3701\n",
            "Epoch 1516/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3686\n",
            "Epoch 1517/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.3643\n",
            "Epoch 1518/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3657\n",
            "Epoch 1519/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3679\n",
            "Epoch 1520/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3629\n",
            "Epoch 1521/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3659\n",
            "Epoch 1522/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3708 - val_loss: 0.3663\n",
            "Epoch 1523/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3721 - val_loss: 0.3678\n",
            "Epoch 1524/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3698 - val_loss: 0.3633\n",
            "Epoch 1525/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3710 - val_loss: 0.3628\n",
            "Epoch 1526/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3706 - val_loss: 0.3709\n",
            "Epoch 1527/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3708 - val_loss: 0.3655\n",
            "Epoch 1528/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3708 - val_loss: 0.3654\n",
            "Epoch 1529/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3704 - val_loss: 0.3739\n",
            "Epoch 1530/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3704 - val_loss: 0.3673\n",
            "Epoch 1531/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3696 - val_loss: 0.3690\n",
            "Epoch 1532/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3690\n",
            "Epoch 1533/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3693\n",
            "Epoch 1534/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.3694\n",
            "Epoch 1535/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3691\n",
            "Epoch 1536/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3645\n",
            "Epoch 1537/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3666\n",
            "Epoch 1538/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3702 - val_loss: 0.3699\n",
            "Epoch 1539/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3711 - val_loss: 0.3655\n",
            "Epoch 1540/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3711 - val_loss: 0.3699\n",
            "Epoch 1541/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3717 - val_loss: 0.3680\n",
            "Epoch 1542/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3685\n",
            "Epoch 1543/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3735\n",
            "Epoch 1544/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3631\n",
            "Epoch 1545/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3637\n",
            "Epoch 1546/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3705 - val_loss: 0.3655\n",
            "Epoch 1547/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3660\n",
            "Epoch 1548/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3634\n",
            "Epoch 1549/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3601\n",
            "Epoch 1550/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3650\n",
            "Epoch 1551/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3637\n",
            "Epoch 1552/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3703 - val_loss: 0.3707\n",
            "Epoch 1553/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3708 - val_loss: 0.3642\n",
            "Epoch 1554/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3702 - val_loss: 0.3644\n",
            "Epoch 1555/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3716 - val_loss: 0.3702\n",
            "Epoch 1556/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3701 - val_loss: 0.3637\n",
            "Epoch 1557/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3715 - val_loss: 0.3616\n",
            "Epoch 1558/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3662\n",
            "Epoch 1559/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3711 - val_loss: 0.3705\n",
            "Epoch 1560/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.3695\n",
            "Epoch 1561/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.3666\n",
            "Epoch 1562/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3696 - val_loss: 0.3662\n",
            "Epoch 1563/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.3680\n",
            "Epoch 1564/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3633\n",
            "Epoch 1565/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3710 - val_loss: 0.3709\n",
            "Epoch 1566/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3693 - val_loss: 0.3701\n",
            "Epoch 1567/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3719 - val_loss: 0.3662\n",
            "Epoch 1568/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3703 - val_loss: 0.3699\n",
            "Epoch 1569/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3700 - val_loss: 0.3658\n",
            "Epoch 1570/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3694 - val_loss: 0.3660\n",
            "Epoch 1571/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3697 - val_loss: 0.3706\n",
            "Epoch 1572/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.3642\n",
            "Epoch 1573/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3702 - val_loss: 0.3677\n",
            "Epoch 1574/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3689 - val_loss: 0.3655\n",
            "Epoch 1575/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3707 - val_loss: 0.3682\n",
            "Epoch 1576/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3675\n",
            "Epoch 1577/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3685\n",
            "Epoch 1578/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3703 - val_loss: 0.3690\n",
            "Epoch 1579/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3697\n",
            "Epoch 1580/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3698 - val_loss: 0.3704\n",
            "Epoch 1581/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3697 - val_loss: 0.3702\n",
            "Epoch 1582/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3702 - val_loss: 0.3692\n",
            "Epoch 1583/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3698 - val_loss: 0.3679\n",
            "Epoch 1584/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3715 - val_loss: 0.3656\n",
            "Epoch 1585/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3703 - val_loss: 0.3678\n",
            "Epoch 1586/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3705 - val_loss: 0.3699\n",
            "Epoch 1587/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3708 - val_loss: 0.3634\n",
            "Epoch 1588/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3679\n",
            "Epoch 1589/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3700 - val_loss: 0.3671\n",
            "Epoch 1590/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3652\n",
            "Epoch 1591/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3653\n",
            "Epoch 1592/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3666\n",
            "Epoch 1593/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3698\n",
            "Epoch 1594/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3702 - val_loss: 0.3656\n",
            "Epoch 1595/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3679\n",
            "Epoch 1596/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.3618\n",
            "Epoch 1597/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3690 - val_loss: 0.3629\n",
            "Epoch 1598/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3698 - val_loss: 0.3671\n",
            "Epoch 1599/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3710 - val_loss: 0.3622\n",
            "Epoch 1600/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3725 - val_loss: 0.3647\n",
            "Epoch 1601/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3699 - val_loss: 0.3621\n",
            "Epoch 1602/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3712\n",
            "Epoch 1603/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3736\n",
            "Epoch 1604/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3707 - val_loss: 0.3680\n",
            "Epoch 1605/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3620\n",
            "Epoch 1606/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.3675\n",
            "Epoch 1607/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3695 - val_loss: 0.3671\n",
            "Epoch 1608/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3707\n",
            "Epoch 1609/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3694 - val_loss: 0.3693\n",
            "Epoch 1610/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3700 - val_loss: 0.3744\n",
            "Epoch 1611/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3701 - val_loss: 0.3688\n",
            "Epoch 1612/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3713 - val_loss: 0.3707\n",
            "Epoch 1613/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3719 - val_loss: 0.3621\n",
            "Epoch 1614/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3701 - val_loss: 0.3642\n",
            "Epoch 1615/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3705 - val_loss: 0.3725\n",
            "Epoch 1616/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3719 - val_loss: 0.3645\n",
            "Epoch 1617/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3752\n",
            "Epoch 1618/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3707 - val_loss: 0.3690\n",
            "Epoch 1619/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3708\n",
            "Epoch 1620/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3631\n",
            "Epoch 1621/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3674\n",
            "Epoch 1622/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3705 - val_loss: 0.3679\n",
            "Epoch 1623/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3694 - val_loss: 0.3644\n",
            "Epoch 1624/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3709 - val_loss: 0.3678\n",
            "Epoch 1625/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3702 - val_loss: 0.3698\n",
            "Epoch 1626/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3628\n",
            "Epoch 1627/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3703 - val_loss: 0.3761\n",
            "Epoch 1628/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3723 - val_loss: 0.3637\n",
            "Epoch 1629/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3709 - val_loss: 0.3646\n",
            "Epoch 1630/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3700 - val_loss: 0.3654\n",
            "Epoch 1631/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3652\n",
            "Epoch 1632/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3650\n",
            "Epoch 1633/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.3668\n",
            "Epoch 1634/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3676\n",
            "Epoch 1635/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3660\n",
            "Epoch 1636/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3659\n",
            "Epoch 1637/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3730\n",
            "Epoch 1638/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3749\n",
            "Epoch 1639/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.3659\n",
            "Epoch 1640/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3731 - val_loss: 0.3674\n",
            "Epoch 1641/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3707 - val_loss: 0.3636\n",
            "Epoch 1642/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3698 - val_loss: 0.3673\n",
            "Epoch 1643/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3694 - val_loss: 0.3687\n",
            "Epoch 1644/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3702 - val_loss: 0.3712\n",
            "Epoch 1645/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3692 - val_loss: 0.3666\n",
            "Epoch 1646/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3698 - val_loss: 0.3623\n",
            "Epoch 1647/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3688\n",
            "Epoch 1648/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3644\n",
            "Epoch 1649/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.3651\n",
            "Epoch 1650/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3697\n",
            "Epoch 1651/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3669\n",
            "Epoch 1652/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3694\n",
            "Epoch 1653/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3667\n",
            "Epoch 1654/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3662\n",
            "Epoch 1655/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3628\n",
            "Epoch 1656/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.3707\n",
            "Epoch 1657/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3675\n",
            "Epoch 1658/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3700 - val_loss: 0.3659\n",
            "Epoch 1659/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3686 - val_loss: 0.3665\n",
            "Epoch 1660/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3703 - val_loss: 0.3633\n",
            "Epoch 1661/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3701 - val_loss: 0.3670\n",
            "Epoch 1662/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.3651\n",
            "Epoch 1663/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3666\n",
            "Epoch 1664/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3698 - val_loss: 0.3689\n",
            "Epoch 1665/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.3629\n",
            "Epoch 1666/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.3657\n",
            "Epoch 1667/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3763\n",
            "Epoch 1668/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3624\n",
            "Epoch 1669/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.3706\n",
            "Epoch 1670/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3637\n",
            "Epoch 1671/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3649\n",
            "Epoch 1672/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3694 - val_loss: 0.3675\n",
            "Epoch 1673/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3711 - val_loss: 0.3663\n",
            "Epoch 1674/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3697 - val_loss: 0.3635\n",
            "Epoch 1675/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3694 - val_loss: 0.3673\n",
            "Epoch 1676/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3702 - val_loss: 0.3667\n",
            "Epoch 1677/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3698 - val_loss: 0.3711\n",
            "Epoch 1678/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3643\n",
            "Epoch 1679/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3603\n",
            "Epoch 1680/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3734\n",
            "Epoch 1681/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3710\n",
            "Epoch 1682/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3662\n",
            "Epoch 1683/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.3629\n",
            "Epoch 1684/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3660\n",
            "Epoch 1685/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3653\n",
            "Epoch 1686/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3698 - val_loss: 0.3702\n",
            "Epoch 1687/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.3679\n",
            "Epoch 1688/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3705 - val_loss: 0.3657\n",
            "Epoch 1689/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3696 - val_loss: 0.3717\n",
            "Epoch 1690/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3696 - val_loss: 0.3698\n",
            "Epoch 1691/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3694 - val_loss: 0.3659\n",
            "Epoch 1692/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3649\n",
            "Epoch 1693/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.3732\n",
            "Epoch 1694/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3725\n",
            "Epoch 1695/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3645\n",
            "Epoch 1696/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3680\n",
            "Epoch 1697/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3695 - val_loss: 0.3699\n",
            "Epoch 1698/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3623\n",
            "Epoch 1699/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3683\n",
            "Epoch 1700/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3720\n",
            "Epoch 1701/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3615\n",
            "Epoch 1702/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3668\n",
            "Epoch 1703/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3687 - val_loss: 0.3680\n",
            "Epoch 1704/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3690 - val_loss: 0.3633\n",
            "Epoch 1705/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3704 - val_loss: 0.3650\n",
            "Epoch 1706/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3705 - val_loss: 0.3645\n",
            "Epoch 1707/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3703 - val_loss: 0.3616\n",
            "Epoch 1708/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3645\n",
            "Epoch 1709/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3691\n",
            "Epoch 1710/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.3661\n",
            "Epoch 1711/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.3672\n",
            "Epoch 1712/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.3728\n",
            "Epoch 1713/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3660\n",
            "Epoch 1714/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.3727\n",
            "Epoch 1715/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.3674\n",
            "Epoch 1716/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.3725\n",
            "Epoch 1717/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3711 - val_loss: 0.3697\n",
            "Epoch 1718/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3706 - val_loss: 0.3677\n",
            "Epoch 1719/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3684 - val_loss: 0.3714\n",
            "Epoch 1720/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3695 - val_loss: 0.3625\n",
            "Epoch 1721/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.3755\n",
            "Epoch 1722/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3708 - val_loss: 0.3721\n",
            "Epoch 1723/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.3684\n",
            "Epoch 1724/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.3664\n",
            "Epoch 1725/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.3657\n",
            "Epoch 1726/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3674\n",
            "Epoch 1727/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3682\n",
            "Epoch 1728/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3677\n",
            "Epoch 1729/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3674\n",
            "Epoch 1730/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.3645\n",
            "Epoch 1731/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.3661\n",
            "Epoch 1732/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3664\n",
            "Epoch 1733/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3641\n",
            "Epoch 1734/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3691 - val_loss: 0.3657\n",
            "Epoch 1735/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3695 - val_loss: 0.3674\n",
            "Epoch 1736/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3691 - val_loss: 0.3639\n",
            "Epoch 1737/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.3699\n",
            "Epoch 1738/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3699 - val_loss: 0.3693\n",
            "Epoch 1739/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.3689\n",
            "Epoch 1740/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.3666\n",
            "Epoch 1741/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.3688\n",
            "Epoch 1742/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3697 - val_loss: 0.3617\n",
            "Epoch 1743/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3727\n",
            "Epoch 1744/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.3702\n",
            "Epoch 1745/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3686\n",
            "Epoch 1746/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3649\n",
            "Epoch 1747/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3694 - val_loss: 0.3691\n",
            "Epoch 1748/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3695 - val_loss: 0.3675\n",
            "Epoch 1749/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3698 - val_loss: 0.3673\n",
            "Epoch 1750/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3697 - val_loss: 0.3671\n",
            "Epoch 1751/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3696 - val_loss: 0.3681\n",
            "Epoch 1752/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3700 - val_loss: 0.3695\n",
            "Epoch 1753/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3690 - val_loss: 0.3690\n",
            "Epoch 1754/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3664\n",
            "Epoch 1755/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3642\n",
            "Epoch 1756/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3669\n",
            "Epoch 1757/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3681\n",
            "Epoch 1758/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3674\n",
            "Epoch 1759/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3682\n",
            "Epoch 1760/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3700 - val_loss: 0.3682\n",
            "Epoch 1761/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3681\n",
            "Epoch 1762/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3721\n",
            "Epoch 1763/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3694 - val_loss: 0.3702\n",
            "Epoch 1764/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3702 - val_loss: 0.3668\n",
            "Epoch 1765/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3704 - val_loss: 0.3669\n",
            "Epoch 1766/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3690 - val_loss: 0.3665\n",
            "Epoch 1767/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.3656\n",
            "Epoch 1768/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3702 - val_loss: 0.3726\n",
            "Epoch 1769/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3708 - val_loss: 0.3674\n",
            "Epoch 1770/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3665\n",
            "Epoch 1771/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3697 - val_loss: 0.3660\n",
            "Epoch 1772/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.3743\n",
            "Epoch 1773/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3708 - val_loss: 0.3691\n",
            "Epoch 1774/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3682\n",
            "Epoch 1775/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3689 - val_loss: 0.3734\n",
            "Epoch 1776/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3689 - val_loss: 0.3671\n",
            "Epoch 1777/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3726\n",
            "Epoch 1778/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3704 - val_loss: 0.3607\n",
            "Epoch 1779/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3689 - val_loss: 0.3664\n",
            "Epoch 1780/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3689 - val_loss: 0.3634\n",
            "Epoch 1781/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3685 - val_loss: 0.3659\n",
            "Epoch 1782/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3701 - val_loss: 0.3658\n",
            "Epoch 1783/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3689 - val_loss: 0.3698\n",
            "Epoch 1784/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.3654\n",
            "Epoch 1785/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.3666\n",
            "Epoch 1786/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3698\n",
            "Epoch 1787/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.3639\n",
            "Epoch 1788/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3669\n",
            "Epoch 1789/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3731\n",
            "Epoch 1790/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.3723\n",
            "Epoch 1791/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3687\n",
            "Epoch 1792/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.3657\n",
            "Epoch 1793/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3707\n",
            "Epoch 1794/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3707 - val_loss: 0.3662\n",
            "Epoch 1795/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3704 - val_loss: 0.3688\n",
            "Epoch 1796/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3705 - val_loss: 0.3681\n",
            "Epoch 1797/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3693 - val_loss: 0.3667\n",
            "Epoch 1798/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3685 - val_loss: 0.3696\n",
            "Epoch 1799/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.3726\n",
            "Epoch 1800/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.3695\n",
            "Epoch 1801/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3671\n",
            "Epoch 1802/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3679\n",
            "Epoch 1803/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.3665\n",
            "Epoch 1804/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.3709\n",
            "Epoch 1805/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3702 - val_loss: 0.3704\n",
            "Epoch 1806/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.3696\n",
            "Epoch 1807/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3668\n",
            "Epoch 1808/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.3682\n",
            "Epoch 1809/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3703 - val_loss: 0.3683\n",
            "Epoch 1810/2000\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.3696 - val_loss: 0.3658\n",
            "Epoch 1811/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3694 - val_loss: 0.3720\n",
            "Epoch 1812/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3688 - val_loss: 0.3677\n",
            "Epoch 1813/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3702 - val_loss: 0.3668\n",
            "Epoch 1814/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3701 - val_loss: 0.3625\n",
            "Epoch 1815/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3689 - val_loss: 0.3636\n",
            "Epoch 1816/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3663\n",
            "Epoch 1817/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3665\n",
            "Epoch 1818/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3687 - val_loss: 0.3666\n",
            "Epoch 1819/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3630\n",
            "Epoch 1820/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.3693\n",
            "Epoch 1821/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3706\n",
            "Epoch 1822/2000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.3633\n",
            "Epoch 1823/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.3647\n",
            "Epoch 1824/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3689 - val_loss: 0.3686\n",
            "Epoch 1825/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3689 - val_loss: 0.3698\n",
            "Epoch 1826/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3697 - val_loss: 0.3694\n",
            "Epoch 1827/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3713 - val_loss: 0.3675\n",
            "Epoch 1828/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3693 - val_loss: 0.3638\n",
            "Epoch 1829/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3687 - val_loss: 0.3694\n",
            "Epoch 1830/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3687 - val_loss: 0.3680\n",
            "Epoch 1831/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.3684\n",
            "Epoch 1832/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3695 - val_loss: 0.3695\n",
            "Epoch 1833/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3708\n",
            "Epoch 1834/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.3692\n",
            "Epoch 1835/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3684 - val_loss: 0.3699\n",
            "Epoch 1836/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3631\n",
            "Epoch 1837/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3652\n",
            "Epoch 1838/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.3639\n",
            "Epoch 1839/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.3677\n",
            "Epoch 1840/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3656\n",
            "Epoch 1841/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3694 - val_loss: 0.3675\n",
            "Epoch 1842/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3696 - val_loss: 0.3627\n",
            "Epoch 1843/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3689 - val_loss: 0.3680\n",
            "Epoch 1844/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3703 - val_loss: 0.3664\n",
            "Epoch 1845/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.3627\n",
            "Epoch 1846/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.3680\n",
            "Epoch 1847/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.3675\n",
            "Epoch 1848/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.3676\n",
            "Epoch 1849/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3687 - val_loss: 0.3684\n",
            "Epoch 1850/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3683\n",
            "Epoch 1851/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3672\n",
            "Epoch 1852/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.3722\n",
            "Epoch 1853/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.3674\n",
            "Epoch 1854/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3694 - val_loss: 0.3592\n",
            "Epoch 1855/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3666\n",
            "Epoch 1856/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3700 - val_loss: 0.3629\n",
            "Epoch 1857/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3694 - val_loss: 0.3646\n",
            "Epoch 1858/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3694 - val_loss: 0.3642\n",
            "Epoch 1859/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3690 - val_loss: 0.3653\n",
            "Epoch 1860/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3695 - val_loss: 0.3671\n",
            "Epoch 1861/2000\n",
            "80/80 [==============================] - 2s 23ms/step - loss: 0.3694 - val_loss: 0.3637\n",
            "Epoch 1862/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3669\n",
            "Epoch 1863/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3694 - val_loss: 0.3685\n",
            "Epoch 1864/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3691 - val_loss: 0.3640\n",
            "Epoch 1865/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3691 - val_loss: 0.3722\n",
            "Epoch 1866/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3686 - val_loss: 0.3634\n",
            "Epoch 1867/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3680\n",
            "Epoch 1868/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3692 - val_loss: 0.3681\n",
            "Epoch 1869/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3702 - val_loss: 0.3721\n",
            "Epoch 1870/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3702 - val_loss: 0.3658\n",
            "Epoch 1871/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3690 - val_loss: 0.3666\n",
            "Epoch 1872/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3688 - val_loss: 0.3690\n",
            "Epoch 1873/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.3698\n",
            "Epoch 1874/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3687 - val_loss: 0.3647\n",
            "Epoch 1875/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3683 - val_loss: 0.3655\n",
            "Epoch 1876/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3687\n",
            "Epoch 1877/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3702 - val_loss: 0.3788\n",
            "Epoch 1878/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3695 - val_loss: 0.3675\n",
            "Epoch 1879/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.3730\n",
            "Epoch 1880/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.3656\n",
            "Epoch 1881/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.3673\n",
            "Epoch 1882/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.3696\n",
            "Epoch 1883/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3697 - val_loss: 0.3656\n",
            "Epoch 1884/2000\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.3683 - val_loss: 0.3656\n",
            "Epoch 1885/2000\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.3681 - val_loss: 0.3658\n",
            "Epoch 1886/2000\n",
            "80/80 [==============================] - 2s 22ms/step - loss: 0.3691 - val_loss: 0.3660\n",
            "Epoch 1887/2000\n",
            "80/80 [==============================] - 2s 23ms/step - loss: 0.3687 - val_loss: 0.3674\n",
            "Epoch 1888/2000\n",
            "80/80 [==============================] - 2s 23ms/step - loss: 0.3687 - val_loss: 0.3644\n",
            "Epoch 1889/2000\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.3689 - val_loss: 0.3747\n",
            "Epoch 1890/2000\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.3694 - val_loss: 0.3690\n",
            "Epoch 1891/2000\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.3686 - val_loss: 0.3686\n",
            "Epoch 1892/2000\n",
            "80/80 [==============================] - 2s 30ms/step - loss: 0.3707 - val_loss: 0.3622\n",
            "Epoch 1893/2000\n",
            "80/80 [==============================] - 3s 31ms/step - loss: 0.3707 - val_loss: 0.3688\n",
            "Epoch 1894/2000\n",
            "80/80 [==============================] - 2s 31ms/step - loss: 0.3692 - val_loss: 0.3711\n",
            "Epoch 1895/2000\n",
            "80/80 [==============================] - 2s 30ms/step - loss: 0.3718 - val_loss: 0.3664\n",
            "Epoch 1896/2000\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.3702 - val_loss: 0.3660\n",
            "Epoch 1897/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3693 - val_loss: 0.3635\n",
            "Epoch 1898/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3685 - val_loss: 0.3673\n",
            "Epoch 1899/2000\n",
            "80/80 [==============================] - 2s 21ms/step - loss: 0.3679 - val_loss: 0.3624\n",
            "Epoch 1900/2000\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.3684 - val_loss: 0.3661\n",
            "Epoch 1901/2000\n",
            "80/80 [==============================] - 2s 29ms/step - loss: 0.3696 - val_loss: 0.3647\n",
            "Epoch 1902/2000\n",
            "80/80 [==============================] - 2s 29ms/step - loss: 0.3694 - val_loss: 0.3644\n",
            "Epoch 1903/2000\n",
            "80/80 [==============================] - 2s 29ms/step - loss: 0.3693 - val_loss: 0.3624\n",
            "Epoch 1904/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3682 - val_loss: 0.3677\n",
            "Epoch 1905/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3694 - val_loss: 0.3716\n",
            "Epoch 1906/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3697 - val_loss: 0.3674\n",
            "Epoch 1907/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3706\n",
            "Epoch 1908/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.3663\n",
            "Epoch 1909/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3673\n",
            "Epoch 1910/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3646\n",
            "Epoch 1911/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3653\n",
            "Epoch 1912/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3670\n",
            "Epoch 1913/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3696 - val_loss: 0.3685\n",
            "Epoch 1914/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3682 - val_loss: 0.3648\n",
            "Epoch 1915/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3691 - val_loss: 0.3706\n",
            "Epoch 1916/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3687 - val_loss: 0.3665\n",
            "Epoch 1917/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3686 - val_loss: 0.3666\n",
            "Epoch 1918/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.3677\n",
            "Epoch 1919/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.3708\n",
            "Epoch 1920/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.3678\n",
            "Epoch 1921/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3681 - val_loss: 0.3636\n",
            "Epoch 1922/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3688 - val_loss: 0.3629\n",
            "Epoch 1923/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.3660\n",
            "Epoch 1924/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3676 - val_loss: 0.3639\n",
            "Epoch 1925/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3684 - val_loss: 0.3736\n",
            "Epoch 1926/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3629\n",
            "Epoch 1927/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.3672\n",
            "Epoch 1928/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3702 - val_loss: 0.3668\n",
            "Epoch 1929/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3701 - val_loss: 0.3655\n",
            "Epoch 1930/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3688 - val_loss: 0.3692\n",
            "Epoch 1931/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3688 - val_loss: 0.3649\n",
            "Epoch 1932/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3692 - val_loss: 0.3680\n",
            "Epoch 1933/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3691 - val_loss: 0.3634\n",
            "Epoch 1934/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3698 - val_loss: 0.3643\n",
            "Epoch 1935/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.3695\n",
            "Epoch 1936/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.3647\n",
            "Epoch 1937/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.3641\n",
            "Epoch 1938/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.3659\n",
            "Epoch 1939/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.3680\n",
            "Epoch 1940/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.3646\n",
            "Epoch 1941/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3627\n",
            "Epoch 1942/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3683\n",
            "Epoch 1943/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3692 - val_loss: 0.3664\n",
            "Epoch 1944/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3695 - val_loss: 0.3735\n",
            "Epoch 1945/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3717 - val_loss: 0.3698\n",
            "Epoch 1946/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3699 - val_loss: 0.3636\n",
            "Epoch 1947/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3682 - val_loss: 0.3653\n",
            "Epoch 1948/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3676\n",
            "Epoch 1949/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3689 - val_loss: 0.3663\n",
            "Epoch 1950/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.3656\n",
            "Epoch 1951/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.3620\n",
            "Epoch 1952/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.3643\n",
            "Epoch 1953/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.3647\n",
            "Epoch 1954/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.3624\n",
            "Epoch 1955/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3685 - val_loss: 0.3661\n",
            "Epoch 1956/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3684 - val_loss: 0.3671\n",
            "Epoch 1957/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.3693\n",
            "Epoch 1958/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3686 - val_loss: 0.3648\n",
            "Epoch 1959/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3686 - val_loss: 0.3637\n",
            "Epoch 1960/2000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3691 - val_loss: 0.3689\n",
            "Epoch 1961/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3685 - val_loss: 0.3660\n",
            "Epoch 1962/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3694 - val_loss: 0.3635\n",
            "Epoch 1963/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3684 - val_loss: 0.3694\n",
            "Epoch 1964/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3691 - val_loss: 0.3655\n",
            "Epoch 1965/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.3684\n",
            "Epoch 1966/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3684 - val_loss: 0.3617\n",
            "Epoch 1967/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.3658\n",
            "Epoch 1968/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3641\n",
            "Epoch 1969/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3700 - val_loss: 0.3651\n",
            "Epoch 1970/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.3681\n",
            "Epoch 1971/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3698 - val_loss: 0.3692\n",
            "Epoch 1972/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3708 - val_loss: 0.3649\n",
            "Epoch 1973/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3688 - val_loss: 0.3679\n",
            "Epoch 1974/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3684 - val_loss: 0.3658\n",
            "Epoch 1975/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3683 - val_loss: 0.3657\n",
            "Epoch 1976/2000\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.3685 - val_loss: 0.3673\n",
            "Epoch 1977/2000\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3684 - val_loss: 0.3655\n",
            "Epoch 1978/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.3673\n",
            "Epoch 1979/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3675 - val_loss: 0.3640\n",
            "Epoch 1980/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3705 - val_loss: 0.3670\n",
            "Epoch 1981/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3669\n",
            "Epoch 1982/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3681 - val_loss: 0.3669\n",
            "Epoch 1983/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3687 - val_loss: 0.3681\n",
            "Epoch 1984/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3686 - val_loss: 0.3678\n",
            "Epoch 1985/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3686 - val_loss: 0.3647\n",
            "Epoch 1986/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.3648\n",
            "Epoch 1987/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3687 - val_loss: 0.3670\n",
            "Epoch 1988/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3681 - val_loss: 0.3650\n",
            "Epoch 1989/2000\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.3678 - val_loss: 0.3660\n",
            "Epoch 1990/2000\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.3685 - val_loss: 0.3669\n",
            "Epoch 1991/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3681 - val_loss: 0.3671\n",
            "Epoch 1992/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.3665\n",
            "Epoch 1993/2000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.3663\n",
            "Epoch 1994/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3689 - val_loss: 0.3656\n",
            "Epoch 1995/2000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.3689 - val_loss: 0.3655\n",
            "Epoch 1996/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3693 - val_loss: 0.3671\n",
            "Epoch 1997/2000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.3689 - val_loss: 0.3685\n",
            "Epoch 1998/2000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.3679 - val_loss: 0.3639\n",
            "Epoch 1999/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3685 - val_loss: 0.3677\n",
            "Epoch 2000/2000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3704 - val_loss: 0.3653\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.3653\n",
            "5/5 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f07597a2680>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=2000;    # training epochs\n",
        "temperature = 0.1    # temprature control the smooth of the probabilities\n",
        "\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(x)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "gJxJD_M41H_v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok-LvipnKoD0"
      },
      "source": [
        "##2.5 Use Normalization layer ##"
      ],
      "id": "ok-LvipnKoD0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "waeLOQUbYF_I",
        "outputId": "1acb65c7-cdc5-4866-af76-ebd14c1f7baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "70/70 [==============================] - 66s 15ms/step - loss: 1.1348 - val_loss: 1.0971\n",
            "Epoch 2/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 1.0938 - val_loss: 1.0919\n",
            "Epoch 3/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0906 - val_loss: 1.0885\n",
            "Epoch 4/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0874 - val_loss: 1.0852\n",
            "Epoch 5/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0842 - val_loss: 1.0818\n",
            "Epoch 6/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0809 - val_loss: 1.0785\n",
            "Epoch 7/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0777 - val_loss: 1.0751\n",
            "Epoch 8/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0746 - val_loss: 1.0718\n",
            "Epoch 9/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 1.0714 - val_loss: 1.0685\n",
            "Epoch 10/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 1.0682 - val_loss: 1.0652\n",
            "Epoch 11/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 1.0651 - val_loss: 1.0619\n",
            "Epoch 12/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 1.0619 - val_loss: 1.0586\n",
            "Epoch 13/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0588 - val_loss: 1.0554\n",
            "Epoch 14/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0557 - val_loss: 1.0521\n",
            "Epoch 15/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 1.0526 - val_loss: 1.0489\n",
            "Epoch 16/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0495 - val_loss: 1.0457\n",
            "Epoch 17/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0464 - val_loss: 1.0425\n",
            "Epoch 18/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0434 - val_loss: 1.0393\n",
            "Epoch 19/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0403 - val_loss: 1.0361\n",
            "Epoch 20/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0373 - val_loss: 1.0329\n",
            "Epoch 21/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0343 - val_loss: 1.0298\n",
            "Epoch 22/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0313 - val_loss: 1.0266\n",
            "Epoch 23/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0283 - val_loss: 1.0235\n",
            "Epoch 24/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0253 - val_loss: 1.0204\n",
            "Epoch 25/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0223 - val_loss: 1.0173\n",
            "Epoch 26/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 1.0194 - val_loss: 1.0142\n",
            "Epoch 27/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 1.0164 - val_loss: 1.0111\n",
            "Epoch 28/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 1.0135 - val_loss: 1.0081\n",
            "Epoch 29/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 1.0106 - val_loss: 1.0051\n",
            "Epoch 30/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 1.0077 - val_loss: 1.0020\n",
            "Epoch 31/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0048 - val_loss: 0.9989\n",
            "Epoch 32/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 1.0019 - val_loss: 0.9960\n",
            "Epoch 33/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.9991 - val_loss: 0.9930\n",
            "Epoch 34/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9962 - val_loss: 0.9900\n",
            "Epoch 35/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9934 - val_loss: 0.9870\n",
            "Epoch 36/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9905 - val_loss: 0.9840\n",
            "Epoch 37/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9877 - val_loss: 0.9811\n",
            "Epoch 38/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9849 - val_loss: 0.9782\n",
            "Epoch 39/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9821 - val_loss: 0.9752\n",
            "Epoch 40/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.9794 - val_loss: 0.9723\n",
            "Epoch 41/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.9766 - val_loss: 0.9694\n",
            "Epoch 42/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.9738 - val_loss: 0.9666\n",
            "Epoch 43/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9711 - val_loss: 0.9637\n",
            "Epoch 44/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.9684 - val_loss: 0.9608\n",
            "Epoch 45/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.9656 - val_loss: 0.9579\n",
            "Epoch 46/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.9629 - val_loss: 0.9551\n",
            "Epoch 47/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.9603 - val_loss: 0.9523\n",
            "Epoch 48/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9576 - val_loss: 0.9495\n",
            "Epoch 49/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9549 - val_loss: 0.9467\n",
            "Epoch 50/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9522 - val_loss: 0.9439\n",
            "Epoch 51/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9496 - val_loss: 0.9411\n",
            "Epoch 52/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9470 - val_loss: 0.9383\n",
            "Epoch 53/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9443 - val_loss: 0.9356\n",
            "Epoch 54/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.9417 - val_loss: 0.9329\n",
            "Epoch 55/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9391 - val_loss: 0.9302\n",
            "Epoch 56/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.9365 - val_loss: 0.9274\n",
            "Epoch 57/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9340 - val_loss: 0.9247\n",
            "Epoch 58/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9314 - val_loss: 0.9220\n",
            "Epoch 59/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9289 - val_loss: 0.9193\n",
            "Epoch 60/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9263 - val_loss: 0.9167\n",
            "Epoch 61/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.9238 - val_loss: 0.9140\n",
            "Epoch 62/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.9213 - val_loss: 0.9114\n",
            "Epoch 63/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.9188 - val_loss: 0.9087\n",
            "Epoch 64/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.9163 - val_loss: 0.9061\n",
            "Epoch 65/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.9138 - val_loss: 0.9035\n",
            "Epoch 66/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.9113 - val_loss: 0.9009\n",
            "Epoch 67/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9089 - val_loss: 0.8983\n",
            "Epoch 68/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9064 - val_loss: 0.8958\n",
            "Epoch 69/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9040 - val_loss: 0.8932\n",
            "Epoch 70/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9016 - val_loss: 0.8906\n",
            "Epoch 71/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8991 - val_loss: 0.8881\n",
            "Epoch 72/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8967 - val_loss: 0.8855\n",
            "Epoch 73/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8943 - val_loss: 0.8830\n",
            "Epoch 74/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8919 - val_loss: 0.8805\n",
            "Epoch 75/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8896 - val_loss: 0.8780\n",
            "Epoch 76/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8872 - val_loss: 0.8755\n",
            "Epoch 77/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8849 - val_loss: 0.8730\n",
            "Epoch 78/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8825 - val_loss: 0.8706\n",
            "Epoch 79/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.8802 - val_loss: 0.8681\n",
            "Epoch 80/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.8779 - val_loss: 0.8657\n",
            "Epoch 81/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.8756 - val_loss: 0.8632\n",
            "Epoch 82/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8733 - val_loss: 0.8608\n",
            "Epoch 83/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8710 - val_loss: 0.8584\n",
            "Epoch 84/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8687 - val_loss: 0.8560\n",
            "Epoch 85/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8665 - val_loss: 0.8536\n",
            "Epoch 86/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8642 - val_loss: 0.8512\n",
            "Epoch 87/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8620 - val_loss: 0.8489\n",
            "Epoch 88/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8598 - val_loss: 0.8465\n",
            "Epoch 89/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8575 - val_loss: 0.8442\n",
            "Epoch 90/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8553 - val_loss: 0.8418\n",
            "Epoch 91/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8532 - val_loss: 0.8395\n",
            "Epoch 92/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8510 - val_loss: 0.8372\n",
            "Epoch 93/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8488 - val_loss: 0.8349\n",
            "Epoch 94/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8466 - val_loss: 0.8326\n",
            "Epoch 95/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8445 - val_loss: 0.8304\n",
            "Epoch 96/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.8424 - val_loss: 0.8281\n",
            "Epoch 97/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.8402 - val_loss: 0.8258\n",
            "Epoch 98/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.8381 - val_loss: 0.8236\n",
            "Epoch 99/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.8360 - val_loss: 0.8214\n",
            "Epoch 100/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8339 - val_loss: 0.8192\n",
            "Epoch 101/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8318 - val_loss: 0.8170\n",
            "Epoch 102/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8298 - val_loss: 0.8147\n",
            "Epoch 103/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8277 - val_loss: 0.8126\n",
            "Epoch 104/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8257 - val_loss: 0.8104\n",
            "Epoch 105/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8236 - val_loss: 0.8083\n",
            "Epoch 106/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8216 - val_loss: 0.8061\n",
            "Epoch 107/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.8196 - val_loss: 0.8039\n",
            "Epoch 108/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8176 - val_loss: 0.8018\n",
            "Epoch 109/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8156 - val_loss: 0.7997\n",
            "Epoch 110/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8136 - val_loss: 0.7976\n",
            "Epoch 111/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8116 - val_loss: 0.7955\n",
            "Epoch 112/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8097 - val_loss: 0.7934\n",
            "Epoch 113/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.8077 - val_loss: 0.7913\n",
            "Epoch 114/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.8058 - val_loss: 0.7893\n",
            "Epoch 115/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.8039 - val_loss: 0.7873\n",
            "Epoch 116/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.8020 - val_loss: 0.7852\n",
            "Epoch 117/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.8001 - val_loss: 0.7832\n",
            "Epoch 118/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7982 - val_loss: 0.7812\n",
            "Epoch 119/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7963 - val_loss: 0.7791\n",
            "Epoch 120/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7944 - val_loss: 0.7772\n",
            "Epoch 121/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7925 - val_loss: 0.7752\n",
            "Epoch 122/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7907 - val_loss: 0.7732\n",
            "Epoch 123/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7889 - val_loss: 0.7712\n",
            "Epoch 124/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7870 - val_loss: 0.7693\n",
            "Epoch 125/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7852 - val_loss: 0.7673\n",
            "Epoch 126/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7834 - val_loss: 0.7654\n",
            "Epoch 127/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7816 - val_loss: 0.7635\n",
            "Epoch 128/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7798 - val_loss: 0.7616\n",
            "Epoch 129/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7781 - val_loss: 0.7597\n",
            "Epoch 130/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7763 - val_loss: 0.7578\n",
            "Epoch 131/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.7745 - val_loss: 0.7559\n",
            "Epoch 132/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.7728 - val_loss: 0.7541\n",
            "Epoch 133/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.7711 - val_loss: 0.7522\n",
            "Epoch 134/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.7694 - val_loss: 0.7504\n",
            "Epoch 135/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7676 - val_loss: 0.7486\n",
            "Epoch 136/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7660 - val_loss: 0.7467\n",
            "Epoch 137/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7643 - val_loss: 0.7449\n",
            "Epoch 138/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7626 - val_loss: 0.7431\n",
            "Epoch 139/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7609 - val_loss: 0.7413\n",
            "Epoch 140/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7593 - val_loss: 0.7396\n",
            "Epoch 141/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7576 - val_loss: 0.7378\n",
            "Epoch 142/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7560 - val_loss: 0.7360\n",
            "Epoch 143/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7544 - val_loss: 0.7343\n",
            "Epoch 144/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7527 - val_loss: 0.7326\n",
            "Epoch 145/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7511 - val_loss: 0.7308\n",
            "Epoch 146/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7495 - val_loss: 0.7291\n",
            "Epoch 147/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7480 - val_loss: 0.7275\n",
            "Epoch 148/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.7464 - val_loss: 0.7257\n",
            "Epoch 149/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.7448 - val_loss: 0.7241\n",
            "Epoch 150/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.7433 - val_loss: 0.7224\n",
            "Epoch 151/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.7417 - val_loss: 0.7207\n",
            "Epoch 152/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7402 - val_loss: 0.7191\n",
            "Epoch 153/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7387 - val_loss: 0.7175\n",
            "Epoch 154/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7372 - val_loss: 0.7158\n",
            "Epoch 155/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.7357 - val_loss: 0.7142\n",
            "Epoch 156/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7342 - val_loss: 0.7126\n",
            "Epoch 157/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7327 - val_loss: 0.7110\n",
            "Epoch 158/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7312 - val_loss: 0.7094\n",
            "Epoch 159/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7298 - val_loss: 0.7078\n",
            "Epoch 160/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7283 - val_loss: 0.7063\n",
            "Epoch 161/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7269 - val_loss: 0.7047\n",
            "Epoch 162/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7255 - val_loss: 0.7032\n",
            "Epoch 163/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7240 - val_loss: 0.7016\n",
            "Epoch 164/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.7226 - val_loss: 0.7001\n",
            "Epoch 165/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.7212 - val_loss: 0.6986\n",
            "Epoch 166/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.7198 - val_loss: 0.6971\n",
            "Epoch 167/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.7185 - val_loss: 0.6956\n",
            "Epoch 168/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7171 - val_loss: 0.6941\n",
            "Epoch 169/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7157 - val_loss: 0.6926\n",
            "Epoch 170/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7144 - val_loss: 0.6911\n",
            "Epoch 171/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7130 - val_loss: 0.6897\n",
            "Epoch 172/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7117 - val_loss: 0.6882\n",
            "Epoch 173/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7104 - val_loss: 0.6868\n",
            "Epoch 174/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7091 - val_loss: 0.6854\n",
            "Epoch 175/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7078 - val_loss: 0.6840\n",
            "Epoch 176/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7065 - val_loss: 0.6825\n",
            "Epoch 177/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.7052 - val_loss: 0.6812\n",
            "Epoch 178/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7039 - val_loss: 0.6798\n",
            "Epoch 179/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7026 - val_loss: 0.6784\n",
            "Epoch 180/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.7014 - val_loss: 0.6770\n",
            "Epoch 181/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.7001 - val_loss: 0.6757\n",
            "Epoch 182/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6989 - val_loss: 0.6743\n",
            "Epoch 183/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6977 - val_loss: 0.6730\n",
            "Epoch 184/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.6965 - val_loss: 0.6717\n",
            "Epoch 185/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6953 - val_loss: 0.6703\n",
            "Epoch 186/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6941 - val_loss: 0.6690\n",
            "Epoch 187/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6929 - val_loss: 0.6677\n",
            "Epoch 188/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6917 - val_loss: 0.6664\n",
            "Epoch 189/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6905 - val_loss: 0.6652\n",
            "Epoch 190/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6893 - val_loss: 0.6639\n",
            "Epoch 191/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6882 - val_loss: 0.6626\n",
            "Epoch 192/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6870 - val_loss: 0.6613\n",
            "Epoch 193/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6859 - val_loss: 0.6601\n",
            "Epoch 194/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6848 - val_loss: 0.6589\n",
            "Epoch 195/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6837 - val_loss: 0.6576\n",
            "Epoch 196/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6826 - val_loss: 0.6564\n",
            "Epoch 197/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6815 - val_loss: 0.6552\n",
            "Epoch 198/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.6804 - val_loss: 0.6540\n",
            "Epoch 199/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6793 - val_loss: 0.6528\n",
            "Epoch 200/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6782 - val_loss: 0.6516\n",
            "Epoch 201/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6771 - val_loss: 0.6504\n",
            "Epoch 202/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6761 - val_loss: 0.6493\n",
            "Epoch 203/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6750 - val_loss: 0.6482\n",
            "Epoch 204/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6740 - val_loss: 0.6470\n",
            "Epoch 205/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6729 - val_loss: 0.6459\n",
            "Epoch 206/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6719 - val_loss: 0.6447\n",
            "Epoch 207/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6709 - val_loss: 0.6436\n",
            "Epoch 208/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6699 - val_loss: 0.6425\n",
            "Epoch 209/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6689 - val_loss: 0.6414\n",
            "Epoch 210/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6679 - val_loss: 0.6403\n",
            "Epoch 211/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6669 - val_loss: 0.6392\n",
            "Epoch 212/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6659 - val_loss: 0.6381\n",
            "Epoch 213/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6650 - val_loss: 0.6371\n",
            "Epoch 214/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6640 - val_loss: 0.6360\n",
            "Epoch 215/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6631 - val_loss: 0.6349\n",
            "Epoch 216/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6621 - val_loss: 0.6339\n",
            "Epoch 217/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6612 - val_loss: 0.6328\n",
            "Epoch 218/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.6603 - val_loss: 0.6318\n",
            "Epoch 219/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.6593 - val_loss: 0.6308\n",
            "Epoch 220/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6584 - val_loss: 0.6298\n",
            "Epoch 221/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6575 - val_loss: 0.6287\n",
            "Epoch 222/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6566 - val_loss: 0.6278\n",
            "Epoch 223/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6557 - val_loss: 0.6268\n",
            "Epoch 224/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6548 - val_loss: 0.6258\n",
            "Epoch 225/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6540 - val_loss: 0.6248\n",
            "Epoch 226/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6531 - val_loss: 0.6239\n",
            "Epoch 227/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.6522 - val_loss: 0.6229\n",
            "Epoch 228/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6514 - val_loss: 0.6219\n",
            "Epoch 229/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6505 - val_loss: 0.6210\n",
            "Epoch 230/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6497 - val_loss: 0.6201\n",
            "Epoch 231/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6489 - val_loss: 0.6191\n",
            "Epoch 232/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6481 - val_loss: 0.6182\n",
            "Epoch 233/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6472 - val_loss: 0.6173\n",
            "Epoch 234/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6464 - val_loss: 0.6164\n",
            "Epoch 235/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6456 - val_loss: 0.6155\n",
            "Epoch 236/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.6448 - val_loss: 0.6146\n",
            "Epoch 237/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6440 - val_loss: 0.6137\n",
            "Epoch 238/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6433 - val_loss: 0.6128\n",
            "Epoch 239/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6425 - val_loss: 0.6120\n",
            "Epoch 240/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6417 - val_loss: 0.6111\n",
            "Epoch 241/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6410 - val_loss: 0.6102\n",
            "Epoch 242/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6402 - val_loss: 0.6094\n",
            "Epoch 243/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6395 - val_loss: 0.6086\n",
            "Epoch 244/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6387 - val_loss: 0.6077\n",
            "Epoch 245/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6380 - val_loss: 0.6069\n",
            "Epoch 246/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6373 - val_loss: 0.6061\n",
            "Epoch 247/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6365 - val_loss: 0.6053\n",
            "Epoch 248/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6358 - val_loss: 0.6045\n",
            "Epoch 249/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.6351 - val_loss: 0.6037\n",
            "Epoch 250/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6344 - val_loss: 0.6029\n",
            "Epoch 251/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6337 - val_loss: 0.6021\n",
            "Epoch 252/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6331 - val_loss: 0.6013\n",
            "Epoch 253/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6324 - val_loss: 0.6005\n",
            "Epoch 254/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6317 - val_loss: 0.5998\n",
            "Epoch 255/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6310 - val_loss: 0.5990\n",
            "Epoch 256/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6304 - val_loss: 0.5983\n",
            "Epoch 257/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6297 - val_loss: 0.5975\n",
            "Epoch 258/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6291 - val_loss: 0.5968\n",
            "Epoch 259/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.6284 - val_loss: 0.5961\n",
            "Epoch 260/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6278 - val_loss: 0.5953\n",
            "Epoch 261/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6271 - val_loss: 0.5946\n",
            "Epoch 262/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6265 - val_loss: 0.5939\n",
            "Epoch 263/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6259 - val_loss: 0.5932\n",
            "Epoch 264/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6253 - val_loss: 0.5925\n",
            "Epoch 265/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6247 - val_loss: 0.5918\n",
            "Epoch 266/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.6241 - val_loss: 0.5911\n",
            "Epoch 267/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6235 - val_loss: 0.5904\n",
            "Epoch 268/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6229 - val_loss: 0.5897\n",
            "Epoch 269/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.6223 - val_loss: 0.5891\n",
            "Epoch 270/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.6217 - val_loss: 0.5884\n",
            "Epoch 271/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6211 - val_loss: 0.5877\n",
            "Epoch 272/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6206 - val_loss: 0.5871\n",
            "Epoch 273/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6200 - val_loss: 0.5864\n",
            "Epoch 274/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.6195 - val_loss: 0.5858\n",
            "Epoch 275/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6189 - val_loss: 0.5852\n",
            "Epoch 276/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6184 - val_loss: 0.5845\n",
            "Epoch 277/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6178 - val_loss: 0.5839\n",
            "Epoch 278/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6173 - val_loss: 0.5833\n",
            "Epoch 279/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6167 - val_loss: 0.5827\n",
            "Epoch 280/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.6162 - val_loss: 0.5821\n",
            "Epoch 281/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6157 - val_loss: 0.5814\n",
            "Epoch 282/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6152 - val_loss: 0.5808\n",
            "Epoch 283/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6147 - val_loss: 0.5802\n",
            "Epoch 284/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6142 - val_loss: 0.5797\n",
            "Epoch 285/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.6136 - val_loss: 0.5791\n",
            "Epoch 286/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6131 - val_loss: 0.5785\n",
            "Epoch 287/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6127 - val_loss: 0.5779\n",
            "Epoch 288/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6122 - val_loss: 0.5774\n",
            "Epoch 289/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6117 - val_loss: 0.5768\n",
            "Epoch 290/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6112 - val_loss: 0.5763\n",
            "Epoch 291/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6107 - val_loss: 0.5757\n",
            "Epoch 292/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6103 - val_loss: 0.5751\n",
            "Epoch 293/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6098 - val_loss: 0.5746\n",
            "Epoch 294/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6093 - val_loss: 0.5741\n",
            "Epoch 295/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6089 - val_loss: 0.5735\n",
            "Epoch 296/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6084 - val_loss: 0.5730\n",
            "Epoch 297/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6080 - val_loss: 0.5725\n",
            "Epoch 298/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6076 - val_loss: 0.5719\n",
            "Epoch 299/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6071 - val_loss: 0.5715\n",
            "Epoch 300/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6067 - val_loss: 0.5709\n",
            "Epoch 301/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6063 - val_loss: 0.5704\n",
            "Epoch 302/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6058 - val_loss: 0.5699\n",
            "Epoch 303/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6054 - val_loss: 0.5695\n",
            "Epoch 304/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6050 - val_loss: 0.5690\n",
            "Epoch 305/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6046 - val_loss: 0.5685\n",
            "Epoch 306/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6042 - val_loss: 0.5680\n",
            "Epoch 307/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6038 - val_loss: 0.5675\n",
            "Epoch 308/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6034 - val_loss: 0.5671\n",
            "Epoch 309/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6030 - val_loss: 0.5666\n",
            "Epoch 310/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6026 - val_loss: 0.5661\n",
            "Epoch 311/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6022 - val_loss: 0.5657\n",
            "Epoch 312/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6019 - val_loss: 0.5652\n",
            "Epoch 313/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6015 - val_loss: 0.5648\n",
            "Epoch 314/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.6011 - val_loss: 0.5643\n",
            "Epoch 315/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.6007 - val_loss: 0.5639\n",
            "Epoch 316/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6004 - val_loss: 0.5634\n",
            "Epoch 317/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.6000 - val_loss: 0.5630\n",
            "Epoch 318/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5997 - val_loss: 0.5626\n",
            "Epoch 319/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5993 - val_loss: 0.5622\n",
            "Epoch 320/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5989 - val_loss: 0.5618\n",
            "Epoch 321/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5986 - val_loss: 0.5613\n",
            "Epoch 322/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5983 - val_loss: 0.5609\n",
            "Epoch 323/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5979 - val_loss: 0.5605\n",
            "Epoch 324/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5976 - val_loss: 0.5601\n",
            "Epoch 325/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5973 - val_loss: 0.5597\n",
            "Epoch 326/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5969 - val_loss: 0.5593\n",
            "Epoch 327/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5966 - val_loss: 0.5589\n",
            "Epoch 328/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5963 - val_loss: 0.5585\n",
            "Epoch 329/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5960 - val_loss: 0.5581\n",
            "Epoch 330/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5956 - val_loss: 0.5577\n",
            "Epoch 331/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5953 - val_loss: 0.5574\n",
            "Epoch 332/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5950 - val_loss: 0.5570\n",
            "Epoch 333/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5947 - val_loss: 0.5566\n",
            "Epoch 334/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5944 - val_loss: 0.5563\n",
            "Epoch 335/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5941 - val_loss: 0.5559\n",
            "Epoch 336/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5938 - val_loss: 0.5556\n",
            "Epoch 337/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5935 - val_loss: 0.5552\n",
            "Epoch 338/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5933 - val_loss: 0.5549\n",
            "Epoch 339/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5930 - val_loss: 0.5545\n",
            "Epoch 340/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5927 - val_loss: 0.5542\n",
            "Epoch 341/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5924 - val_loss: 0.5538\n",
            "Epoch 342/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5921 - val_loss: 0.5535\n",
            "Epoch 343/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5919 - val_loss: 0.5531\n",
            "Epoch 344/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5916 - val_loss: 0.5528\n",
            "Epoch 345/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5913 - val_loss: 0.5525\n",
            "Epoch 346/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5911 - val_loss: 0.5522\n",
            "Epoch 347/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5908 - val_loss: 0.5518\n",
            "Epoch 348/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5905 - val_loss: 0.5515\n",
            "Epoch 349/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5903 - val_loss: 0.5512\n",
            "Epoch 350/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5900 - val_loss: 0.5509\n",
            "Epoch 351/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5898 - val_loss: 0.5506\n",
            "Epoch 352/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5896 - val_loss: 0.5503\n",
            "Epoch 353/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5893 - val_loss: 0.5499\n",
            "Epoch 354/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5891 - val_loss: 0.5497\n",
            "Epoch 355/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5888 - val_loss: 0.5494\n",
            "Epoch 356/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5886 - val_loss: 0.5491\n",
            "Epoch 357/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5884 - val_loss: 0.5488\n",
            "Epoch 358/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5881 - val_loss: 0.5485\n",
            "Epoch 359/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5879 - val_loss: 0.5482\n",
            "Epoch 360/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5877 - val_loss: 0.5479\n",
            "Epoch 361/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5875 - val_loss: 0.5476\n",
            "Epoch 362/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5873 - val_loss: 0.5474\n",
            "Epoch 363/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5870 - val_loss: 0.5471\n",
            "Epoch 364/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5868 - val_loss: 0.5468\n",
            "Epoch 365/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5866 - val_loss: 0.5466\n",
            "Epoch 366/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5864 - val_loss: 0.5463\n",
            "Epoch 367/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5862 - val_loss: 0.5460\n",
            "Epoch 368/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5860 - val_loss: 0.5458\n",
            "Epoch 369/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5858 - val_loss: 0.5455\n",
            "Epoch 370/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5856 - val_loss: 0.5453\n",
            "Epoch 371/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5854 - val_loss: 0.5450\n",
            "Epoch 372/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5852 - val_loss: 0.5448\n",
            "Epoch 373/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5850 - val_loss: 0.5445\n",
            "Epoch 374/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5848 - val_loss: 0.5443\n",
            "Epoch 375/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5847 - val_loss: 0.5440\n",
            "Epoch 376/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5845 - val_loss: 0.5438\n",
            "Epoch 377/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5843 - val_loss: 0.5436\n",
            "Epoch 378/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5841 - val_loss: 0.5433\n",
            "Epoch 379/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5839 - val_loss: 0.5431\n",
            "Epoch 380/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5838 - val_loss: 0.5429\n",
            "Epoch 381/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5836 - val_loss: 0.5427\n",
            "Epoch 382/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5834 - val_loss: 0.5424\n",
            "Epoch 383/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5833 - val_loss: 0.5422\n",
            "Epoch 384/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5831 - val_loss: 0.5420\n",
            "Epoch 385/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5829 - val_loss: 0.5418\n",
            "Epoch 386/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5828 - val_loss: 0.5416\n",
            "Epoch 387/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5826 - val_loss: 0.5414\n",
            "Epoch 388/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5825 - val_loss: 0.5412\n",
            "Epoch 389/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5823 - val_loss: 0.5409\n",
            "Epoch 390/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5822 - val_loss: 0.5408\n",
            "Epoch 391/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5820 - val_loss: 0.5406\n",
            "Epoch 392/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5819 - val_loss: 0.5404\n",
            "Epoch 393/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5817 - val_loss: 0.5402\n",
            "Epoch 394/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5816 - val_loss: 0.5400\n",
            "Epoch 395/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5814 - val_loss: 0.5398\n",
            "Epoch 396/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5813 - val_loss: 0.5396\n",
            "Epoch 397/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5812 - val_loss: 0.5394\n",
            "Epoch 398/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5810 - val_loss: 0.5393\n",
            "Epoch 399/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5809 - val_loss: 0.5391\n",
            "Epoch 400/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5808 - val_loss: 0.5389\n",
            "Epoch 401/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5806 - val_loss: 0.5387\n",
            "Epoch 402/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5805 - val_loss: 0.5385\n",
            "Epoch 403/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5804 - val_loss: 0.5384\n",
            "Epoch 404/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5803 - val_loss: 0.5382\n",
            "Epoch 405/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5801 - val_loss: 0.5380\n",
            "Epoch 406/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5800 - val_loss: 0.5379\n",
            "Epoch 407/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5799 - val_loss: 0.5377\n",
            "Epoch 408/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5798 - val_loss: 0.5375\n",
            "Epoch 409/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5797 - val_loss: 0.5374\n",
            "Epoch 410/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5795 - val_loss: 0.5372\n",
            "Epoch 411/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5794 - val_loss: 0.5370\n",
            "Epoch 412/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5793 - val_loss: 0.5369\n",
            "Epoch 413/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5792 - val_loss: 0.5367\n",
            "Epoch 414/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5791 - val_loss: 0.5366\n",
            "Epoch 415/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5790 - val_loss: 0.5365\n",
            "Epoch 416/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5789 - val_loss: 0.5363\n",
            "Epoch 417/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5788 - val_loss: 0.5362\n",
            "Epoch 418/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5787 - val_loss: 0.5360\n",
            "Epoch 419/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5786 - val_loss: 0.5359\n",
            "Epoch 420/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5785 - val_loss: 0.5357\n",
            "Epoch 421/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5784 - val_loss: 0.5356\n",
            "Epoch 422/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5783 - val_loss: 0.5355\n",
            "Epoch 423/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5782 - val_loss: 0.5353\n",
            "Epoch 424/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5781 - val_loss: 0.5352\n",
            "Epoch 425/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5780 - val_loss: 0.5350\n",
            "Epoch 426/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5779 - val_loss: 0.5349\n",
            "Epoch 427/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5778 - val_loss: 0.5348\n",
            "Epoch 428/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5777 - val_loss: 0.5346\n",
            "Epoch 429/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5776 - val_loss: 0.5345\n",
            "Epoch 430/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5776 - val_loss: 0.5344\n",
            "Epoch 431/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5775 - val_loss: 0.5343\n",
            "Epoch 432/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5774 - val_loss: 0.5341\n",
            "Epoch 433/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5773 - val_loss: 0.5340\n",
            "Epoch 434/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5772 - val_loss: 0.5339\n",
            "Epoch 435/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5771 - val_loss: 0.5338\n",
            "Epoch 436/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5771 - val_loss: 0.5337\n",
            "Epoch 437/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5770 - val_loss: 0.5335\n",
            "Epoch 438/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5769 - val_loss: 0.5334\n",
            "Epoch 439/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5768 - val_loss: 0.5333\n",
            "Epoch 440/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5768 - val_loss: 0.5332\n",
            "Epoch 441/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5767 - val_loss: 0.5331\n",
            "Epoch 442/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5766 - val_loss: 0.5330\n",
            "Epoch 443/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5765 - val_loss: 0.5329\n",
            "Epoch 444/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5765 - val_loss: 0.5328\n",
            "Epoch 445/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5764 - val_loss: 0.5326\n",
            "Epoch 446/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5763 - val_loss: 0.5326\n",
            "Epoch 447/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5763 - val_loss: 0.5325\n",
            "Epoch 448/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5762 - val_loss: 0.5324\n",
            "Epoch 449/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5761 - val_loss: 0.5323\n",
            "Epoch 450/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5761 - val_loss: 0.5322\n",
            "Epoch 451/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5760 - val_loss: 0.5321\n",
            "Epoch 452/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5760 - val_loss: 0.5320\n",
            "Epoch 453/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5759 - val_loss: 0.5319\n",
            "Epoch 454/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5758 - val_loss: 0.5318\n",
            "Epoch 455/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5758 - val_loss: 0.5317\n",
            "Epoch 456/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5757 - val_loss: 0.5316\n",
            "Epoch 457/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5757 - val_loss: 0.5315\n",
            "Epoch 458/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5756 - val_loss: 0.5314\n",
            "Epoch 459/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5756 - val_loss: 0.5313\n",
            "Epoch 460/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5755 - val_loss: 0.5313\n",
            "Epoch 461/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5755 - val_loss: 0.5312\n",
            "Epoch 462/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5754 - val_loss: 0.5311\n",
            "Epoch 463/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5754 - val_loss: 0.5310\n",
            "Epoch 464/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5753 - val_loss: 0.5309\n",
            "Epoch 465/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5753 - val_loss: 0.5309\n",
            "Epoch 466/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5752 - val_loss: 0.5308\n",
            "Epoch 467/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5752 - val_loss: 0.5307\n",
            "Epoch 468/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5751 - val_loss: 0.5306\n",
            "Epoch 469/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5751 - val_loss: 0.5305\n",
            "Epoch 470/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5750 - val_loss: 0.5305\n",
            "Epoch 471/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5750 - val_loss: 0.5304\n",
            "Epoch 472/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5749 - val_loss: 0.5303\n",
            "Epoch 473/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5749 - val_loss: 0.5302\n",
            "Epoch 474/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5748 - val_loss: 0.5302\n",
            "Epoch 475/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5748 - val_loss: 0.5301\n",
            "Epoch 476/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5748 - val_loss: 0.5300\n",
            "Epoch 477/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5747 - val_loss: 0.5300\n",
            "Epoch 478/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5747 - val_loss: 0.5299\n",
            "Epoch 479/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5746 - val_loss: 0.5298\n",
            "Epoch 480/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5746 - val_loss: 0.5298\n",
            "Epoch 481/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5746 - val_loss: 0.5297\n",
            "Epoch 482/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5745 - val_loss: 0.5296\n",
            "Epoch 483/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5745 - val_loss: 0.5296\n",
            "Epoch 484/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5745 - val_loss: 0.5295\n",
            "Epoch 485/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5744 - val_loss: 0.5295\n",
            "Epoch 486/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5744 - val_loss: 0.5294\n",
            "Epoch 487/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5743 - val_loss: 0.5293\n",
            "Epoch 488/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5743 - val_loss: 0.5293\n",
            "Epoch 489/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5743 - val_loss: 0.5292\n",
            "Epoch 490/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5742 - val_loss: 0.5291\n",
            "Epoch 491/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5742 - val_loss: 0.5291\n",
            "Epoch 492/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5742 - val_loss: 0.5290\n",
            "Epoch 493/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5741 - val_loss: 0.5290\n",
            "Epoch 494/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5741 - val_loss: 0.5289\n",
            "Epoch 495/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5741 - val_loss: 0.5289\n",
            "Epoch 496/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5741 - val_loss: 0.5288\n",
            "Epoch 497/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5740 - val_loss: 0.5287\n",
            "Epoch 498/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5740 - val_loss: 0.5287\n",
            "Epoch 499/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5740 - val_loss: 0.5286\n",
            "Epoch 500/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5739 - val_loss: 0.5286\n",
            "Epoch 501/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5739 - val_loss: 0.5286\n",
            "Epoch 502/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5739 - val_loss: 0.5285\n",
            "Epoch 503/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5739 - val_loss: 0.5285\n",
            "Epoch 504/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5738 - val_loss: 0.5284\n",
            "Epoch 505/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5738 - val_loss: 0.5283\n",
            "Epoch 506/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5738 - val_loss: 0.5283\n",
            "Epoch 507/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5738 - val_loss: 0.5283\n",
            "Epoch 508/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5737 - val_loss: 0.5282\n",
            "Epoch 509/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5737 - val_loss: 0.5282\n",
            "Epoch 510/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5737 - val_loss: 0.5281\n",
            "Epoch 511/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5737 - val_loss: 0.5281\n",
            "Epoch 512/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5736 - val_loss: 0.5280\n",
            "Epoch 513/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5736 - val_loss: 0.5280\n",
            "Epoch 514/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5736 - val_loss: 0.5279\n",
            "Epoch 515/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5736 - val_loss: 0.5279\n",
            "Epoch 516/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5736 - val_loss: 0.5279\n",
            "Epoch 517/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5735 - val_loss: 0.5278\n",
            "Epoch 518/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5735 - val_loss: 0.5278\n",
            "Epoch 519/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5735 - val_loss: 0.5277\n",
            "Epoch 520/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5735 - val_loss: 0.5277\n",
            "Epoch 521/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5735 - val_loss: 0.5277\n",
            "Epoch 522/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5734 - val_loss: 0.5276\n",
            "Epoch 523/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5734 - val_loss: 0.5276\n",
            "Epoch 524/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5734 - val_loss: 0.5276\n",
            "Epoch 525/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5734 - val_loss: 0.5275\n",
            "Epoch 526/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5734 - val_loss: 0.5275\n",
            "Epoch 527/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5733 - val_loss: 0.5274\n",
            "Epoch 528/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5733 - val_loss: 0.5274\n",
            "Epoch 529/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5733 - val_loss: 0.5274\n",
            "Epoch 530/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5733 - val_loss: 0.5273\n",
            "Epoch 531/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5733 - val_loss: 0.5273\n",
            "Epoch 532/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5733 - val_loss: 0.5273\n",
            "Epoch 533/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5732 - val_loss: 0.5272\n",
            "Epoch 534/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5732 - val_loss: 0.5272\n",
            "Epoch 535/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5732 - val_loss: 0.5272\n",
            "Epoch 536/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5732 - val_loss: 0.5271\n",
            "Epoch 537/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5732 - val_loss: 0.5271\n",
            "Epoch 538/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5732 - val_loss: 0.5271\n",
            "Epoch 539/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5732 - val_loss: 0.5270\n",
            "Epoch 540/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5731 - val_loss: 0.5270\n",
            "Epoch 541/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5731 - val_loss: 0.5270\n",
            "Epoch 542/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5731 - val_loss: 0.5269\n",
            "Epoch 543/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5731 - val_loss: 0.5269\n",
            "Epoch 544/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5731 - val_loss: 0.5269\n",
            "Epoch 545/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5731 - val_loss: 0.5269\n",
            "Epoch 546/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5731 - val_loss: 0.5268\n",
            "Epoch 547/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5731 - val_loss: 0.5268\n",
            "Epoch 548/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5730 - val_loss: 0.5268\n",
            "Epoch 549/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5730 - val_loss: 0.5267\n",
            "Epoch 550/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5730 - val_loss: 0.5267\n",
            "Epoch 551/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5730 - val_loss: 0.5267\n",
            "Epoch 552/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5730 - val_loss: 0.5267\n",
            "Epoch 553/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5730 - val_loss: 0.5266\n",
            "Epoch 554/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5730 - val_loss: 0.5266\n",
            "Epoch 555/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5730 - val_loss: 0.5266\n",
            "Epoch 556/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5729 - val_loss: 0.5266\n",
            "Epoch 557/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5729 - val_loss: 0.5265\n",
            "Epoch 558/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5729 - val_loss: 0.5265\n",
            "Epoch 559/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5729 - val_loss: 0.5265\n",
            "Epoch 560/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5729 - val_loss: 0.5265\n",
            "Epoch 561/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5729 - val_loss: 0.5264\n",
            "Epoch 562/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5729 - val_loss: 0.5264\n",
            "Epoch 563/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5729 - val_loss: 0.5264\n",
            "Epoch 564/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5729 - val_loss: 0.5264\n",
            "Epoch 565/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5729 - val_loss: 0.5263\n",
            "Epoch 566/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5728 - val_loss: 0.5263\n",
            "Epoch 567/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5728 - val_loss: 0.5263\n",
            "Epoch 568/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5728 - val_loss: 0.5263\n",
            "Epoch 569/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5728 - val_loss: 0.5262\n",
            "Epoch 570/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5728 - val_loss: 0.5262\n",
            "Epoch 571/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5728 - val_loss: 0.5262\n",
            "Epoch 572/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5728 - val_loss: 0.5262\n",
            "Epoch 573/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5728 - val_loss: 0.5262\n",
            "Epoch 574/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5728 - val_loss: 0.5261\n",
            "Epoch 575/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5728 - val_loss: 0.5261\n",
            "Epoch 576/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5728 - val_loss: 0.5261\n",
            "Epoch 577/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5728 - val_loss: 0.5261\n",
            "Epoch 578/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5727 - val_loss: 0.5260\n",
            "Epoch 579/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5727 - val_loss: 0.5260\n",
            "Epoch 580/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5727 - val_loss: 0.5260\n",
            "Epoch 581/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5727 - val_loss: 0.5260\n",
            "Epoch 582/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5727 - val_loss: 0.5260\n",
            "Epoch 583/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5727 - val_loss: 0.5260\n",
            "Epoch 584/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5727 - val_loss: 0.5259\n",
            "Epoch 585/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5727 - val_loss: 0.5259\n",
            "Epoch 586/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5727 - val_loss: 0.5259\n",
            "Epoch 587/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5727 - val_loss: 0.5259\n",
            "Epoch 588/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5727 - val_loss: 0.5259\n",
            "Epoch 589/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5727 - val_loss: 0.5258\n",
            "Epoch 590/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5727 - val_loss: 0.5258\n",
            "Epoch 591/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5727 - val_loss: 0.5258\n",
            "Epoch 592/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5727 - val_loss: 0.5258\n",
            "Epoch 593/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.5258\n",
            "Epoch 594/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.5258\n",
            "Epoch 595/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.5257\n",
            "Epoch 596/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5726 - val_loss: 0.5257\n",
            "Epoch 597/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5726 - val_loss: 0.5257\n",
            "Epoch 598/1000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.5726 - val_loss: 0.5257\n",
            "Epoch 599/1000\n",
            "70/70 [==============================] - 1s 21ms/step - loss: 0.5726 - val_loss: 0.5257\n",
            "Epoch 600/1000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.5726 - val_loss: 0.5257\n",
            "Epoch 601/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5726 - val_loss: 0.5256\n",
            "Epoch 602/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5726 - val_loss: 0.5256\n",
            "Epoch 603/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.5256\n",
            "Epoch 604/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.5256\n",
            "Epoch 605/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.5256\n",
            "Epoch 606/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.5256\n",
            "Epoch 607/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.5255\n",
            "Epoch 608/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5726 - val_loss: 0.5255\n",
            "Epoch 609/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5726 - val_loss: 0.5255\n",
            "Epoch 610/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.5255\n",
            "Epoch 611/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5726 - val_loss: 0.5255\n",
            "Epoch 612/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5725 - val_loss: 0.5255\n",
            "Epoch 613/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5255\n",
            "Epoch 614/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5255\n",
            "Epoch 615/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5725 - val_loss: 0.5254\n",
            "Epoch 616/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5725 - val_loss: 0.5254\n",
            "Epoch 617/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5725 - val_loss: 0.5254\n",
            "Epoch 618/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5725 - val_loss: 0.5254\n",
            "Epoch 619/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5725 - val_loss: 0.5254\n",
            "Epoch 620/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5725 - val_loss: 0.5254\n",
            "Epoch 621/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5254\n",
            "Epoch 622/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5254\n",
            "Epoch 623/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5253\n",
            "Epoch 624/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5725 - val_loss: 0.5253\n",
            "Epoch 625/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5253\n",
            "Epoch 626/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5253\n",
            "Epoch 627/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5725 - val_loss: 0.5253\n",
            "Epoch 628/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5253\n",
            "Epoch 629/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5253\n",
            "Epoch 630/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5253\n",
            "Epoch 631/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5252\n",
            "Epoch 632/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5725 - val_loss: 0.5252\n",
            "Epoch 633/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5725 - val_loss: 0.5252\n",
            "Epoch 634/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5725 - val_loss: 0.5252\n",
            "Epoch 635/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5725 - val_loss: 0.5252\n",
            "Epoch 636/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5725 - val_loss: 0.5252\n",
            "Epoch 637/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5252\n",
            "Epoch 638/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5252\n",
            "Epoch 639/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.5252\n",
            "Epoch 640/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 641/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 642/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 643/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 644/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 645/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 646/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 647/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 648/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 649/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5724 - val_loss: 0.5251\n",
            "Epoch 650/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 651/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 652/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 653/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 654/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 655/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 656/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 657/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 658/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 659/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 660/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 661/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5250\n",
            "Epoch 662/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 663/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 664/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 665/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 666/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 667/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 668/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 669/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 670/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 671/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 672/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 673/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 674/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5249\n",
            "Epoch 675/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 676/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 677/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 678/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 679/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 680/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 681/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 682/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 683/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 684/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 685/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 686/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 687/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 688/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 689/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5248\n",
            "Epoch 690/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5247\n",
            "Epoch 691/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5247\n",
            "Epoch 692/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5247\n",
            "Epoch 693/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5724 - val_loss: 0.5247\n",
            "Epoch 694/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5724 - val_loss: 0.5247\n",
            "Epoch 695/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 696/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 697/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 698/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 699/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 700/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 701/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 702/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 703/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 704/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 705/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 706/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 707/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5247\n",
            "Epoch 708/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 709/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 710/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 711/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 712/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 713/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 714/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 715/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 716/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 717/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 718/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 719/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 720/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 721/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 722/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 723/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 724/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 725/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 726/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 727/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5246\n",
            "Epoch 728/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 729/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 730/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 731/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 732/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 733/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 734/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 735/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 736/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 737/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 738/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 739/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 740/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 741/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 742/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 743/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 744/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 745/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 746/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 747/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 748/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 749/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 750/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 751/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 752/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5245\n",
            "Epoch 753/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 754/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 755/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 756/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 757/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 758/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 759/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 760/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 761/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 762/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 763/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 764/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 765/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 766/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 767/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 768/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 769/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 770/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 771/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 772/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 773/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 774/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 775/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 776/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 777/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 778/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 779/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 780/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 781/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 782/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 783/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 784/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 785/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 786/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5244\n",
            "Epoch 787/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 788/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 789/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 790/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 791/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 792/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 793/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 794/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 795/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 796/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 797/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 798/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 799/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 800/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 801/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 802/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 803/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 804/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 805/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 806/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 807/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 808/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 809/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 810/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 811/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 812/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 813/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 814/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 815/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 816/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 817/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 818/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 819/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 820/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 821/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 822/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 823/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 824/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 825/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 826/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 827/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 828/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 829/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 830/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 831/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 832/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 833/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 834/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 835/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5243\n",
            "Epoch 836/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 837/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 838/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 839/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 840/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 841/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 842/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 843/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 844/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 845/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 846/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 847/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 848/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 849/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 850/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 851/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 852/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 853/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 854/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 855/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 856/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 857/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 858/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 859/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 860/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 861/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 862/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 863/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 864/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 865/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 866/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 867/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 868/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 869/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 870/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 871/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 872/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 873/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 874/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 875/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 876/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 877/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 878/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 879/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 880/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 881/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 882/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 883/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 884/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 885/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 886/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 887/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 888/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 889/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 890/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 891/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 892/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 893/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 894/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 895/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 896/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 897/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 898/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 899/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 900/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 901/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 902/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 903/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 904/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 905/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 906/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 907/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 908/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 909/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 910/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 911/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5723 - val_loss: 0.5242\n",
            "Epoch 912/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 913/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 914/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 915/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 916/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 917/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 918/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 919/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 920/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 921/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 922/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 923/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 924/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 925/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 926/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 927/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 928/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 929/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 930/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 931/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 932/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 933/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 934/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 935/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 936/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 937/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 938/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 939/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 940/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 941/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 942/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 943/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 944/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 945/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 946/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 947/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 948/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 949/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 950/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 951/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 952/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 953/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 954/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 955/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 956/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 957/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 958/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 959/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 960/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 961/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 962/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 963/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 964/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 965/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 966/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 967/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 968/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 969/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 970/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 971/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 972/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 973/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 974/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 975/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 976/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 977/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 978/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 979/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 980/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 981/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 982/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 983/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 984/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 985/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 986/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 987/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 988/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 989/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 990/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 991/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 992/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 993/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 994/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 995/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 996/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 997/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 998/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 999/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "Epoch 1000/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5723 - val_loss: 0.5241\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5241\n",
            "8/8 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbf845d8c70>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000;    # training epochs\n",
        "temperature = 0.1    # temprature control the smooth of the probabilities\n",
        "\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "# replace softmax\n",
        "x = Dense(3, activation=\"sigmoid\")(x)\n",
        "\n",
        "# add normalization layer\n",
        "pred = layers.Lambda(lambda x: x/(K.sum(x)+K.epsilon()))(x)\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "waeLOQUbYF_I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adzI5AVDMv70"
      },
      "source": [
        "#3. Data Splitting Test for Euclidean distance loss#\n",
        "In thie part, the data will be splitted to:\n",
        "- 3.1 train on one part of the color space and test on an the other part\n",
        "- 3.2 train on data under 1 light and test on 1 light condition \n",
        "- 3.3 train on data under 3 light and test on 1 light condition \n",
        "  \n",
        "In total, 200 color pairs and 4 lights."
      ],
      "id": "adzI5AVDMv70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk6qZ4HcObF_"
      },
      "source": [
        "##3.1 Train on part of the color space and test on the rest part ##\n",
        "\n",
        "loss: 0.0094 - val_loss: 0.0064"
      ],
      "id": "qk6qZ4HcObF_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4s2eIDL2Npp0",
        "outputId": "03a277c7-327c-4c06-f62e-5b243615b65e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "35/35 [==============================] - 65s 24ms/step - loss: 0.7400 - val_loss: 0.4098\n",
            "Epoch 2/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.4580 - val_loss: 0.4559\n",
            "Epoch 3/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.4334 - val_loss: 0.4022\n",
            "Epoch 4/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.4358 - val_loss: 0.3608\n",
            "Epoch 5/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.3869 - val_loss: 0.3687\n",
            "Epoch 6/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.3783 - val_loss: 0.3613\n",
            "Epoch 7/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.3620 - val_loss: 0.3959\n",
            "Epoch 8/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.4185 - val_loss: 0.3907\n",
            "Epoch 9/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.3907 - val_loss: 0.3912\n",
            "Epoch 10/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.3591 - val_loss: 0.3872\n",
            "Epoch 11/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.3842 - val_loss: 0.3556\n",
            "Epoch 12/500\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.3392 - val_loss: 0.3220\n",
            "Epoch 13/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.3946 - val_loss: 0.3566\n",
            "Epoch 14/500\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.3687 - val_loss: 0.3983\n",
            "Epoch 15/500\n",
            "35/35 [==============================] - 1s 30ms/step - loss: 0.3494 - val_loss: 0.3349\n",
            "Epoch 16/500\n",
            "35/35 [==============================] - 1s 13ms/step - loss: 0.3358 - val_loss: 0.3452\n",
            "Epoch 17/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3534 - val_loss: 0.3482\n",
            "Epoch 18/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3388 - val_loss: 0.3787\n",
            "Epoch 19/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.3418 - val_loss: 0.3431\n",
            "Epoch 20/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.3744 - val_loss: 0.3564\n",
            "Epoch 21/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.3498 - val_loss: 0.3445\n",
            "Epoch 22/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3495 - val_loss: 0.3392\n",
            "Epoch 23/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.3219 - val_loss: 0.3105\n",
            "Epoch 24/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2975 - val_loss: 0.3716\n",
            "Epoch 25/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3346 - val_loss: 0.3372\n",
            "Epoch 26/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3022 - val_loss: 0.3194\n",
            "Epoch 27/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2850 - val_loss: 0.2977\n",
            "Epoch 28/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2878 - val_loss: 0.3243\n",
            "Epoch 29/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2982 - val_loss: 0.3041\n",
            "Epoch 30/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2821 - val_loss: 0.3018\n",
            "Epoch 31/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2818 - val_loss: 0.3164\n",
            "Epoch 32/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3000 - val_loss: 0.3404\n",
            "Epoch 33/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2826 - val_loss: 0.2958\n",
            "Epoch 34/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2585 - val_loss: 0.2890\n",
            "Epoch 35/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2428 - val_loss: 0.2563\n",
            "Epoch 36/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2654 - val_loss: 0.2762\n",
            "Epoch 37/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2305 - val_loss: 0.2879\n",
            "Epoch 38/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2438 - val_loss: 0.2860\n",
            "Epoch 39/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2393 - val_loss: 0.3076\n",
            "Epoch 40/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2754 - val_loss: 0.2812\n",
            "Epoch 41/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2475 - val_loss: 0.2617\n",
            "Epoch 42/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.2658 - val_loss: 0.3015\n",
            "Epoch 43/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.2878 - val_loss: 0.2769\n",
            "Epoch 44/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.2520 - val_loss: 0.2500\n",
            "Epoch 45/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.2526 - val_loss: 0.2593\n",
            "Epoch 46/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.2040 - val_loss: 0.2494\n",
            "Epoch 47/500\n",
            "35/35 [==============================] - 0s 10ms/step - loss: 0.2035 - val_loss: 0.2735\n",
            "Epoch 48/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2394 - val_loss: 0.2751\n",
            "Epoch 49/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2462 - val_loss: 0.2637\n",
            "Epoch 50/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2542 - val_loss: 0.2578\n",
            "Epoch 51/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2326 - val_loss: 0.2776\n",
            "Epoch 52/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2167 - val_loss: 0.2554\n",
            "Epoch 53/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2359 - val_loss: 0.2542\n",
            "Epoch 54/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2392 - val_loss: 0.2947\n",
            "Epoch 55/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2207 - val_loss: 0.2607\n",
            "Epoch 56/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2207 - val_loss: 0.2532\n",
            "Epoch 57/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2404 - val_loss: 0.2483\n",
            "Epoch 58/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2197 - val_loss: 0.2677\n",
            "Epoch 59/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2178 - val_loss: 0.2466\n",
            "Epoch 60/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.1860 - val_loss: 0.2462\n",
            "Epoch 61/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1830 - val_loss: 0.2493\n",
            "Epoch 62/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2143 - val_loss: 0.2453\n",
            "Epoch 63/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1767 - val_loss: 0.2509\n",
            "Epoch 64/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.2048 - val_loss: 0.2458\n",
            "Epoch 65/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1489 - val_loss: 0.2150\n",
            "Epoch 66/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1552 - val_loss: 0.1939\n",
            "Epoch 67/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1521 - val_loss: 0.2238\n",
            "Epoch 68/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1642 - val_loss: 0.2120\n",
            "Epoch 69/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1758 - val_loss: 0.2341\n",
            "Epoch 70/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1736 - val_loss: 0.2283\n",
            "Epoch 71/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.1574 - val_loss: 0.2100\n",
            "Epoch 72/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.1571 - val_loss: 0.2118\n",
            "Epoch 73/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.1822 - val_loss: 0.2910\n",
            "Epoch 74/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.1919 - val_loss: 0.2135\n",
            "Epoch 75/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.1579 - val_loss: 0.2148\n",
            "Epoch 76/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1845 - val_loss: 0.2075\n",
            "Epoch 77/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1896 - val_loss: 0.2415\n",
            "Epoch 78/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1941 - val_loss: 0.2588\n",
            "Epoch 79/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.2349 - val_loss: 0.2807\n",
            "Epoch 80/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.2012 - val_loss: 0.2569\n",
            "Epoch 81/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1834 - val_loss: 0.2283\n",
            "Epoch 82/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1563 - val_loss: 0.2105\n",
            "Epoch 83/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1327 - val_loss: 0.1952\n",
            "Epoch 84/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1437 - val_loss: 0.2028\n",
            "Epoch 85/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1291 - val_loss: 0.1959\n",
            "Epoch 86/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1359 - val_loss: 0.1825\n",
            "Epoch 87/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1337 - val_loss: 0.2313\n",
            "Epoch 88/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1895 - val_loss: 0.2265\n",
            "Epoch 89/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1685 - val_loss: 0.1888\n",
            "Epoch 90/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1535 - val_loss: 0.2240\n",
            "Epoch 91/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.1488 - val_loss: 0.2117\n",
            "Epoch 92/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1395 - val_loss: 0.2102\n",
            "Epoch 93/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1562 - val_loss: 0.1968\n",
            "Epoch 94/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1839 - val_loss: 0.2098\n",
            "Epoch 95/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.1455 - val_loss: 0.2089\n",
            "Epoch 96/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.1427 - val_loss: 0.2086\n",
            "Epoch 97/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1378 - val_loss: 0.1849\n",
            "Epoch 98/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1498 - val_loss: 0.1915\n",
            "Epoch 99/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1302 - val_loss: 0.1907\n",
            "Epoch 100/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1362 - val_loss: 0.1827\n",
            "Epoch 101/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.1335 - val_loss: 0.1875\n",
            "Epoch 102/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.1270 - val_loss: 0.2030\n",
            "Epoch 103/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1303 - val_loss: 0.1696\n",
            "Epoch 104/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.1125 - val_loss: 0.1580\n",
            "Epoch 105/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.1284 - val_loss: 0.1839\n",
            "Epoch 106/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.1433 - val_loss: 0.2022\n",
            "Epoch 107/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1519 - val_loss: 0.1954\n",
            "Epoch 108/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1368 - val_loss: 0.1903\n",
            "Epoch 109/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1252 - val_loss: 0.1716\n",
            "Epoch 110/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.1227 - val_loss: 0.1786\n",
            "Epoch 111/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1285 - val_loss: 0.1754\n",
            "Epoch 112/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1339 - val_loss: 0.1649\n",
            "Epoch 113/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1101 - val_loss: 0.1657\n",
            "Epoch 114/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1241 - val_loss: 0.1828\n",
            "Epoch 115/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1273 - val_loss: 0.1775\n",
            "Epoch 116/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1295 - val_loss: 0.1617\n",
            "Epoch 117/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1072 - val_loss: 0.1702\n",
            "Epoch 118/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1283 - val_loss: 0.1991\n",
            "Epoch 119/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1201 - val_loss: 0.1572\n",
            "Epoch 120/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1048 - val_loss: 0.1607\n",
            "Epoch 121/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1034 - val_loss: 0.1546\n",
            "Epoch 122/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1124 - val_loss: 0.1839\n",
            "Epoch 123/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1017 - val_loss: 0.1592\n",
            "Epoch 124/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1232 - val_loss: 0.1988\n",
            "Epoch 125/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1168 - val_loss: 0.1773\n",
            "Epoch 126/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1275 - val_loss: 0.1392\n",
            "Epoch 127/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0983 - val_loss: 0.1478\n",
            "Epoch 128/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0932 - val_loss: 0.1469\n",
            "Epoch 129/500\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 0.1015 - val_loss: 0.1571\n",
            "Epoch 130/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0929 - val_loss: 0.1561\n",
            "Epoch 131/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0922 - val_loss: 0.1649\n",
            "Epoch 132/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1432 - val_loss: 0.1519\n",
            "Epoch 133/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1097 - val_loss: 0.1622\n",
            "Epoch 134/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0907 - val_loss: 0.1492\n",
            "Epoch 135/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1037 - val_loss: 0.1540\n",
            "Epoch 136/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0942 - val_loss: 0.1419\n",
            "Epoch 137/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0954 - val_loss: 0.1485\n",
            "Epoch 138/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1067 - val_loss: 0.1751\n",
            "Epoch 139/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1085 - val_loss: 0.1509\n",
            "Epoch 140/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0985 - val_loss: 0.1671\n",
            "Epoch 141/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0982 - val_loss: 0.1553\n",
            "Epoch 142/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1192 - val_loss: 0.1629\n",
            "Epoch 143/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1288 - val_loss: 0.1476\n",
            "Epoch 144/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0940 - val_loss: 0.1454\n",
            "Epoch 145/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0869 - val_loss: 0.1398\n",
            "Epoch 146/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1023 - val_loss: 0.1381\n",
            "Epoch 147/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0933 - val_loss: 0.1428\n",
            "Epoch 148/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1044 - val_loss: 0.1459\n",
            "Epoch 149/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0868 - val_loss: 0.1368\n",
            "Epoch 150/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0860 - val_loss: 0.1391\n",
            "Epoch 151/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0825 - val_loss: 0.1253\n",
            "Epoch 152/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0948 - val_loss: 0.1473\n",
            "Epoch 153/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0945 - val_loss: 0.1396\n",
            "Epoch 154/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0881 - val_loss: 0.1407\n",
            "Epoch 155/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0771 - val_loss: 0.1483\n",
            "Epoch 156/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0901 - val_loss: 0.1471\n",
            "Epoch 157/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0901 - val_loss: 0.1552\n",
            "Epoch 158/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1106 - val_loss: 0.1458\n",
            "Epoch 159/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0771 - val_loss: 0.1500\n",
            "Epoch 160/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0962 - val_loss: 0.1437\n",
            "Epoch 161/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.1040 - val_loss: 0.1438\n",
            "Epoch 162/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0966 - val_loss: 0.1411\n",
            "Epoch 163/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0834 - val_loss: 0.1546\n",
            "Epoch 164/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1050 - val_loss: 0.1552\n",
            "Epoch 165/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.1008 - val_loss: 0.1338\n",
            "Epoch 166/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0978 - val_loss: 0.1405\n",
            "Epoch 167/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0966 - val_loss: 0.1421\n",
            "Epoch 168/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0930 - val_loss: 0.1610\n",
            "Epoch 169/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1182 - val_loss: 0.1526\n",
            "Epoch 170/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1202 - val_loss: 0.1489\n",
            "Epoch 171/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1076 - val_loss: 0.1357\n",
            "Epoch 172/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0953 - val_loss: 0.1370\n",
            "Epoch 173/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1022 - val_loss: 0.1440\n",
            "Epoch 174/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1122 - val_loss: 0.1590\n",
            "Epoch 175/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1086 - val_loss: 0.1615\n",
            "Epoch 176/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1337 - val_loss: 0.1547\n",
            "Epoch 177/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1067 - val_loss: 0.1382\n",
            "Epoch 178/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1031 - val_loss: 0.1624\n",
            "Epoch 179/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1174 - val_loss: 0.1631\n",
            "Epoch 180/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1167 - val_loss: 0.1428\n",
            "Epoch 181/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0962 - val_loss: 0.1510\n",
            "Epoch 182/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0799 - val_loss: 0.1440\n",
            "Epoch 183/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0995 - val_loss: 0.1315\n",
            "Epoch 184/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.1023 - val_loss: 0.1319\n",
            "Epoch 185/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0857 - val_loss: 0.1598\n",
            "Epoch 186/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0980 - val_loss: 0.1387\n",
            "Epoch 187/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0995 - val_loss: 0.1433\n",
            "Epoch 188/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.1174 - val_loss: 0.1592\n",
            "Epoch 189/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.1010 - val_loss: 0.1282\n",
            "Epoch 190/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0944 - val_loss: 0.1360\n",
            "Epoch 191/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0918 - val_loss: 0.1315\n",
            "Epoch 192/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0808 - val_loss: 0.1222\n",
            "Epoch 193/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0801 - val_loss: 0.1407\n",
            "Epoch 194/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0739 - val_loss: 0.1393\n",
            "Epoch 195/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0849 - val_loss: 0.1414\n",
            "Epoch 196/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0866 - val_loss: 0.1194\n",
            "Epoch 197/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0738 - val_loss: 0.1280\n",
            "Epoch 198/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0984 - val_loss: 0.1299\n",
            "Epoch 199/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0790 - val_loss: 0.1249\n",
            "Epoch 200/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0846 - val_loss: 0.1260\n",
            "Epoch 201/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0890 - val_loss: 0.1508\n",
            "Epoch 202/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1010 - val_loss: 0.1284\n",
            "Epoch 203/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0911 - val_loss: 0.1400\n",
            "Epoch 204/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1093 - val_loss: 0.1324\n",
            "Epoch 205/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0831 - val_loss: 0.1553\n",
            "Epoch 206/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1278 - val_loss: 0.1321\n",
            "Epoch 207/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0877 - val_loss: 0.1232\n",
            "Epoch 208/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0871 - val_loss: 0.1435\n",
            "Epoch 209/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0977 - val_loss: 0.1221\n",
            "Epoch 210/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0745 - val_loss: 0.1419\n",
            "Epoch 211/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0784 - val_loss: 0.1216\n",
            "Epoch 212/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0946 - val_loss: 0.1321\n",
            "Epoch 213/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0955 - val_loss: 0.1553\n",
            "Epoch 214/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0943 - val_loss: 0.1451\n",
            "Epoch 215/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1183 - val_loss: 0.1518\n",
            "Epoch 216/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.1072 - val_loss: 0.1269\n",
            "Epoch 217/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0620 - val_loss: 0.1149\n",
            "Epoch 218/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0839 - val_loss: 0.1224\n",
            "Epoch 219/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0863 - val_loss: 0.1299\n",
            "Epoch 220/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0886 - val_loss: 0.1166\n",
            "Epoch 221/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0788 - val_loss: 0.1092\n",
            "Epoch 222/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0832 - val_loss: 0.1168\n",
            "Epoch 223/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0975 - val_loss: 0.1222\n",
            "Epoch 224/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0679 - val_loss: 0.1151\n",
            "Epoch 225/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0767 - val_loss: 0.1143\n",
            "Epoch 226/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0590 - val_loss: 0.1148\n",
            "Epoch 227/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0831 - val_loss: 0.1110\n",
            "Epoch 228/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0724 - val_loss: 0.1220\n",
            "Epoch 229/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0705 - val_loss: 0.1212\n",
            "Epoch 230/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0752 - val_loss: 0.1188\n",
            "Epoch 231/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0734 - val_loss: 0.1246\n",
            "Epoch 232/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0789 - val_loss: 0.1273\n",
            "Epoch 233/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0911 - val_loss: 0.1186\n",
            "Epoch 234/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0767 - val_loss: 0.1201\n",
            "Epoch 235/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0884 - val_loss: 0.1202\n",
            "Epoch 236/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0628 - val_loss: 0.1252\n",
            "Epoch 237/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0755 - val_loss: 0.1233\n",
            "Epoch 238/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0772 - val_loss: 0.1156\n",
            "Epoch 239/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0676 - val_loss: 0.1215\n",
            "Epoch 240/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0560 - val_loss: 0.1155\n",
            "Epoch 241/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0646 - val_loss: 0.1023\n",
            "Epoch 242/500\n",
            "35/35 [==============================] - 1s 27ms/step - loss: 0.0740 - val_loss: 0.1188\n",
            "Epoch 243/500\n",
            "35/35 [==============================] - 1s 20ms/step - loss: 0.1058 - val_loss: 0.1283\n",
            "Epoch 244/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0716 - val_loss: 0.1198\n",
            "Epoch 245/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0772 - val_loss: 0.1199\n",
            "Epoch 246/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0746 - val_loss: 0.1217\n",
            "Epoch 247/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0739 - val_loss: 0.1234\n",
            "Epoch 248/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0763 - val_loss: 0.1152\n",
            "Epoch 249/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0663 - val_loss: 0.1131\n",
            "Epoch 250/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0820 - val_loss: 0.1096\n",
            "Epoch 251/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0598 - val_loss: 0.1234\n",
            "Epoch 252/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0701 - val_loss: 0.1307\n",
            "Epoch 253/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0924 - val_loss: 0.1448\n",
            "Epoch 254/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0977 - val_loss: 0.1187\n",
            "Epoch 255/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0613 - val_loss: 0.1139\n",
            "Epoch 256/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0786 - val_loss: 0.1134\n",
            "Epoch 257/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0744 - val_loss: 0.1127\n",
            "Epoch 258/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0694 - val_loss: 0.1113\n",
            "Epoch 259/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0642 - val_loss: 0.1177\n",
            "Epoch 260/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0694 - val_loss: 0.1176\n",
            "Epoch 261/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.1002 - val_loss: 0.1267\n",
            "Epoch 262/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0888 - val_loss: 0.1342\n",
            "Epoch 263/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0901 - val_loss: 0.1098\n",
            "Epoch 264/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0804 - val_loss: 0.1078\n",
            "Epoch 265/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0853 - val_loss: 0.1265\n",
            "Epoch 266/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0732 - val_loss: 0.1114\n",
            "Epoch 267/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0783 - val_loss: 0.1224\n",
            "Epoch 268/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0790 - val_loss: 0.1151\n",
            "Epoch 269/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0744 - val_loss: 0.0990\n",
            "Epoch 270/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0835 - val_loss: 0.1175\n",
            "Epoch 271/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0900 - val_loss: 0.1443\n",
            "Epoch 272/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0795 - val_loss: 0.1164\n",
            "Epoch 273/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0848 - val_loss: 0.1408\n",
            "Epoch 274/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0842 - val_loss: 0.1281\n",
            "Epoch 275/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0951 - val_loss: 0.1342\n",
            "Epoch 276/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0922 - val_loss: 0.1207\n",
            "Epoch 277/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0801 - val_loss: 0.1220\n",
            "Epoch 278/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0869 - val_loss: 0.1306\n",
            "Epoch 279/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1000 - val_loss: 0.1285\n",
            "Epoch 280/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0857 - val_loss: 0.1329\n",
            "Epoch 281/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0766 - val_loss: 0.1217\n",
            "Epoch 282/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0935 - val_loss: 0.1075\n",
            "Epoch 283/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0974 - val_loss: 0.1234\n",
            "Epoch 284/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0708 - val_loss: 0.1242\n",
            "Epoch 285/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0816 - val_loss: 0.1111\n",
            "Epoch 286/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0702 - val_loss: 0.1150\n",
            "Epoch 287/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0853 - val_loss: 0.1066\n",
            "Epoch 288/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0814 - val_loss: 0.1270\n",
            "Epoch 289/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0891 - val_loss: 0.1099\n",
            "Epoch 290/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0733 - val_loss: 0.1153\n",
            "Epoch 291/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0492 - val_loss: 0.1029\n",
            "Epoch 292/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0725 - val_loss: 0.1204\n",
            "Epoch 293/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0701 - val_loss: 0.1460\n",
            "Epoch 294/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0799 - val_loss: 0.1057\n",
            "Epoch 295/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0751 - val_loss: 0.1089\n",
            "Epoch 296/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.1008 - val_loss: 0.1193\n",
            "Epoch 297/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0838 - val_loss: 0.1011\n",
            "Epoch 298/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0815 - val_loss: 0.1024\n",
            "Epoch 299/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0659 - val_loss: 0.1106\n",
            "Epoch 300/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0772 - val_loss: 0.1146\n",
            "Epoch 301/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0756 - val_loss: 0.1017\n",
            "Epoch 302/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0707 - val_loss: 0.1001\n",
            "Epoch 303/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0736 - val_loss: 0.0981\n",
            "Epoch 304/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0651 - val_loss: 0.0893\n",
            "Epoch 305/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0473 - val_loss: 0.0947\n",
            "Epoch 306/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0601 - val_loss: 0.0943\n",
            "Epoch 307/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0619 - val_loss: 0.0985\n",
            "Epoch 308/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0576 - val_loss: 0.0950\n",
            "Epoch 309/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0510 - val_loss: 0.0962\n",
            "Epoch 310/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0891 - val_loss: 0.1105\n",
            "Epoch 311/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0973 - val_loss: 0.1277\n",
            "Epoch 312/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0767 - val_loss: 0.1275\n",
            "Epoch 313/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0778 - val_loss: 0.1113\n",
            "Epoch 314/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0785 - val_loss: 0.1031\n",
            "Epoch 315/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0609 - val_loss: 0.1070\n",
            "Epoch 316/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0685 - val_loss: 0.1089\n",
            "Epoch 317/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0757 - val_loss: 0.1124\n",
            "Epoch 318/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0922 - val_loss: 0.1034\n",
            "Epoch 319/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0751 - val_loss: 0.1082\n",
            "Epoch 320/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0763 - val_loss: 0.0972\n",
            "Epoch 321/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0659 - val_loss: 0.1024\n",
            "Epoch 322/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0489 - val_loss: 0.0904\n",
            "Epoch 323/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0503 - val_loss: 0.0891\n",
            "Epoch 324/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0749 - val_loss: 0.1000\n",
            "Epoch 325/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0756 - val_loss: 0.1021\n",
            "Epoch 326/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0704 - val_loss: 0.0940\n",
            "Epoch 327/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0699 - val_loss: 0.0891\n",
            "Epoch 328/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0642 - val_loss: 0.1043\n",
            "Epoch 329/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0560 - val_loss: 0.0971\n",
            "Epoch 330/500\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0602 - val_loss: 0.0928\n",
            "Epoch 331/500\n",
            "35/35 [==============================] - 1s 21ms/step - loss: 0.0512 - val_loss: 0.0983\n",
            "Epoch 332/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0595 - val_loss: 0.1080\n",
            "Epoch 333/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0614 - val_loss: 0.1036\n",
            "Epoch 334/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0754 - val_loss: 0.1135\n",
            "Epoch 335/500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0744 - val_loss: 0.1006\n",
            "Epoch 336/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0649 - val_loss: 0.1041\n",
            "Epoch 337/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0605 - val_loss: 0.0923\n",
            "Epoch 338/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0560 - val_loss: 0.0966\n",
            "Epoch 339/500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0706 - val_loss: 0.1035\n",
            "Epoch 340/500\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0707 - val_loss: 0.0992\n",
            "Epoch 341/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0559 - val_loss: 0.0981\n",
            "Epoch 342/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0660 - val_loss: 0.0989\n",
            "Epoch 343/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0829 - val_loss: 0.1164\n",
            "Epoch 344/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0725 - val_loss: 0.0987\n",
            "Epoch 345/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0585 - val_loss: 0.1064\n",
            "Epoch 346/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0684 - val_loss: 0.0920\n",
            "Epoch 347/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0481 - val_loss: 0.1023\n",
            "Epoch 348/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0613 - val_loss: 0.0893\n",
            "Epoch 349/500\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0738 - val_loss: 0.1011\n",
            "Epoch 350/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0724 - val_loss: 0.1109\n",
            "Epoch 351/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0843 - val_loss: 0.1323\n",
            "Epoch 352/500\n",
            "35/35 [==============================] - 1s 21ms/step - loss: 0.0903 - val_loss: 0.1205\n",
            "Epoch 353/500\n",
            "35/35 [==============================] - 1s 20ms/step - loss: 0.0897 - val_loss: 0.1093\n",
            "Epoch 354/500\n",
            "35/35 [==============================] - 1s 19ms/step - loss: 0.0739 - val_loss: 0.1123\n",
            "Epoch 355/500\n",
            "35/35 [==============================] - 1s 26ms/step - loss: 0.0632 - val_loss: 0.0997\n",
            "Epoch 356/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0756 - val_loss: 0.1126\n",
            "Epoch 357/500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0810 - val_loss: 0.1013\n",
            "Epoch 358/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0531 - val_loss: 0.0903\n",
            "Epoch 359/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0769 - val_loss: 0.0997\n",
            "Epoch 360/500\n",
            "35/35 [==============================] - 1s 21ms/step - loss: 0.0718 - val_loss: 0.0937\n",
            "Epoch 361/500\n",
            "35/35 [==============================] - 1s 22ms/step - loss: 0.0636 - val_loss: 0.0961\n",
            "Epoch 362/500\n",
            "35/35 [==============================] - 1s 23ms/step - loss: 0.0588 - val_loss: 0.0868\n",
            "Epoch 363/500\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0534 - val_loss: 0.0911\n",
            "Epoch 364/500\n",
            "35/35 [==============================] - 1s 24ms/step - loss: 0.0528 - val_loss: 0.1056\n",
            "Epoch 365/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0594 - val_loss: 0.1320\n",
            "Epoch 366/500\n",
            "35/35 [==============================] - 1s 21ms/step - loss: 0.0688 - val_loss: 0.1018\n",
            "Epoch 367/500\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0507 - val_loss: 0.1063\n",
            "Epoch 368/500\n",
            "35/35 [==============================] - 1s 20ms/step - loss: 0.0629 - val_loss: 0.1065\n",
            "Epoch 369/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0694 - val_loss: 0.1003\n",
            "Epoch 370/500\n",
            "35/35 [==============================] - 1s 25ms/step - loss: 0.0595 - val_loss: 0.1040\n",
            "Epoch 371/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0598 - val_loss: 0.1050\n",
            "Epoch 372/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0726 - val_loss: 0.1010\n",
            "Epoch 373/500\n",
            "35/35 [==============================] - 1s 21ms/step - loss: 0.0714 - val_loss: 0.1083\n",
            "Epoch 374/500\n",
            "35/35 [==============================] - 1s 20ms/step - loss: 0.0519 - val_loss: 0.0968\n",
            "Epoch 375/500\n",
            "35/35 [==============================] - 1s 21ms/step - loss: 0.0577 - val_loss: 0.0979\n",
            "Epoch 376/500\n",
            "35/35 [==============================] - 1s 20ms/step - loss: 0.0601 - val_loss: 0.1057\n",
            "Epoch 377/500\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 0.0616 - val_loss: 0.0954\n",
            "Epoch 378/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0585 - val_loss: 0.1038\n",
            "Epoch 379/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0525 - val_loss: 0.1005\n",
            "Epoch 380/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0612 - val_loss: 0.0862\n",
            "Epoch 381/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0599 - val_loss: 0.0875\n",
            "Epoch 382/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0557 - val_loss: 0.0882\n",
            "Epoch 383/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0741 - val_loss: 0.1151\n",
            "Epoch 384/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0765 - val_loss: 0.1083\n",
            "Epoch 385/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0581 - val_loss: 0.1057\n",
            "Epoch 386/500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0751 - val_loss: 0.1002\n",
            "Epoch 387/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0686 - val_loss: 0.1019\n",
            "Epoch 388/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0627 - val_loss: 0.1025\n",
            "Epoch 389/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0777 - val_loss: 0.0866\n",
            "Epoch 390/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0491 - val_loss: 0.0868\n",
            "Epoch 391/500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0548 - val_loss: 0.0957\n",
            "Epoch 392/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0577 - val_loss: 0.0985\n",
            "Epoch 393/500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0588 - val_loss: 0.0900\n",
            "Epoch 394/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0694 - val_loss: 0.0879\n",
            "Epoch 395/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0549 - val_loss: 0.0919\n",
            "Epoch 396/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0524 - val_loss: 0.0822\n",
            "Epoch 397/500\n",
            "35/35 [==============================] - 1s 17ms/step - loss: 0.0576 - val_loss: 0.0960\n",
            "Epoch 398/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0513 - val_loss: 0.0936\n",
            "Epoch 399/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0670 - val_loss: 0.0970\n",
            "Epoch 400/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0621 - val_loss: 0.1032\n",
            "Epoch 401/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0703 - val_loss: 0.0950\n",
            "Epoch 402/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0591 - val_loss: 0.0968\n",
            "Epoch 403/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0515 - val_loss: 0.1014\n",
            "Epoch 404/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0596 - val_loss: 0.1026\n",
            "Epoch 405/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0595 - val_loss: 0.1056\n",
            "Epoch 406/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0513 - val_loss: 0.0947\n",
            "Epoch 407/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0584 - val_loss: 0.0925\n",
            "Epoch 408/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0642 - val_loss: 0.0937\n",
            "Epoch 409/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0425 - val_loss: 0.0854\n",
            "Epoch 410/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0510 - val_loss: 0.1054\n",
            "Epoch 411/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0657 - val_loss: 0.1115\n",
            "Epoch 412/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0642 - val_loss: 0.0910\n",
            "Epoch 413/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0545 - val_loss: 0.0892\n",
            "Epoch 414/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0592 - val_loss: 0.1038\n",
            "Epoch 415/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0625 - val_loss: 0.1002\n",
            "Epoch 416/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0527 - val_loss: 0.0968\n",
            "Epoch 417/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0479 - val_loss: 0.0843\n",
            "Epoch 418/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0481 - val_loss: 0.0883\n",
            "Epoch 419/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0496 - val_loss: 0.0843\n",
            "Epoch 420/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0493 - val_loss: 0.0905\n",
            "Epoch 421/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0540 - val_loss: 0.1060\n",
            "Epoch 422/500\n",
            "35/35 [==============================] - 0s 10ms/step - loss: 0.0722 - val_loss: 0.1100\n",
            "Epoch 423/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0646 - val_loss: 0.0922\n",
            "Epoch 424/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0575 - val_loss: 0.0977\n",
            "Epoch 425/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0523 - val_loss: 0.1016\n",
            "Epoch 426/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0595 - val_loss: 0.0990\n",
            "Epoch 427/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0552 - val_loss: 0.0846\n",
            "Epoch 428/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0629 - val_loss: 0.1018\n",
            "Epoch 429/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0650 - val_loss: 0.1057\n",
            "Epoch 430/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0732 - val_loss: 0.1033\n",
            "Epoch 431/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0672 - val_loss: 0.0929\n",
            "Epoch 432/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0573 - val_loss: 0.1052\n",
            "Epoch 433/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0565 - val_loss: 0.1132\n",
            "Epoch 434/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0678 - val_loss: 0.0966\n",
            "Epoch 435/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0604 - val_loss: 0.0947\n",
            "Epoch 436/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0534 - val_loss: 0.1039\n",
            "Epoch 437/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0664 - val_loss: 0.0910\n",
            "Epoch 438/500\n",
            "35/35 [==============================] - 0s 10ms/step - loss: 0.0514 - val_loss: 0.0980\n",
            "Epoch 439/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0659 - val_loss: 0.0934\n",
            "Epoch 440/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0577 - val_loss: 0.0867\n",
            "Epoch 441/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0716 - val_loss: 0.0896\n",
            "Epoch 442/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0753 - val_loss: 0.1112\n",
            "Epoch 443/500\n",
            "35/35 [==============================] - 0s 10ms/step - loss: 0.0734 - val_loss: 0.0920\n",
            "Epoch 444/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0869 - val_loss: 0.1115\n",
            "Epoch 445/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.1012 - val_loss: 0.1038\n",
            "Epoch 446/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0770 - val_loss: 0.0905\n",
            "Epoch 447/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0653 - val_loss: 0.0941\n",
            "Epoch 448/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0667 - val_loss: 0.1031\n",
            "Epoch 449/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0828 - val_loss: 0.1087\n",
            "Epoch 450/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0667 - val_loss: 0.0867\n",
            "Epoch 451/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0531 - val_loss: 0.0939\n",
            "Epoch 452/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0749 - val_loss: 0.0991\n",
            "Epoch 453/500\n",
            "35/35 [==============================] - 1s 14ms/step - loss: 0.0589 - val_loss: 0.0884\n",
            "Epoch 454/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0526 - val_loss: 0.0918\n",
            "Epoch 455/500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0542 - val_loss: 0.0845\n",
            "Epoch 456/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0449 - val_loss: 0.0936\n",
            "Epoch 457/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0529 - val_loss: 0.0851\n",
            "Epoch 458/500\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0475 - val_loss: 0.0832\n",
            "Epoch 459/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0520 - val_loss: 0.0941\n",
            "Epoch 460/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0507 - val_loss: 0.0841\n",
            "Epoch 461/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0577 - val_loss: 0.0869\n",
            "Epoch 462/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0440 - val_loss: 0.0770\n",
            "Epoch 463/500\n",
            "35/35 [==============================] - 0s 10ms/step - loss: 0.0471 - val_loss: 0.0928\n",
            "Epoch 464/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0428 - val_loss: 0.0971\n",
            "Epoch 465/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0541 - val_loss: 0.0922\n",
            "Epoch 466/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0557 - val_loss: 0.0886\n",
            "Epoch 467/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0446 - val_loss: 0.0876\n",
            "Epoch 468/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0537 - val_loss: 0.0812\n",
            "Epoch 469/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0477 - val_loss: 0.0854\n",
            "Epoch 470/500\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.0418 - val_loss: 0.0786\n",
            "Epoch 471/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0559 - val_loss: 0.0865\n",
            "Epoch 472/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0501 - val_loss: 0.0940\n",
            "Epoch 473/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0848\n",
            "Epoch 474/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0415 - val_loss: 0.0787\n",
            "Epoch 475/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0825 - val_loss: 0.0872\n",
            "Epoch 476/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0486 - val_loss: 0.0867\n",
            "Epoch 477/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0472 - val_loss: 0.0833\n",
            "Epoch 478/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0485 - val_loss: 0.0820\n",
            "Epoch 479/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0929\n",
            "Epoch 480/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0877\n",
            "Epoch 481/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0471 - val_loss: 0.0916\n",
            "Epoch 482/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0498 - val_loss: 0.0835\n",
            "Epoch 483/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0536 - val_loss: 0.0872\n",
            "Epoch 484/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0570 - val_loss: 0.1007\n",
            "Epoch 485/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0692 - val_loss: 0.0895\n",
            "Epoch 486/500\n",
            "35/35 [==============================] - 0s 11ms/step - loss: 0.0510 - val_loss: 0.0894\n",
            "Epoch 487/500\n",
            "35/35 [==============================] - 0s 12ms/step - loss: 0.0567 - val_loss: 0.0790\n",
            "Epoch 488/500\n",
            "35/35 [==============================] - 0s 13ms/step - loss: 0.0520 - val_loss: 0.0810\n",
            "Epoch 489/500\n",
            "35/35 [==============================] - 1s 15ms/step - loss: 0.0501 - val_loss: 0.0702\n",
            "Epoch 490/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0430 - val_loss: 0.0815\n",
            "Epoch 491/500\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0487 - val_loss: 0.0886\n",
            "Epoch 492/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0449 - val_loss: 0.0793\n",
            "Epoch 493/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0510 - val_loss: 0.0875\n",
            "Epoch 494/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0467 - val_loss: 0.0886\n",
            "Epoch 495/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0476 - val_loss: 0.0811\n",
            "Epoch 496/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0662 - val_loss: 0.0873\n",
            "Epoch 497/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0573 - val_loss: 0.0964\n",
            "Epoch 498/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0564 - val_loss: 0.0837\n",
            "Epoch 499/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0505 - val_loss: 0.0809\n",
            "Epoch 500/500\n",
            "35/35 [==============================] - 0s 9ms/step - loss: 0.0522 - val_loss: 0.0910\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0910\n",
            "25/25 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbfd4fa4550>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number set as 256\n",
        "nb_epochs=500;    # training epochs change to 1000\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "# Randomly choose indices for training and validation\n",
        "train_indices = random.sample(range(200), k=160)\n",
        "val_indices = [i for i in range(200) if i not in train_indices]\n",
        "\n",
        "# Construct the training set\n",
        "train_data = pd.concat([condition1.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition2.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition3.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition4.iloc[train_indices[0]:train_indices[-1]+1, :]])\n",
        "\n",
        "# Construct the validation set\n",
        "val_data = pd.concat([condition1.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition2.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition3.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition4.iloc[val_indices[0]:val_indices[-1]+1, :]])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = np.asarray(train_data).astype(np.float32)\n",
        "val_set = np.asarray(val_data).astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-4))    # Learning rate changes 1e-5 to 1e-4\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "4s2eIDL2Npp0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWLs2EQZi7OW"
      },
      "source": [
        "##3.2 Train on data under 1 light and test on 1 light condition ##\n",
        "Randomly choose one lighting condition for testing and the 1 condition for training.\n",
        "\n",
        "loss: 0.0216 - val_loss: 0.6389  \n",
        "Significantly higher than naive prediction(0.11)"
      ],
      "id": "oWLs2EQZi7OW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fDPx2kPSow29",
        "outputId": "dfd636dc-7c53-4c07-a2fa-90771d66634c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "25/25 [==============================] - 4s 17ms/step - loss: 2.1460 - val_loss: 1.9866\n",
            "Epoch 2/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 1.9185 - val_loss: 1.8915\n",
            "Epoch 3/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 1.6983 - val_loss: 1.7905\n",
            "Epoch 4/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 1.5107 - val_loss: 1.6821\n",
            "Epoch 5/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 1.3406 - val_loss: 1.5765\n",
            "Epoch 6/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 1.1806 - val_loss: 1.4711\n",
            "Epoch 7/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 1.0238 - val_loss: 1.3734\n",
            "Epoch 8/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.9355 - val_loss: 1.2910\n",
            "Epoch 9/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.8394 - val_loss: 1.2137\n",
            "Epoch 10/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.7574 - val_loss: 1.1519\n",
            "Epoch 11/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.7135 - val_loss: 1.0960\n",
            "Epoch 12/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.6831 - val_loss: 1.0540\n",
            "Epoch 13/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6547 - val_loss: 1.0223\n",
            "Epoch 14/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6314 - val_loss: 0.9868\n",
            "Epoch 15/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5907 - val_loss: 0.9663\n",
            "Epoch 16/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5702 - val_loss: 0.9433\n",
            "Epoch 17/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5462 - val_loss: 0.9289\n",
            "Epoch 18/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5276 - val_loss: 0.8969\n",
            "Epoch 19/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5332 - val_loss: 0.8836\n",
            "Epoch 20/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5156 - val_loss: 0.8686\n",
            "Epoch 21/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.5181 - val_loss: 0.8562\n",
            "Epoch 22/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4819 - val_loss: 0.8339\n",
            "Epoch 23/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.4979 - val_loss: 0.8171\n",
            "Epoch 24/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.4516 - val_loss: 0.8064\n",
            "Epoch 25/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.4495 - val_loss: 0.7886\n",
            "Epoch 26/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4510 - val_loss: 0.7738\n",
            "Epoch 27/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4494 - val_loss: 0.7549\n",
            "Epoch 28/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.4330 - val_loss: 0.7449\n",
            "Epoch 29/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4235 - val_loss: 0.7302\n",
            "Epoch 30/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4022 - val_loss: 0.7155\n",
            "Epoch 31/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4001 - val_loss: 0.7107\n",
            "Epoch 32/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4049 - val_loss: 0.6929\n",
            "Epoch 33/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4010 - val_loss: 0.6896\n",
            "Epoch 34/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3913 - val_loss: 0.6691\n",
            "Epoch 35/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3687 - val_loss: 0.6777\n",
            "Epoch 36/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3816 - val_loss: 0.6561\n",
            "Epoch 37/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3697 - val_loss: 0.6456\n",
            "Epoch 38/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3570 - val_loss: 0.6365\n",
            "Epoch 39/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3571 - val_loss: 0.6279\n",
            "Epoch 40/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3590 - val_loss: 0.6203\n",
            "Epoch 41/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3579 - val_loss: 0.6104\n",
            "Epoch 42/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3751 - val_loss: 0.6028\n",
            "Epoch 43/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3523 - val_loss: 0.5989\n",
            "Epoch 44/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3361 - val_loss: 0.5918\n",
            "Epoch 45/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3527 - val_loss: 0.5832\n",
            "Epoch 46/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3339 - val_loss: 0.5773\n",
            "Epoch 47/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.3408 - val_loss: 0.5749\n",
            "Epoch 48/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.3201 - val_loss: 0.5732\n",
            "Epoch 49/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.3258 - val_loss: 0.5689\n",
            "Epoch 50/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3277 - val_loss: 0.5669\n",
            "Epoch 51/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.3561 - val_loss: 0.5619\n",
            "Epoch 52/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.3586 - val_loss: 0.5554\n",
            "Epoch 53/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.3311 - val_loss: 0.5494\n",
            "Epoch 54/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3302 - val_loss: 0.5465\n",
            "Epoch 55/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3531 - val_loss: 0.5468\n",
            "Epoch 56/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3288 - val_loss: 0.5394\n",
            "Epoch 57/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3425 - val_loss: 0.5373\n",
            "Epoch 58/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3427 - val_loss: 0.5391\n",
            "Epoch 59/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3326 - val_loss: 0.5306\n",
            "Epoch 60/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3135 - val_loss: 0.5276\n",
            "Epoch 61/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3284 - val_loss: 0.5284\n",
            "Epoch 62/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3412 - val_loss: 0.5263\n",
            "Epoch 63/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3111 - val_loss: 0.5236\n",
            "Epoch 64/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3211 - val_loss: 0.5197\n",
            "Epoch 65/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3214 - val_loss: 0.5196\n",
            "Epoch 66/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3261 - val_loss: 0.5183\n",
            "Epoch 67/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3124 - val_loss: 0.5192\n",
            "Epoch 68/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3140 - val_loss: 0.5136\n",
            "Epoch 69/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3266 - val_loss: 0.5127\n",
            "Epoch 70/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3077 - val_loss: 0.5097\n",
            "Epoch 71/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3318 - val_loss: 0.5132\n",
            "Epoch 72/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3127 - val_loss: 0.5122\n",
            "Epoch 73/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3115 - val_loss: 0.5123\n",
            "Epoch 74/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3031 - val_loss: 0.5082\n",
            "Epoch 75/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3303 - val_loss: 0.5147\n",
            "Epoch 76/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3149 - val_loss: 0.5100\n",
            "Epoch 77/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3250 - val_loss: 0.5084\n",
            "Epoch 78/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3044 - val_loss: 0.5108\n",
            "Epoch 79/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3167 - val_loss: 0.5065\n",
            "Epoch 80/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3021 - val_loss: 0.5049\n",
            "Epoch 81/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3232 - val_loss: 0.5072\n",
            "Epoch 82/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2895 - val_loss: 0.5098\n",
            "Epoch 83/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3151 - val_loss: 0.5094\n",
            "Epoch 84/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2922 - val_loss: 0.5094\n",
            "Epoch 85/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.3309 - val_loss: 0.5085\n",
            "Epoch 86/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.3199 - val_loss: 0.5015\n",
            "Epoch 87/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3136 - val_loss: 0.5003\n",
            "Epoch 88/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3010 - val_loss: 0.5020\n",
            "Epoch 89/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3079 - val_loss: 0.5006\n",
            "Epoch 90/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3136 - val_loss: 0.4979\n",
            "Epoch 91/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2853 - val_loss: 0.4960\n",
            "Epoch 92/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2947 - val_loss: 0.4979\n",
            "Epoch 93/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2886 - val_loss: 0.4955\n",
            "Epoch 94/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2966 - val_loss: 0.4930\n",
            "Epoch 95/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2950 - val_loss: 0.4931\n",
            "Epoch 96/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3067 - val_loss: 0.4952\n",
            "Epoch 97/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.3060 - val_loss: 0.4944\n",
            "Epoch 98/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2959 - val_loss: 0.4914\n",
            "Epoch 99/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2923 - val_loss: 0.4900\n",
            "Epoch 100/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3034 - val_loss: 0.4921\n",
            "Epoch 101/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2910 - val_loss: 0.4881\n",
            "Epoch 102/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2976 - val_loss: 0.4913\n",
            "Epoch 103/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2817 - val_loss: 0.4903\n",
            "Epoch 104/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2980 - val_loss: 0.4885\n",
            "Epoch 105/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2961 - val_loss: 0.4898\n",
            "Epoch 106/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3036 - val_loss: 0.4919\n",
            "Epoch 107/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2995 - val_loss: 0.4840\n",
            "Epoch 108/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3134 - val_loss: 0.4816\n",
            "Epoch 109/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3182 - val_loss: 0.4834\n",
            "Epoch 110/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2760 - val_loss: 0.4817\n",
            "Epoch 111/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2906 - val_loss: 0.4865\n",
            "Epoch 112/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2958 - val_loss: 0.4827\n",
            "Epoch 113/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3013 - val_loss: 0.4824\n",
            "Epoch 114/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3024 - val_loss: 0.4784\n",
            "Epoch 115/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2974 - val_loss: 0.4831\n",
            "Epoch 116/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3084 - val_loss: 0.4838\n",
            "Epoch 117/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2972 - val_loss: 0.4774\n",
            "Epoch 118/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2954 - val_loss: 0.4751\n",
            "Epoch 119/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2951 - val_loss: 0.4748\n",
            "Epoch 120/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2597 - val_loss: 0.4769\n",
            "Epoch 121/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3240 - val_loss: 0.4732\n",
            "Epoch 122/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2986 - val_loss: 0.4759\n",
            "Epoch 123/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3383 - val_loss: 0.4754\n",
            "Epoch 124/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3008 - val_loss: 0.4723\n",
            "Epoch 125/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2913 - val_loss: 0.4699\n",
            "Epoch 126/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2956 - val_loss: 0.4723\n",
            "Epoch 127/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2912 - val_loss: 0.4728\n",
            "Epoch 128/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2940 - val_loss: 0.4741\n",
            "Epoch 129/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2847 - val_loss: 0.4730\n",
            "Epoch 130/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2801 - val_loss: 0.4750\n",
            "Epoch 131/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3000 - val_loss: 0.4745\n",
            "Epoch 132/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2841 - val_loss: 0.4721\n",
            "Epoch 133/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2916 - val_loss: 0.4724\n",
            "Epoch 134/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2803 - val_loss: 0.4731\n",
            "Epoch 135/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3066 - val_loss: 0.4638\n",
            "Epoch 136/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2700 - val_loss: 0.4594\n",
            "Epoch 137/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2971 - val_loss: 0.4631\n",
            "Epoch 138/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2950 - val_loss: 0.4654\n",
            "Epoch 139/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2694 - val_loss: 0.4619\n",
            "Epoch 140/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2749 - val_loss: 0.4600\n",
            "Epoch 141/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2757 - val_loss: 0.4659\n",
            "Epoch 142/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2842 - val_loss: 0.4711\n",
            "Epoch 143/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2845 - val_loss: 0.4701\n",
            "Epoch 144/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2915 - val_loss: 0.4692\n",
            "Epoch 145/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2880 - val_loss: 0.4683\n",
            "Epoch 146/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3025 - val_loss: 0.4689\n",
            "Epoch 147/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2748 - val_loss: 0.4771\n",
            "Epoch 148/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2963 - val_loss: 0.4716\n",
            "Epoch 149/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2848 - val_loss: 0.4641\n",
            "Epoch 150/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2575 - val_loss: 0.4672\n",
            "Epoch 151/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2720 - val_loss: 0.4652\n",
            "Epoch 152/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2625 - val_loss: 0.4536\n",
            "Epoch 153/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2877 - val_loss: 0.4611\n",
            "Epoch 154/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2769 - val_loss: 0.4644\n",
            "Epoch 155/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2808 - val_loss: 0.4602\n",
            "Epoch 156/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3180 - val_loss: 0.4625\n",
            "Epoch 157/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2934 - val_loss: 0.4638\n",
            "Epoch 158/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2784 - val_loss: 0.4505\n",
            "Epoch 159/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2796 - val_loss: 0.4509\n",
            "Epoch 160/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2853 - val_loss: 0.4513\n",
            "Epoch 161/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2875 - val_loss: 0.4566\n",
            "Epoch 162/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2982 - val_loss: 0.4567\n",
            "Epoch 163/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2907 - val_loss: 0.4572\n",
            "Epoch 164/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2754 - val_loss: 0.4577\n",
            "Epoch 165/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2716 - val_loss: 0.4565\n",
            "Epoch 166/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2671 - val_loss: 0.4572\n",
            "Epoch 167/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2803 - val_loss: 0.4553\n",
            "Epoch 168/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2908 - val_loss: 0.4532\n",
            "Epoch 169/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2747 - val_loss: 0.4518\n",
            "Epoch 170/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2812 - val_loss: 0.4438\n",
            "Epoch 171/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2767 - val_loss: 0.4446\n",
            "Epoch 172/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2944 - val_loss: 0.4480\n",
            "Epoch 173/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2859 - val_loss: 0.4450\n",
            "Epoch 174/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2767 - val_loss: 0.4513\n",
            "Epoch 175/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2719 - val_loss: 0.4545\n",
            "Epoch 176/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2769 - val_loss: 0.4466\n",
            "Epoch 177/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2924 - val_loss: 0.4481\n",
            "Epoch 178/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2850 - val_loss: 0.4474\n",
            "Epoch 179/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2663 - val_loss: 0.4505\n",
            "Epoch 180/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2725 - val_loss: 0.4490\n",
            "Epoch 181/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2789 - val_loss: 0.4487\n",
            "Epoch 182/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2673 - val_loss: 0.4498\n",
            "Epoch 183/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2714 - val_loss: 0.4495\n",
            "Epoch 184/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2627 - val_loss: 0.4466\n",
            "Epoch 185/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2865 - val_loss: 0.4426\n",
            "Epoch 186/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2806 - val_loss: 0.4397\n",
            "Epoch 187/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2979 - val_loss: 0.4412\n",
            "Epoch 188/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2777 - val_loss: 0.4395\n",
            "Epoch 189/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2695 - val_loss: 0.4389\n",
            "Epoch 190/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2786 - val_loss: 0.4390\n",
            "Epoch 191/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2767 - val_loss: 0.4388\n",
            "Epoch 192/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2662 - val_loss: 0.4404\n",
            "Epoch 193/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2687 - val_loss: 0.4481\n",
            "Epoch 194/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2643 - val_loss: 0.4449\n",
            "Epoch 195/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2760 - val_loss: 0.4483\n",
            "Epoch 196/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2767 - val_loss: 0.4501\n",
            "Epoch 197/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2508 - val_loss: 0.4435\n",
            "Epoch 198/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2623 - val_loss: 0.4449\n",
            "Epoch 199/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2697 - val_loss: 0.4395\n",
            "Epoch 200/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2633 - val_loss: 0.4323\n",
            "Epoch 201/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2689 - val_loss: 0.4352\n",
            "Epoch 202/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2765 - val_loss: 0.4338\n",
            "Epoch 203/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2745 - val_loss: 0.4288\n",
            "Epoch 204/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2580 - val_loss: 0.4284\n",
            "Epoch 205/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2645 - val_loss: 0.4310\n",
            "Epoch 206/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2496 - val_loss: 0.4323\n",
            "Epoch 207/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2619 - val_loss: 0.4301\n",
            "Epoch 208/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2755 - val_loss: 0.4293\n",
            "Epoch 209/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2709 - val_loss: 0.4314\n",
            "Epoch 210/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2526 - val_loss: 0.4285\n",
            "Epoch 211/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2596 - val_loss: 0.4296\n",
            "Epoch 212/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2641 - val_loss: 0.4317\n",
            "Epoch 213/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2644 - val_loss: 0.4347\n",
            "Epoch 214/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2621 - val_loss: 0.4366\n",
            "Epoch 215/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2573 - val_loss: 0.4335\n",
            "Epoch 216/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2665 - val_loss: 0.4316\n",
            "Epoch 217/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2488 - val_loss: 0.4328\n",
            "Epoch 218/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2711 - val_loss: 0.4351\n",
            "Epoch 219/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2569 - val_loss: 0.4340\n",
            "Epoch 220/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2469 - val_loss: 0.4321\n",
            "Epoch 221/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2463 - val_loss: 0.4334\n",
            "Epoch 222/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2699 - val_loss: 0.4366\n",
            "Epoch 223/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2565 - val_loss: 0.4347\n",
            "Epoch 224/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2540 - val_loss: 0.4353\n",
            "Epoch 225/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2443 - val_loss: 0.4286\n",
            "Epoch 226/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2602 - val_loss: 0.4265\n",
            "Epoch 227/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2605 - val_loss: 0.4284\n",
            "Epoch 228/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2522 - val_loss: 0.4282\n",
            "Epoch 229/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2375 - val_loss: 0.4257\n",
            "Epoch 230/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2535 - val_loss: 0.4288\n",
            "Epoch 231/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2468 - val_loss: 0.4278\n",
            "Epoch 232/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2410 - val_loss: 0.4256\n",
            "Epoch 233/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2596 - val_loss: 0.4233\n",
            "Epoch 234/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2585 - val_loss: 0.4296\n",
            "Epoch 235/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2454 - val_loss: 0.4337\n",
            "Epoch 236/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2687 - val_loss: 0.4273\n",
            "Epoch 237/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2590 - val_loss: 0.4262\n",
            "Epoch 238/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2678 - val_loss: 0.4274\n",
            "Epoch 239/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2370 - val_loss: 0.4233\n",
            "Epoch 240/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2609 - val_loss: 0.4208\n",
            "Epoch 241/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2409 - val_loss: 0.4297\n",
            "Epoch 242/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2717 - val_loss: 0.4276\n",
            "Epoch 243/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2562 - val_loss: 0.4231\n",
            "Epoch 244/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2565 - val_loss: 0.4224\n",
            "Epoch 245/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2481 - val_loss: 0.4227\n",
            "Epoch 246/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2569 - val_loss: 0.4200\n",
            "Epoch 247/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2484 - val_loss: 0.4165\n",
            "Epoch 248/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2553 - val_loss: 0.4162\n",
            "Epoch 249/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2386 - val_loss: 0.4166\n",
            "Epoch 250/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2365 - val_loss: 0.4222\n",
            "Epoch 251/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2385 - val_loss: 0.4236\n",
            "Epoch 252/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2448 - val_loss: 0.4246\n",
            "Epoch 253/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2413 - val_loss: 0.4259\n",
            "Epoch 254/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2386 - val_loss: 0.4297\n",
            "Epoch 255/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2356 - val_loss: 0.4324\n",
            "Epoch 256/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2585 - val_loss: 0.4286\n",
            "Epoch 257/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2371 - val_loss: 0.4241\n",
            "Epoch 258/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2511 - val_loss: 0.4262\n",
            "Epoch 259/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2464 - val_loss: 0.4245\n",
            "Epoch 260/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2477 - val_loss: 0.4222\n",
            "Epoch 261/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2415 - val_loss: 0.4327\n",
            "Epoch 262/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2426 - val_loss: 0.4333\n",
            "Epoch 263/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2271 - val_loss: 0.4273\n",
            "Epoch 264/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2510 - val_loss: 0.4257\n",
            "Epoch 265/1000\n",
            "25/25 [==============================] - 0s 16ms/step - loss: 0.2346 - val_loss: 0.4258\n",
            "Epoch 266/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2409 - val_loss: 0.4286\n",
            "Epoch 267/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2496 - val_loss: 0.4260\n",
            "Epoch 268/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2371 - val_loss: 0.4272\n",
            "Epoch 269/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2371 - val_loss: 0.4329\n",
            "Epoch 270/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2807 - val_loss: 0.4335\n",
            "Epoch 271/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2537 - val_loss: 0.4280\n",
            "Epoch 272/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2456 - val_loss: 0.4286\n",
            "Epoch 273/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2317 - val_loss: 0.4298\n",
            "Epoch 274/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2439 - val_loss: 0.4284\n",
            "Epoch 275/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2453 - val_loss: 0.4280\n",
            "Epoch 276/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2493 - val_loss: 0.4274\n",
            "Epoch 277/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2403 - val_loss: 0.4329\n",
            "Epoch 278/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2264 - val_loss: 0.4265\n",
            "Epoch 279/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2303 - val_loss: 0.4254\n",
            "Epoch 280/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2209 - val_loss: 0.4226\n",
            "Epoch 281/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2336 - val_loss: 0.4254\n",
            "Epoch 282/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2294 - val_loss: 0.4306\n",
            "Epoch 283/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2529 - val_loss: 0.4286\n",
            "Epoch 284/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2485 - val_loss: 0.4277\n",
            "Epoch 285/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2188 - val_loss: 0.4315\n",
            "Epoch 286/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2486 - val_loss: 0.4352\n",
            "Epoch 287/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2221 - val_loss: 0.4339\n",
            "Epoch 288/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2400 - val_loss: 0.4337\n",
            "Epoch 289/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2358 - val_loss: 0.4369\n",
            "Epoch 290/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2285 - val_loss: 0.4359\n",
            "Epoch 291/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2551 - val_loss: 0.4373\n",
            "Epoch 292/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2397 - val_loss: 0.4355\n",
            "Epoch 293/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2198 - val_loss: 0.4365\n",
            "Epoch 294/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2111 - val_loss: 0.4343\n",
            "Epoch 295/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2353 - val_loss: 0.4322\n",
            "Epoch 296/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2523 - val_loss: 0.4342\n",
            "Epoch 297/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2040 - val_loss: 0.4322\n",
            "Epoch 298/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2334 - val_loss: 0.4348\n",
            "Epoch 299/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2414 - val_loss: 0.4373\n",
            "Epoch 300/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2516 - val_loss: 0.4414\n",
            "Epoch 301/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2427 - val_loss: 0.4423\n",
            "Epoch 302/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2250 - val_loss: 0.4401\n",
            "Epoch 303/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2324 - val_loss: 0.4392\n",
            "Epoch 304/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2518 - val_loss: 0.4433\n",
            "Epoch 305/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2277 - val_loss: 0.4416\n",
            "Epoch 306/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2221 - val_loss: 0.4433\n",
            "Epoch 307/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2482 - val_loss: 0.4368\n",
            "Epoch 308/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2270 - val_loss: 0.4429\n",
            "Epoch 309/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2243 - val_loss: 0.4406\n",
            "Epoch 310/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2211 - val_loss: 0.4355\n",
            "Epoch 311/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2340 - val_loss: 0.4381\n",
            "Epoch 312/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2133 - val_loss: 0.4478\n",
            "Epoch 313/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2276 - val_loss: 0.4402\n",
            "Epoch 314/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2365 - val_loss: 0.4497\n",
            "Epoch 315/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2335 - val_loss: 0.4447\n",
            "Epoch 316/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2204 - val_loss: 0.4398\n",
            "Epoch 317/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2221 - val_loss: 0.4475\n",
            "Epoch 318/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2105 - val_loss: 0.4513\n",
            "Epoch 319/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2154 - val_loss: 0.4499\n",
            "Epoch 320/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2327 - val_loss: 0.4520\n",
            "Epoch 321/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2019 - val_loss: 0.4553\n",
            "Epoch 322/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2140 - val_loss: 0.4510\n",
            "Epoch 323/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2154 - val_loss: 0.4546\n",
            "Epoch 324/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2191 - val_loss: 0.4529\n",
            "Epoch 325/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2124 - val_loss: 0.4555\n",
            "Epoch 326/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2092 - val_loss: 0.4562\n",
            "Epoch 327/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2234 - val_loss: 0.4599\n",
            "Epoch 328/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2208 - val_loss: 0.4611\n",
            "Epoch 329/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2127 - val_loss: 0.4624\n",
            "Epoch 330/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2137 - val_loss: 0.4613\n",
            "Epoch 331/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2241 - val_loss: 0.4624\n",
            "Epoch 332/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2196 - val_loss: 0.4630\n",
            "Epoch 333/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2173 - val_loss: 0.4665\n",
            "Epoch 334/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2163 - val_loss: 0.4722\n",
            "Epoch 335/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2266 - val_loss: 0.4652\n",
            "Epoch 336/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2027 - val_loss: 0.4689\n",
            "Epoch 337/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2170 - val_loss: 0.4711\n",
            "Epoch 338/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2166 - val_loss: 0.4793\n",
            "Epoch 339/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2035 - val_loss: 0.4750\n",
            "Epoch 340/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2365 - val_loss: 0.4743\n",
            "Epoch 341/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2060 - val_loss: 0.4690\n",
            "Epoch 342/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2294 - val_loss: 0.4756\n",
            "Epoch 343/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2139 - val_loss: 0.4758\n",
            "Epoch 344/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2171 - val_loss: 0.4789\n",
            "Epoch 345/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2201 - val_loss: 0.4799\n",
            "Epoch 346/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2031 - val_loss: 0.4808\n",
            "Epoch 347/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1975 - val_loss: 0.4797\n",
            "Epoch 348/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2004 - val_loss: 0.4801\n",
            "Epoch 349/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2386 - val_loss: 0.4851\n",
            "Epoch 350/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2154 - val_loss: 0.4830\n",
            "Epoch 351/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2147 - val_loss: 0.4832\n",
            "Epoch 352/1000\n",
            "25/25 [==============================] - 0s 16ms/step - loss: 0.2122 - val_loss: 0.4831\n",
            "Epoch 353/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2046 - val_loss: 0.4895\n",
            "Epoch 354/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2011 - val_loss: 0.4893\n",
            "Epoch 355/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2253 - val_loss: 0.4876\n",
            "Epoch 356/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2013 - val_loss: 0.4810\n",
            "Epoch 357/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2249 - val_loss: 0.4891\n",
            "Epoch 358/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2041 - val_loss: 0.4905\n",
            "Epoch 359/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2178 - val_loss: 0.4902\n",
            "Epoch 360/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1893 - val_loss: 0.4944\n",
            "Epoch 361/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2098 - val_loss: 0.4993\n",
            "Epoch 362/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1910 - val_loss: 0.4968\n",
            "Epoch 363/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2049 - val_loss: 0.4969\n",
            "Epoch 364/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2209 - val_loss: 0.4984\n",
            "Epoch 365/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1972 - val_loss: 0.4961\n",
            "Epoch 366/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1973 - val_loss: 0.4980\n",
            "Epoch 367/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1879 - val_loss: 0.5110\n",
            "Epoch 368/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1980 - val_loss: 0.5079\n",
            "Epoch 369/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2162 - val_loss: 0.5058\n",
            "Epoch 370/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1896 - val_loss: 0.5127\n",
            "Epoch 371/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1931 - val_loss: 0.5154\n",
            "Epoch 372/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2258 - val_loss: 0.5092\n",
            "Epoch 373/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2109 - val_loss: 0.5048\n",
            "Epoch 374/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1978 - val_loss: 0.5047\n",
            "Epoch 375/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1880 - val_loss: 0.5042\n",
            "Epoch 376/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1955 - val_loss: 0.5104\n",
            "Epoch 377/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1976 - val_loss: 0.5205\n",
            "Epoch 378/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2224 - val_loss: 0.5195\n",
            "Epoch 379/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1802 - val_loss: 0.5181\n",
            "Epoch 380/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2124 - val_loss: 0.5238\n",
            "Epoch 381/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1947 - val_loss: 0.5267\n",
            "Epoch 382/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2098 - val_loss: 0.5256\n",
            "Epoch 383/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2188 - val_loss: 0.5277\n",
            "Epoch 384/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2193 - val_loss: 0.5234\n",
            "Epoch 385/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2014 - val_loss: 0.5294\n",
            "Epoch 386/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2174 - val_loss: 0.5272\n",
            "Epoch 387/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1782 - val_loss: 0.5283\n",
            "Epoch 388/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1928 - val_loss: 0.5397\n",
            "Epoch 389/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1796 - val_loss: 0.5422\n",
            "Epoch 390/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2104 - val_loss: 0.5351\n",
            "Epoch 391/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1957 - val_loss: 0.5373\n",
            "Epoch 392/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1991 - val_loss: 0.5368\n",
            "Epoch 393/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2064 - val_loss: 0.5395\n",
            "Epoch 394/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1931 - val_loss: 0.5421\n",
            "Epoch 395/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1957 - val_loss: 0.5428\n",
            "Epoch 396/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1880 - val_loss: 0.5538\n",
            "Epoch 397/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1775 - val_loss: 0.5478\n",
            "Epoch 398/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1923 - val_loss: 0.5471\n",
            "Epoch 399/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2306 - val_loss: 0.5510\n",
            "Epoch 400/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1885 - val_loss: 0.5539\n",
            "Epoch 401/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1901 - val_loss: 0.5608\n",
            "Epoch 402/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1849 - val_loss: 0.5555\n",
            "Epoch 403/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1768 - val_loss: 0.5559\n",
            "Epoch 404/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2108 - val_loss: 0.5577\n",
            "Epoch 405/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2088 - val_loss: 0.5546\n",
            "Epoch 406/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2091 - val_loss: 0.5633\n",
            "Epoch 407/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1835 - val_loss: 0.5663\n",
            "Epoch 408/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1921 - val_loss: 0.5652\n",
            "Epoch 409/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2016 - val_loss: 0.5737\n",
            "Epoch 410/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1878 - val_loss: 0.5733\n",
            "Epoch 411/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1678 - val_loss: 0.5742\n",
            "Epoch 412/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1973 - val_loss: 0.5668\n",
            "Epoch 413/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1999 - val_loss: 0.5606\n",
            "Epoch 414/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1899 - val_loss: 0.5592\n",
            "Epoch 415/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1683 - val_loss: 0.5585\n",
            "Epoch 416/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1894 - val_loss: 0.5592\n",
            "Epoch 417/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1947 - val_loss: 0.5638\n",
            "Epoch 418/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1823 - val_loss: 0.5644\n",
            "Epoch 419/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2036 - val_loss: 0.5684\n",
            "Epoch 420/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1832 - val_loss: 0.5692\n",
            "Epoch 421/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1942 - val_loss: 0.5654\n",
            "Epoch 422/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1702 - val_loss: 0.5662\n",
            "Epoch 423/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1772 - val_loss: 0.5638\n",
            "Epoch 424/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1741 - val_loss: 0.5601\n",
            "Epoch 425/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1967 - val_loss: 0.5599\n",
            "Epoch 426/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1790 - val_loss: 0.5678\n",
            "Epoch 427/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1783 - val_loss: 0.5703\n",
            "Epoch 428/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1798 - val_loss: 0.5720\n",
            "Epoch 429/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1934 - val_loss: 0.5661\n",
            "Epoch 430/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1909 - val_loss: 0.5639\n",
            "Epoch 431/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2008 - val_loss: 0.5663\n",
            "Epoch 432/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1839 - val_loss: 0.5718\n",
            "Epoch 433/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1995 - val_loss: 0.5760\n",
            "Epoch 434/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1892 - val_loss: 0.5832\n",
            "Epoch 435/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1787 - val_loss: 0.5943\n",
            "Epoch 436/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1730 - val_loss: 0.5828\n",
            "Epoch 437/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1816 - val_loss: 0.5841\n",
            "Epoch 438/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1813 - val_loss: 0.5817\n",
            "Epoch 439/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1968 - val_loss: 0.5937\n",
            "Epoch 440/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1781 - val_loss: 0.5974\n",
            "Epoch 441/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1854 - val_loss: 0.5919\n",
            "Epoch 442/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1860 - val_loss: 0.5886\n",
            "Epoch 443/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1786 - val_loss: 0.5950\n",
            "Epoch 444/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1747 - val_loss: 0.5894\n",
            "Epoch 445/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1741 - val_loss: 0.5858\n",
            "Epoch 446/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1911 - val_loss: 0.5829\n",
            "Epoch 447/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1880 - val_loss: 0.5908\n",
            "Epoch 448/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1707 - val_loss: 0.5925\n",
            "Epoch 449/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1725 - val_loss: 0.5950\n",
            "Epoch 450/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1775 - val_loss: 0.6051\n",
            "Epoch 451/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1694 - val_loss: 0.6093\n",
            "Epoch 452/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1812 - val_loss: 0.6137\n",
            "Epoch 453/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1804 - val_loss: 0.6163\n",
            "Epoch 454/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1654 - val_loss: 0.6156\n",
            "Epoch 455/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1833 - val_loss: 0.6159\n",
            "Epoch 456/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1961 - val_loss: 0.6204\n",
            "Epoch 457/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1818 - val_loss: 0.6129\n",
            "Epoch 458/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1605 - val_loss: 0.6144\n",
            "Epoch 459/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1830 - val_loss: 0.6062\n",
            "Epoch 460/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1654 - val_loss: 0.6089\n",
            "Epoch 461/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1805 - val_loss: 0.6063\n",
            "Epoch 462/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1855 - val_loss: 0.6061\n",
            "Epoch 463/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1814 - val_loss: 0.6023\n",
            "Epoch 464/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1760 - val_loss: 0.6055\n",
            "Epoch 465/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1956 - val_loss: 0.6022\n",
            "Epoch 466/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1775 - val_loss: 0.5976\n",
            "Epoch 467/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1698 - val_loss: 0.6000\n",
            "Epoch 468/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1862 - val_loss: 0.6111\n",
            "Epoch 469/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1744 - val_loss: 0.6115\n",
            "Epoch 470/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1569 - val_loss: 0.6069\n",
            "Epoch 471/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1789 - val_loss: 0.6033\n",
            "Epoch 472/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1727 - val_loss: 0.6079\n",
            "Epoch 473/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1603 - val_loss: 0.6144\n",
            "Epoch 474/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1714 - val_loss: 0.6169\n",
            "Epoch 475/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1811 - val_loss: 0.6231\n",
            "Epoch 476/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1553 - val_loss: 0.6152\n",
            "Epoch 477/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1842 - val_loss: 0.6117\n",
            "Epoch 478/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1648 - val_loss: 0.6174\n",
            "Epoch 479/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1694 - val_loss: 0.6213\n",
            "Epoch 480/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1856 - val_loss: 0.6213\n",
            "Epoch 481/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1681 - val_loss: 0.6196\n",
            "Epoch 482/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1657 - val_loss: 0.6211\n",
            "Epoch 483/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1705 - val_loss: 0.6220\n",
            "Epoch 484/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1696 - val_loss: 0.6095\n",
            "Epoch 485/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1667 - val_loss: 0.6120\n",
            "Epoch 486/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1760 - val_loss: 0.6140\n",
            "Epoch 487/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1766 - val_loss: 0.6159\n",
            "Epoch 488/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1519 - val_loss: 0.6224\n",
            "Epoch 489/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1607 - val_loss: 0.6251\n",
            "Epoch 490/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1514 - val_loss: 0.6225\n",
            "Epoch 491/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1943 - val_loss: 0.6148\n",
            "Epoch 492/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1585 - val_loss: 0.6247\n",
            "Epoch 493/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1612 - val_loss: 0.6207\n",
            "Epoch 494/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1570 - val_loss: 0.6173\n",
            "Epoch 495/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1689 - val_loss: 0.6176\n",
            "Epoch 496/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1741 - val_loss: 0.6204\n",
            "Epoch 497/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1881 - val_loss: 0.6219\n",
            "Epoch 498/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1550 - val_loss: 0.6103\n",
            "Epoch 499/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1506 - val_loss: 0.6126\n",
            "Epoch 500/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1555 - val_loss: 0.6234\n",
            "Epoch 501/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1861 - val_loss: 0.6271\n",
            "Epoch 502/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1586 - val_loss: 0.6252\n",
            "Epoch 503/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1647 - val_loss: 0.6255\n",
            "Epoch 504/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1539 - val_loss: 0.6305\n",
            "Epoch 505/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1847 - val_loss: 0.6288\n",
            "Epoch 506/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1439 - val_loss: 0.6300\n",
            "Epoch 507/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1644 - val_loss: 0.6180\n",
            "Epoch 508/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1548 - val_loss: 0.6144\n",
            "Epoch 509/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1590 - val_loss: 0.6205\n",
            "Epoch 510/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1823 - val_loss: 0.6322\n",
            "Epoch 511/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1516 - val_loss: 0.6399\n",
            "Epoch 512/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1740 - val_loss: 0.6382\n",
            "Epoch 513/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1651 - val_loss: 0.6399\n",
            "Epoch 514/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1648 - val_loss: 0.6441\n",
            "Epoch 515/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1579 - val_loss: 0.6455\n",
            "Epoch 516/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1641 - val_loss: 0.6374\n",
            "Epoch 517/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1702 - val_loss: 0.6370\n",
            "Epoch 518/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1627 - val_loss: 0.6308\n",
            "Epoch 519/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1442 - val_loss: 0.6321\n",
            "Epoch 520/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1652 - val_loss: 0.6325\n",
            "Epoch 521/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1684 - val_loss: 0.6321\n",
            "Epoch 522/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1800 - val_loss: 0.6318\n",
            "Epoch 523/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1807 - val_loss: 0.6417\n",
            "Epoch 524/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1669 - val_loss: 0.6309\n",
            "Epoch 525/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1583 - val_loss: 0.6364\n",
            "Epoch 526/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1585 - val_loss: 0.6266\n",
            "Epoch 527/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1540 - val_loss: 0.6230\n",
            "Epoch 528/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1705 - val_loss: 0.6300\n",
            "Epoch 529/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1680 - val_loss: 0.6290\n",
            "Epoch 530/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1636 - val_loss: 0.6328\n",
            "Epoch 531/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1568 - val_loss: 0.6412\n",
            "Epoch 532/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1546 - val_loss: 0.6460\n",
            "Epoch 533/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1639 - val_loss: 0.6460\n",
            "Epoch 534/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1750 - val_loss: 0.6428\n",
            "Epoch 535/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1565 - val_loss: 0.6490\n",
            "Epoch 536/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1506 - val_loss: 0.6448\n",
            "Epoch 537/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1566 - val_loss: 0.6446\n",
            "Epoch 538/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1611 - val_loss: 0.6421\n",
            "Epoch 539/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1632 - val_loss: 0.6354\n",
            "Epoch 540/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1549 - val_loss: 0.6335\n",
            "Epoch 541/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1457 - val_loss: 0.6420\n",
            "Epoch 542/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1615 - val_loss: 0.6392\n",
            "Epoch 543/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1506 - val_loss: 0.6325\n",
            "Epoch 544/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1467 - val_loss: 0.6414\n",
            "Epoch 545/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1656 - val_loss: 0.6354\n",
            "Epoch 546/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1695 - val_loss: 0.6344\n",
            "Epoch 547/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1598 - val_loss: 0.6313\n",
            "Epoch 548/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1515 - val_loss: 0.6294\n",
            "Epoch 549/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1780 - val_loss: 0.6155\n",
            "Epoch 550/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1567 - val_loss: 0.6235\n",
            "Epoch 551/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1754 - val_loss: 0.6320\n",
            "Epoch 552/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1614 - val_loss: 0.6303\n",
            "Epoch 553/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1601 - val_loss: 0.6375\n",
            "Epoch 554/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1420 - val_loss: 0.6323\n",
            "Epoch 555/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1408 - val_loss: 0.6282\n",
            "Epoch 556/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1549 - val_loss: 0.6262\n",
            "Epoch 557/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1419 - val_loss: 0.6325\n",
            "Epoch 558/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1647 - val_loss: 0.6252\n",
            "Epoch 559/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1794 - val_loss: 0.6257\n",
            "Epoch 560/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1503 - val_loss: 0.6220\n",
            "Epoch 561/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1334 - val_loss: 0.6231\n",
            "Epoch 562/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1717 - val_loss: 0.6255\n",
            "Epoch 563/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1645 - val_loss: 0.6286\n",
            "Epoch 564/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1696 - val_loss: 0.6296\n",
            "Epoch 565/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1610 - val_loss: 0.6193\n",
            "Epoch 566/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1542 - val_loss: 0.6285\n",
            "Epoch 567/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1458 - val_loss: 0.6341\n",
            "Epoch 568/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1470 - val_loss: 0.6400\n",
            "Epoch 569/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1620 - val_loss: 0.6418\n",
            "Epoch 570/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1453 - val_loss: 0.6452\n",
            "Epoch 571/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1545 - val_loss: 0.6392\n",
            "Epoch 572/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1662 - val_loss: 0.6386\n",
            "Epoch 573/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1436 - val_loss: 0.6373\n",
            "Epoch 574/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1501 - val_loss: 0.6321\n",
            "Epoch 575/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1500 - val_loss: 0.6370\n",
            "Epoch 576/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1517 - val_loss: 0.6511\n",
            "Epoch 577/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1541 - val_loss: 0.6436\n",
            "Epoch 578/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1571 - val_loss: 0.6353\n",
            "Epoch 579/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1625 - val_loss: 0.6474\n",
            "Epoch 580/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1642 - val_loss: 0.6494\n",
            "Epoch 581/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1630 - val_loss: 0.6397\n",
            "Epoch 582/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1398 - val_loss: 0.6386\n",
            "Epoch 583/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1364 - val_loss: 0.6496\n",
            "Epoch 584/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1567 - val_loss: 0.6340\n",
            "Epoch 585/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1434 - val_loss: 0.6258\n",
            "Epoch 586/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1479 - val_loss: 0.6307\n",
            "Epoch 587/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1604 - val_loss: 0.6366\n",
            "Epoch 588/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1471 - val_loss: 0.6262\n",
            "Epoch 589/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1661 - val_loss: 0.6330\n",
            "Epoch 590/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1476 - val_loss: 0.6334\n",
            "Epoch 591/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1549 - val_loss: 0.6224\n",
            "Epoch 592/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1472 - val_loss: 0.6276\n",
            "Epoch 593/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1476 - val_loss: 0.6328\n",
            "Epoch 594/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1502 - val_loss: 0.6389\n",
            "Epoch 595/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1611 - val_loss: 0.6383\n",
            "Epoch 596/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1615 - val_loss: 0.6396\n",
            "Epoch 597/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1557 - val_loss: 0.6381\n",
            "Epoch 598/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1538 - val_loss: 0.6251\n",
            "Epoch 599/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1790 - val_loss: 0.6201\n",
            "Epoch 600/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1351 - val_loss: 0.6237\n",
            "Epoch 601/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1373 - val_loss: 0.6283\n",
            "Epoch 602/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1594 - val_loss: 0.6435\n",
            "Epoch 603/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1519 - val_loss: 0.6460\n",
            "Epoch 604/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1301 - val_loss: 0.6442\n",
            "Epoch 605/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1483 - val_loss: 0.6436\n",
            "Epoch 606/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1462 - val_loss: 0.6393\n",
            "Epoch 607/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1577 - val_loss: 0.6382\n",
            "Epoch 608/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1700 - val_loss: 0.6287\n",
            "Epoch 609/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1523 - val_loss: 0.6250\n",
            "Epoch 610/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1492 - val_loss: 0.6269\n",
            "Epoch 611/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1678 - val_loss: 0.6226\n",
            "Epoch 612/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1364 - val_loss: 0.6269\n",
            "Epoch 613/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1758 - val_loss: 0.6218\n",
            "Epoch 614/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1411 - val_loss: 0.6164\n",
            "Epoch 615/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1501 - val_loss: 0.6235\n",
            "Epoch 616/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1419 - val_loss: 0.6225\n",
            "Epoch 617/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1647 - val_loss: 0.6217\n",
            "Epoch 618/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1265 - val_loss: 0.6217\n",
            "Epoch 619/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1565 - val_loss: 0.6313\n",
            "Epoch 620/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1316 - val_loss: 0.6403\n",
            "Epoch 621/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1467 - val_loss: 0.6350\n",
            "Epoch 622/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1759 - val_loss: 0.6330\n",
            "Epoch 623/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1598 - val_loss: 0.6298\n",
            "Epoch 624/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1410 - val_loss: 0.6345\n",
            "Epoch 625/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1621 - val_loss: 0.6213\n",
            "Epoch 626/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1272 - val_loss: 0.6239\n",
            "Epoch 627/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1308 - val_loss: 0.6303\n",
            "Epoch 628/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1501 - val_loss: 0.6285\n",
            "Epoch 629/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1548 - val_loss: 0.6224\n",
            "Epoch 630/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1555 - val_loss: 0.6176\n",
            "Epoch 631/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1390 - val_loss: 0.6277\n",
            "Epoch 632/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1480 - val_loss: 0.6206\n",
            "Epoch 633/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1622 - val_loss: 0.6118\n",
            "Epoch 634/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1485 - val_loss: 0.6096\n",
            "Epoch 635/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1458 - val_loss: 0.6105\n",
            "Epoch 636/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1567 - val_loss: 0.6187\n",
            "Epoch 637/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1573 - val_loss: 0.6173\n",
            "Epoch 638/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1412 - val_loss: 0.6218\n",
            "Epoch 639/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1512 - val_loss: 0.6186\n",
            "Epoch 640/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1575 - val_loss: 0.6137\n",
            "Epoch 641/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1274 - val_loss: 0.6092\n",
            "Epoch 642/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1365 - val_loss: 0.6132\n",
            "Epoch 643/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1396 - val_loss: 0.6179\n",
            "Epoch 644/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1211 - val_loss: 0.6190\n",
            "Epoch 645/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1370 - val_loss: 0.6082\n",
            "Epoch 646/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1445 - val_loss: 0.6009\n",
            "Epoch 647/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1450 - val_loss: 0.6099\n",
            "Epoch 648/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1578 - val_loss: 0.6102\n",
            "Epoch 649/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1591 - val_loss: 0.6202\n",
            "Epoch 650/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1384 - val_loss: 0.6052\n",
            "Epoch 651/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1499 - val_loss: 0.6108\n",
            "Epoch 652/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1382 - val_loss: 0.6050\n",
            "Epoch 653/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1363 - val_loss: 0.6088\n",
            "Epoch 654/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1288 - val_loss: 0.6131\n",
            "Epoch 655/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1230 - val_loss: 0.6213\n",
            "Epoch 656/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1482 - val_loss: 0.6150\n",
            "Epoch 657/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1401 - val_loss: 0.6206\n",
            "Epoch 658/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1329 - val_loss: 0.6189\n",
            "Epoch 659/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1406 - val_loss: 0.6180\n",
            "Epoch 660/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1453 - val_loss: 0.6245\n",
            "Epoch 661/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1410 - val_loss: 0.6211\n",
            "Epoch 662/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1570 - val_loss: 0.6187\n",
            "Epoch 663/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1381 - val_loss: 0.6117\n",
            "Epoch 664/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1451 - val_loss: 0.6143\n",
            "Epoch 665/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1461 - val_loss: 0.6089\n",
            "Epoch 666/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1516 - val_loss: 0.6104\n",
            "Epoch 667/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1436 - val_loss: 0.6169\n",
            "Epoch 668/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1481 - val_loss: 0.6177\n",
            "Epoch 669/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1568 - val_loss: 0.6193\n",
            "Epoch 670/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1534 - val_loss: 0.6266\n",
            "Epoch 671/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1658 - val_loss: 0.6171\n",
            "Epoch 672/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1455 - val_loss: 0.6180\n",
            "Epoch 673/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1382 - val_loss: 0.6206\n",
            "Epoch 674/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1518 - val_loss: 0.6245\n",
            "Epoch 675/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1412 - val_loss: 0.6127\n",
            "Epoch 676/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1421 - val_loss: 0.6045\n",
            "Epoch 677/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1250 - val_loss: 0.6031\n",
            "Epoch 678/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1402 - val_loss: 0.6146\n",
            "Epoch 679/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1431 - val_loss: 0.6103\n",
            "Epoch 680/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1346 - val_loss: 0.6124\n",
            "Epoch 681/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1312 - val_loss: 0.6056\n",
            "Epoch 682/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1376 - val_loss: 0.6102\n",
            "Epoch 683/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1279 - val_loss: 0.6140\n",
            "Epoch 684/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1589 - val_loss: 0.6021\n",
            "Epoch 685/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1468 - val_loss: 0.6050\n",
            "Epoch 686/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1332 - val_loss: 0.6092\n",
            "Epoch 687/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1200 - val_loss: 0.6019\n",
            "Epoch 688/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1433 - val_loss: 0.5951\n",
            "Epoch 689/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1463 - val_loss: 0.6048\n",
            "Epoch 690/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1438 - val_loss: 0.5980\n",
            "Epoch 691/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1459 - val_loss: 0.5952\n",
            "Epoch 692/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1458 - val_loss: 0.5975\n",
            "Epoch 693/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1209 - val_loss: 0.5939\n",
            "Epoch 694/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1532 - val_loss: 0.5983\n",
            "Epoch 695/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1493 - val_loss: 0.6014\n",
            "Epoch 696/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1439 - val_loss: 0.6003\n",
            "Epoch 697/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1196 - val_loss: 0.6053\n",
            "Epoch 698/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1190 - val_loss: 0.6024\n",
            "Epoch 699/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1462 - val_loss: 0.6095\n",
            "Epoch 700/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1463 - val_loss: 0.6174\n",
            "Epoch 701/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1372 - val_loss: 0.6121\n",
            "Epoch 702/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1464 - val_loss: 0.6071\n",
            "Epoch 703/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1285 - val_loss: 0.6161\n",
            "Epoch 704/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1386 - val_loss: 0.6186\n",
            "Epoch 705/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1343 - val_loss: 0.6244\n",
            "Epoch 706/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1321 - val_loss: 0.6290\n",
            "Epoch 707/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1439 - val_loss: 0.6263\n",
            "Epoch 708/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1190 - val_loss: 0.6357\n",
            "Epoch 709/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1410 - val_loss: 0.6390\n",
            "Epoch 710/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1382 - val_loss: 0.6392\n",
            "Epoch 711/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1339 - val_loss: 0.6184\n",
            "Epoch 712/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1354 - val_loss: 0.6115\n",
            "Epoch 713/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1310 - val_loss: 0.6150\n",
            "Epoch 714/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1347 - val_loss: 0.6034\n",
            "Epoch 715/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1417 - val_loss: 0.6052\n",
            "Epoch 716/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1328 - val_loss: 0.6131\n",
            "Epoch 717/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1363 - val_loss: 0.6118\n",
            "Epoch 718/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1116 - val_loss: 0.6093\n",
            "Epoch 719/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1175 - val_loss: 0.6161\n",
            "Epoch 720/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1272 - val_loss: 0.6088\n",
            "Epoch 721/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1277 - val_loss: 0.5993\n",
            "Epoch 722/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1372 - val_loss: 0.6055\n",
            "Epoch 723/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1342 - val_loss: 0.6017\n",
            "Epoch 724/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1284 - val_loss: 0.6049\n",
            "Epoch 725/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1332 - val_loss: 0.6049\n",
            "Epoch 726/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1479 - val_loss: 0.6010\n",
            "Epoch 727/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1439 - val_loss: 0.6001\n",
            "Epoch 728/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1509 - val_loss: 0.5991\n",
            "Epoch 729/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1340 - val_loss: 0.5897\n",
            "Epoch 730/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1397 - val_loss: 0.5913\n",
            "Epoch 731/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1297 - val_loss: 0.5977\n",
            "Epoch 732/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1392 - val_loss: 0.5942\n",
            "Epoch 733/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1341 - val_loss: 0.5960\n",
            "Epoch 734/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1310 - val_loss: 0.5842\n",
            "Epoch 735/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1466 - val_loss: 0.5848\n",
            "Epoch 736/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1165 - val_loss: 0.5839\n",
            "Epoch 737/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1340 - val_loss: 0.5806\n",
            "Epoch 738/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1248 - val_loss: 0.5922\n",
            "Epoch 739/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1291 - val_loss: 0.5976\n",
            "Epoch 740/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1557 - val_loss: 0.5913\n",
            "Epoch 741/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1402 - val_loss: 0.5951\n",
            "Epoch 742/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1268 - val_loss: 0.5997\n",
            "Epoch 743/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1346 - val_loss: 0.6022\n",
            "Epoch 744/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1283 - val_loss: 0.6026\n",
            "Epoch 745/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1282 - val_loss: 0.6065\n",
            "Epoch 746/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1255 - val_loss: 0.6130\n",
            "Epoch 747/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1408 - val_loss: 0.6096\n",
            "Epoch 748/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1152 - val_loss: 0.6043\n",
            "Epoch 749/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1044 - val_loss: 0.5964\n",
            "Epoch 750/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1332 - val_loss: 0.5998\n",
            "Epoch 751/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1325 - val_loss: 0.5971\n",
            "Epoch 752/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1318 - val_loss: 0.5953\n",
            "Epoch 753/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1246 - val_loss: 0.5969\n",
            "Epoch 754/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1335 - val_loss: 0.5961\n",
            "Epoch 755/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1184 - val_loss: 0.5974\n",
            "Epoch 756/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1392 - val_loss: 0.5970\n",
            "Epoch 757/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1388 - val_loss: 0.6053\n",
            "Epoch 758/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1312 - val_loss: 0.5978\n",
            "Epoch 759/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1125 - val_loss: 0.5954\n",
            "Epoch 760/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1324 - val_loss: 0.5986\n",
            "Epoch 761/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1401 - val_loss: 0.5933\n",
            "Epoch 762/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1089 - val_loss: 0.5864\n",
            "Epoch 763/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1309 - val_loss: 0.5931\n",
            "Epoch 764/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1262 - val_loss: 0.5924\n",
            "Epoch 765/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1310 - val_loss: 0.5921\n",
            "Epoch 766/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1390 - val_loss: 0.5946\n",
            "Epoch 767/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1240 - val_loss: 0.5924\n",
            "Epoch 768/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1270 - val_loss: 0.5871\n",
            "Epoch 769/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1269 - val_loss: 0.5856\n",
            "Epoch 770/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1236 - val_loss: 0.5860\n",
            "Epoch 771/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1230 - val_loss: 0.5894\n",
            "Epoch 772/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1209 - val_loss: 0.5912\n",
            "Epoch 773/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1144 - val_loss: 0.5865\n",
            "Epoch 774/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1259 - val_loss: 0.5812\n",
            "Epoch 775/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1098 - val_loss: 0.5830\n",
            "Epoch 776/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1343 - val_loss: 0.5832\n",
            "Epoch 777/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1354 - val_loss: 0.5879\n",
            "Epoch 778/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1182 - val_loss: 0.5914\n",
            "Epoch 779/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1320 - val_loss: 0.5971\n",
            "Epoch 780/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1270 - val_loss: 0.5943\n",
            "Epoch 781/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1464 - val_loss: 0.5919\n",
            "Epoch 782/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1168 - val_loss: 0.5989\n",
            "Epoch 783/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1468 - val_loss: 0.5961\n",
            "Epoch 784/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1266 - val_loss: 0.5940\n",
            "Epoch 785/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1529 - val_loss: 0.5949\n",
            "Epoch 786/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1166 - val_loss: 0.5944\n",
            "Epoch 787/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1325 - val_loss: 0.5930\n",
            "Epoch 788/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1163 - val_loss: 0.5821\n",
            "Epoch 789/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1272 - val_loss: 0.5900\n",
            "Epoch 790/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1210 - val_loss: 0.5836\n",
            "Epoch 791/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1108 - val_loss: 0.5867\n",
            "Epoch 792/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1132 - val_loss: 0.5833\n",
            "Epoch 793/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1326 - val_loss: 0.5855\n",
            "Epoch 794/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1305 - val_loss: 0.5895\n",
            "Epoch 795/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1275 - val_loss: 0.5975\n",
            "Epoch 796/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1377 - val_loss: 0.5900\n",
            "Epoch 797/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1169 - val_loss: 0.5790\n",
            "Epoch 798/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1332 - val_loss: 0.5860\n",
            "Epoch 799/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1270 - val_loss: 0.5936\n",
            "Epoch 800/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1053 - val_loss: 0.5846\n",
            "Epoch 801/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1480 - val_loss: 0.5925\n",
            "Epoch 802/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1287 - val_loss: 0.5946\n",
            "Epoch 803/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1226 - val_loss: 0.5932\n",
            "Epoch 804/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1245 - val_loss: 0.5901\n",
            "Epoch 805/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1194 - val_loss: 0.5900\n",
            "Epoch 806/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1145 - val_loss: 0.5888\n",
            "Epoch 807/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1195 - val_loss: 0.5940\n",
            "Epoch 808/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1267 - val_loss: 0.5987\n",
            "Epoch 809/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1104 - val_loss: 0.6021\n",
            "Epoch 810/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1310 - val_loss: 0.5937\n",
            "Epoch 811/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1306 - val_loss: 0.5850\n",
            "Epoch 812/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1197 - val_loss: 0.5893\n",
            "Epoch 813/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1125 - val_loss: 0.5850\n",
            "Epoch 814/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1147 - val_loss: 0.5902\n",
            "Epoch 815/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1440 - val_loss: 0.5928\n",
            "Epoch 816/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1162 - val_loss: 0.5917\n",
            "Epoch 817/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1265 - val_loss: 0.5862\n",
            "Epoch 818/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1257 - val_loss: 0.5823\n",
            "Epoch 819/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1345 - val_loss: 0.5857\n",
            "Epoch 820/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1180 - val_loss: 0.5784\n",
            "Epoch 821/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1072 - val_loss: 0.5860\n",
            "Epoch 822/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1121 - val_loss: 0.5860\n",
            "Epoch 823/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1456 - val_loss: 0.5781\n",
            "Epoch 824/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1209 - val_loss: 0.5720\n",
            "Epoch 825/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1168 - val_loss: 0.5808\n",
            "Epoch 826/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1216 - val_loss: 0.5876\n",
            "Epoch 827/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1254 - val_loss: 0.5843\n",
            "Epoch 828/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1358 - val_loss: 0.5811\n",
            "Epoch 829/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1281 - val_loss: 0.5767\n",
            "Epoch 830/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1309 - val_loss: 0.5775\n",
            "Epoch 831/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1231 - val_loss: 0.5819\n",
            "Epoch 832/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1146 - val_loss: 0.5781\n",
            "Epoch 833/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1220 - val_loss: 0.5741\n",
            "Epoch 834/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1074 - val_loss: 0.5819\n",
            "Epoch 835/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1161 - val_loss: 0.5839\n",
            "Epoch 836/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1097 - val_loss: 0.5786\n",
            "Epoch 837/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1148 - val_loss: 0.5777\n",
            "Epoch 838/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1202 - val_loss: 0.5826\n",
            "Epoch 839/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1324 - val_loss: 0.5841\n",
            "Epoch 840/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1223 - val_loss: 0.5781\n",
            "Epoch 841/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1254 - val_loss: 0.5824\n",
            "Epoch 842/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1314 - val_loss: 0.5811\n",
            "Epoch 843/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1206 - val_loss: 0.5838\n",
            "Epoch 844/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1298 - val_loss: 0.5783\n",
            "Epoch 845/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1159 - val_loss: 0.5833\n",
            "Epoch 846/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1257 - val_loss: 0.5877\n",
            "Epoch 847/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1192 - val_loss: 0.5822\n",
            "Epoch 848/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1275 - val_loss: 0.5811\n",
            "Epoch 849/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1258 - val_loss: 0.5801\n",
            "Epoch 850/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1214 - val_loss: 0.5878\n",
            "Epoch 851/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1255 - val_loss: 0.5887\n",
            "Epoch 852/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1325 - val_loss: 0.5911\n",
            "Epoch 853/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1257 - val_loss: 0.5920\n",
            "Epoch 854/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1126 - val_loss: 0.5854\n",
            "Epoch 855/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1228 - val_loss: 0.5835\n",
            "Epoch 856/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1285 - val_loss: 0.5865\n",
            "Epoch 857/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1396 - val_loss: 0.5908\n",
            "Epoch 858/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1441 - val_loss: 0.6045\n",
            "Epoch 859/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1251 - val_loss: 0.5985\n",
            "Epoch 860/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1060 - val_loss: 0.5976\n",
            "Epoch 861/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1194 - val_loss: 0.6014\n",
            "Epoch 862/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1233 - val_loss: 0.6027\n",
            "Epoch 863/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1019 - val_loss: 0.5965\n",
            "Epoch 864/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1184 - val_loss: 0.5926\n",
            "Epoch 865/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1221 - val_loss: 0.5851\n",
            "Epoch 866/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1102 - val_loss: 0.5819\n",
            "Epoch 867/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1255 - val_loss: 0.5828\n",
            "Epoch 868/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1066 - val_loss: 0.5901\n",
            "Epoch 869/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1218 - val_loss: 0.5826\n",
            "Epoch 870/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1215 - val_loss: 0.5802\n",
            "Epoch 871/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1256 - val_loss: 0.5765\n",
            "Epoch 872/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1098 - val_loss: 0.5832\n",
            "Epoch 873/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1418 - val_loss: 0.5692\n",
            "Epoch 874/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1188 - val_loss: 0.5722\n",
            "Epoch 875/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1048 - val_loss: 0.5825\n",
            "Epoch 876/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1309 - val_loss: 0.5860\n",
            "Epoch 877/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1116 - val_loss: 0.5786\n",
            "Epoch 878/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1288 - val_loss: 0.5813\n",
            "Epoch 879/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1387 - val_loss: 0.5722\n",
            "Epoch 880/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1184 - val_loss: 0.5692\n",
            "Epoch 881/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1302 - val_loss: 0.5778\n",
            "Epoch 882/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1253 - val_loss: 0.5823\n",
            "Epoch 883/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1317 - val_loss: 0.5874\n",
            "Epoch 884/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1106 - val_loss: 0.5885\n",
            "Epoch 885/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1375 - val_loss: 0.5809\n",
            "Epoch 886/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1132 - val_loss: 0.5876\n",
            "Epoch 887/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1248 - val_loss: 0.5882\n",
            "Epoch 888/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1050 - val_loss: 0.5910\n",
            "Epoch 889/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1309 - val_loss: 0.5842\n",
            "Epoch 890/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1149 - val_loss: 0.5749\n",
            "Epoch 891/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1068 - val_loss: 0.5806\n",
            "Epoch 892/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1217 - val_loss: 0.5880\n",
            "Epoch 893/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1126 - val_loss: 0.6002\n",
            "Epoch 894/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1313 - val_loss: 0.5936\n",
            "Epoch 895/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1287 - val_loss: 0.5867\n",
            "Epoch 896/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1200 - val_loss: 0.5861\n",
            "Epoch 897/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1112 - val_loss: 0.5890\n",
            "Epoch 898/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1169 - val_loss: 0.5887\n",
            "Epoch 899/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1050 - val_loss: 0.5998\n",
            "Epoch 900/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1098 - val_loss: 0.6016\n",
            "Epoch 901/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1203 - val_loss: 0.5965\n",
            "Epoch 902/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1138 - val_loss: 0.5991\n",
            "Epoch 903/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1227 - val_loss: 0.5957\n",
            "Epoch 904/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.0991 - val_loss: 0.5949\n",
            "Epoch 905/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0970 - val_loss: 0.5997\n",
            "Epoch 906/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.0984 - val_loss: 0.6027\n",
            "Epoch 907/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1153 - val_loss: 0.6023\n",
            "Epoch 908/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1268 - val_loss: 0.5978\n",
            "Epoch 909/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1109 - val_loss: 0.5884\n",
            "Epoch 910/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1001 - val_loss: 0.5871\n",
            "Epoch 911/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1159 - val_loss: 0.5958\n",
            "Epoch 912/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1141 - val_loss: 0.5955\n",
            "Epoch 913/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1163 - val_loss: 0.5872\n",
            "Epoch 914/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1152 - val_loss: 0.5892\n",
            "Epoch 915/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1152 - val_loss: 0.5866\n",
            "Epoch 916/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1165 - val_loss: 0.5831\n",
            "Epoch 917/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1099 - val_loss: 0.5972\n",
            "Epoch 918/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1188 - val_loss: 0.6029\n",
            "Epoch 919/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1194 - val_loss: 0.6056\n",
            "Epoch 920/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1079 - val_loss: 0.6002\n",
            "Epoch 921/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1079 - val_loss: 0.5970\n",
            "Epoch 922/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1038 - val_loss: 0.6000\n",
            "Epoch 923/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1201 - val_loss: 0.6020\n",
            "Epoch 924/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1315 - val_loss: 0.5944\n",
            "Epoch 925/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1290 - val_loss: 0.5947\n",
            "Epoch 926/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1149 - val_loss: 0.5809\n",
            "Epoch 927/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1128 - val_loss: 0.5812\n",
            "Epoch 928/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1101 - val_loss: 0.5926\n",
            "Epoch 929/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1319 - val_loss: 0.5848\n",
            "Epoch 930/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1384 - val_loss: 0.5889\n",
            "Epoch 931/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1161 - val_loss: 0.5899\n",
            "Epoch 932/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1161 - val_loss: 0.5859\n",
            "Epoch 933/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1095 - val_loss: 0.5852\n",
            "Epoch 934/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1163 - val_loss: 0.5824\n",
            "Epoch 935/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1079 - val_loss: 0.5844\n",
            "Epoch 936/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1022 - val_loss: 0.5855\n",
            "Epoch 937/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1273 - val_loss: 0.5950\n",
            "Epoch 938/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1111 - val_loss: 0.5902\n",
            "Epoch 939/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1202 - val_loss: 0.5972\n",
            "Epoch 940/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1358 - val_loss: 0.5977\n",
            "Epoch 941/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1003 - val_loss: 0.5894\n",
            "Epoch 942/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1151 - val_loss: 0.5909\n",
            "Epoch 943/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1219 - val_loss: 0.5870\n",
            "Epoch 944/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1038 - val_loss: 0.5926\n",
            "Epoch 945/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1094 - val_loss: 0.5881\n",
            "Epoch 946/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1012 - val_loss: 0.5967\n",
            "Epoch 947/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1262 - val_loss: 0.6036\n",
            "Epoch 948/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0912 - val_loss: 0.5957\n",
            "Epoch 949/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1083 - val_loss: 0.5887\n",
            "Epoch 950/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1176 - val_loss: 0.5920\n",
            "Epoch 951/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1038 - val_loss: 0.5943\n",
            "Epoch 952/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1191 - val_loss: 0.5954\n",
            "Epoch 953/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1105 - val_loss: 0.6070\n",
            "Epoch 954/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1127 - val_loss: 0.6097\n",
            "Epoch 955/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1057 - val_loss: 0.5982\n",
            "Epoch 956/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1191 - val_loss: 0.5996\n",
            "Epoch 957/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1115 - val_loss: 0.5968\n",
            "Epoch 958/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1079 - val_loss: 0.5855\n",
            "Epoch 959/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1167 - val_loss: 0.5891\n",
            "Epoch 960/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1297 - val_loss: 0.5877\n",
            "Epoch 961/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1173 - val_loss: 0.5841\n",
            "Epoch 962/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1083 - val_loss: 0.5802\n",
            "Epoch 963/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1185 - val_loss: 0.5847\n",
            "Epoch 964/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1201 - val_loss: 0.5997\n",
            "Epoch 965/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1047 - val_loss: 0.5965\n",
            "Epoch 966/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1111 - val_loss: 0.5902\n",
            "Epoch 967/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1088 - val_loss: 0.5931\n",
            "Epoch 968/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1122 - val_loss: 0.5850\n",
            "Epoch 969/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1155 - val_loss: 0.5736\n",
            "Epoch 970/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1066 - val_loss: 0.5784\n",
            "Epoch 971/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1016 - val_loss: 0.5762\n",
            "Epoch 972/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1082 - val_loss: 0.5828\n",
            "Epoch 973/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1145 - val_loss: 0.5907\n",
            "Epoch 974/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1083 - val_loss: 0.5899\n",
            "Epoch 975/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1035 - val_loss: 0.5814\n",
            "Epoch 976/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1164 - val_loss: 0.5849\n",
            "Epoch 977/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1058 - val_loss: 0.5790\n",
            "Epoch 978/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1027 - val_loss: 0.5795\n",
            "Epoch 979/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1416 - val_loss: 0.5717\n",
            "Epoch 980/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1260 - val_loss: 0.5785\n",
            "Epoch 981/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1030 - val_loss: 0.5735\n",
            "Epoch 982/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1043 - val_loss: 0.5633\n",
            "Epoch 983/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1166 - val_loss: 0.5673\n",
            "Epoch 984/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1168 - val_loss: 0.5702\n",
            "Epoch 985/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1118 - val_loss: 0.5772\n",
            "Epoch 986/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1158 - val_loss: 0.5831\n",
            "Epoch 987/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1233 - val_loss: 0.5759\n",
            "Epoch 988/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1181 - val_loss: 0.5766\n",
            "Epoch 989/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1190 - val_loss: 0.5642\n",
            "Epoch 990/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1063 - val_loss: 0.5627\n",
            "Epoch 991/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1143 - val_loss: 0.5618\n",
            "Epoch 992/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1145 - val_loss: 0.5632\n",
            "Epoch 993/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1121 - val_loss: 0.5718\n",
            "Epoch 994/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1166 - val_loss: 0.5691\n",
            "Epoch 995/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.0932 - val_loss: 0.5659\n",
            "Epoch 996/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1084 - val_loss: 0.5746\n",
            "Epoch 997/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1081 - val_loss: 0.5708\n",
            "Epoch 998/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1089 - val_loss: 0.5769\n",
            "Epoch 999/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1206 - val_loss: 0.5807\n",
            "Epoch 1000/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1271 - val_loss: 0.5738\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5738\n",
            "7/7 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae6a02b160>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number set as 256\n",
        "nb_epochs=1000;    # training epochs change to 1000\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = random.choice([condition1, condition2, condition3, condition4])    # randomly choose 1 lighting condition for test set\n",
        "train_conditions = random.choice([c for c in [condition1, condition2, condition3, condition4] if c is not val_condition])   # randomly choose 1 left condition for training\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_conditions.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=5e-6))    # Learning rate changes 1e-5 to 3e-6\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "fDPx2kPSow29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vqXNWwoTkqf"
      },
      "source": [
        "##3.3 Train on data under 3 light and test on 1 light condition ##"
      ],
      "id": "4vqXNWwoTkqf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uHGpIjBAGC8"
      },
      "source": [
        "###3.3.1 Randomly Choose###\n",
        "Randomly choose one lighting condition for testing and the left 3 conditions will be conditions for training.\n",
        "\n",
        "--> 4 fold cross validation...\n",
        "\n",
        "loss: 0.0301 - val_loss: 0.3302  \n",
        "Significantly higher than naive prediction(0.11)"
      ],
      "id": "9uHGpIjBAGC8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "me4W9XdNTpUM",
        "outputId": "fffdc09c-c58b-4c95-d784-85334aed2876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 4s 9ms/step - loss: 2.0647 - val_loss: 2.1753\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.9710 - val_loss: 2.1016\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.8817 - val_loss: 2.0255\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.7821 - val_loss: 1.9488\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 1.6925 - val_loss: 1.8702\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.5877 - val_loss: 1.7930\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.4994 - val_loss: 1.7141\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.4214 - val_loss: 1.6359\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 6ms/step - loss: 1.3281 - val_loss: 1.5540\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.2553 - val_loss: 1.4765\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.1698 - val_loss: 1.3962\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.1103 - val_loss: 1.3198\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.0375 - val_loss: 1.2456\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.9750 - val_loss: 1.1732\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.9239 - val_loss: 1.1058\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.8627 - val_loss: 1.0402\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.8231 - val_loss: 0.9802\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7770 - val_loss: 0.9215\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7326 - val_loss: 0.8705\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6924 - val_loss: 0.8231\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6651 - val_loss: 0.7798\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6380 - val_loss: 0.7423\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6062 - val_loss: 0.7079\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5967 - val_loss: 0.6798\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5736 - val_loss: 0.6541\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5591 - val_loss: 0.6315\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5452 - val_loss: 0.6111\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.5296 - val_loss: 0.5939\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5330 - val_loss: 0.5791\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5204 - val_loss: 0.5647\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4957 - val_loss: 0.5535\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5068 - val_loss: 0.5446\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4917 - val_loss: 0.5363\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4787 - val_loss: 0.5309\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4910 - val_loss: 0.5252\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4766 - val_loss: 0.5195\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4863 - val_loss: 0.5174\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4621 - val_loss: 0.5122\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4732 - val_loss: 0.5082\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4622 - val_loss: 0.5057\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4586 - val_loss: 0.5024\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4550 - val_loss: 0.5000\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4513 - val_loss: 0.4983\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4392 - val_loss: 0.4966\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4598 - val_loss: 0.4966\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4431 - val_loss: 0.4941\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4457 - val_loss: 0.4941\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4287 - val_loss: 0.4943\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4320 - val_loss: 0.4938\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4306 - val_loss: 0.4904\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4138 - val_loss: 0.4900\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4249 - val_loss: 0.4888\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4279 - val_loss: 0.4858\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4276 - val_loss: 0.4859\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4170 - val_loss: 0.4862\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4229 - val_loss: 0.4856\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4154 - val_loss: 0.4846\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4239 - val_loss: 0.4843\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4087 - val_loss: 0.4842\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4133 - val_loss: 0.4839\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4124 - val_loss: 0.4829\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4192 - val_loss: 0.4837\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4268 - val_loss: 0.4816\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4079 - val_loss: 0.4814\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4127 - val_loss: 0.4830\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4136 - val_loss: 0.4822\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4071 - val_loss: 0.4817\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4210 - val_loss: 0.4810\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4160 - val_loss: 0.4822\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4013 - val_loss: 0.4817\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3985 - val_loss: 0.4816\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3997 - val_loss: 0.4826\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4113 - val_loss: 0.4820\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3950 - val_loss: 0.4798\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3919 - val_loss: 0.4807\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3877 - val_loss: 0.4787\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3856 - val_loss: 0.4807\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3871 - val_loss: 0.4811\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4084 - val_loss: 0.4813\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3806 - val_loss: 0.4811\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4008 - val_loss: 0.4800\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3908 - val_loss: 0.4783\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3889 - val_loss: 0.4803\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3914 - val_loss: 0.4788\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3994 - val_loss: 0.4771\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3927 - val_loss: 0.4770\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3965 - val_loss: 0.4760\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3885 - val_loss: 0.4754\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3889 - val_loss: 0.4756\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.4761\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3885 - val_loss: 0.4764\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3868 - val_loss: 0.4738\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3805 - val_loss: 0.4742\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3874 - val_loss: 0.4748\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3840 - val_loss: 0.4745\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3827 - val_loss: 0.4731\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3890 - val_loss: 0.4730\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3700 - val_loss: 0.4723\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3736 - val_loss: 0.4737\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3773 - val_loss: 0.4733\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3795 - val_loss: 0.4731\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3721 - val_loss: 0.4732\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3693 - val_loss: 0.4732\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3755 - val_loss: 0.4714\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3851 - val_loss: 0.4724\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3657 - val_loss: 0.4710\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3692 - val_loss: 0.4716\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3763 - val_loss: 0.4725\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3869 - val_loss: 0.4696\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.4700\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3660 - val_loss: 0.4701\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3664 - val_loss: 0.4712\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3662 - val_loss: 0.4699\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3640 - val_loss: 0.4702\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3763 - val_loss: 0.4694\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3657 - val_loss: 0.4687\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3699 - val_loss: 0.4703\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3568 - val_loss: 0.4704\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3700 - val_loss: 0.4693\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3664 - val_loss: 0.4687\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3792 - val_loss: 0.4665\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3583 - val_loss: 0.4667\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3675 - val_loss: 0.4640\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3572 - val_loss: 0.4657\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3552 - val_loss: 0.4620\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3653 - val_loss: 0.4614\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3566 - val_loss: 0.4616\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3675 - val_loss: 0.4615\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3642 - val_loss: 0.4610\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3588 - val_loss: 0.4632\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3600 - val_loss: 0.4618\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3621 - val_loss: 0.4624\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3523 - val_loss: 0.4603\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3582 - val_loss: 0.4610\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3664 - val_loss: 0.4590\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3604 - val_loss: 0.4605\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3561 - val_loss: 0.4584\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3628 - val_loss: 0.4609\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3663 - val_loss: 0.4618\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3655 - val_loss: 0.4600\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3621 - val_loss: 0.4624\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3627 - val_loss: 0.4612\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3534 - val_loss: 0.4601\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3571 - val_loss: 0.4568\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3667 - val_loss: 0.4560\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3503 - val_loss: 0.4565\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3451 - val_loss: 0.4571\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3571 - val_loss: 0.4552\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3533 - val_loss: 0.4561\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3558 - val_loss: 0.4541\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3537 - val_loss: 0.4537\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3551 - val_loss: 0.4534\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3449 - val_loss: 0.4532\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3505 - val_loss: 0.4532\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3439 - val_loss: 0.4525\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3418 - val_loss: 0.4523\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3446 - val_loss: 0.4542\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3426 - val_loss: 0.4548\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3422 - val_loss: 0.4564\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3488 - val_loss: 0.4547\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3445 - val_loss: 0.4559\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3416 - val_loss: 0.4554\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3424 - val_loss: 0.4517\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3406 - val_loss: 0.4515\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3476 - val_loss: 0.4527\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3582 - val_loss: 0.4498\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3407 - val_loss: 0.4493\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3469 - val_loss: 0.4489\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3435 - val_loss: 0.4481\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3413 - val_loss: 0.4470\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3425 - val_loss: 0.4462\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3484 - val_loss: 0.4461\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3507 - val_loss: 0.4470\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3352 - val_loss: 0.4486\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3306 - val_loss: 0.4484\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3351 - val_loss: 0.4458\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3392 - val_loss: 0.4448\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3307 - val_loss: 0.4450\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3398 - val_loss: 0.4443\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3410 - val_loss: 0.4427\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3321 - val_loss: 0.4432\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3404 - val_loss: 0.4412\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3332 - val_loss: 0.4413\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3332 - val_loss: 0.4439\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3288 - val_loss: 0.4449\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3422 - val_loss: 0.4421\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3367 - val_loss: 0.4437\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3292 - val_loss: 0.4421\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3359 - val_loss: 0.4421\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3246 - val_loss: 0.4417\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3346 - val_loss: 0.4430\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3310 - val_loss: 0.4417\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3254 - val_loss: 0.4430\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3304 - val_loss: 0.4419\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3224 - val_loss: 0.4434\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3413 - val_loss: 0.4408\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3276 - val_loss: 0.4388\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3356 - val_loss: 0.4394\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3356 - val_loss: 0.4388\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3370 - val_loss: 0.4378\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3406 - val_loss: 0.4339\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3400 - val_loss: 0.4356\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3271 - val_loss: 0.4350\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3203 - val_loss: 0.4348\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3366 - val_loss: 0.4342\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3302 - val_loss: 0.4335\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3307 - val_loss: 0.4352\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3300 - val_loss: 0.4353\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3401 - val_loss: 0.4355\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3344 - val_loss: 0.4328\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3244 - val_loss: 0.4352\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3336 - val_loss: 0.4353\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3330 - val_loss: 0.4335\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3213 - val_loss: 0.4326\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3234 - val_loss: 0.4317\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3249 - val_loss: 0.4323\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3281 - val_loss: 0.4324\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3260 - val_loss: 0.4311\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3271 - val_loss: 0.4297\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3295 - val_loss: 0.4287\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3317 - val_loss: 0.4299\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3293 - val_loss: 0.4284\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3196 - val_loss: 0.4286\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3291 - val_loss: 0.4286\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3166 - val_loss: 0.4273\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3265 - val_loss: 0.4265\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3244 - val_loss: 0.4263\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3224 - val_loss: 0.4290\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3310 - val_loss: 0.4277\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3317 - val_loss: 0.4257\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3249 - val_loss: 0.4239\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3235 - val_loss: 0.4242\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3256 - val_loss: 0.4251\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3248 - val_loss: 0.4234\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3148 - val_loss: 0.4247\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3204 - val_loss: 0.4253\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3128 - val_loss: 0.4260\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3241 - val_loss: 0.4248\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3128 - val_loss: 0.4248\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3172 - val_loss: 0.4249\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3348 - val_loss: 0.4255\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3217 - val_loss: 0.4215\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3186 - val_loss: 0.4214\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3205 - val_loss: 0.4215\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3202 - val_loss: 0.4176\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3154 - val_loss: 0.4181\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3111 - val_loss: 0.4166\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3075 - val_loss: 0.4192\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3177 - val_loss: 0.4193\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3167 - val_loss: 0.4177\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3170 - val_loss: 0.4177\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3147 - val_loss: 0.4189\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3255 - val_loss: 0.4173\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3089 - val_loss: 0.4177\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3200 - val_loss: 0.4144\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3156 - val_loss: 0.4129\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3038 - val_loss: 0.4122\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3092 - val_loss: 0.4115\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3161 - val_loss: 0.4137\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3113 - val_loss: 0.4126\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3091 - val_loss: 0.4134\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3191 - val_loss: 0.4109\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3015 - val_loss: 0.4113\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3106 - val_loss: 0.4101\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3051 - val_loss: 0.4093\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3214 - val_loss: 0.4102\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3015 - val_loss: 0.4098\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3145 - val_loss: 0.4102\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3082 - val_loss: 0.4119\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3117 - val_loss: 0.4126\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3139 - val_loss: 0.4099\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3132 - val_loss: 0.4115\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3234 - val_loss: 0.4113\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3149 - val_loss: 0.4064\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3106 - val_loss: 0.4059\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3090 - val_loss: 0.4046\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2986 - val_loss: 0.4061\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3080 - val_loss: 0.4067\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3030 - val_loss: 0.4078\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3119 - val_loss: 0.4065\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3068 - val_loss: 0.4045\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3146 - val_loss: 0.4048\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3133 - val_loss: 0.4051\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3092 - val_loss: 0.4047\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2958 - val_loss: 0.4035\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3014 - val_loss: 0.4040\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3118 - val_loss: 0.4020\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3040 - val_loss: 0.4019\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3140 - val_loss: 0.4020\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2978 - val_loss: 0.4031\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2975 - val_loss: 0.4041\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3116 - val_loss: 0.4036\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3028 - val_loss: 0.3997\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3080 - val_loss: 0.4001\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2901 - val_loss: 0.3995\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3034 - val_loss: 0.3973\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3023 - val_loss: 0.3984\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3019 - val_loss: 0.3980\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3012 - val_loss: 0.3978\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2962 - val_loss: 0.3987\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2945 - val_loss: 0.3977\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2982 - val_loss: 0.3984\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3006 - val_loss: 0.3985\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2911 - val_loss: 0.3990\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2994 - val_loss: 0.3962\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2979 - val_loss: 0.3967\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3080 - val_loss: 0.3970\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3090 - val_loss: 0.3961\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2980 - val_loss: 0.3956\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2941 - val_loss: 0.3941\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2995 - val_loss: 0.3944\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3058 - val_loss: 0.3938\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2973 - val_loss: 0.3957\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2923 - val_loss: 0.3952\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3023 - val_loss: 0.3929\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3032 - val_loss: 0.3910\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2976 - val_loss: 0.3934\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3015 - val_loss: 0.3931\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3034 - val_loss: 0.3892\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3078 - val_loss: 0.3907\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3047 - val_loss: 0.3891\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2941 - val_loss: 0.3902\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2925 - val_loss: 0.3912\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2974 - val_loss: 0.3902\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2888 - val_loss: 0.3908\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2955 - val_loss: 0.3905\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3072 - val_loss: 0.3906\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2965 - val_loss: 0.3902\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2946 - val_loss: 0.3887\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2976 - val_loss: 0.3871\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3175 - val_loss: 0.3839\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2894 - val_loss: 0.3867\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2887 - val_loss: 0.3852\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2875 - val_loss: 0.3850\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2981 - val_loss: 0.3855\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2916 - val_loss: 0.3862\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2817 - val_loss: 0.3849\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2941 - val_loss: 0.3839\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2968 - val_loss: 0.3819\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2979 - val_loss: 0.3831\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3064 - val_loss: 0.3836\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3024 - val_loss: 0.3800\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2874 - val_loss: 0.3807\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2822 - val_loss: 0.3807\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2947 - val_loss: 0.3800\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2902 - val_loss: 0.3787\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2850 - val_loss: 0.3786\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2779 - val_loss: 0.3775\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2864 - val_loss: 0.3780\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2844 - val_loss: 0.3785\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2790 - val_loss: 0.3761\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2933 - val_loss: 0.3777\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2937 - val_loss: 0.3766\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2867 - val_loss: 0.3758\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2842 - val_loss: 0.3761\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2976 - val_loss: 0.3747\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2847 - val_loss: 0.3739\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2864 - val_loss: 0.3741\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2920 - val_loss: 0.3701\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2796 - val_loss: 0.3735\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2821 - val_loss: 0.3739\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2975 - val_loss: 0.3729\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2855 - val_loss: 0.3763\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2809 - val_loss: 0.3755\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2798 - val_loss: 0.3741\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2878 - val_loss: 0.3713\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2909 - val_loss: 0.3736\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2830 - val_loss: 0.3723\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2875 - val_loss: 0.3721\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2850 - val_loss: 0.3687\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2835 - val_loss: 0.3700\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2796 - val_loss: 0.3683\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2766 - val_loss: 0.3701\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2891 - val_loss: 0.3670\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2861 - val_loss: 0.3647\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2867 - val_loss: 0.3670\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2720 - val_loss: 0.3681\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2831 - val_loss: 0.3685\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2810 - val_loss: 0.3695\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2828 - val_loss: 0.3685\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2871 - val_loss: 0.3668\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2837 - val_loss: 0.3664\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2697 - val_loss: 0.3660\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2757 - val_loss: 0.3635\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2798 - val_loss: 0.3650\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2767 - val_loss: 0.3640\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2789 - val_loss: 0.3644\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2801 - val_loss: 0.3637\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2862 - val_loss: 0.3641\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2904 - val_loss: 0.3637\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2822 - val_loss: 0.3625\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2739 - val_loss: 0.3640\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2920 - val_loss: 0.3656\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2960 - val_loss: 0.3639\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2839 - val_loss: 0.3620\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2883 - val_loss: 0.3618\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2884 - val_loss: 0.3610\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2813 - val_loss: 0.3607\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2874 - val_loss: 0.3595\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2758 - val_loss: 0.3596\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2789 - val_loss: 0.3575\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2872 - val_loss: 0.3572\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2773 - val_loss: 0.3583\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2816 - val_loss: 0.3577\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2833 - val_loss: 0.3576\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2726 - val_loss: 0.3566\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2683 - val_loss: 0.3579\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2787 - val_loss: 0.3574\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2743 - val_loss: 0.3571\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2798 - val_loss: 0.3558\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2823 - val_loss: 0.3559\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2797 - val_loss: 0.3552\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2710 - val_loss: 0.3536\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2838 - val_loss: 0.3566\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2761 - val_loss: 0.3562\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2766 - val_loss: 0.3556\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2766 - val_loss: 0.3526\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2750 - val_loss: 0.3523\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2802 - val_loss: 0.3550\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2792 - val_loss: 0.3537\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2795 - val_loss: 0.3550\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2695 - val_loss: 0.3561\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.2746 - val_loss: 0.3545\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2577 - val_loss: 0.3578\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2703 - val_loss: 0.3555\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2881 - val_loss: 0.3532\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2767 - val_loss: 0.3531\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2679 - val_loss: 0.3525\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2713 - val_loss: 0.3527\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2641 - val_loss: 0.3556\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2680 - val_loss: 0.3526\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2737 - val_loss: 0.3536\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2772 - val_loss: 0.3534\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2738 - val_loss: 0.3535\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2771 - val_loss: 0.3531\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2597 - val_loss: 0.3533\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2564 - val_loss: 0.3524\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2644 - val_loss: 0.3536\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2723 - val_loss: 0.3531\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2676 - val_loss: 0.3530\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2651 - val_loss: 0.3529\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2646 - val_loss: 0.3546\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2665 - val_loss: 0.3509\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2591 - val_loss: 0.3512\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2648 - val_loss: 0.3510\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2736 - val_loss: 0.3524\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2774 - val_loss: 0.3500\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2714 - val_loss: 0.3505\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2612 - val_loss: 0.3488\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2741 - val_loss: 0.3472\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2607 - val_loss: 0.3471\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2562 - val_loss: 0.3475\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2671 - val_loss: 0.3468\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2722 - val_loss: 0.3484\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2654 - val_loss: 0.3457\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2488 - val_loss: 0.3455\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2638 - val_loss: 0.3445\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2611 - val_loss: 0.3442\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2705 - val_loss: 0.3440\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2595 - val_loss: 0.3455\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2734 - val_loss: 0.3467\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2640 - val_loss: 0.3464\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2562 - val_loss: 0.3448\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2591 - val_loss: 0.3465\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2678 - val_loss: 0.3462\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2681 - val_loss: 0.3448\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.3444\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2639 - val_loss: 0.3424\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2579 - val_loss: 0.3440\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2592 - val_loss: 0.3434\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2661 - val_loss: 0.3442\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2585 - val_loss: 0.3437\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2630 - val_loss: 0.3434\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2682 - val_loss: 0.3433\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2658 - val_loss: 0.3441\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2606 - val_loss: 0.3452\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2657 - val_loss: 0.3436\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2621 - val_loss: 0.3447\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2567 - val_loss: 0.3418\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2654 - val_loss: 0.3435\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2616 - val_loss: 0.3444\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2621 - val_loss: 0.3433\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2592 - val_loss: 0.3415\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2584 - val_loss: 0.3403\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2596 - val_loss: 0.3408\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2647 - val_loss: 0.3412\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2613 - val_loss: 0.3406\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2669 - val_loss: 0.3410\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2616 - val_loss: 0.3405\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2600 - val_loss: 0.3400\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2535 - val_loss: 0.3393\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2604 - val_loss: 0.3383\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2530 - val_loss: 0.3358\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2506 - val_loss: 0.3378\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2600 - val_loss: 0.3391\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2427 - val_loss: 0.3384\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2632 - val_loss: 0.3397\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2486 - val_loss: 0.3415\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2423 - val_loss: 0.3406\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2659 - val_loss: 0.3405\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2608 - val_loss: 0.3390\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2562 - val_loss: 0.3379\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2592 - val_loss: 0.3397\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2614 - val_loss: 0.3400\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2687 - val_loss: 0.3380\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2602 - val_loss: 0.3362\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2601 - val_loss: 0.3353\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2543 - val_loss: 0.3347\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2537 - val_loss: 0.3348\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2559 - val_loss: 0.3365\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2563 - val_loss: 0.3349\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2487 - val_loss: 0.3344\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2509 - val_loss: 0.3353\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2537 - val_loss: 0.3345\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2539 - val_loss: 0.3344\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2505 - val_loss: 0.3327\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2642 - val_loss: 0.3297\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2543 - val_loss: 0.3311\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2580 - val_loss: 0.3313\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2419 - val_loss: 0.3355\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2461 - val_loss: 0.3337\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2622 - val_loss: 0.3326\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2524 - val_loss: 0.3319\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2561 - val_loss: 0.3306\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2491 - val_loss: 0.3319\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2526 - val_loss: 0.3316\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2574 - val_loss: 0.3329\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2561 - val_loss: 0.3350\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2599 - val_loss: 0.3318\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2508 - val_loss: 0.3326\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2573 - val_loss: 0.3327\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2513 - val_loss: 0.3327\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2472 - val_loss: 0.3342\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2524 - val_loss: 0.3345\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2550 - val_loss: 0.3348\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2534 - val_loss: 0.3320\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2569 - val_loss: 0.3324\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2511 - val_loss: 0.3350\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2442 - val_loss: 0.3337\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2591 - val_loss: 0.3347\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2459 - val_loss: 0.3340\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2446 - val_loss: 0.3341\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2401 - val_loss: 0.3325\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2557 - val_loss: 0.3330\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2571 - val_loss: 0.3323\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2630 - val_loss: 0.3323\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2583 - val_loss: 0.3326\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2379 - val_loss: 0.3325\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2576 - val_loss: 0.3331\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2493 - val_loss: 0.3340\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2387 - val_loss: 0.3328\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2497 - val_loss: 0.3338\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2510 - val_loss: 0.3346\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2514 - val_loss: 0.3327\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.3335\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2554 - val_loss: 0.3304\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2521 - val_loss: 0.3333\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2497 - val_loss: 0.3332\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2442 - val_loss: 0.3306\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2423 - val_loss: 0.3336\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2453 - val_loss: 0.3331\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2493 - val_loss: 0.3328\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2369 - val_loss: 0.3341\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2445 - val_loss: 0.3352\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2462 - val_loss: 0.3339\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2422 - val_loss: 0.3356\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2540 - val_loss: 0.3312\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2363 - val_loss: 0.3314\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2417 - val_loss: 0.3306\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2496 - val_loss: 0.3313\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2457 - val_loss: 0.3320\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2423 - val_loss: 0.3310\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2444 - val_loss: 0.3315\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2475 - val_loss: 0.3333\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.3311\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2602 - val_loss: 0.3325\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2549 - val_loss: 0.3338\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2461 - val_loss: 0.3323\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2442 - val_loss: 0.3331\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2403 - val_loss: 0.3316\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2393 - val_loss: 0.3327\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2580 - val_loss: 0.3319\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2374 - val_loss: 0.3323\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2423 - val_loss: 0.3301\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2526 - val_loss: 0.3315\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2415 - val_loss: 0.3294\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2369 - val_loss: 0.3293\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2482 - val_loss: 0.3289\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2370 - val_loss: 0.3314\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2390 - val_loss: 0.3313\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2429 - val_loss: 0.3324\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2472 - val_loss: 0.3333\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2431 - val_loss: 0.3318\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2375 - val_loss: 0.3321\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2351 - val_loss: 0.3309\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2378 - val_loss: 0.3333\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2467 - val_loss: 0.3319\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2453 - val_loss: 0.3311\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2409 - val_loss: 0.3305\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2420 - val_loss: 0.3303\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2423 - val_loss: 0.3297\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2462 - val_loss: 0.3305\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2298 - val_loss: 0.3310\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2477 - val_loss: 0.3293\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2385 - val_loss: 0.3261\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.3275\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2451 - val_loss: 0.3278\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2482 - val_loss: 0.3268\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2326 - val_loss: 0.3271\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2432 - val_loss: 0.3274\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2412 - val_loss: 0.3277\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2331 - val_loss: 0.3292\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2425 - val_loss: 0.3271\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2380 - val_loss: 0.3260\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2276 - val_loss: 0.3272\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2380 - val_loss: 0.3279\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2321 - val_loss: 0.3286\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2455 - val_loss: 0.3290\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2467 - val_loss: 0.3280\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2525 - val_loss: 0.3278\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2485 - val_loss: 0.3311\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2419 - val_loss: 0.3318\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2331 - val_loss: 0.3313\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2313 - val_loss: 0.3290\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2426 - val_loss: 0.3267\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2416 - val_loss: 0.3293\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2444 - val_loss: 0.3305\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2363 - val_loss: 0.3295\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2413 - val_loss: 0.3288\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.2391 - val_loss: 0.3265\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2446 - val_loss: 0.3283\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2310 - val_loss: 0.3313\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2467 - val_loss: 0.3309\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.2362 - val_loss: 0.3294\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.2408 - val_loss: 0.3304\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.2295 - val_loss: 0.3308\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.2365 - val_loss: 0.3338\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.2340 - val_loss: 0.3325\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.2306 - val_loss: 0.3310\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.2419 - val_loss: 0.3322\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.2313 - val_loss: 0.3306\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.2345 - val_loss: 0.3297\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2307 - val_loss: 0.3298\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2449 - val_loss: 0.3262\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2382 - val_loss: 0.3289\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2397 - val_loss: 0.3294\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2448 - val_loss: 0.3305\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2202 - val_loss: 0.3304\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2360 - val_loss: 0.3277\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2428 - val_loss: 0.3261\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2305 - val_loss: 0.3278\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2415 - val_loss: 0.3282\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2377 - val_loss: 0.3300\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2279 - val_loss: 0.3302\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2236 - val_loss: 0.3316\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2344 - val_loss: 0.3282\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2404 - val_loss: 0.3295\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2328 - val_loss: 0.3274\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2387 - val_loss: 0.3276\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2260 - val_loss: 0.3278\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2305 - val_loss: 0.3297\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2264 - val_loss: 0.3291\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2319 - val_loss: 0.3269\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.2277 - val_loss: 0.3267\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2476 - val_loss: 0.3247\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2221 - val_loss: 0.3263\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2392 - val_loss: 0.3260\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2364 - val_loss: 0.3259\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2279 - val_loss: 0.3261\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2400 - val_loss: 0.3272\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.3273\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2314 - val_loss: 0.3293\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2364 - val_loss: 0.3307\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2363 - val_loss: 0.3291\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2276 - val_loss: 0.3305\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2243 - val_loss: 0.3302\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2313 - val_loss: 0.3329\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2346 - val_loss: 0.3333\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2267 - val_loss: 0.3339\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2455 - val_loss: 0.3314\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2316 - val_loss: 0.3314\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2317 - val_loss: 0.3313\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2307 - val_loss: 0.3300\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2285 - val_loss: 0.3311\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2337 - val_loss: 0.3303\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2283 - val_loss: 0.3294\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2225 - val_loss: 0.3291\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2309 - val_loss: 0.3300\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2265 - val_loss: 0.3300\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2302 - val_loss: 0.3304\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2260 - val_loss: 0.3297\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2287 - val_loss: 0.3266\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2253 - val_loss: 0.3276\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2277 - val_loss: 0.3285\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2266 - val_loss: 0.3282\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2338 - val_loss: 0.3279\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2226 - val_loss: 0.3301\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2121 - val_loss: 0.3298\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2229 - val_loss: 0.3290\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2256 - val_loss: 0.3289\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2192 - val_loss: 0.3278\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2299 - val_loss: 0.3316\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2288 - val_loss: 0.3294\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2254 - val_loss: 0.3297\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2267 - val_loss: 0.3294\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2244 - val_loss: 0.3307\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2336 - val_loss: 0.3281\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.3273\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.3268\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2334 - val_loss: 0.3273\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2229 - val_loss: 0.3270\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.3290\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2319 - val_loss: 0.3303\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2295 - val_loss: 0.3286\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2276 - val_loss: 0.3270\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2239 - val_loss: 0.3285\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2297 - val_loss: 0.3291\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2243 - val_loss: 0.3303\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2227 - val_loss: 0.3288\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2275 - val_loss: 0.3281\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2196 - val_loss: 0.3288\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.3293\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.3273\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2256 - val_loss: 0.3279\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2270 - val_loss: 0.3282\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2281 - val_loss: 0.3268\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2237 - val_loss: 0.3289\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2185 - val_loss: 0.3296\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2272 - val_loss: 0.3308\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2201 - val_loss: 0.3298\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2240 - val_loss: 0.3295\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2421 - val_loss: 0.3292\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2062 - val_loss: 0.3297\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2241 - val_loss: 0.3304\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2206 - val_loss: 0.3302\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2280 - val_loss: 0.3321\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2152 - val_loss: 0.3324\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2244 - val_loss: 0.3318\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2330 - val_loss: 0.3332\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2187 - val_loss: 0.3336\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2382 - val_loss: 0.3322\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2205 - val_loss: 0.3328\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.3299\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.3286\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2312 - val_loss: 0.3287\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2257 - val_loss: 0.3310\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.3312\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.3322\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2213 - val_loss: 0.3320\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2250 - val_loss: 0.3300\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2258 - val_loss: 0.3296\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2128 - val_loss: 0.3306\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.3308\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2150 - val_loss: 0.3316\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2245 - val_loss: 0.3309\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2139 - val_loss: 0.3330\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2238 - val_loss: 0.3327\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2059 - val_loss: 0.3349\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2162 - val_loss: 0.3346\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2196 - val_loss: 0.3370\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2268 - val_loss: 0.3365\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2190 - val_loss: 0.3355\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2190 - val_loss: 0.3329\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2217 - val_loss: 0.3347\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2193 - val_loss: 0.3324\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2149 - val_loss: 0.3329\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2131 - val_loss: 0.3339\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2217 - val_loss: 0.3312\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2300 - val_loss: 0.3317\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2148 - val_loss: 0.3311\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.3318\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.3305\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2225 - val_loss: 0.3318\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2267 - val_loss: 0.3320\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2257 - val_loss: 0.3327\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2128 - val_loss: 0.3356\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2159 - val_loss: 0.3309\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2200 - val_loss: 0.3307\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2099 - val_loss: 0.3319\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2189 - val_loss: 0.3322\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2130 - val_loss: 0.3320\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2300 - val_loss: 0.3332\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2190 - val_loss: 0.3327\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2223 - val_loss: 0.3339\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2317 - val_loss: 0.3321\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2307 - val_loss: 0.3311\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2265 - val_loss: 0.3309\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.3319\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.3279\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2183 - val_loss: 0.3274\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2154 - val_loss: 0.3289\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2234 - val_loss: 0.3296\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2311 - val_loss: 0.3309\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2179 - val_loss: 0.3307\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2298 - val_loss: 0.3325\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2253 - val_loss: 0.3308\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2143 - val_loss: 0.3324\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2189 - val_loss: 0.3300\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.3301\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.3312\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2254 - val_loss: 0.3298\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2149 - val_loss: 0.3303\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2105 - val_loss: 0.3307\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2321 - val_loss: 0.3291\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2238 - val_loss: 0.3312\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2240 - val_loss: 0.3337\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2185 - val_loss: 0.3331\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2123 - val_loss: 0.3317\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2272 - val_loss: 0.3333\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2164 - val_loss: 0.3300\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2272 - val_loss: 0.3300\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2132 - val_loss: 0.3315\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2158 - val_loss: 0.3295\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2094 - val_loss: 0.3318\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2282 - val_loss: 0.3345\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2304 - val_loss: 0.3329\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2221 - val_loss: 0.3325\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2144 - val_loss: 0.3338\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2190 - val_loss: 0.3330\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.3340\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2251 - val_loss: 0.3345\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2194 - val_loss: 0.3363\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.3362\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.3347\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2071 - val_loss: 0.3351\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2209 - val_loss: 0.3329\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2120 - val_loss: 0.3334\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.3349\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2159 - val_loss: 0.3346\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2127 - val_loss: 0.3339\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2175 - val_loss: 0.3306\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2078 - val_loss: 0.3324\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2198 - val_loss: 0.3335\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2139 - val_loss: 0.3338\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2201 - val_loss: 0.3363\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2307 - val_loss: 0.3338\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2188 - val_loss: 0.3288\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2145 - val_loss: 0.3268\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2155 - val_loss: 0.3284\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2083 - val_loss: 0.3294\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2126 - val_loss: 0.3291\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2123 - val_loss: 0.3302\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2101 - val_loss: 0.3317\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2206 - val_loss: 0.3317\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2140 - val_loss: 0.3333\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2169 - val_loss: 0.3347\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2068 - val_loss: 0.3331\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.3325\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2130 - val_loss: 0.3321\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2083 - val_loss: 0.3319\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2079 - val_loss: 0.3324\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2134 - val_loss: 0.3335\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2071 - val_loss: 0.3336\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2035 - val_loss: 0.3329\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2127 - val_loss: 0.3331\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2197 - val_loss: 0.3318\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2248 - val_loss: 0.3293\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2150 - val_loss: 0.3282\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2161 - val_loss: 0.3315\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2064 - val_loss: 0.3283\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2244 - val_loss: 0.3289\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2156 - val_loss: 0.3308\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2121 - val_loss: 0.3292\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2092 - val_loss: 0.3314\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2079 - val_loss: 0.3307\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2078 - val_loss: 0.3318\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2103 - val_loss: 0.3300\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2063 - val_loss: 0.3284\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2105 - val_loss: 0.3295\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2137 - val_loss: 0.3320\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2106 - val_loss: 0.3316\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1961 - val_loss: 0.3311\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2071 - val_loss: 0.3325\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2189 - val_loss: 0.3322\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2097 - val_loss: 0.3326\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2133 - val_loss: 0.3333\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1989 - val_loss: 0.3310\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2166 - val_loss: 0.3331\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2096 - val_loss: 0.3309\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2255 - val_loss: 0.3303\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2152 - val_loss: 0.3328\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2060 - val_loss: 0.3339\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1944 - val_loss: 0.3338\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2077 - val_loss: 0.3346\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2199 - val_loss: 0.3345\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2163 - val_loss: 0.3331\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2155 - val_loss: 0.3357\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2175 - val_loss: 0.3346\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.3335\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2110 - val_loss: 0.3321\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2119 - val_loss: 0.3327\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2079 - val_loss: 0.3356\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2095 - val_loss: 0.3334\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2194 - val_loss: 0.3323\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2080 - val_loss: 0.3334\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.3327\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2166 - val_loss: 0.3349\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2063 - val_loss: 0.3346\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2098 - val_loss: 0.3365\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2118 - val_loss: 0.3392\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.3380\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2055 - val_loss: 0.3380\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2061 - val_loss: 0.3377\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2088 - val_loss: 0.3382\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2023 - val_loss: 0.3410\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2106 - val_loss: 0.3388\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2129 - val_loss: 0.3389\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2142 - val_loss: 0.3389\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2055 - val_loss: 0.3380\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2141 - val_loss: 0.3364\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2214 - val_loss: 0.3372\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2061 - val_loss: 0.3379\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2071 - val_loss: 0.3371\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2132 - val_loss: 0.3352\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2008 - val_loss: 0.3366\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2045 - val_loss: 0.3371\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1991 - val_loss: 0.3364\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.3342\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2070 - val_loss: 0.3337\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2026 - val_loss: 0.3340\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1988 - val_loss: 0.3330\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2069 - val_loss: 0.3332\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.3334\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.3354\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2048 - val_loss: 0.3369\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1984 - val_loss: 0.3358\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1998 - val_loss: 0.3364\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.3334\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2097 - val_loss: 0.3346\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2114 - val_loss: 0.3358\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2003 - val_loss: 0.3332\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.1959 - val_loss: 0.3357\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2042 - val_loss: 0.3364\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2169 - val_loss: 0.3363\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2083 - val_loss: 0.3378\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1993 - val_loss: 0.3376\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2046 - val_loss: 0.3359\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2046 - val_loss: 0.3341\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.3355\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1990 - val_loss: 0.3352\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2069 - val_loss: 0.3339\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.3339\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2108 - val_loss: 0.3339\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2095 - val_loss: 0.3361\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2065 - val_loss: 0.3374\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2006 - val_loss: 0.3367\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2042 - val_loss: 0.3386\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2036 - val_loss: 0.3391\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1967 - val_loss: 0.3377\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1999 - val_loss: 0.3375\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2022 - val_loss: 0.3338\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1959 - val_loss: 0.3348\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1969 - val_loss: 0.3362\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2057 - val_loss: 0.3371\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1970 - val_loss: 0.3384\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2076 - val_loss: 0.3379\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2046 - val_loss: 0.3356\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1955 - val_loss: 0.3333\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2187 - val_loss: 0.3333\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2094 - val_loss: 0.3358\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.3365\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2036 - val_loss: 0.3383\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2012 - val_loss: 0.3374\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1900 - val_loss: 0.3377\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1978 - val_loss: 0.3369\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2022 - val_loss: 0.3382\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1979 - val_loss: 0.3393\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1933 - val_loss: 0.3369\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2115 - val_loss: 0.3378\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2023 - val_loss: 0.3390\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1987 - val_loss: 0.3400\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2127 - val_loss: 0.3383\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2104 - val_loss: 0.3386\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2001 - val_loss: 0.3364\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2026 - val_loss: 0.3359\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2001 - val_loss: 0.3380\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2064 - val_loss: 0.3375\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2141 - val_loss: 0.3359\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1942 - val_loss: 0.3337\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.3334\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2067 - val_loss: 0.3336\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1987 - val_loss: 0.3347\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1994 - val_loss: 0.3379\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2052 - val_loss: 0.3339\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2032 - val_loss: 0.3330\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1979 - val_loss: 0.3351\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1990 - val_loss: 0.3349\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2058 - val_loss: 0.3355\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1999 - val_loss: 0.3343\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1888 - val_loss: 0.3367\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1931 - val_loss: 0.3381\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1973 - val_loss: 0.3365\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1937 - val_loss: 0.3378\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.3330\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2061 - val_loss: 0.3376\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2057 - val_loss: 0.3378\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1881 - val_loss: 0.3370\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2015 - val_loss: 0.3361\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2043 - val_loss: 0.3357\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1976 - val_loss: 0.3339\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3339\n",
            "7/7 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae6a973850>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number set as 256\n",
        "nb_epochs=1000;    # training epochs change to 1000\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = random.choice([condition1, condition2, condition3, condition4])    # randomly choose 1 lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "me4W9XdNTpUM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NraLzXmAPKk"
      },
      "source": [
        "###3.3.2 1st light condition as test###  "
      ],
      "id": "0NraLzXmAPKk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SsLHq1n3Aduz",
        "outputId": "83e38cd7-661b-44cd-ede8-1f45d1d5330c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 5s 14ms/step - loss: 2.3550 - val_loss: 2.0191\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 2.2750 - val_loss: 1.9542\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 2.1915 - val_loss: 1.8874\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 2.0953 - val_loss: 1.8175\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 2.0067 - val_loss: 1.7481\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 1.9132 - val_loss: 1.6802\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.8134 - val_loss: 1.6119\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.7214 - val_loss: 1.5435\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.6284 - val_loss: 1.4777\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.5361 - val_loss: 1.4125\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.4500 - val_loss: 1.3488\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.3713 - val_loss: 1.2871\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 1.2910 - val_loss: 1.2251\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.2177 - val_loss: 1.1630\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.1436 - val_loss: 1.1015\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 1.0653 - val_loss: 1.0401\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.9969 - val_loss: 0.9805\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9413 - val_loss: 0.9227\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8979 - val_loss: 0.8697\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8323 - val_loss: 0.8148\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7939 - val_loss: 0.7646\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.7507 - val_loss: 0.7178\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7184 - val_loss: 0.6744\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6963 - val_loss: 0.6330\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6702 - val_loss: 0.5949\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6292 - val_loss: 0.5612\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6228 - val_loss: 0.5305\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6037 - val_loss: 0.5026\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5923 - val_loss: 0.4785\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5726 - val_loss: 0.4561\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5524 - val_loss: 0.4368\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5587 - val_loss: 0.4205\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5499 - val_loss: 0.4063\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5360 - val_loss: 0.3914\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.5266 - val_loss: 0.3781\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5318 - val_loss: 0.3692\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5163 - val_loss: 0.3595\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5215 - val_loss: 0.3505\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5007 - val_loss: 0.3430\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4975 - val_loss: 0.3356\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5038 - val_loss: 0.3299\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4939 - val_loss: 0.3251\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4856 - val_loss: 0.3203\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4872 - val_loss: 0.3154\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4803 - val_loss: 0.3130\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4666 - val_loss: 0.3086\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4724 - val_loss: 0.3060\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4804 - val_loss: 0.3025\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4581 - val_loss: 0.3018\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4639 - val_loss: 0.2990\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4692 - val_loss: 0.2977\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4638 - val_loss: 0.2956\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4535 - val_loss: 0.2933\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4521 - val_loss: 0.2913\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4492 - val_loss: 0.2903\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4542 - val_loss: 0.2886\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4531 - val_loss: 0.2878\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4530 - val_loss: 0.2859\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4473 - val_loss: 0.2851\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4413 - val_loss: 0.2847\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4337 - val_loss: 0.2830\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4432 - val_loss: 0.2814\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4407 - val_loss: 0.2802\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4383 - val_loss: 0.2800\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4257 - val_loss: 0.2802\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4227 - val_loss: 0.2809\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4166 - val_loss: 0.2798\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4087 - val_loss: 0.2796\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4272 - val_loss: 0.2797\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4232 - val_loss: 0.2785\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4219 - val_loss: 0.2793\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4219 - val_loss: 0.2793\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4244 - val_loss: 0.2790\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4273 - val_loss: 0.2784\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4165 - val_loss: 0.2777\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4152 - val_loss: 0.2787\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4115 - val_loss: 0.2775\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4052 - val_loss: 0.2773\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4023 - val_loss: 0.2766\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4044 - val_loss: 0.2762\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4020 - val_loss: 0.2770\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4028 - val_loss: 0.2775\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4117 - val_loss: 0.2763\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3991 - val_loss: 0.2772\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3918 - val_loss: 0.2758\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3986 - val_loss: 0.2772\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3991 - val_loss: 0.2768\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3973 - val_loss: 0.2760\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4001 - val_loss: 0.2764\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3990 - val_loss: 0.2765\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3982 - val_loss: 0.2757\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3919 - val_loss: 0.2751\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3881 - val_loss: 0.2755\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3851 - val_loss: 0.2745\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3696 - val_loss: 0.2740\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3889 - val_loss: 0.2737\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3983 - val_loss: 0.2729\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3722 - val_loss: 0.2740\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3854 - val_loss: 0.2733\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3708 - val_loss: 0.2735\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3755 - val_loss: 0.2735\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3812 - val_loss: 0.2735\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3922 - val_loss: 0.2722\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3698 - val_loss: 0.2736\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3767 - val_loss: 0.2730\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3843 - val_loss: 0.2733\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3674 - val_loss: 0.2726\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3646 - val_loss: 0.2728\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3679 - val_loss: 0.2725\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3836 - val_loss: 0.2707\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3780 - val_loss: 0.2718\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3760 - val_loss: 0.2707\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3611 - val_loss: 0.2706\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3630 - val_loss: 0.2695\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3684 - val_loss: 0.2694\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3622 - val_loss: 0.2698\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3581 - val_loss: 0.2691\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3726 - val_loss: 0.2685\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3754 - val_loss: 0.2680\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3687 - val_loss: 0.2667\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3764 - val_loss: 0.2660\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3664 - val_loss: 0.2661\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3542 - val_loss: 0.2652\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3580 - val_loss: 0.2647\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3564 - val_loss: 0.2645\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3645 - val_loss: 0.2649\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3539 - val_loss: 0.2649\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3577 - val_loss: 0.2641\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3607 - val_loss: 0.2641\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3582 - val_loss: 0.2632\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3556 - val_loss: 0.2635\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3515 - val_loss: 0.2641\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3478 - val_loss: 0.2644\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3446 - val_loss: 0.2632\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3565 - val_loss: 0.2625\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3623 - val_loss: 0.2627\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3502 - val_loss: 0.2626\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3480 - val_loss: 0.2623\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3642 - val_loss: 0.2611\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3352 - val_loss: 0.2618\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3591 - val_loss: 0.2622\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3499 - val_loss: 0.2617\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3521 - val_loss: 0.2604\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3348 - val_loss: 0.2602\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3388 - val_loss: 0.2597\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3453 - val_loss: 0.2581\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3487 - val_loss: 0.2580\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3501 - val_loss: 0.2574\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3511 - val_loss: 0.2565\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3523 - val_loss: 0.2570\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3433 - val_loss: 0.2566\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3401 - val_loss: 0.2560\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3405 - val_loss: 0.2556\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3408 - val_loss: 0.2558\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3490 - val_loss: 0.2567\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3354 - val_loss: 0.2555\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3399 - val_loss: 0.2550\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3452 - val_loss: 0.2537\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3341 - val_loss: 0.2533\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3319 - val_loss: 0.2528\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3452 - val_loss: 0.2532\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3451 - val_loss: 0.2535\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3330 - val_loss: 0.2528\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3311 - val_loss: 0.2520\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3343 - val_loss: 0.2521\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3274 - val_loss: 0.2517\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3292 - val_loss: 0.2512\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3262 - val_loss: 0.2500\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3411 - val_loss: 0.2502\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3336 - val_loss: 0.2504\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3279 - val_loss: 0.2499\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3340 - val_loss: 0.2505\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3260 - val_loss: 0.2508\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3369 - val_loss: 0.2503\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3342 - val_loss: 0.2501\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3378 - val_loss: 0.2508\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3224 - val_loss: 0.2511\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3301 - val_loss: 0.2503\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3267 - val_loss: 0.2499\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3260 - val_loss: 0.2488\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3267 - val_loss: 0.2486\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3358 - val_loss: 0.2477\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3263 - val_loss: 0.2466\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3395 - val_loss: 0.2455\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3294 - val_loss: 0.2452\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3177 - val_loss: 0.2460\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3282 - val_loss: 0.2467\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3333 - val_loss: 0.2464\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3325 - val_loss: 0.2457\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3316 - val_loss: 0.2447\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3247 - val_loss: 0.2440\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3240 - val_loss: 0.2444\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3212 - val_loss: 0.2443\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3283 - val_loss: 0.2436\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3240 - val_loss: 0.2437\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3262 - val_loss: 0.2431\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3199 - val_loss: 0.2422\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3222 - val_loss: 0.2429\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3151 - val_loss: 0.2429\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3226 - val_loss: 0.2429\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3215 - val_loss: 0.2433\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3167 - val_loss: 0.2430\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3252 - val_loss: 0.2424\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3244 - val_loss: 0.2423\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3296 - val_loss: 0.2420\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3150 - val_loss: 0.2421\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3185 - val_loss: 0.2416\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3162 - val_loss: 0.2411\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3101 - val_loss: 0.2416\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3078 - val_loss: 0.2422\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3249 - val_loss: 0.2414\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2979 - val_loss: 0.2411\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3198 - val_loss: 0.2411\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3178 - val_loss: 0.2416\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3141 - val_loss: 0.2413\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3223 - val_loss: 0.2412\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3083 - val_loss: 0.2417\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3100 - val_loss: 0.2423\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3118 - val_loss: 0.2416\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3178 - val_loss: 0.2415\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3020 - val_loss: 0.2411\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3096 - val_loss: 0.2398\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3171 - val_loss: 0.2392\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3115 - val_loss: 0.2402\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3138 - val_loss: 0.2388\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3096 - val_loss: 0.2395\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3154 - val_loss: 0.2397\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3002 - val_loss: 0.2387\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3147 - val_loss: 0.2380\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3079 - val_loss: 0.2382\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3150 - val_loss: 0.2388\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3027 - val_loss: 0.2397\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3145 - val_loss: 0.2393\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3105 - val_loss: 0.2383\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3086 - val_loss: 0.2395\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3196 - val_loss: 0.2403\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3099 - val_loss: 0.2398\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3062 - val_loss: 0.2392\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3129 - val_loss: 0.2390\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3036 - val_loss: 0.2389\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3143 - val_loss: 0.2392\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3078 - val_loss: 0.2392\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3065 - val_loss: 0.2398\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3088 - val_loss: 0.2398\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3054 - val_loss: 0.2403\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2956 - val_loss: 0.2399\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3072 - val_loss: 0.2398\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2989 - val_loss: 0.2389\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3097 - val_loss: 0.2388\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3006 - val_loss: 0.2395\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3111 - val_loss: 0.2389\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2938 - val_loss: 0.2381\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3055 - val_loss: 0.2376\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3000 - val_loss: 0.2369\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3043 - val_loss: 0.2377\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2904 - val_loss: 0.2377\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2974 - val_loss: 0.2368\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2976 - val_loss: 0.2367\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3020 - val_loss: 0.2366\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2956 - val_loss: 0.2375\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2989 - val_loss: 0.2379\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3011 - val_loss: 0.2370\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3054 - val_loss: 0.2387\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.2997 - val_loss: 0.2372\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3061 - val_loss: 0.2387\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2944 - val_loss: 0.2384\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3095 - val_loss: 0.2381\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2999 - val_loss: 0.2388\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3047 - val_loss: 0.2389\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2961 - val_loss: 0.2376\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3064 - val_loss: 0.2373\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3053 - val_loss: 0.2371\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3053 - val_loss: 0.2371\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2984 - val_loss: 0.2364\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2913 - val_loss: 0.2373\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2963 - val_loss: 0.2364\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2840 - val_loss: 0.2367\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2914 - val_loss: 0.2367\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2898 - val_loss: 0.2373\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2851 - val_loss: 0.2378\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2896 - val_loss: 0.2377\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3010 - val_loss: 0.2367\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2940 - val_loss: 0.2373\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2972 - val_loss: 0.2366\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2886 - val_loss: 0.2363\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3030 - val_loss: 0.2367\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3051 - val_loss: 0.2362\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2881 - val_loss: 0.2365\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2912 - val_loss: 0.2369\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.2871 - val_loss: 0.2366\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2934 - val_loss: 0.2358\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2935 - val_loss: 0.2359\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2959 - val_loss: 0.2361\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3038 - val_loss: 0.2364\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2883 - val_loss: 0.2362\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2917 - val_loss: 0.2367\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2834 - val_loss: 0.2361\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2906 - val_loss: 0.2362\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2844 - val_loss: 0.2358\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2965 - val_loss: 0.2357\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2840 - val_loss: 0.2354\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2919 - val_loss: 0.2364\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2829 - val_loss: 0.2359\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3011 - val_loss: 0.2357\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3024 - val_loss: 0.2358\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2996 - val_loss: 0.2357\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2783 - val_loss: 0.2353\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2890 - val_loss: 0.2356\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2893 - val_loss: 0.2365\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2859 - val_loss: 0.2363\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2979 - val_loss: 0.2363\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2907 - val_loss: 0.2370\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2859 - val_loss: 0.2364\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2781 - val_loss: 0.2350\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2909 - val_loss: 0.2358\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2871 - val_loss: 0.2353\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2847 - val_loss: 0.2342\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2921 - val_loss: 0.2343\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2903 - val_loss: 0.2331\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2756 - val_loss: 0.2349\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2848 - val_loss: 0.2360\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2861 - val_loss: 0.2356\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2845 - val_loss: 0.2352\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2877 - val_loss: 0.2357\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2812 - val_loss: 0.2346\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2821 - val_loss: 0.2349\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2838 - val_loss: 0.2349\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2944 - val_loss: 0.2358\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2710 - val_loss: 0.2350\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2866 - val_loss: 0.2362\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2915 - val_loss: 0.2364\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2708 - val_loss: 0.2366\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2881 - val_loss: 0.2374\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2838 - val_loss: 0.2367\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2884 - val_loss: 0.2379\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2825 - val_loss: 0.2374\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2803 - val_loss: 0.2369\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2832 - val_loss: 0.2376\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2876 - val_loss: 0.2374\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2789 - val_loss: 0.2366\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2884 - val_loss: 0.2370\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2862 - val_loss: 0.2363\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2818 - val_loss: 0.2352\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2896 - val_loss: 0.2352\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2878 - val_loss: 0.2356\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2828 - val_loss: 0.2352\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.2362\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2849 - val_loss: 0.2363\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2804 - val_loss: 0.2359\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2713 - val_loss: 0.2357\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2857 - val_loss: 0.2357\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2859 - val_loss: 0.2354\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2871 - val_loss: 0.2355\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2745 - val_loss: 0.2341\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2842 - val_loss: 0.2346\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2818 - val_loss: 0.2353\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2824 - val_loss: 0.2351\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2789 - val_loss: 0.2348\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2794 - val_loss: 0.2357\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.2359\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.2353\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2746 - val_loss: 0.2359\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2768 - val_loss: 0.2338\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2740 - val_loss: 0.2343\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2719 - val_loss: 0.2339\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2699 - val_loss: 0.2344\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2825 - val_loss: 0.2338\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2705 - val_loss: 0.2346\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2747 - val_loss: 0.2364\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2733 - val_loss: 0.2347\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2769 - val_loss: 0.2351\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2759 - val_loss: 0.2355\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2883 - val_loss: 0.2350\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2750 - val_loss: 0.2343\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.2361\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2781 - val_loss: 0.2363\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2839 - val_loss: 0.2363\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2888 - val_loss: 0.2353\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2718 - val_loss: 0.2352\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2657 - val_loss: 0.2352\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2777 - val_loss: 0.2350\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2635 - val_loss: 0.2363\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2747 - val_loss: 0.2354\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2627 - val_loss: 0.2357\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2770 - val_loss: 0.2348\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2701 - val_loss: 0.2357\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2740 - val_loss: 0.2353\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.2358\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2679 - val_loss: 0.2352\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2713 - val_loss: 0.2357\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2674 - val_loss: 0.2368\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2645 - val_loss: 0.2364\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2814 - val_loss: 0.2367\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2752 - val_loss: 0.2369\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2761 - val_loss: 0.2364\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2785 - val_loss: 0.2366\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2745 - val_loss: 0.2370\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2727 - val_loss: 0.2359\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2692 - val_loss: 0.2367\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2610 - val_loss: 0.2369\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.2372\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2747 - val_loss: 0.2372\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2817 - val_loss: 0.2371\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2611 - val_loss: 0.2356\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2613 - val_loss: 0.2357\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2746 - val_loss: 0.2358\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2669 - val_loss: 0.2358\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2658 - val_loss: 0.2363\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2734 - val_loss: 0.2374\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2604 - val_loss: 0.2373\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2809 - val_loss: 0.2369\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2781 - val_loss: 0.2381\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2586 - val_loss: 0.2377\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2625 - val_loss: 0.2372\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2701 - val_loss: 0.2368\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2617 - val_loss: 0.2366\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2605 - val_loss: 0.2365\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2795 - val_loss: 0.2373\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.2377\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2663 - val_loss: 0.2369\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2755 - val_loss: 0.2377\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2793 - val_loss: 0.2376\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2672 - val_loss: 0.2373\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2710 - val_loss: 0.2375\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.2372\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2633 - val_loss: 0.2375\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2576 - val_loss: 0.2375\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2653 - val_loss: 0.2376\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2761 - val_loss: 0.2381\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2776 - val_loss: 0.2386\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2682 - val_loss: 0.2394\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2597 - val_loss: 0.2399\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2805 - val_loss: 0.2399\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.2386\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2786 - val_loss: 0.2378\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2648 - val_loss: 0.2372\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2658 - val_loss: 0.2383\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2738 - val_loss: 0.2398\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2609 - val_loss: 0.2395\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2647 - val_loss: 0.2393\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2580 - val_loss: 0.2388\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.2387\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2665 - val_loss: 0.2390\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2577 - val_loss: 0.2398\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2700 - val_loss: 0.2379\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2577 - val_loss: 0.2374\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2677 - val_loss: 0.2382\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2635 - val_loss: 0.2382\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2539 - val_loss: 0.2376\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2690 - val_loss: 0.2377\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2647 - val_loss: 0.2375\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2790 - val_loss: 0.2376\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.2376\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2721 - val_loss: 0.2384\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2561 - val_loss: 0.2371\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2681 - val_loss: 0.2375\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2587 - val_loss: 0.2378\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2535 - val_loss: 0.2382\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2683 - val_loss: 0.2399\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2589 - val_loss: 0.2399\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2588 - val_loss: 0.2385\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2583 - val_loss: 0.2368\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2564 - val_loss: 0.2367\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2580 - val_loss: 0.2373\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2642 - val_loss: 0.2402\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2452 - val_loss: 0.2380\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2610 - val_loss: 0.2376\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2607 - val_loss: 0.2377\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2655 - val_loss: 0.2386\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.2381\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2533 - val_loss: 0.2374\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2660 - val_loss: 0.2383\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2571 - val_loss: 0.2380\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2618 - val_loss: 0.2385\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2506 - val_loss: 0.2371\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.2368\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2557 - val_loss: 0.2385\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2513 - val_loss: 0.2375\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2616 - val_loss: 0.2381\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2531 - val_loss: 0.2381\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2600 - val_loss: 0.2385\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2491 - val_loss: 0.2390\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2489 - val_loss: 0.2394\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2587 - val_loss: 0.2387\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2577 - val_loss: 0.2390\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2580 - val_loss: 0.2391\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2564 - val_loss: 0.2404\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2601 - val_loss: 0.2407\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2579 - val_loss: 0.2399\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2645 - val_loss: 0.2396\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2622 - val_loss: 0.2399\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2692 - val_loss: 0.2390\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2563 - val_loss: 0.2402\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2539 - val_loss: 0.2399\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2629 - val_loss: 0.2393\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2475 - val_loss: 0.2390\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2587 - val_loss: 0.2390\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2600 - val_loss: 0.2387\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2499 - val_loss: 0.2404\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2503 - val_loss: 0.2399\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2481 - val_loss: 0.2406\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2549 - val_loss: 0.2396\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2588 - val_loss: 0.2406\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2538 - val_loss: 0.2383\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2468 - val_loss: 0.2389\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2653 - val_loss: 0.2392\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2610 - val_loss: 0.2387\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2540 - val_loss: 0.2400\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2677 - val_loss: 0.2392\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2554 - val_loss: 0.2401\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2502 - val_loss: 0.2393\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2504 - val_loss: 0.2395\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2605 - val_loss: 0.2397\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2412 - val_loss: 0.2386\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2459 - val_loss: 0.2374\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2573 - val_loss: 0.2385\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2546 - val_loss: 0.2390\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2467 - val_loss: 0.2410\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2443 - val_loss: 0.2391\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2540 - val_loss: 0.2401\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2645 - val_loss: 0.2394\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2455 - val_loss: 0.2406\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2548 - val_loss: 0.2399\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2499 - val_loss: 0.2389\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2540 - val_loss: 0.2390\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2560 - val_loss: 0.2395\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2655 - val_loss: 0.2386\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2569 - val_loss: 0.2382\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2527 - val_loss: 0.2399\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2468 - val_loss: 0.2401\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2559 - val_loss: 0.2398\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2373 - val_loss: 0.2401\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2528 - val_loss: 0.2399\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2510 - val_loss: 0.2403\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2576 - val_loss: 0.2400\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2549 - val_loss: 0.2407\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2552 - val_loss: 0.2386\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2525 - val_loss: 0.2407\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2518 - val_loss: 0.2402\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2465 - val_loss: 0.2402\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2609 - val_loss: 0.2401\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2470 - val_loss: 0.2412\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2505 - val_loss: 0.2412\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2482 - val_loss: 0.2404\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2547 - val_loss: 0.2411\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2473 - val_loss: 0.2394\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2510 - val_loss: 0.2392\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2495 - val_loss: 0.2409\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2502 - val_loss: 0.2393\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.2407\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2455 - val_loss: 0.2386\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2517 - val_loss: 0.2407\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2559 - val_loss: 0.2413\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2573 - val_loss: 0.2418\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2429 - val_loss: 0.2404\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2511 - val_loss: 0.2397\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2412 - val_loss: 0.2405\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2475 - val_loss: 0.2401\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.2394\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2482 - val_loss: 0.2400\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2327 - val_loss: 0.2396\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.2409\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2409 - val_loss: 0.2402\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2513 - val_loss: 0.2423\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2521 - val_loss: 0.2413\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2604 - val_loss: 0.2416\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2516 - val_loss: 0.2415\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2473 - val_loss: 0.2400\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2436 - val_loss: 0.2403\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2456 - val_loss: 0.2407\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2469 - val_loss: 0.2397\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2466 - val_loss: 0.2408\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2483 - val_loss: 0.2402\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2495 - val_loss: 0.2399\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2423 - val_loss: 0.2406\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2481 - val_loss: 0.2415\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2503 - val_loss: 0.2426\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2526 - val_loss: 0.2426\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2556 - val_loss: 0.2416\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2472 - val_loss: 0.2419\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2583 - val_loss: 0.2429\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2418 - val_loss: 0.2415\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2391 - val_loss: 0.2412\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2530 - val_loss: 0.2413\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.2425\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2499 - val_loss: 0.2421\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2448 - val_loss: 0.2402\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2359 - val_loss: 0.2408\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2374 - val_loss: 0.2418\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2268 - val_loss: 0.2421\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2449 - val_loss: 0.2415\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2406 - val_loss: 0.2426\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2415 - val_loss: 0.2411\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2447 - val_loss: 0.2410\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2424 - val_loss: 0.2402\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2338 - val_loss: 0.2396\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2436 - val_loss: 0.2401\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2460 - val_loss: 0.2403\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2545 - val_loss: 0.2405\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2492 - val_loss: 0.2398\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.2381\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2443 - val_loss: 0.2391\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2407 - val_loss: 0.2389\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2404 - val_loss: 0.2382\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2599 - val_loss: 0.2390\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2391 - val_loss: 0.2388\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2442 - val_loss: 0.2386\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2524 - val_loss: 0.2393\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.2392\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2376 - val_loss: 0.2391\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.2380\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2332 - val_loss: 0.2386\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2456 - val_loss: 0.2391\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2411 - val_loss: 0.2388\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2445 - val_loss: 0.2387\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2431 - val_loss: 0.2381\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2442 - val_loss: 0.2388\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2457 - val_loss: 0.2393\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2382 - val_loss: 0.2390\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2409 - val_loss: 0.2384\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2477 - val_loss: 0.2390\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2483 - val_loss: 0.2393\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2419 - val_loss: 0.2388\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2479 - val_loss: 0.2373\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2277 - val_loss: 0.2381\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2435 - val_loss: 0.2370\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2382 - val_loss: 0.2374\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2460 - val_loss: 0.2377\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2380 - val_loss: 0.2377\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2456 - val_loss: 0.2389\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2336 - val_loss: 0.2370\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2443 - val_loss: 0.2389\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2339 - val_loss: 0.2385\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2386 - val_loss: 0.2387\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2512 - val_loss: 0.2391\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2309 - val_loss: 0.2406\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.2399\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2400 - val_loss: 0.2401\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2339 - val_loss: 0.2410\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2473 - val_loss: 0.2409\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2349 - val_loss: 0.2407\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2402 - val_loss: 0.2400\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2296 - val_loss: 0.2400\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2317 - val_loss: 0.2411\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2436 - val_loss: 0.2393\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2335 - val_loss: 0.2395\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2349 - val_loss: 0.2408\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.2407\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2380 - val_loss: 0.2392\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2428 - val_loss: 0.2404\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2347 - val_loss: 0.2420\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2442 - val_loss: 0.2400\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2368 - val_loss: 0.2409\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.2409\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2280 - val_loss: 0.2412\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2405 - val_loss: 0.2406\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2280 - val_loss: 0.2407\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2337 - val_loss: 0.2419\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2383 - val_loss: 0.2417\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2446 - val_loss: 0.2415\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2316 - val_loss: 0.2408\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2310 - val_loss: 0.2401\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2516 - val_loss: 0.2423\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2354 - val_loss: 0.2434\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2411 - val_loss: 0.2413\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.2415\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2323 - val_loss: 0.2412\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.2398\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.2400\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2314 - val_loss: 0.2411\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2341 - val_loss: 0.2389\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2293 - val_loss: 0.2380\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2341 - val_loss: 0.2376\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.2395\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2337 - val_loss: 0.2387\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2353 - val_loss: 0.2388\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2420 - val_loss: 0.2388\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2302 - val_loss: 0.2391\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2334 - val_loss: 0.2384\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2390 - val_loss: 0.2386\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.2378\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2398 - val_loss: 0.2373\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.2381\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2253 - val_loss: 0.2372\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2346 - val_loss: 0.2374\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2434 - val_loss: 0.2379\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2447 - val_loss: 0.2381\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2303 - val_loss: 0.2383\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2406 - val_loss: 0.2381\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2251 - val_loss: 0.2390\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2392 - val_loss: 0.2390\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2337 - val_loss: 0.2385\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2326 - val_loss: 0.2388\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2364 - val_loss: 0.2389\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.2369\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2318 - val_loss: 0.2369\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2321 - val_loss: 0.2375\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2351 - val_loss: 0.2380\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2332 - val_loss: 0.2379\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2316 - val_loss: 0.2382\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2205 - val_loss: 0.2395\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2299 - val_loss: 0.2396\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2295 - val_loss: 0.2392\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2282 - val_loss: 0.2393\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2295 - val_loss: 0.2399\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2286 - val_loss: 0.2396\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2366 - val_loss: 0.2407\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.2380\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2400 - val_loss: 0.2394\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2337 - val_loss: 0.2383\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2344 - val_loss: 0.2389\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2212 - val_loss: 0.2391\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2254 - val_loss: 0.2392\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2336 - val_loss: 0.2391\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2279 - val_loss: 0.2395\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.2396\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2276 - val_loss: 0.2405\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2292 - val_loss: 0.2393\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.2394\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2272 - val_loss: 0.2397\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2430 - val_loss: 0.2405\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2212 - val_loss: 0.2390\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2244 - val_loss: 0.2398\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2393 - val_loss: 0.2389\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.2396\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2235 - val_loss: 0.2392\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.2389\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2345 - val_loss: 0.2388\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2304 - val_loss: 0.2386\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2224 - val_loss: 0.2380\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2378 - val_loss: 0.2385\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2251 - val_loss: 0.2392\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2351 - val_loss: 0.2381\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2252 - val_loss: 0.2376\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2279 - val_loss: 0.2373\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2348 - val_loss: 0.2384\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2397 - val_loss: 0.2383\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2414 - val_loss: 0.2387\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2334 - val_loss: 0.2391\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2288 - val_loss: 0.2400\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.2399\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2225 - val_loss: 0.2384\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2330 - val_loss: 0.2386\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2366 - val_loss: 0.2386\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2345 - val_loss: 0.2385\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2197 - val_loss: 0.2369\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2320 - val_loss: 0.2382\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2393 - val_loss: 0.2382\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.2379\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2284 - val_loss: 0.2369\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2221 - val_loss: 0.2382\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2315 - val_loss: 0.2374\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2234 - val_loss: 0.2374\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2274 - val_loss: 0.2359\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.2349\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2245 - val_loss: 0.2346\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.2334\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.2346\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2276 - val_loss: 0.2357\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2332 - val_loss: 0.2357\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2260 - val_loss: 0.2363\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.2381\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2252 - val_loss: 0.2371\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.2380\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2304 - val_loss: 0.2377\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2316 - val_loss: 0.2379\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2183 - val_loss: 0.2368\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2221 - val_loss: 0.2373\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2254 - val_loss: 0.2384\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2302 - val_loss: 0.2374\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2233 - val_loss: 0.2369\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2385 - val_loss: 0.2366\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2222 - val_loss: 0.2363\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2266 - val_loss: 0.2369\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.2381\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2233 - val_loss: 0.2380\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2205 - val_loss: 0.2371\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2151 - val_loss: 0.2380\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2222 - val_loss: 0.2375\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2270 - val_loss: 0.2375\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.2373\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.2387\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2263 - val_loss: 0.2369\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2211 - val_loss: 0.2376\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2288 - val_loss: 0.2388\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2273 - val_loss: 0.2389\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2254 - val_loss: 0.2371\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2251 - val_loss: 0.2381\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2207 - val_loss: 0.2384\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2243 - val_loss: 0.2369\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2168 - val_loss: 0.2366\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.2361\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2245 - val_loss: 0.2364\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.2357\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2339 - val_loss: 0.2361\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2180 - val_loss: 0.2360\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2148 - val_loss: 0.2350\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2187 - val_loss: 0.2347\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.2361\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.2377\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2247 - val_loss: 0.2369\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.2381\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2290 - val_loss: 0.2373\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2237 - val_loss: 0.2360\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2228 - val_loss: 0.2360\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2152 - val_loss: 0.2358\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2210 - val_loss: 0.2367\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2230 - val_loss: 0.2352\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2213 - val_loss: 0.2364\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2291 - val_loss: 0.2364\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2292 - val_loss: 0.2361\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.2376\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2255 - val_loss: 0.2367\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2224 - val_loss: 0.2373\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2188 - val_loss: 0.2366\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2183 - val_loss: 0.2368\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2096 - val_loss: 0.2366\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2110 - val_loss: 0.2358\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2270 - val_loss: 0.2362\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.2365\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2226 - val_loss: 0.2377\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2180 - val_loss: 0.2351\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2174 - val_loss: 0.2352\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2314 - val_loss: 0.2352\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2061 - val_loss: 0.2356\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2279 - val_loss: 0.2364\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2243 - val_loss: 0.2376\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2165 - val_loss: 0.2371\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2174 - val_loss: 0.2355\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2135 - val_loss: 0.2374\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2190 - val_loss: 0.2368\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.2356\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2158 - val_loss: 0.2349\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2212 - val_loss: 0.2367\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2145 - val_loss: 0.2352\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2240 - val_loss: 0.2344\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.2341\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.2343\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2128 - val_loss: 0.2331\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2280 - val_loss: 0.2329\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.2338\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2250 - val_loss: 0.2331\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2176 - val_loss: 0.2341\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2034 - val_loss: 0.2331\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2152 - val_loss: 0.2331\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2111 - val_loss: 0.2333\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2175 - val_loss: 0.2339\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2208 - val_loss: 0.2363\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.2358\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2218 - val_loss: 0.2360\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2226 - val_loss: 0.2360\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.2357\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2187 - val_loss: 0.2354\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2261 - val_loss: 0.2354\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2202 - val_loss: 0.2350\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2160 - val_loss: 0.2349\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2119 - val_loss: 0.2347\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2117 - val_loss: 0.2349\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2109 - val_loss: 0.2346\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2191 - val_loss: 0.2342\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2123 - val_loss: 0.2340\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.2363\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2153 - val_loss: 0.2344\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2252 - val_loss: 0.2353\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2038 - val_loss: 0.2350\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2148 - val_loss: 0.2344\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2035 - val_loss: 0.2346\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2021 - val_loss: 0.2349\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2078 - val_loss: 0.2349\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2179 - val_loss: 0.2365\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2172 - val_loss: 0.2355\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2206 - val_loss: 0.2351\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.2357\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2160 - val_loss: 0.2362\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2357 - val_loss: 0.2362\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2252 - val_loss: 0.2346\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2207 - val_loss: 0.2346\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2173 - val_loss: 0.2339\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2090 - val_loss: 0.2351\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2035 - val_loss: 0.2341\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2119 - val_loss: 0.2342\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2131 - val_loss: 0.2345\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2203 - val_loss: 0.2325\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2158 - val_loss: 0.2331\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2103 - val_loss: 0.2319\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2126 - val_loss: 0.2336\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.2342\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.2332\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2219 - val_loss: 0.2325\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2041 - val_loss: 0.2346\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.2350\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2209 - val_loss: 0.2329\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2140 - val_loss: 0.2328\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2062 - val_loss: 0.2315\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2179 - val_loss: 0.2322\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2096 - val_loss: 0.2338\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.2330\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2033 - val_loss: 0.2333\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.2339\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2135 - val_loss: 0.2338\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2240 - val_loss: 0.2331\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2060 - val_loss: 0.2318\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2167 - val_loss: 0.2312\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2169 - val_loss: 0.2316\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2083 - val_loss: 0.2328\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.2323\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2104 - val_loss: 0.2328\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2102 - val_loss: 0.2327\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2148 - val_loss: 0.2324\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2110 - val_loss: 0.2325\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2068 - val_loss: 0.2332\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2085 - val_loss: 0.2339\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2032 - val_loss: 0.2331\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1970 - val_loss: 0.2328\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2155 - val_loss: 0.2338\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2109 - val_loss: 0.2338\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2157 - val_loss: 0.2335\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2128 - val_loss: 0.2336\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2034 - val_loss: 0.2330\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2109 - val_loss: 0.2331\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2172 - val_loss: 0.2328\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1970 - val_loss: 0.2316\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2133 - val_loss: 0.2322\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2097 - val_loss: 0.2321\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2064 - val_loss: 0.2323\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2068 - val_loss: 0.2335\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2339 - val_loss: 0.2322\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2050 - val_loss: 0.2315\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.2316\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2155 - val_loss: 0.2324\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2045 - val_loss: 0.2324\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2036 - val_loss: 0.2319\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2005 - val_loss: 0.2334\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2059 - val_loss: 0.2330\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1952 - val_loss: 0.2323\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2043 - val_loss: 0.2332\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.2326\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2098 - val_loss: 0.2334\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.2337\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2078 - val_loss: 0.2329\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2030 - val_loss: 0.2325\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2131 - val_loss: 0.2318\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2096 - val_loss: 0.2329\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2069 - val_loss: 0.2321\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2071 - val_loss: 0.2318\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2118 - val_loss: 0.2318\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.2322\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2152 - val_loss: 0.2320\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1982 - val_loss: 0.2323\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2041 - val_loss: 0.2326\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2043 - val_loss: 0.2334\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.2332\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2153 - val_loss: 0.2316\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2047 - val_loss: 0.2314\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2089 - val_loss: 0.2308\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2026 - val_loss: 0.2310\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2134 - val_loss: 0.2308\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2015 - val_loss: 0.2312\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2134 - val_loss: 0.2313\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2127 - val_loss: 0.2316\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2004 - val_loss: 0.2326\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2044 - val_loss: 0.2331\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2048 - val_loss: 0.2303\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1973 - val_loss: 0.2297\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2035 - val_loss: 0.2296\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2199 - val_loss: 0.2303\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2022 - val_loss: 0.2301\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2057 - val_loss: 0.2324\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2115 - val_loss: 0.2304\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2039 - val_loss: 0.2311\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2063 - val_loss: 0.2302\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.2300\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2111 - val_loss: 0.2309\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1985 - val_loss: 0.2307\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2089 - val_loss: 0.2302\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2101 - val_loss: 0.2313\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2041 - val_loss: 0.2310\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.2297\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2019 - val_loss: 0.2304\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2034 - val_loss: 0.2312\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2074 - val_loss: 0.2302\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2011 - val_loss: 0.2298\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1948 - val_loss: 0.2297\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2029 - val_loss: 0.2299\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1945 - val_loss: 0.2308\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.2298\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.2298\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2041 - val_loss: 0.2292\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2103 - val_loss: 0.2282\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2082 - val_loss: 0.2288\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2012 - val_loss: 0.2276\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1996 - val_loss: 0.2284\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2097 - val_loss: 0.2294\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1959 - val_loss: 0.2298\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2018 - val_loss: 0.2292\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2077 - val_loss: 0.2291\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.2298\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2139 - val_loss: 0.2290\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2108 - val_loss: 0.2301\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2057 - val_loss: 0.2293\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.2293\n",
            "7/7 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae6a1dc910>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number\n",
        "nb_epochs=1000;    # training epochs change\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition1   # choose 1st lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "SsLHq1n3Aduz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsUDTN5OO9sH"
      },
      "source": [
        "###3.3.3 2nd light condition as test###  "
      ],
      "id": "OsUDTN5OO9sH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GU2yN8WFPGQs",
        "outputId": "46655479-2a10-4f02-b892-66ff096da897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 4s 9ms/step - loss: 2.2083 - val_loss: 2.1821\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 2.1285 - val_loss: 2.1126\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 2.0406 - val_loss: 2.0435\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.9568 - val_loss: 1.9753\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.8722 - val_loss: 1.9028\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.7833 - val_loss: 1.8279\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.6937 - val_loss: 1.7510\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.6137 - val_loss: 1.6727\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.5224 - val_loss: 1.5935\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.4349 - val_loss: 1.5153\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 1.3517 - val_loss: 1.4381\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.2678 - val_loss: 1.3630\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.2037 - val_loss: 1.2896\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.1265 - val_loss: 1.2174\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.0595 - val_loss: 1.1473\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.9895 - val_loss: 1.0780\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.9325 - val_loss: 1.0155\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.8856 - val_loss: 0.9582\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.8391 - val_loss: 0.9029\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7964 - val_loss: 0.8526\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.7468 - val_loss: 0.8043\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7197 - val_loss: 0.7630\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6894 - val_loss: 0.7248\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6733 - val_loss: 0.6904\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6318 - val_loss: 0.6617\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6269 - val_loss: 0.6338\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6159 - val_loss: 0.6100\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5966 - val_loss: 0.5884\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5816 - val_loss: 0.5706\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5724 - val_loss: 0.5533\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.5596 - val_loss: 0.5404\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5530 - val_loss: 0.5299\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5411 - val_loss: 0.5182\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5233 - val_loss: 0.5109\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5439 - val_loss: 0.5032\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5146 - val_loss: 0.4954\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5116 - val_loss: 0.4918\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5088 - val_loss: 0.4853\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5093 - val_loss: 0.4811\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4976 - val_loss: 0.4788\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4931 - val_loss: 0.4767\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4808 - val_loss: 0.4727\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4730 - val_loss: 0.4696\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4654 - val_loss: 0.4687\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4720 - val_loss: 0.4661\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4731 - val_loss: 0.4649\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4638 - val_loss: 0.4623\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4577 - val_loss: 0.4615\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4574 - val_loss: 0.4594\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4447 - val_loss: 0.4584\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4635 - val_loss: 0.4574\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4633 - val_loss: 0.4547\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4446 - val_loss: 0.4548\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4512 - val_loss: 0.4511\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4563 - val_loss: 0.4501\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4557 - val_loss: 0.4519\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4452 - val_loss: 0.4506\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4286 - val_loss: 0.4495\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4366 - val_loss: 0.4483\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4525 - val_loss: 0.4476\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4359 - val_loss: 0.4465\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4390 - val_loss: 0.4465\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4308 - val_loss: 0.4441\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4300 - val_loss: 0.4444\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4216 - val_loss: 0.4428\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4312 - val_loss: 0.4401\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4211 - val_loss: 0.4399\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4318 - val_loss: 0.4397\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4211 - val_loss: 0.4378\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4094 - val_loss: 0.4374\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4189 - val_loss: 0.4378\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4155 - val_loss: 0.4362\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4117 - val_loss: 0.4360\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4115 - val_loss: 0.4353\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4134 - val_loss: 0.4360\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4070 - val_loss: 0.4347\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4034 - val_loss: 0.4341\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4047 - val_loss: 0.4347\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4066 - val_loss: 0.4334\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4184 - val_loss: 0.4325\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4072 - val_loss: 0.4320\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4128 - val_loss: 0.4312\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4101 - val_loss: 0.4291\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4049 - val_loss: 0.4307\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3989 - val_loss: 0.4285\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4041 - val_loss: 0.4279\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4067 - val_loss: 0.4260\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3969 - val_loss: 0.4286\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4025 - val_loss: 0.4284\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3964 - val_loss: 0.4283\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4112 - val_loss: 0.4275\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4036 - val_loss: 0.4250\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3880 - val_loss: 0.4234\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3968 - val_loss: 0.4222\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3879 - val_loss: 0.4234\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3954 - val_loss: 0.4220\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3822 - val_loss: 0.4217\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3817 - val_loss: 0.4227\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3931 - val_loss: 0.4211\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3887 - val_loss: 0.4208\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3815 - val_loss: 0.4234\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3926 - val_loss: 0.4221\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3930 - val_loss: 0.4212\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3876 - val_loss: 0.4219\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3775 - val_loss: 0.4208\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3804 - val_loss: 0.4200\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3793 - val_loss: 0.4201\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3792 - val_loss: 0.4183\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3725 - val_loss: 0.4187\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3919 - val_loss: 0.4181\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3797 - val_loss: 0.4183\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3750 - val_loss: 0.4154\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3774 - val_loss: 0.4162\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3735 - val_loss: 0.4138\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3765 - val_loss: 0.4145\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3654 - val_loss: 0.4161\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3814 - val_loss: 0.4148\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3696 - val_loss: 0.4151\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3746 - val_loss: 0.4136\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3688 - val_loss: 0.4148\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3683 - val_loss: 0.4142\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3674 - val_loss: 0.4138\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3745 - val_loss: 0.4104\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3637 - val_loss: 0.4118\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3644 - val_loss: 0.4128\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3619 - val_loss: 0.4117\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3554 - val_loss: 0.4106\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3629 - val_loss: 0.4097\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3624 - val_loss: 0.4081\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3579 - val_loss: 0.4092\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3611 - val_loss: 0.4089\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3555 - val_loss: 0.4095\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3592 - val_loss: 0.4067\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3679 - val_loss: 0.4074\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3637 - val_loss: 0.4084\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3414 - val_loss: 0.4074\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3562 - val_loss: 0.4076\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3563 - val_loss: 0.4075\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3592 - val_loss: 0.4084\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3522 - val_loss: 0.4071\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3542 - val_loss: 0.4060\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3538 - val_loss: 0.4059\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3550 - val_loss: 0.4063\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3487 - val_loss: 0.4067\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3661 - val_loss: 0.4058\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3660 - val_loss: 0.4047\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3384 - val_loss: 0.4044\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3417 - val_loss: 0.4058\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3568 - val_loss: 0.4059\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3489 - val_loss: 0.4059\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3549 - val_loss: 0.4065\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3413 - val_loss: 0.4054\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3446 - val_loss: 0.4063\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3396 - val_loss: 0.4062\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3579 - val_loss: 0.4049\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3558 - val_loss: 0.4069\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3491 - val_loss: 0.4053\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3565 - val_loss: 0.4048\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3585 - val_loss: 0.4048\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3314 - val_loss: 0.4047\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3536 - val_loss: 0.4039\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3464 - val_loss: 0.4035\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3429 - val_loss: 0.4021\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3602 - val_loss: 0.4008\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3396 - val_loss: 0.3998\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3391 - val_loss: 0.4012\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3369 - val_loss: 0.4021\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3294 - val_loss: 0.4016\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3353 - val_loss: 0.4025\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3402 - val_loss: 0.4003\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3460 - val_loss: 0.4004\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3359 - val_loss: 0.4009\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3296 - val_loss: 0.4009\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3391 - val_loss: 0.4006\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3436 - val_loss: 0.4008\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3269 - val_loss: 0.3983\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3398 - val_loss: 0.3975\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3361 - val_loss: 0.3978\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3421 - val_loss: 0.4003\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3338 - val_loss: 0.4005\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3339 - val_loss: 0.3985\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3368 - val_loss: 0.3984\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3272 - val_loss: 0.3995\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3319 - val_loss: 0.4004\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3351 - val_loss: 0.3984\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3325 - val_loss: 0.3994\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3204 - val_loss: 0.3973\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3364 - val_loss: 0.3976\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3390 - val_loss: 0.3979\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3284 - val_loss: 0.3970\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3281 - val_loss: 0.3972\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3210 - val_loss: 0.3977\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3267 - val_loss: 0.3974\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3251 - val_loss: 0.3956\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3354 - val_loss: 0.3967\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3286 - val_loss: 0.3965\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3301 - val_loss: 0.3965\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3372 - val_loss: 0.3956\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3240 - val_loss: 0.3955\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3282 - val_loss: 0.3941\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3248 - val_loss: 0.3947\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3232 - val_loss: 0.3944\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3329 - val_loss: 0.3941\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3211 - val_loss: 0.3940\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3289 - val_loss: 0.3939\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3229 - val_loss: 0.3934\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3177 - val_loss: 0.3935\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3179 - val_loss: 0.3928\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3251 - val_loss: 0.3926\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3103 - val_loss: 0.3924\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3217 - val_loss: 0.3937\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3201 - val_loss: 0.3937\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3326 - val_loss: 0.3924\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3304 - val_loss: 0.3928\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3180 - val_loss: 0.3933\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3136 - val_loss: 0.3931\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3159 - val_loss: 0.3936\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3191 - val_loss: 0.3935\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3155 - val_loss: 0.3951\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3172 - val_loss: 0.3953\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3175 - val_loss: 0.3932\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3236 - val_loss: 0.3942\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3116 - val_loss: 0.3941\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3239 - val_loss: 0.3931\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3210 - val_loss: 0.3934\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3186 - val_loss: 0.3934\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3205 - val_loss: 0.3950\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3193 - val_loss: 0.3939\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3101 - val_loss: 0.3945\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3200 - val_loss: 0.3942\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3125 - val_loss: 0.3942\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3014 - val_loss: 0.3958\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3130 - val_loss: 0.3946\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3151 - val_loss: 0.3949\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3197 - val_loss: 0.3944\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3268 - val_loss: 0.3924\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3002 - val_loss: 0.3927\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3219 - val_loss: 0.3938\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3094 - val_loss: 0.3927\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3056 - val_loss: 0.3926\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3091 - val_loss: 0.3932\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3173 - val_loss: 0.3923\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3131 - val_loss: 0.3927\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3096 - val_loss: 0.3920\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3160 - val_loss: 0.3918\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2998 - val_loss: 0.3916\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3019 - val_loss: 0.3913\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3230 - val_loss: 0.3917\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3082 - val_loss: 0.3904\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3217 - val_loss: 0.3914\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3117 - val_loss: 0.3910\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3227 - val_loss: 0.3930\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3126 - val_loss: 0.3924\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3091 - val_loss: 0.3906\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3115 - val_loss: 0.3896\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3037 - val_loss: 0.3917\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3054 - val_loss: 0.3919\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3114 - val_loss: 0.3918\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3139 - val_loss: 0.3918\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3155 - val_loss: 0.3916\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3138 - val_loss: 0.3883\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3103 - val_loss: 0.3885\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3018 - val_loss: 0.3902\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3005 - val_loss: 0.3898\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2950 - val_loss: 0.3897\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3004 - val_loss: 0.3909\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3027 - val_loss: 0.3911\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2983 - val_loss: 0.3903\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2996 - val_loss: 0.3901\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2963 - val_loss: 0.3898\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3078 - val_loss: 0.3896\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3020 - val_loss: 0.3897\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3056 - val_loss: 0.3908\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2996 - val_loss: 0.3918\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2921 - val_loss: 0.3920\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3077 - val_loss: 0.3908\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2978 - val_loss: 0.3907\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3110 - val_loss: 0.3908\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2985 - val_loss: 0.3907\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3017 - val_loss: 0.3887\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2946 - val_loss: 0.3891\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2962 - val_loss: 0.3885\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3005 - val_loss: 0.3887\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3037 - val_loss: 0.3896\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3035 - val_loss: 0.3899\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3033 - val_loss: 0.3890\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2919 - val_loss: 0.3872\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2947 - val_loss: 0.3871\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2991 - val_loss: 0.3892\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3013 - val_loss: 0.3891\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2929 - val_loss: 0.3901\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2939 - val_loss: 0.3906\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2951 - val_loss: 0.3899\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3047 - val_loss: 0.3908\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2974 - val_loss: 0.3895\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3064 - val_loss: 0.3904\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2993 - val_loss: 0.3910\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3022 - val_loss: 0.3890\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2908 - val_loss: 0.3894\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2984 - val_loss: 0.3904\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2807 - val_loss: 0.3891\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2944 - val_loss: 0.3895\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2896 - val_loss: 0.3902\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2922 - val_loss: 0.3887\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2898 - val_loss: 0.3891\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2984 - val_loss: 0.3903\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2970 - val_loss: 0.3901\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2935 - val_loss: 0.3891\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2914 - val_loss: 0.3904\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3037 - val_loss: 0.3917\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2960 - val_loss: 0.3918\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2839 - val_loss: 0.3923\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2920 - val_loss: 0.3920\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2887 - val_loss: 0.3915\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2916 - val_loss: 0.3918\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2970 - val_loss: 0.3904\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3019 - val_loss: 0.3902\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2861 - val_loss: 0.3919\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2960 - val_loss: 0.3919\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2841 - val_loss: 0.3919\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2893 - val_loss: 0.3930\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2886 - val_loss: 0.3924\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2988 - val_loss: 0.3937\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2920 - val_loss: 0.3933\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2895 - val_loss: 0.3925\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2899 - val_loss: 0.3939\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2820 - val_loss: 0.3944\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2960 - val_loss: 0.3930\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2933 - val_loss: 0.3937\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2822 - val_loss: 0.3949\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2893 - val_loss: 0.3940\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.2801 - val_loss: 0.3933\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2942 - val_loss: 0.3928\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2825 - val_loss: 0.3930\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2874 - val_loss: 0.3919\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2799 - val_loss: 0.3909\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2827 - val_loss: 0.3913\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2788 - val_loss: 0.3921\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.3946\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2811 - val_loss: 0.3941\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2869 - val_loss: 0.3929\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2872 - val_loss: 0.3933\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2824 - val_loss: 0.3929\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2855 - val_loss: 0.3947\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2891 - val_loss: 0.3935\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2902 - val_loss: 0.3937\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2836 - val_loss: 0.3920\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2792 - val_loss: 0.3928\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2823 - val_loss: 0.3933\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2857 - val_loss: 0.3944\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2861 - val_loss: 0.3949\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2787 - val_loss: 0.3936\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2752 - val_loss: 0.3934\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2870 - val_loss: 0.3934\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2844 - val_loss: 0.3944\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2759 - val_loss: 0.3918\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2772 - val_loss: 0.3904\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.3916\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.3915\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2740 - val_loss: 0.3915\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2889 - val_loss: 0.3912\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2802 - val_loss: 0.3920\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2773 - val_loss: 0.3933\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2870 - val_loss: 0.3946\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2761 - val_loss: 0.3959\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2785 - val_loss: 0.3961\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2738 - val_loss: 0.3966\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2854 - val_loss: 0.3975\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2706 - val_loss: 0.3963\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2840 - val_loss: 0.3940\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2775 - val_loss: 0.3958\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2795 - val_loss: 0.3965\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2682 - val_loss: 0.3957\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2787 - val_loss: 0.3950\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2769 - val_loss: 0.3948\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.3941\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2856 - val_loss: 0.3949\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.3940\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2813 - val_loss: 0.3935\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2756 - val_loss: 0.3927\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2730 - val_loss: 0.3928\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2655 - val_loss: 0.3942\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2702 - val_loss: 0.3937\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2749 - val_loss: 0.3930\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2672 - val_loss: 0.3936\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2718 - val_loss: 0.3933\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2720 - val_loss: 0.3919\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2810 - val_loss: 0.3933\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2744 - val_loss: 0.3934\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2806 - val_loss: 0.3923\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2626 - val_loss: 0.3926\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2742 - val_loss: 0.3927\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2773 - val_loss: 0.3924\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2755 - val_loss: 0.3922\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2711 - val_loss: 0.3908\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2821 - val_loss: 0.3890\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2707 - val_loss: 0.3895\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2775 - val_loss: 0.3904\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2758 - val_loss: 0.3921\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2656 - val_loss: 0.3930\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2794 - val_loss: 0.3925\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2838 - val_loss: 0.3913\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2672 - val_loss: 0.3923\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2703 - val_loss: 0.3913\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2578 - val_loss: 0.3918\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2834 - val_loss: 0.3924\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2813 - val_loss: 0.3928\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2684 - val_loss: 0.3937\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2777 - val_loss: 0.3920\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2706 - val_loss: 0.3922\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2690 - val_loss: 0.3919\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2824 - val_loss: 0.3928\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2611 - val_loss: 0.3941\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2646 - val_loss: 0.3925\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2662 - val_loss: 0.3920\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2762 - val_loss: 0.3921\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2674 - val_loss: 0.3912\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2730 - val_loss: 0.3929\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2753 - val_loss: 0.3925\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2690 - val_loss: 0.3931\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.3951\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2710 - val_loss: 0.3936\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2732 - val_loss: 0.3922\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2737 - val_loss: 0.3939\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2800 - val_loss: 0.3928\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2570 - val_loss: 0.3920\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2780 - val_loss: 0.3933\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.3925\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.3957\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2630 - val_loss: 0.3930\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2560 - val_loss: 0.3931\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2688 - val_loss: 0.3930\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2572 - val_loss: 0.3936\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2676 - val_loss: 0.3920\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2780 - val_loss: 0.3909\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2769 - val_loss: 0.3918\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2697 - val_loss: 0.3904\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2653 - val_loss: 0.3899\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2641 - val_loss: 0.3913\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2676 - val_loss: 0.3924\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2643 - val_loss: 0.3931\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.3924\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2651 - val_loss: 0.3935\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2619 - val_loss: 0.3929\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2605 - val_loss: 0.3922\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2649 - val_loss: 0.3907\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2701 - val_loss: 0.3903\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2600 - val_loss: 0.3900\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2643 - val_loss: 0.3882\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.3911\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2671 - val_loss: 0.3897\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2686 - val_loss: 0.3911\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2754 - val_loss: 0.3914\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.3931\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2579 - val_loss: 0.3907\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2538 - val_loss: 0.3895\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2534 - val_loss: 0.3886\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2680 - val_loss: 0.3877\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2633 - val_loss: 0.3899\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2588 - val_loss: 0.3903\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2676 - val_loss: 0.3888\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2624 - val_loss: 0.3888\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2556 - val_loss: 0.3892\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2639 - val_loss: 0.3900\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2679 - val_loss: 0.3901\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2658 - val_loss: 0.3873\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2666 - val_loss: 0.3880\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2705 - val_loss: 0.3872\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2691 - val_loss: 0.3883\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2600 - val_loss: 0.3895\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2647 - val_loss: 0.3904\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2754 - val_loss: 0.3914\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2630 - val_loss: 0.3918\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2552 - val_loss: 0.3887\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2623 - val_loss: 0.3879\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2652 - val_loss: 0.3884\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2567 - val_loss: 0.3879\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2541 - val_loss: 0.3873\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2608 - val_loss: 0.3881\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2612 - val_loss: 0.3862\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2523 - val_loss: 0.3878\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2550 - val_loss: 0.3880\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2658 - val_loss: 0.3882\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2589 - val_loss: 0.3896\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2481 - val_loss: 0.3905\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2567 - val_loss: 0.3879\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2587 - val_loss: 0.3863\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.3882\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2611 - val_loss: 0.3873\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2554 - val_loss: 0.3861\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2570 - val_loss: 0.3873\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2589 - val_loss: 0.3866\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2460 - val_loss: 0.3873\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2542 - val_loss: 0.3869\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2554 - val_loss: 0.3875\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2739 - val_loss: 0.3866\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2553 - val_loss: 0.3867\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2624 - val_loss: 0.3868\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2628 - val_loss: 0.3865\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2618 - val_loss: 0.3869\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2529 - val_loss: 0.3859\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2595 - val_loss: 0.3860\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2498 - val_loss: 0.3853\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2629 - val_loss: 0.3850\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2582 - val_loss: 0.3839\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2564 - val_loss: 0.3847\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2652 - val_loss: 0.3871\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2455 - val_loss: 0.3870\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2465 - val_loss: 0.3904\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2461 - val_loss: 0.3879\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2567 - val_loss: 0.3885\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2404 - val_loss: 0.3882\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2550 - val_loss: 0.3884\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2521 - val_loss: 0.3896\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2645 - val_loss: 0.3901\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2535 - val_loss: 0.3897\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2557 - val_loss: 0.3895\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2518 - val_loss: 0.3895\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2513 - val_loss: 0.3888\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2536 - val_loss: 0.3916\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2642 - val_loss: 0.3898\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2558 - val_loss: 0.3896\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2513 - val_loss: 0.3896\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2550 - val_loss: 0.3905\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.3885\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2562 - val_loss: 0.3886\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2581 - val_loss: 0.3859\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2549 - val_loss: 0.3866\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2427 - val_loss: 0.3863\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2665 - val_loss: 0.3855\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2601 - val_loss: 0.3877\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2522 - val_loss: 0.3875\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2485 - val_loss: 0.3858\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2530 - val_loss: 0.3848\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2583 - val_loss: 0.3856\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2456 - val_loss: 0.3862\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2492 - val_loss: 0.3871\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2500 - val_loss: 0.3875\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2441 - val_loss: 0.3872\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2510 - val_loss: 0.3879\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2527 - val_loss: 0.3867\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2482 - val_loss: 0.3866\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2416 - val_loss: 0.3865\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2522 - val_loss: 0.3866\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2451 - val_loss: 0.3867\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2662 - val_loss: 0.3876\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2474 - val_loss: 0.3874\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2476 - val_loss: 0.3857\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2618 - val_loss: 0.3851\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.3856\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2432 - val_loss: 0.3854\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2480 - val_loss: 0.3853\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2502 - val_loss: 0.3851\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2587 - val_loss: 0.3845\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2397 - val_loss: 0.3838\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2410 - val_loss: 0.3856\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2439 - val_loss: 0.3852\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2522 - val_loss: 0.3862\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2488 - val_loss: 0.3852\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2394 - val_loss: 0.3844\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2487 - val_loss: 0.3851\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2452 - val_loss: 0.3866\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2583 - val_loss: 0.3858\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2499 - val_loss: 0.3857\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2472 - val_loss: 0.3857\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2366 - val_loss: 0.3861\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2527 - val_loss: 0.3867\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2472 - val_loss: 0.3853\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.3846\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2364 - val_loss: 0.3838\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2339 - val_loss: 0.3854\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2532 - val_loss: 0.3875\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.3877\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.3857\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2440 - val_loss: 0.3858\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.3855\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2459 - val_loss: 0.3854\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2511 - val_loss: 0.3838\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2496 - val_loss: 0.3838\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2440 - val_loss: 0.3835\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2517 - val_loss: 0.3823\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2506 - val_loss: 0.3841\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2359 - val_loss: 0.3858\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2399 - val_loss: 0.3856\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2476 - val_loss: 0.3862\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2407 - val_loss: 0.3845\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2392 - val_loss: 0.3855\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2476 - val_loss: 0.3850\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2382 - val_loss: 0.3844\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2416 - val_loss: 0.3861\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2475 - val_loss: 0.3864\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2363 - val_loss: 0.3859\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2324 - val_loss: 0.3852\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2368 - val_loss: 0.3848\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2485 - val_loss: 0.3858\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2475 - val_loss: 0.3853\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2457 - val_loss: 0.3846\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2491 - val_loss: 0.3865\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2387 - val_loss: 0.3845\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2437 - val_loss: 0.3850\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2424 - val_loss: 0.3832\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2488 - val_loss: 0.3851\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2296 - val_loss: 0.3860\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2385 - val_loss: 0.3863\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2426 - val_loss: 0.3857\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2466 - val_loss: 0.3850\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2458 - val_loss: 0.3835\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2399 - val_loss: 0.3852\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2402 - val_loss: 0.3822\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2494 - val_loss: 0.3812\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2503 - val_loss: 0.3800\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2387 - val_loss: 0.3810\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2452 - val_loss: 0.3808\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2370 - val_loss: 0.3816\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2379 - val_loss: 0.3816\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2394 - val_loss: 0.3805\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2325 - val_loss: 0.3820\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2459 - val_loss: 0.3813\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2326 - val_loss: 0.3799\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2391 - val_loss: 0.3825\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2312 - val_loss: 0.3822\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.3832\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2325 - val_loss: 0.3816\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2328 - val_loss: 0.3814\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2444 - val_loss: 0.3808\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2444 - val_loss: 0.3796\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2423 - val_loss: 0.3800\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2318 - val_loss: 0.3809\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2492 - val_loss: 0.3811\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2363 - val_loss: 0.3805\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2395 - val_loss: 0.3814\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2353 - val_loss: 0.3796\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2455 - val_loss: 0.3789\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2396 - val_loss: 0.3778\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.3774\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2347 - val_loss: 0.3796\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2470 - val_loss: 0.3788\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2355 - val_loss: 0.3785\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2432 - val_loss: 0.3796\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2443 - val_loss: 0.3776\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2318 - val_loss: 0.3776\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2372 - val_loss: 0.3774\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2349 - val_loss: 0.3780\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2403 - val_loss: 0.3756\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2370 - val_loss: 0.3760\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2370 - val_loss: 0.3766\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.3758\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2429 - val_loss: 0.3765\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2419 - val_loss: 0.3768\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2354 - val_loss: 0.3759\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2268 - val_loss: 0.3769\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2350 - val_loss: 0.3751\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.3762\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.3764\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2495 - val_loss: 0.3756\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2364 - val_loss: 0.3772\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2457 - val_loss: 0.3767\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2389 - val_loss: 0.3757\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2436 - val_loss: 0.3766\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2323 - val_loss: 0.3760\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2400 - val_loss: 0.3755\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2335 - val_loss: 0.3740\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2311 - val_loss: 0.3762\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2330 - val_loss: 0.3763\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2403 - val_loss: 0.3743\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2454 - val_loss: 0.3745\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2280 - val_loss: 0.3766\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.3765\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2420 - val_loss: 0.3780\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2327 - val_loss: 0.3770\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2343 - val_loss: 0.3763\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.3749\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.3748\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2341 - val_loss: 0.3776\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2405 - val_loss: 0.3765\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2428 - val_loss: 0.3743\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2404 - val_loss: 0.3759\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2316 - val_loss: 0.3752\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2464 - val_loss: 0.3724\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2291 - val_loss: 0.3741\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2374 - val_loss: 0.3753\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2322 - val_loss: 0.3769\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.3743\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2354 - val_loss: 0.3756\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2268 - val_loss: 0.3760\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2303 - val_loss: 0.3771\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2421 - val_loss: 0.3763\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2298 - val_loss: 0.3764\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.3772\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2400 - val_loss: 0.3763\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.3761\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2261 - val_loss: 0.3755\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2321 - val_loss: 0.3752\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2324 - val_loss: 0.3748\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2400 - val_loss: 0.3766\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2335 - val_loss: 0.3756\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.3755\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.3744\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2340 - val_loss: 0.3723\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2389 - val_loss: 0.3719\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2367 - val_loss: 0.3710\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2271 - val_loss: 0.3723\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2401 - val_loss: 0.3733\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2268 - val_loss: 0.3726\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.3729\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.3726\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2315 - val_loss: 0.3720\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2256 - val_loss: 0.3709\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2249 - val_loss: 0.3716\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2362 - val_loss: 0.3698\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2301 - val_loss: 0.3680\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2261 - val_loss: 0.3690\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2258 - val_loss: 0.3700\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2263 - val_loss: 0.3709\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2142 - val_loss: 0.3713\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2223 - val_loss: 0.3698\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2243 - val_loss: 0.3709\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2303 - val_loss: 0.3695\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2260 - val_loss: 0.3695\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2345 - val_loss: 0.3701\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.3687\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2216 - val_loss: 0.3696\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2218 - val_loss: 0.3709\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2276 - val_loss: 0.3724\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2270 - val_loss: 0.3714\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2282 - val_loss: 0.3705\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2246 - val_loss: 0.3715\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2277 - val_loss: 0.3712\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.3722\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2282 - val_loss: 0.3703\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2194 - val_loss: 0.3683\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2395 - val_loss: 0.3681\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2192 - val_loss: 0.3691\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2300 - val_loss: 0.3711\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2257 - val_loss: 0.3695\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.3667\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.3669\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2321 - val_loss: 0.3669\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.3652\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2323 - val_loss: 0.3656\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2292 - val_loss: 0.3642\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2310 - val_loss: 0.3634\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.3636\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2309 - val_loss: 0.3636\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2108 - val_loss: 0.3658\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2274 - val_loss: 0.3669\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2237 - val_loss: 0.3658\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2310 - val_loss: 0.3678\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.3668\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2267 - val_loss: 0.3662\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2332 - val_loss: 0.3654\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2368 - val_loss: 0.3646\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2230 - val_loss: 0.3642\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2333 - val_loss: 0.3645\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2290 - val_loss: 0.3657\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2352 - val_loss: 0.3667\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2302 - val_loss: 0.3678\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2361 - val_loss: 0.3669\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2221 - val_loss: 0.3694\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2166 - val_loss: 0.3680\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2269 - val_loss: 0.3657\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2177 - val_loss: 0.3644\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2223 - val_loss: 0.3636\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2227 - val_loss: 0.3648\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2163 - val_loss: 0.3635\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2170 - val_loss: 0.3622\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2267 - val_loss: 0.3636\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2276 - val_loss: 0.3626\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2343 - val_loss: 0.3649\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.3644\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2322 - val_loss: 0.3644\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2199 - val_loss: 0.3653\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2202 - val_loss: 0.3647\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2240 - val_loss: 0.3648\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2260 - val_loss: 0.3659\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2233 - val_loss: 0.3647\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.3662\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.3645\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2323 - val_loss: 0.3659\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2173 - val_loss: 0.3651\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2304 - val_loss: 0.3666\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2331 - val_loss: 0.3682\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2230 - val_loss: 0.3680\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2283 - val_loss: 0.3676\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2308 - val_loss: 0.3666\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2258 - val_loss: 0.3652\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.3649\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.3649\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2248 - val_loss: 0.3642\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2097 - val_loss: 0.3626\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2182 - val_loss: 0.3642\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2227 - val_loss: 0.3650\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2124 - val_loss: 0.3642\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2305 - val_loss: 0.3664\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2109 - val_loss: 0.3665\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.3663\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2180 - val_loss: 0.3666\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2227 - val_loss: 0.3659\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2217 - val_loss: 0.3650\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2140 - val_loss: 0.3649\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2244 - val_loss: 0.3645\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.3656\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2307 - val_loss: 0.3648\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.3640\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2212 - val_loss: 0.3637\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2123 - val_loss: 0.3636\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2187 - val_loss: 0.3651\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2147 - val_loss: 0.3650\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2157 - val_loss: 0.3642\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2201 - val_loss: 0.3630\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2242 - val_loss: 0.3633\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2237 - val_loss: 0.3641\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2263 - val_loss: 0.3630\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2214 - val_loss: 0.3623\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2181 - val_loss: 0.3645\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2232 - val_loss: 0.3645\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2319 - val_loss: 0.3632\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2175 - val_loss: 0.3633\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2161 - val_loss: 0.3628\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2139 - val_loss: 0.3630\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.3644\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2246 - val_loss: 0.3637\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2275 - val_loss: 0.3622\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2162 - val_loss: 0.3623\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.3626\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2192 - val_loss: 0.3632\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.3615\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2155 - val_loss: 0.3619\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2196 - val_loss: 0.3615\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2169 - val_loss: 0.3611\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2177 - val_loss: 0.3634\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2265 - val_loss: 0.3610\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2102 - val_loss: 0.3600\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2147 - val_loss: 0.3609\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2257 - val_loss: 0.3605\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2105 - val_loss: 0.3601\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2151 - val_loss: 0.3598\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2158 - val_loss: 0.3592\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2105 - val_loss: 0.3600\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.3612\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.3595\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2122 - val_loss: 0.3596\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2082 - val_loss: 0.3574\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.3582\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2107 - val_loss: 0.3588\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2168 - val_loss: 0.3592\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2184 - val_loss: 0.3594\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2251 - val_loss: 0.3584\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2092 - val_loss: 0.3594\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2213 - val_loss: 0.3588\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2082 - val_loss: 0.3600\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2190 - val_loss: 0.3609\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2252 - val_loss: 0.3587\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2171 - val_loss: 0.3600\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2190 - val_loss: 0.3606\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2144 - val_loss: 0.3590\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2142 - val_loss: 0.3609\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2234 - val_loss: 0.3595\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2085 - val_loss: 0.3586\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2098 - val_loss: 0.3589\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2118 - val_loss: 0.3571\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2051 - val_loss: 0.3565\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2209 - val_loss: 0.3542\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2158 - val_loss: 0.3542\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2102 - val_loss: 0.3548\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2111 - val_loss: 0.3547\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2208 - val_loss: 0.3553\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2188 - val_loss: 0.3547\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2146 - val_loss: 0.3554\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2055 - val_loss: 0.3554\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2078 - val_loss: 0.3571\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.3569\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2117 - val_loss: 0.3564\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2182 - val_loss: 0.3570\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2212 - val_loss: 0.3571\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2138 - val_loss: 0.3580\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2079 - val_loss: 0.3586\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2127 - val_loss: 0.3551\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2185 - val_loss: 0.3548\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2222 - val_loss: 0.3548\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2144 - val_loss: 0.3550\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2152 - val_loss: 0.3552\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2160 - val_loss: 0.3554\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2137 - val_loss: 0.3549\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2090 - val_loss: 0.3563\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2104 - val_loss: 0.3561\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2027 - val_loss: 0.3547\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2108 - val_loss: 0.3565\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.3546\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2183 - val_loss: 0.3539\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2110 - val_loss: 0.3525\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2140 - val_loss: 0.3509\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.3501\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2090 - val_loss: 0.3514\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2113 - val_loss: 0.3506\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2153 - val_loss: 0.3532\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.3539\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2110 - val_loss: 0.3534\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2015 - val_loss: 0.3519\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2121 - val_loss: 0.3507\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.3511\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2162 - val_loss: 0.3517\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2106 - val_loss: 0.3524\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2135 - val_loss: 0.3510\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2133 - val_loss: 0.3505\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2005 - val_loss: 0.3522\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2089 - val_loss: 0.3530\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2091 - val_loss: 0.3502\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2057 - val_loss: 0.3491\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2021 - val_loss: 0.3488\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2093 - val_loss: 0.3512\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2079 - val_loss: 0.3498\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2130 - val_loss: 0.3516\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2068 - val_loss: 0.3533\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2104 - val_loss: 0.3534\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2139 - val_loss: 0.3521\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2104 - val_loss: 0.3492\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2045 - val_loss: 0.3517\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2177 - val_loss: 0.3520\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2079 - val_loss: 0.3517\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2167 - val_loss: 0.3527\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2103 - val_loss: 0.3538\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.3525\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2163 - val_loss: 0.3516\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2076 - val_loss: 0.3505\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2088 - val_loss: 0.3490\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2025 - val_loss: 0.3479\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2052 - val_loss: 0.3486\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2081 - val_loss: 0.3485\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.3503\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2095 - val_loss: 0.3507\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2072 - val_loss: 0.3513\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.3502\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.3497\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2044 - val_loss: 0.3502\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2090 - val_loss: 0.3512\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1951 - val_loss: 0.3503\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2071 - val_loss: 0.3495\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2106 - val_loss: 0.3500\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.3469\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1970 - val_loss: 0.3488\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2103 - val_loss: 0.3474\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2081 - val_loss: 0.3490\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2098 - val_loss: 0.3473\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2153 - val_loss: 0.3484\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2159 - val_loss: 0.3468\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2045 - val_loss: 0.3479\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2005 - val_loss: 0.3475\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.3465\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2173 - val_loss: 0.3469\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2049 - val_loss: 0.3465\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.3465\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2103 - val_loss: 0.3468\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2138 - val_loss: 0.3455\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2091 - val_loss: 0.3455\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2056 - val_loss: 0.3455\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2029 - val_loss: 0.3456\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2047 - val_loss: 0.3480\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2138 - val_loss: 0.3467\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2053 - val_loss: 0.3463\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1979 - val_loss: 0.3461\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1992 - val_loss: 0.3455\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2105 - val_loss: 0.3453\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1910 - val_loss: 0.3467\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2003 - val_loss: 0.3460\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2010 - val_loss: 0.3452\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2009 - val_loss: 0.3453\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1910 - val_loss: 0.3425\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2021 - val_loss: 0.3431\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2067 - val_loss: 0.3435\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1867 - val_loss: 0.3445\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1968 - val_loss: 0.3465\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1974 - val_loss: 0.3473\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2076 - val_loss: 0.3454\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1946 - val_loss: 0.3442\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2018 - val_loss: 0.3437\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2048 - val_loss: 0.3419\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2054 - val_loss: 0.3424\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1982 - val_loss: 0.3432\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2033 - val_loss: 0.3426\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2226 - val_loss: 0.3440\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2007 - val_loss: 0.3421\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2024 - val_loss: 0.3431\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1974 - val_loss: 0.3438\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2006 - val_loss: 0.3429\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2039 - val_loss: 0.3452\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2000 - val_loss: 0.3438\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2120 - val_loss: 0.3435\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2050 - val_loss: 0.3434\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2002 - val_loss: 0.3425\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1953 - val_loss: 0.3425\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2017 - val_loss: 0.3428\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2022 - val_loss: 0.3437\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1969 - val_loss: 0.3424\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2028 - val_loss: 0.3426\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2038 - val_loss: 0.3419\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2013 - val_loss: 0.3421\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2001 - val_loss: 0.3414\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1951 - val_loss: 0.3419\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1994 - val_loss: 0.3394\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3394\n",
            "7/7 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae1637f610>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number\n",
        "nb_epochs=1000;    # training epochs change\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition2   # choose 2nd lighting condition for test set\n",
        "\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "GU2yN8WFPGQs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNKobK2ePIbf"
      },
      "source": [
        "###3.3.4 3rd light condition as test###  "
      ],
      "id": "cNKobK2ePIbf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B2_a5xoIPJTv",
        "outputId": "3515123e-a3ca-441e-e54c-e06eab0b024b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 4s 10ms/step - loss: 1.8740 - val_loss: 1.7680\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 1.7586 - val_loss: 1.6980\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.6712 - val_loss: 1.6347\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.5785 - val_loss: 1.5690\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.4707 - val_loss: 1.5052\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.3773 - val_loss: 1.4389\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.2709 - val_loss: 1.3743\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.1774 - val_loss: 1.3107\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.1031 - val_loss: 1.2498\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.0070 - val_loss: 1.1919\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.9308 - val_loss: 1.1369\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.8468 - val_loss: 1.0862\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7893 - val_loss: 1.0393\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7240 - val_loss: 0.9970\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6601 - val_loss: 0.9576\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6141 - val_loss: 0.9241\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5684 - val_loss: 0.8951\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5331 - val_loss: 0.8703\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5117 - val_loss: 0.8521\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4728 - val_loss: 0.8357\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4516 - val_loss: 0.8230\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4289 - val_loss: 0.8148\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4157 - val_loss: 0.8088\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4046 - val_loss: 0.8051\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.8041\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3814 - val_loss: 0.8041\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3859 - val_loss: 0.8047\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3667 - val_loss: 0.8075\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3671 - val_loss: 0.8107\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3553 - val_loss: 0.8137\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3464 - val_loss: 0.8172\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3428 - val_loss: 0.8200\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3453 - val_loss: 0.8260\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3450 - val_loss: 0.8292\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3388 - val_loss: 0.8321\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3340 - val_loss: 0.8365\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3338 - val_loss: 0.8396\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3306 - val_loss: 0.8429\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3246 - val_loss: 0.8466\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3302 - val_loss: 0.8478\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3403 - val_loss: 0.8508\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3327 - val_loss: 0.8543\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3250 - val_loss: 0.8556\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3163 - val_loss: 0.8586\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3192 - val_loss: 0.8588\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3145 - val_loss: 0.8613\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3165 - val_loss: 0.8637\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3167 - val_loss: 0.8626\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3120 - val_loss: 0.8652\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3090 - val_loss: 0.8673\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3104 - val_loss: 0.8685\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3142 - val_loss: 0.8685\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3112 - val_loss: 0.8703\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3242 - val_loss: 0.8683\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3055 - val_loss: 0.8704\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3084 - val_loss: 0.8711\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3164 - val_loss: 0.8728\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3071 - val_loss: 0.8743\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2987 - val_loss: 0.8750\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3082 - val_loss: 0.8771\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2997 - val_loss: 0.8819\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3043 - val_loss: 0.8770\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3102 - val_loss: 0.8784\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2922 - val_loss: 0.8789\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3103 - val_loss: 0.8804\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2973 - val_loss: 0.8814\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2935 - val_loss: 0.8833\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2965 - val_loss: 0.8852\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2989 - val_loss: 0.8816\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3072 - val_loss: 0.8828\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3120 - val_loss: 0.8806\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3003 - val_loss: 0.8799\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3025 - val_loss: 0.8781\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3029 - val_loss: 0.8794\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2875 - val_loss: 0.8825\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3060 - val_loss: 0.8802\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2925 - val_loss: 0.8828\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2905 - val_loss: 0.8824\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2947 - val_loss: 0.8833\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2859 - val_loss: 0.8888\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2956 - val_loss: 0.8860\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2806 - val_loss: 0.8867\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2940 - val_loss: 0.8901\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2957 - val_loss: 0.8870\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2885 - val_loss: 0.8846\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2838 - val_loss: 0.8825\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2938 - val_loss: 0.8805\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2830 - val_loss: 0.8821\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2912 - val_loss: 0.8817\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3014 - val_loss: 0.8788\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2925 - val_loss: 0.8791\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2826 - val_loss: 0.8795\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2910 - val_loss: 0.8790\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2762 - val_loss: 0.8832\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2839 - val_loss: 0.8803\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2973 - val_loss: 0.8746\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2734 - val_loss: 0.8757\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2855 - val_loss: 0.8737\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2892 - val_loss: 0.8724\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2712 - val_loss: 0.8721\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2913 - val_loss: 0.8745\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2798 - val_loss: 0.8717\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2756 - val_loss: 0.8733\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2877 - val_loss: 0.8690\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2832 - val_loss: 0.8693\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2846 - val_loss: 0.8657\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2738 - val_loss: 0.8699\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2795 - val_loss: 0.8680\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2699 - val_loss: 0.8706\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2694 - val_loss: 0.8697\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2760 - val_loss: 0.8672\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2753 - val_loss: 0.8692\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2665 - val_loss: 0.8698\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2826 - val_loss: 0.8721\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2716 - val_loss: 0.8684\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2787 - val_loss: 0.8660\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.8662\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2802 - val_loss: 0.8632\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2716 - val_loss: 0.8633\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2753 - val_loss: 0.8637\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2721 - val_loss: 0.8648\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2791 - val_loss: 0.8639\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2738 - val_loss: 0.8619\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2757 - val_loss: 0.8643\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2679 - val_loss: 0.8635\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2723 - val_loss: 0.8639\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2665 - val_loss: 0.8587\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2692 - val_loss: 0.8586\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2653 - val_loss: 0.8564\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2689 - val_loss: 0.8596\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2652 - val_loss: 0.8614\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2643 - val_loss: 0.8592\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.8554\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2669 - val_loss: 0.8561\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2661 - val_loss: 0.8546\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2696 - val_loss: 0.8540\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2689 - val_loss: 0.8521\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2677 - val_loss: 0.8541\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2709 - val_loss: 0.8551\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.8551\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2718 - val_loss: 0.8506\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2672 - val_loss: 0.8460\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.2659 - val_loss: 0.8450\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.2592 - val_loss: 0.8451\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.2566 - val_loss: 0.8455\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.2709 - val_loss: 0.8460\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2595 - val_loss: 0.8436\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.2600 - val_loss: 0.8448\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2464 - val_loss: 0.8449\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2647 - val_loss: 0.8452\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2628 - val_loss: 0.8448\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2587 - val_loss: 0.8425\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2559 - val_loss: 0.8416\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2582 - val_loss: 0.8413\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2628 - val_loss: 0.8397\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2562 - val_loss: 0.8399\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2586 - val_loss: 0.8381\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2589 - val_loss: 0.8378\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2582 - val_loss: 0.8356\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2635 - val_loss: 0.8365\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2575 - val_loss: 0.8347\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2641 - val_loss: 0.8355\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2537 - val_loss: 0.8333\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2587 - val_loss: 0.8311\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2610 - val_loss: 0.8292\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2581 - val_loss: 0.8298\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2574 - val_loss: 0.8342\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2607 - val_loss: 0.8345\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2598 - val_loss: 0.8296\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2486 - val_loss: 0.8290\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2645 - val_loss: 0.8257\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2520 - val_loss: 0.8295\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2452 - val_loss: 0.8293\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2508 - val_loss: 0.8273\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2504 - val_loss: 0.8277\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2478 - val_loss: 0.8246\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2621 - val_loss: 0.8234\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2507 - val_loss: 0.8215\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2533 - val_loss: 0.8249\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2486 - val_loss: 0.8246\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2607 - val_loss: 0.8242\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2510 - val_loss: 0.8217\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2532 - val_loss: 0.8205\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2520 - val_loss: 0.8192\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2517 - val_loss: 0.8169\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2508 - val_loss: 0.8153\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2427 - val_loss: 0.8131\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2435 - val_loss: 0.8136\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2470 - val_loss: 0.8149\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2557 - val_loss: 0.8154\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2590 - val_loss: 0.8163\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2547 - val_loss: 0.8167\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2507 - val_loss: 0.8174\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2464 - val_loss: 0.8139\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2436 - val_loss: 0.8106\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2491 - val_loss: 0.8146\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2427 - val_loss: 0.8166\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2499 - val_loss: 0.8129\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2449 - val_loss: 0.8099\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2402 - val_loss: 0.8112\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2396 - val_loss: 0.8091\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2512 - val_loss: 0.8133\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2500 - val_loss: 0.8053\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2320 - val_loss: 0.8032\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2326 - val_loss: 0.8027\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2496 - val_loss: 0.8075\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2410 - val_loss: 0.8049\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2417 - val_loss: 0.8046\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2467 - val_loss: 0.7994\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2389 - val_loss: 0.7989\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2424 - val_loss: 0.8041\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.8006\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2415 - val_loss: 0.8047\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2336 - val_loss: 0.8022\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2504 - val_loss: 0.7997\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2440 - val_loss: 0.8012\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2329 - val_loss: 0.8019\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2416 - val_loss: 0.8036\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2421 - val_loss: 0.8052\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2497 - val_loss: 0.8045\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2386 - val_loss: 0.8036\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2453 - val_loss: 0.8025\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2372 - val_loss: 0.8020\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2423 - val_loss: 0.8015\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2417 - val_loss: 0.8006\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2311 - val_loss: 0.8000\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2380 - val_loss: 0.7980\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2475 - val_loss: 0.7980\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2425 - val_loss: 0.7968\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2467 - val_loss: 0.7958\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2326 - val_loss: 0.7939\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2467 - val_loss: 0.7950\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2425 - val_loss: 0.7924\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2293 - val_loss: 0.7948\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2527 - val_loss: 0.7969\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2262 - val_loss: 0.7975\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2398 - val_loss: 0.7978\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2299 - val_loss: 0.7964\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2420 - val_loss: 0.7946\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2399 - val_loss: 0.7920\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2428 - val_loss: 0.7916\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.7914\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2426 - val_loss: 0.7873\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2325 - val_loss: 0.7892\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2267 - val_loss: 0.7888\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2251 - val_loss: 0.7889\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2284 - val_loss: 0.7868\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2320 - val_loss: 0.7867\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2437 - val_loss: 0.7838\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2390 - val_loss: 0.7888\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2310 - val_loss: 0.7912\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2332 - val_loss: 0.7897\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.7916\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2385 - val_loss: 0.7921\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2379 - val_loss: 0.7957\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2376 - val_loss: 0.7948\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2284 - val_loss: 0.7946\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2331 - val_loss: 0.7960\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2313 - val_loss: 0.7896\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2315 - val_loss: 0.7903\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2332 - val_loss: 0.7902\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2335 - val_loss: 0.7879\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.7917\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2353 - val_loss: 0.7831\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2285 - val_loss: 0.7864\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2352 - val_loss: 0.7861\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2277 - val_loss: 0.7895\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2206 - val_loss: 0.7876\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2476 - val_loss: 0.7846\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2288 - val_loss: 0.7853\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2372 - val_loss: 0.7882\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2282 - val_loss: 0.7927\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2265 - val_loss: 0.7906\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2193 - val_loss: 0.7936\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2292 - val_loss: 0.7930\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2230 - val_loss: 0.7918\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2225 - val_loss: 0.7907\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2234 - val_loss: 0.7933\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2250 - val_loss: 0.7942\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2351 - val_loss: 0.7961\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2248 - val_loss: 0.7940\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2314 - val_loss: 0.7933\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.7859\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.7875\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2231 - val_loss: 0.7912\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2172 - val_loss: 0.7898\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2172 - val_loss: 0.7888\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2231 - val_loss: 0.7874\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2250 - val_loss: 0.7870\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2160 - val_loss: 0.7889\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.7861\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2207 - val_loss: 0.7854\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2193 - val_loss: 0.7854\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2338 - val_loss: 0.7845\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2282 - val_loss: 0.7867\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2287 - val_loss: 0.7834\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2163 - val_loss: 0.7811\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2252 - val_loss: 0.7829\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2256 - val_loss: 0.7880\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2195 - val_loss: 0.7870\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2190 - val_loss: 0.7875\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2240 - val_loss: 0.7854\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2198 - val_loss: 0.7889\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2260 - val_loss: 0.7862\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2197 - val_loss: 0.7827\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2117 - val_loss: 0.7840\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2226 - val_loss: 0.7806\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2226 - val_loss: 0.7801\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2324 - val_loss: 0.7809\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2168 - val_loss: 0.7815\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2177 - val_loss: 0.7842\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.7816\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.7787\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.7830\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.7776\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2101 - val_loss: 0.7806\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2125 - val_loss: 0.7804\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2230 - val_loss: 0.7817\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2245 - val_loss: 0.7817\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2158 - val_loss: 0.7812\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2222 - val_loss: 0.7813\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2119 - val_loss: 0.7817\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2238 - val_loss: 0.7816\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.7810\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2218 - val_loss: 0.7782\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2219 - val_loss: 0.7794\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2162 - val_loss: 0.7810\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2223 - val_loss: 0.7837\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2107 - val_loss: 0.7863\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2081 - val_loss: 0.7841\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2163 - val_loss: 0.7838\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1970 - val_loss: 0.7827\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2105 - val_loss: 0.7843\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2147 - val_loss: 0.7847\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2123 - val_loss: 0.7831\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2175 - val_loss: 0.7811\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2190 - val_loss: 0.7815\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2060 - val_loss: 0.7810\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2130 - val_loss: 0.7851\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2110 - val_loss: 0.7880\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2196 - val_loss: 0.7798\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2187 - val_loss: 0.7826\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.7772\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2110 - val_loss: 0.7815\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2096 - val_loss: 0.7805\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2140 - val_loss: 0.7816\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2070 - val_loss: 0.7804\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2074 - val_loss: 0.7837\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2179 - val_loss: 0.7839\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2116 - val_loss: 0.7829\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2085 - val_loss: 0.7814\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2063 - val_loss: 0.7844\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.7870\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2118 - val_loss: 0.7867\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2154 - val_loss: 0.7861\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2154 - val_loss: 0.7828\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.7821\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.7780\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.7744\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2008 - val_loss: 0.7788\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2039 - val_loss: 0.7756\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2066 - val_loss: 0.7728\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2025 - val_loss: 0.7743\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2079 - val_loss: 0.7758\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.7771\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2022 - val_loss: 0.7785\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2162 - val_loss: 0.7763\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2018 - val_loss: 0.7757\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2076 - val_loss: 0.7741\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1983 - val_loss: 0.7771\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2008 - val_loss: 0.7763\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2051 - val_loss: 0.7759\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2012 - val_loss: 0.7776\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2134 - val_loss: 0.7738\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2168 - val_loss: 0.7744\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1988 - val_loss: 0.7746\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2145 - val_loss: 0.7730\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1967 - val_loss: 0.7688\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2016 - val_loss: 0.7721\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2047 - val_loss: 0.7719\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2071 - val_loss: 0.7701\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2080 - val_loss: 0.7684\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2029 - val_loss: 0.7706\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2026 - val_loss: 0.7716\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2034 - val_loss: 0.7719\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1969 - val_loss: 0.7736\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1907 - val_loss: 0.7761\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2114 - val_loss: 0.7774\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2042 - val_loss: 0.7768\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2017 - val_loss: 0.7764\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1957 - val_loss: 0.7737\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.7772\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2001 - val_loss: 0.7776\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2035 - val_loss: 0.7729\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2112 - val_loss: 0.7770\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2034 - val_loss: 0.7726\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2008 - val_loss: 0.7765\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1992 - val_loss: 0.7773\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1921 - val_loss: 0.7790\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1941 - val_loss: 0.7771\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2151 - val_loss: 0.7761\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1989 - val_loss: 0.7773\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2003 - val_loss: 0.7767\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2000 - val_loss: 0.7779\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1961 - val_loss: 0.7787\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2000 - val_loss: 0.7795\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1971 - val_loss: 0.7799\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1970 - val_loss: 0.7832\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2051 - val_loss: 0.7825\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1951 - val_loss: 0.7793\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2065 - val_loss: 0.7751\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1967 - val_loss: 0.7751\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2017 - val_loss: 0.7735\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1918 - val_loss: 0.7733\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1890 - val_loss: 0.7784\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.7783\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1906 - val_loss: 0.7847\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1961 - val_loss: 0.7836\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1997 - val_loss: 0.7852\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2000 - val_loss: 0.7821\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1905 - val_loss: 0.7825\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2015 - val_loss: 0.7862\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1951 - val_loss: 0.7854\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.7821\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2009 - val_loss: 0.7811\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1968 - val_loss: 0.7793\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1903 - val_loss: 0.7780\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2011 - val_loss: 0.7759\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1922 - val_loss: 0.7772\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1987 - val_loss: 0.7781\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2034 - val_loss: 0.7812\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1983 - val_loss: 0.7770\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1921 - val_loss: 0.7767\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1893 - val_loss: 0.7751\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1972 - val_loss: 0.7771\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1977 - val_loss: 0.7754\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1942 - val_loss: 0.7736\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1964 - val_loss: 0.7718\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1915 - val_loss: 0.7745\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1906 - val_loss: 0.7736\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1921 - val_loss: 0.7731\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1934 - val_loss: 0.7738\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1913 - val_loss: 0.7719\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1832 - val_loss: 0.7741\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1953 - val_loss: 0.7765\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1894 - val_loss: 0.7761\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2055 - val_loss: 0.7747\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1908 - val_loss: 0.7757\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1940 - val_loss: 0.7766\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1858 - val_loss: 0.7787\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1951 - val_loss: 0.7799\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1962 - val_loss: 0.7804\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1911 - val_loss: 0.7817\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2006 - val_loss: 0.7801\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1968 - val_loss: 0.7750\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1820 - val_loss: 0.7757\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1825 - val_loss: 0.7778\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1856 - val_loss: 0.7786\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1889 - val_loss: 0.7776\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1948 - val_loss: 0.7784\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1919 - val_loss: 0.7786\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1979 - val_loss: 0.7778\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1994 - val_loss: 0.7791\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1877 - val_loss: 0.7799\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.1881 - val_loss: 0.7771\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1926 - val_loss: 0.7780\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1919 - val_loss: 0.7775\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1867 - val_loss: 0.7821\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1895 - val_loss: 0.7832\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1846 - val_loss: 0.7833\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1850 - val_loss: 0.7854\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1820 - val_loss: 0.7868\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1873 - val_loss: 0.7852\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1834 - val_loss: 0.7893\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1940 - val_loss: 0.7878\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1884 - val_loss: 0.7889\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1908 - val_loss: 0.7880\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1831 - val_loss: 0.7869\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1858 - val_loss: 0.7842\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1893 - val_loss: 0.7855\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1814 - val_loss: 0.7878\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1890 - val_loss: 0.7862\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1805 - val_loss: 0.7850\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1878 - val_loss: 0.7844\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1924 - val_loss: 0.7838\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1837 - val_loss: 0.7846\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1880 - val_loss: 0.7850\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1879 - val_loss: 0.7816\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1856 - val_loss: 0.7787\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1787 - val_loss: 0.7773\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1944 - val_loss: 0.7785\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1807 - val_loss: 0.7790\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1811 - val_loss: 0.7777\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1871 - val_loss: 0.7814\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1811 - val_loss: 0.7869\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1793 - val_loss: 0.7876\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1807 - val_loss: 0.7891\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1768 - val_loss: 0.7887\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1877 - val_loss: 0.7880\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1798 - val_loss: 0.7902\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1886 - val_loss: 0.7835\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1803 - val_loss: 0.7831\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1715 - val_loss: 0.7812\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1839 - val_loss: 0.7829\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1787 - val_loss: 0.7813\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1994 - val_loss: 0.7808\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1786 - val_loss: 0.7794\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1787 - val_loss: 0.7812\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1825 - val_loss: 0.7820\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1796 - val_loss: 0.7811\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1879 - val_loss: 0.7778\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1831 - val_loss: 0.7816\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1731 - val_loss: 0.7865\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1773 - val_loss: 0.7876\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1836 - val_loss: 0.7852\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1743 - val_loss: 0.7866\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1888 - val_loss: 0.7881\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1711 - val_loss: 0.7873\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1770 - val_loss: 0.7881\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1797 - val_loss: 0.7898\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1724 - val_loss: 0.7875\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1806 - val_loss: 0.7900\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1893 - val_loss: 0.7891\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1813 - val_loss: 0.7878\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1820 - val_loss: 0.7898\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1848 - val_loss: 0.7850\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1763 - val_loss: 0.7869\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1705 - val_loss: 0.7892\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1790 - val_loss: 0.7923\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1892 - val_loss: 0.7962\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1795 - val_loss: 0.7903\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1712 - val_loss: 0.7858\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1804 - val_loss: 0.7856\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1841 - val_loss: 0.7856\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1783 - val_loss: 0.7828\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1865 - val_loss: 0.7872\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1786 - val_loss: 0.7874\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1792 - val_loss: 0.7869\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1800 - val_loss: 0.7900\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1834 - val_loss: 0.7901\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1698 - val_loss: 0.7858\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1801 - val_loss: 0.7896\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1768 - val_loss: 0.7896\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1911 - val_loss: 0.7902\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1657 - val_loss: 0.7862\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1760 - val_loss: 0.7862\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1873 - val_loss: 0.7861\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1722 - val_loss: 0.7871\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1811 - val_loss: 0.7860\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1781 - val_loss: 0.7862\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1799 - val_loss: 0.7841\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1724 - val_loss: 0.7853\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1832 - val_loss: 0.7869\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1698 - val_loss: 0.7883\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1763 - val_loss: 0.7922\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1644 - val_loss: 0.7913\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1942 - val_loss: 0.7909\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1889 - val_loss: 0.7919\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1668 - val_loss: 0.7882\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1725 - val_loss: 0.7921\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1693 - val_loss: 0.7948\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1803 - val_loss: 0.7957\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1740 - val_loss: 0.7955\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1752 - val_loss: 0.7944\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1624 - val_loss: 0.7987\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1767 - val_loss: 0.7958\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1748 - val_loss: 0.7902\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1708 - val_loss: 0.7948\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1707 - val_loss: 0.7910\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1738 - val_loss: 0.7979\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1727 - val_loss: 0.7999\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1745 - val_loss: 0.7957\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1691 - val_loss: 0.7969\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1728 - val_loss: 0.7960\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1791 - val_loss: 0.7920\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1775 - val_loss: 0.7885\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1736 - val_loss: 0.7929\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1777 - val_loss: 0.7904\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1708 - val_loss: 0.7898\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1820 - val_loss: 0.7930\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1823 - val_loss: 0.7962\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1787 - val_loss: 0.7973\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1772 - val_loss: 0.7978\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1643 - val_loss: 0.8024\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1661 - val_loss: 0.8005\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1711 - val_loss: 0.8033\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1681 - val_loss: 0.8001\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1704 - val_loss: 0.7987\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1711 - val_loss: 0.7976\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1781 - val_loss: 0.7975\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1581 - val_loss: 0.7975\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1720 - val_loss: 0.8001\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1596 - val_loss: 0.7993\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1716 - val_loss: 0.7998\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1731 - val_loss: 0.8023\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1691 - val_loss: 0.7980\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1793 - val_loss: 0.7976\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1702 - val_loss: 0.8029\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1767 - val_loss: 0.8051\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1629 - val_loss: 0.8003\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1672 - val_loss: 0.8026\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1695 - val_loss: 0.8006\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1667 - val_loss: 0.8001\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1669 - val_loss: 0.8037\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1622 - val_loss: 0.8017\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1707 - val_loss: 0.7989\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1670 - val_loss: 0.8000\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1676 - val_loss: 0.7945\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1709 - val_loss: 0.7952\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1674 - val_loss: 0.7964\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1744 - val_loss: 0.7929\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1748 - val_loss: 0.7961\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1690 - val_loss: 0.7949\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1776 - val_loss: 0.7921\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1731 - val_loss: 0.7986\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1736 - val_loss: 0.7986\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1687 - val_loss: 0.7954\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1651 - val_loss: 0.7963\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1738 - val_loss: 0.7944\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1717 - val_loss: 0.7972\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1637 - val_loss: 0.7955\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1688 - val_loss: 0.7956\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1655 - val_loss: 0.8004\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1572 - val_loss: 0.8020\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1632 - val_loss: 0.8036\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1681 - val_loss: 0.8018\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1648 - val_loss: 0.8020\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1630 - val_loss: 0.8009\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1707 - val_loss: 0.8038\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1635 - val_loss: 0.8041\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1577 - val_loss: 0.8020\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1568 - val_loss: 0.8001\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1708 - val_loss: 0.8055\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1743 - val_loss: 0.7993\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1605 - val_loss: 0.7997\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1655 - val_loss: 0.8027\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1636 - val_loss: 0.8035\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1684 - val_loss: 0.7988\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1680 - val_loss: 0.7937\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1640 - val_loss: 0.7935\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1638 - val_loss: 0.7945\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1618 - val_loss: 0.7934\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1605 - val_loss: 0.7947\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1678 - val_loss: 0.7959\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1586 - val_loss: 0.7997\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1651 - val_loss: 0.8030\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1581 - val_loss: 0.8089\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1708 - val_loss: 0.8025\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1573 - val_loss: 0.8020\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1753 - val_loss: 0.8044\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1619 - val_loss: 0.8036\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1601 - val_loss: 0.8039\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1594 - val_loss: 0.8036\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1547 - val_loss: 0.8031\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1699 - val_loss: 0.8019\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1591 - val_loss: 0.7981\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1685 - val_loss: 0.8004\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1562 - val_loss: 0.8002\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1701 - val_loss: 0.8014\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1614 - val_loss: 0.8082\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1592 - val_loss: 0.8097\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1525 - val_loss: 0.8080\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1629 - val_loss: 0.8088\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1599 - val_loss: 0.8102\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1628 - val_loss: 0.8055\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1686 - val_loss: 0.8061\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1569 - val_loss: 0.8092\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1637 - val_loss: 0.8095\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1586 - val_loss: 0.8104\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1577 - val_loss: 0.8077\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1640 - val_loss: 0.8111\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1661 - val_loss: 0.8134\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1621 - val_loss: 0.8131\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1623 - val_loss: 0.8167\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1584 - val_loss: 0.8120\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1609 - val_loss: 0.8128\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1615 - val_loss: 0.8108\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1657 - val_loss: 0.8103\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1604 - val_loss: 0.8111\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1510 - val_loss: 0.8129\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1569 - val_loss: 0.8202\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1589 - val_loss: 0.8185\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1530 - val_loss: 0.8186\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1594 - val_loss: 0.8194\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1595 - val_loss: 0.8206\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1717 - val_loss: 0.8173\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1633 - val_loss: 0.8195\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1593 - val_loss: 0.8200\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1566 - val_loss: 0.8163\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1582 - val_loss: 0.8166\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1594 - val_loss: 0.8169\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1501 - val_loss: 0.8200\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1640 - val_loss: 0.8153\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1610 - val_loss: 0.8186\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1574 - val_loss: 0.8177\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1541 - val_loss: 0.8162\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1540 - val_loss: 0.8168\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1575 - val_loss: 0.8162\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1645 - val_loss: 0.8175\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1568 - val_loss: 0.8150\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1717 - val_loss: 0.8138\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1584 - val_loss: 0.8119\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1539 - val_loss: 0.8118\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1603 - val_loss: 0.8159\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1556 - val_loss: 0.8139\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1522 - val_loss: 0.8154\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1651 - val_loss: 0.8197\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1547 - val_loss: 0.8161\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1624 - val_loss: 0.8120\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1522 - val_loss: 0.8125\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1589 - val_loss: 0.8171\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1521 - val_loss: 0.8154\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1593 - val_loss: 0.8104\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1531 - val_loss: 0.8136\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1507 - val_loss: 0.8135\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1597 - val_loss: 0.8101\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1545 - val_loss: 0.8122\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1560 - val_loss: 0.8157\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1596 - val_loss: 0.8152\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1465 - val_loss: 0.8113\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1620 - val_loss: 0.8124\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1550 - val_loss: 0.8087\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1479 - val_loss: 0.8094\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1483 - val_loss: 0.8060\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1608 - val_loss: 0.8095\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1542 - val_loss: 0.8115\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1563 - val_loss: 0.8076\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1487 - val_loss: 0.8100\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1547 - val_loss: 0.8097\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1638 - val_loss: 0.8115\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1600 - val_loss: 0.8122\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1577 - val_loss: 0.8078\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1628 - val_loss: 0.8089\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1462 - val_loss: 0.8077\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1528 - val_loss: 0.8084\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1532 - val_loss: 0.8077\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1604 - val_loss: 0.8017\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1541 - val_loss: 0.8092\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1554 - val_loss: 0.8065\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1484 - val_loss: 0.8071\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1488 - val_loss: 0.8081\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1537 - val_loss: 0.8065\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1521 - val_loss: 0.8099\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1432 - val_loss: 0.8086\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1591 - val_loss: 0.8089\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1539 - val_loss: 0.8092\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1628 - val_loss: 0.8044\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1510 - val_loss: 0.8087\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1452 - val_loss: 0.8065\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1494 - val_loss: 0.8066\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1579 - val_loss: 0.8041\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1451 - val_loss: 0.8064\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1528 - val_loss: 0.8108\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1501 - val_loss: 0.8070\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1550 - val_loss: 0.8013\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1566 - val_loss: 0.8030\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1495 - val_loss: 0.8053\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1564 - val_loss: 0.8057\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1545 - val_loss: 0.8007\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1402 - val_loss: 0.8056\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1496 - val_loss: 0.8042\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1599 - val_loss: 0.8049\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1487 - val_loss: 0.8102\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1563 - val_loss: 0.8055\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1419 - val_loss: 0.8085\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1553 - val_loss: 0.8116\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1495 - val_loss: 0.8073\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1489 - val_loss: 0.8103\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1427 - val_loss: 0.8094\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1534 - val_loss: 0.8024\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1481 - val_loss: 0.8086\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1523 - val_loss: 0.8064\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1554 - val_loss: 0.8090\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1485 - val_loss: 0.8126\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1437 - val_loss: 0.8123\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1414 - val_loss: 0.8127\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1523 - val_loss: 0.8104\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1465 - val_loss: 0.8083\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1419 - val_loss: 0.8095\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1558 - val_loss: 0.8093\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1483 - val_loss: 0.8147\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1460 - val_loss: 0.8167\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1535 - val_loss: 0.8118\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1471 - val_loss: 0.8088\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1439 - val_loss: 0.8088\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1493 - val_loss: 0.8100\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1510 - val_loss: 0.8107\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1472 - val_loss: 0.8110\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1459 - val_loss: 0.8139\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1592 - val_loss: 0.8081\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1441 - val_loss: 0.8091\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1464 - val_loss: 0.8037\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1495 - val_loss: 0.8079\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1455 - val_loss: 0.8075\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1474 - val_loss: 0.8107\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1490 - val_loss: 0.8164\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1471 - val_loss: 0.8090\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1478 - val_loss: 0.8118\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1540 - val_loss: 0.8106\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1382 - val_loss: 0.8088\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1367 - val_loss: 0.8082\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1513 - val_loss: 0.8158\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1499 - val_loss: 0.8118\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1483 - val_loss: 0.8104\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1536 - val_loss: 0.8123\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1459 - val_loss: 0.8136\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1591 - val_loss: 0.8142\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1514 - val_loss: 0.8103\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1400 - val_loss: 0.8121\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1484 - val_loss: 0.8061\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1453 - val_loss: 0.8060\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1490 - val_loss: 0.8088\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1410 - val_loss: 0.8122\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1391 - val_loss: 0.8185\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1411 - val_loss: 0.8170\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1456 - val_loss: 0.8153\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1454 - val_loss: 0.8127\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1456 - val_loss: 0.8108\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1368 - val_loss: 0.8097\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1503 - val_loss: 0.8061\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1432 - val_loss: 0.8038\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1424 - val_loss: 0.8074\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1441 - val_loss: 0.8066\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1435 - val_loss: 0.8060\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1410 - val_loss: 0.8019\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1432 - val_loss: 0.8034\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1369 - val_loss: 0.8052\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1452 - val_loss: 0.8038\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1446 - val_loss: 0.8039\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1463 - val_loss: 0.8021\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1435 - val_loss: 0.7992\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1527 - val_loss: 0.8005\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1510 - val_loss: 0.8009\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1497 - val_loss: 0.8031\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1489 - val_loss: 0.8004\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1359 - val_loss: 0.8041\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1426 - val_loss: 0.8016\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1406 - val_loss: 0.8046\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1375 - val_loss: 0.8035\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1456 - val_loss: 0.7988\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1328 - val_loss: 0.7978\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1388 - val_loss: 0.7958\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1430 - val_loss: 0.7954\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1379 - val_loss: 0.7993\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1478 - val_loss: 0.7960\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1330 - val_loss: 0.8008\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1363 - val_loss: 0.8025\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1462 - val_loss: 0.7990\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1360 - val_loss: 0.7998\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1481 - val_loss: 0.8014\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1296 - val_loss: 0.8017\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1471 - val_loss: 0.7975\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1394 - val_loss: 0.8030\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1448 - val_loss: 0.8033\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1384 - val_loss: 0.8025\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1460 - val_loss: 0.8022\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1425 - val_loss: 0.7982\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1447 - val_loss: 0.7966\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1459 - val_loss: 0.7981\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1390 - val_loss: 0.7998\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1434 - val_loss: 0.7985\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1371 - val_loss: 0.7972\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1461 - val_loss: 0.7970\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1343 - val_loss: 0.7992\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1410 - val_loss: 0.8029\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1328 - val_loss: 0.8036\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1378 - val_loss: 0.7993\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1421 - val_loss: 0.7996\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1370 - val_loss: 0.7993\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1416 - val_loss: 0.7983\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1347 - val_loss: 0.8009\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1380 - val_loss: 0.8033\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1407 - val_loss: 0.7989\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1510 - val_loss: 0.7971\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1436 - val_loss: 0.8000\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1424 - val_loss: 0.8015\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1411 - val_loss: 0.7987\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1343 - val_loss: 0.7985\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1303 - val_loss: 0.7984\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1461 - val_loss: 0.7984\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1484 - val_loss: 0.7956\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1379 - val_loss: 0.7988\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1459 - val_loss: 0.8009\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1383 - val_loss: 0.7995\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1440 - val_loss: 0.8017\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1371 - val_loss: 0.8003\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1365 - val_loss: 0.8011\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1440 - val_loss: 0.8043\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1407 - val_loss: 0.8015\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1392 - val_loss: 0.8045\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1374 - val_loss: 0.8067\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1420 - val_loss: 0.8020\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1310 - val_loss: 0.8047\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1352 - val_loss: 0.8037\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1350 - val_loss: 0.8011\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1456 - val_loss: 0.7979\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1329 - val_loss: 0.7970\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1403 - val_loss: 0.7974\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1431 - val_loss: 0.8011\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1377 - val_loss: 0.7973\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1323 - val_loss: 0.8006\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1328 - val_loss: 0.8028\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1425 - val_loss: 0.8007\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1353 - val_loss: 0.8022\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1314 - val_loss: 0.8036\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1279 - val_loss: 0.8078\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1263 - val_loss: 0.8066\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1276 - val_loss: 0.8042\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1310 - val_loss: 0.8067\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1361 - val_loss: 0.8046\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1344 - val_loss: 0.8058\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1261 - val_loss: 0.8090\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1349 - val_loss: 0.8056\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1427 - val_loss: 0.8035\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1388 - val_loss: 0.8017\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1367 - val_loss: 0.8024\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1312 - val_loss: 0.8021\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1395 - val_loss: 0.8035\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1363 - val_loss: 0.8036\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1286 - val_loss: 0.7991\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1370 - val_loss: 0.8018\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1380 - val_loss: 0.8026\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1405 - val_loss: 0.7985\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1382 - val_loss: 0.8006\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1257 - val_loss: 0.8020\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1287 - val_loss: 0.8054\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1351 - val_loss: 0.8027\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1360 - val_loss: 0.8023\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1414 - val_loss: 0.8052\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1297 - val_loss: 0.8087\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1408 - val_loss: 0.8085\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1289 - val_loss: 0.8063\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1360 - val_loss: 0.8069\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1316 - val_loss: 0.8061\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1408 - val_loss: 0.8037\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1301 - val_loss: 0.7991\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1376 - val_loss: 0.7960\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1305 - val_loss: 0.8038\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1361 - val_loss: 0.8022\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1391 - val_loss: 0.8009\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1318 - val_loss: 0.7938\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.7990\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1392 - val_loss: 0.8028\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1306 - val_loss: 0.8005\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1353 - val_loss: 0.8040\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1344 - val_loss: 0.7985\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1457 - val_loss: 0.8001\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1315 - val_loss: 0.8003\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1337 - val_loss: 0.7952\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1241 - val_loss: 0.7954\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1416 - val_loss: 0.7959\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1304 - val_loss: 0.8031\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1336 - val_loss: 0.7995\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1362 - val_loss: 0.8015\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.1297 - val_loss: 0.7995\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.1224 - val_loss: 0.8021\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1338 - val_loss: 0.7984\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1353 - val_loss: 0.7995\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1309 - val_loss: 0.8028\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1286 - val_loss: 0.8005\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1252 - val_loss: 0.8014\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1268 - val_loss: 0.8018\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1380 - val_loss: 0.7986\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1329 - val_loss: 0.7952\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1330 - val_loss: 0.8037\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1382 - val_loss: 0.8022\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1306 - val_loss: 0.8019\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.8010\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1349 - val_loss: 0.8048\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1323 - val_loss: 0.8005\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1274 - val_loss: 0.7991\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1343 - val_loss: 0.7991\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1236 - val_loss: 0.7990\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1272 - val_loss: 0.7945\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1258 - val_loss: 0.7944\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1263 - val_loss: 0.7948\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1306 - val_loss: 0.7961\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1246 - val_loss: 0.7930\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1302 - val_loss: 0.7926\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1278 - val_loss: 0.7952\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1275 - val_loss: 0.7974\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1284 - val_loss: 0.7981\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1362 - val_loss: 0.8023\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1335 - val_loss: 0.7976\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1310 - val_loss: 0.7971\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1370 - val_loss: 0.7970\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1264 - val_loss: 0.7958\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1261 - val_loss: 0.7966\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1299 - val_loss: 0.8017\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1293 - val_loss: 0.8010\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1259 - val_loss: 0.8007\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1287 - val_loss: 0.8012\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1203 - val_loss: 0.8030\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1225 - val_loss: 0.8020\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1275 - val_loss: 0.7959\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1233 - val_loss: 0.7948\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1338 - val_loss: 0.7955\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1292 - val_loss: 0.7949\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1284 - val_loss: 0.7952\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1319 - val_loss: 0.7935\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.7935\n",
            "7/7 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbf84787190>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number\n",
        "nb_epochs=1000;    # training epochs change\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition3   # choose 3rd lighting condition for test set\n",
        "\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "B2_a5xoIPJTv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do9GnXSdPMkJ"
      },
      "source": [
        "###3.3.5 4th light condition as test###  "
      ],
      "id": "Do9GnXSdPMkJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4qkevaibPMua",
        "outputId": "592ee384-b6bb-40f7-8c15-75111c6f893f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 4s 10ms/step - loss: 2.0703 - val_loss: 2.3568\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.9721 - val_loss: 2.2945\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.8722 - val_loss: 2.2357\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.7787 - val_loss: 2.1722\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.7039 - val_loss: 2.1086\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.6168 - val_loss: 2.0454\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.5322 - val_loss: 1.9827\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.4492 - val_loss: 1.9193\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.3700 - val_loss: 1.8541\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.2992 - val_loss: 1.7868\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.2144 - val_loss: 1.7197\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.1380 - val_loss: 1.6534\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.0799 - val_loss: 1.5885\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.9906 - val_loss: 1.5235\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 6ms/step - loss: 0.9335 - val_loss: 1.4615\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.8826 - val_loss: 1.4004\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.8275 - val_loss: 1.3441\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7801 - val_loss: 1.2879\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7233 - val_loss: 1.2355\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6800 - val_loss: 1.1859\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6620 - val_loss: 1.1413\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6152 - val_loss: 1.0979\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5868 - val_loss: 1.0606\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5537 - val_loss: 1.0246\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5267 - val_loss: 0.9923\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5155 - val_loss: 0.9633\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4854 - val_loss: 0.9353\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4774 - val_loss: 0.9112\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4589 - val_loss: 0.8877\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4377 - val_loss: 0.8692\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4276 - val_loss: 0.8519\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4268 - val_loss: 0.8363\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4069 - val_loss: 0.8237\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4055 - val_loss: 0.8103\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4081 - val_loss: 0.7999\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3781 - val_loss: 0.7907\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3778 - val_loss: 0.7822\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3754 - val_loss: 0.7728\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3774 - val_loss: 0.7674\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3613 - val_loss: 0.7612\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3547 - val_loss: 0.7545\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3521 - val_loss: 0.7484\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3456 - val_loss: 0.7436\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3424 - val_loss: 0.7397\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3379 - val_loss: 0.7346\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3441 - val_loss: 0.7315\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3404 - val_loss: 0.7274\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3344 - val_loss: 0.7247\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3302 - val_loss: 0.7219\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3365 - val_loss: 0.7166\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3347 - val_loss: 0.7156\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3378 - val_loss: 0.7134\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3229 - val_loss: 0.7111\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3330 - val_loss: 0.7101\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3269 - val_loss: 0.7066\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3286 - val_loss: 0.7043\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3223 - val_loss: 0.7023\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3210 - val_loss: 0.7015\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3249 - val_loss: 0.6990\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3162 - val_loss: 0.6972\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3155 - val_loss: 0.6952\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3109 - val_loss: 0.6940\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3128 - val_loss: 0.6924\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3061 - val_loss: 0.6912\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3067 - val_loss: 0.6894\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3177 - val_loss: 0.6892\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3202 - val_loss: 0.6888\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2976 - val_loss: 0.6858\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3064 - val_loss: 0.6852\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3127 - val_loss: 0.6841\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3029 - val_loss: 0.6836\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3018 - val_loss: 0.6821\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3056 - val_loss: 0.6823\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3001 - val_loss: 0.6816\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2967 - val_loss: 0.6800\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3069 - val_loss: 0.6799\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3030 - val_loss: 0.6804\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2966 - val_loss: 0.6783\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3047 - val_loss: 0.6789\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3033 - val_loss: 0.6785\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3099 - val_loss: 0.6776\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2961 - val_loss: 0.6774\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 1s 6ms/step - loss: 0.2951 - val_loss: 0.6778\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3010 - val_loss: 0.6781\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2969 - val_loss: 0.6766\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2942 - val_loss: 0.6757\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2974 - val_loss: 0.6765\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2894 - val_loss: 0.6760\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2868 - val_loss: 0.6757\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2924 - val_loss: 0.6751\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2839 - val_loss: 0.6744\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2994 - val_loss: 0.6753\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2889 - val_loss: 0.6754\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2826 - val_loss: 0.6754\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2876 - val_loss: 0.6763\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2907 - val_loss: 0.6753\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2972 - val_loss: 0.6754\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2738 - val_loss: 0.6751\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2817 - val_loss: 0.6747\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2844 - val_loss: 0.6747\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2829 - val_loss: 0.6749\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2841 - val_loss: 0.6749\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2875 - val_loss: 0.6755\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2851 - val_loss: 0.6758\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2870 - val_loss: 0.6757\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2878 - val_loss: 0.6758\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2758 - val_loss: 0.6750\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2883 - val_loss: 0.6750\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.6732\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2902 - val_loss: 0.6736\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2860 - val_loss: 0.6737\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2799 - val_loss: 0.6732\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.6724\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2744 - val_loss: 0.6729\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2782 - val_loss: 0.6720\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2780 - val_loss: 0.6725\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2764 - val_loss: 0.6729\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2686 - val_loss: 0.6728\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2731 - val_loss: 0.6727\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2929 - val_loss: 0.6737\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2782 - val_loss: 0.6748\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2707 - val_loss: 0.6743\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2741 - val_loss: 0.6747\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2764 - val_loss: 0.6738\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2628 - val_loss: 0.6742\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2848 - val_loss: 0.6738\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2638 - val_loss: 0.6744\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2652 - val_loss: 0.6744\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2694 - val_loss: 0.6733\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2670 - val_loss: 0.6731\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2730 - val_loss: 0.6738\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2709 - val_loss: 0.6748\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2724 - val_loss: 0.6748\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2727 - val_loss: 0.6730\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2756 - val_loss: 0.6747\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2743 - val_loss: 0.6752\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2704 - val_loss: 0.6745\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2687 - val_loss: 0.6741\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2797 - val_loss: 0.6736\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2791 - val_loss: 0.6746\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2761 - val_loss: 0.6748\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2739 - val_loss: 0.6750\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2711 - val_loss: 0.6749\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2663 - val_loss: 0.6742\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2826 - val_loss: 0.6747\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2710 - val_loss: 0.6755\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2738 - val_loss: 0.6753\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2777 - val_loss: 0.6746\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2680 - val_loss: 0.6742\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2665 - val_loss: 0.6739\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2682 - val_loss: 0.6740\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2573 - val_loss: 0.6734\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2721 - val_loss: 0.6744\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2713 - val_loss: 0.6752\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2717 - val_loss: 0.6748\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2693 - val_loss: 0.6742\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2721 - val_loss: 0.6739\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2717 - val_loss: 0.6739\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.2648 - val_loss: 0.6748\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2689 - val_loss: 0.6758\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2692 - val_loss: 0.6761\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2693 - val_loss: 0.6756\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2733 - val_loss: 0.6750\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2694 - val_loss: 0.6749\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2644 - val_loss: 0.6747\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2696 - val_loss: 0.6740\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2720 - val_loss: 0.6741\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2655 - val_loss: 0.6743\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2555 - val_loss: 0.6755\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2631 - val_loss: 0.6760\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2719 - val_loss: 0.6766\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2686 - val_loss: 0.6755\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.6757\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2596 - val_loss: 0.6758\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2604 - val_loss: 0.6766\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2622 - val_loss: 0.6759\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2570 - val_loss: 0.6763\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2585 - val_loss: 0.6762\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2624 - val_loss: 0.6759\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2555 - val_loss: 0.6752\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2581 - val_loss: 0.6753\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2557 - val_loss: 0.6743\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2570 - val_loss: 0.6749\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2590 - val_loss: 0.6752\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2622 - val_loss: 0.6759\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2701 - val_loss: 0.6773\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2500 - val_loss: 0.6779\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2699 - val_loss: 0.6777\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2558 - val_loss: 0.6784\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2521 - val_loss: 0.6790\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2580 - val_loss: 0.6797\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2645 - val_loss: 0.6812\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2621 - val_loss: 0.6808\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2611 - val_loss: 0.6806\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2657 - val_loss: 0.6807\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2591 - val_loss: 0.6806\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2684 - val_loss: 0.6793\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2516 - val_loss: 0.6795\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2582 - val_loss: 0.6793\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2666 - val_loss: 0.6792\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2648 - val_loss: 0.6785\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2618 - val_loss: 0.6798\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.6803\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2556 - val_loss: 0.6804\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2580 - val_loss: 0.6809\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2596 - val_loss: 0.6811\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2746 - val_loss: 0.6811\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2547 - val_loss: 0.6811\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2537 - val_loss: 0.6825\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2620 - val_loss: 0.6819\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2547 - val_loss: 0.6809\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2564 - val_loss: 0.6819\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2624 - val_loss: 0.6827\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2533 - val_loss: 0.6825\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2521 - val_loss: 0.6825\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2499 - val_loss: 0.6823\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.6813\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2603 - val_loss: 0.6819\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2529 - val_loss: 0.6828\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2572 - val_loss: 0.6828\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2545 - val_loss: 0.6816\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2556 - val_loss: 0.6820\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2532 - val_loss: 0.6821\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2522 - val_loss: 0.6830\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2606 - val_loss: 0.6813\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2575 - val_loss: 0.6802\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2557 - val_loss: 0.6809\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2587 - val_loss: 0.6818\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2623 - val_loss: 0.6834\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2537 - val_loss: 0.6841\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2555 - val_loss: 0.6831\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2506 - val_loss: 0.6827\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2617 - val_loss: 0.6823\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2597 - val_loss: 0.6833\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2570 - val_loss: 0.6834\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2556 - val_loss: 0.6835\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2483 - val_loss: 0.6830\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2506 - val_loss: 0.6841\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2589 - val_loss: 0.6837\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2550 - val_loss: 0.6839\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2603 - val_loss: 0.6834\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2522 - val_loss: 0.6835\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2510 - val_loss: 0.6846\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2447 - val_loss: 0.6850\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2572 - val_loss: 0.6859\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2516 - val_loss: 0.6851\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2591 - val_loss: 0.6850\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2549 - val_loss: 0.6848\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2507 - val_loss: 0.6855\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2526 - val_loss: 0.6859\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2555 - val_loss: 0.6863\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2588 - val_loss: 0.6863\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2567 - val_loss: 0.6868\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2539 - val_loss: 0.6866\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2512 - val_loss: 0.6881\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2561 - val_loss: 0.6874\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2586 - val_loss: 0.6891\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2479 - val_loss: 0.6883\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2533 - val_loss: 0.6880\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2597 - val_loss: 0.6888\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2382 - val_loss: 0.6873\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2514 - val_loss: 0.6865\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2554 - val_loss: 0.6869\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2483 - val_loss: 0.6866\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2531 - val_loss: 0.6865\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2555 - val_loss: 0.6869\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2497 - val_loss: 0.6866\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2472 - val_loss: 0.6859\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2543 - val_loss: 0.6857\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2545 - val_loss: 0.6851\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2456 - val_loss: 0.6861\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2545 - val_loss: 0.6875\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2596 - val_loss: 0.6888\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2466 - val_loss: 0.6864\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2522 - val_loss: 0.6863\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2475 - val_loss: 0.6865\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2478 - val_loss: 0.6859\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2534 - val_loss: 0.6860\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.6854\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2565 - val_loss: 0.6856\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2470 - val_loss: 0.6860\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2516 - val_loss: 0.6861\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2497 - val_loss: 0.6871\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2449 - val_loss: 0.6880\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2452 - val_loss: 0.6894\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2441 - val_loss: 0.6877\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2525 - val_loss: 0.6882\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2465 - val_loss: 0.6870\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2380 - val_loss: 0.6872\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2556 - val_loss: 0.6868\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2460 - val_loss: 0.6856\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2377 - val_loss: 0.6856\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2488 - val_loss: 0.6864\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2531 - val_loss: 0.6863\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2550 - val_loss: 0.6876\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2552 - val_loss: 0.6871\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2482 - val_loss: 0.6882\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 6ms/step - loss: 0.2409 - val_loss: 0.6870\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2469 - val_loss: 0.6862\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2462 - val_loss: 0.6862\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2495 - val_loss: 0.6872\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2436 - val_loss: 0.6863\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2443 - val_loss: 0.6859\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2417 - val_loss: 0.6867\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2530 - val_loss: 0.6863\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2480 - val_loss: 0.6867\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2432 - val_loss: 0.6872\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2551 - val_loss: 0.6862\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2511 - val_loss: 0.6873\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2425 - val_loss: 0.6853\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2384 - val_loss: 0.6857\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2623 - val_loss: 0.6859\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2527 - val_loss: 0.6863\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2433 - val_loss: 0.6855\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.6865\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2503 - val_loss: 0.6850\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2423 - val_loss: 0.6843\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2591 - val_loss: 0.6839\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2379 - val_loss: 0.6844\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2423 - val_loss: 0.6848\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2279 - val_loss: 0.6866\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2473 - val_loss: 0.6865\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2435 - val_loss: 0.6871\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2506 - val_loss: 0.6881\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2329 - val_loss: 0.6875\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2392 - val_loss: 0.6873\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2349 - val_loss: 0.6878\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2277 - val_loss: 0.6863\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2434 - val_loss: 0.6858\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2406 - val_loss: 0.6861\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2270 - val_loss: 0.6852\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2401 - val_loss: 0.6858\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2363 - val_loss: 0.6847\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2330 - val_loss: 0.6851\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2370 - val_loss: 0.6852\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2434 - val_loss: 0.6856\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2410 - val_loss: 0.6866\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2473 - val_loss: 0.6868\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2441 - val_loss: 0.6869\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2448 - val_loss: 0.6874\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2326 - val_loss: 0.6862\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2427 - val_loss: 0.6852\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2464 - val_loss: 0.6840\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2435 - val_loss: 0.6844\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2400 - val_loss: 0.6860\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2331 - val_loss: 0.6859\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2415 - val_loss: 0.6880\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2375 - val_loss: 0.6867\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2467 - val_loss: 0.6866\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2435 - val_loss: 0.6863\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2459 - val_loss: 0.6868\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2377 - val_loss: 0.6865\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2357 - val_loss: 0.6875\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2425 - val_loss: 0.6873\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2296 - val_loss: 0.6866\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2354 - val_loss: 0.6874\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2364 - val_loss: 0.6855\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2425 - val_loss: 0.6864\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2348 - val_loss: 0.6871\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2291 - val_loss: 0.6866\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2418 - val_loss: 0.6872\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2375 - val_loss: 0.6872\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2274 - val_loss: 0.6867\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.6876\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2438 - val_loss: 0.6873\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2475 - val_loss: 0.6875\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2429 - val_loss: 0.6869\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2406 - val_loss: 0.6878\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2375 - val_loss: 0.6874\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2361 - val_loss: 0.6884\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2307 - val_loss: 0.6865\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2462 - val_loss: 0.6877\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.6865\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2467 - val_loss: 0.6868\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2329 - val_loss: 0.6880\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2363 - val_loss: 0.6885\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.6879\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2275 - val_loss: 0.6883\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2351 - val_loss: 0.6883\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2497 - val_loss: 0.6877\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2193 - val_loss: 0.6877\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2417 - val_loss: 0.6890\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.6880\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2476 - val_loss: 0.6891\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2270 - val_loss: 0.6909\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2433 - val_loss: 0.6899\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2320 - val_loss: 0.6906\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2351 - val_loss: 0.6908\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2264 - val_loss: 0.6911\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2387 - val_loss: 0.6905\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2378 - val_loss: 0.6906\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2392 - val_loss: 0.6907\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2319 - val_loss: 0.6905\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.6899\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2310 - val_loss: 0.6878\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2351 - val_loss: 0.6891\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.6892\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2328 - val_loss: 0.6881\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2391 - val_loss: 0.6876\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2312 - val_loss: 0.6866\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2234 - val_loss: 0.6854\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2310 - val_loss: 0.6866\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.6886\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2316 - val_loss: 0.6887\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2228 - val_loss: 0.6875\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2277 - val_loss: 0.6866\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2385 - val_loss: 0.6867\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2379 - val_loss: 0.6868\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2359 - val_loss: 0.6860\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2384 - val_loss: 0.6872\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2285 - val_loss: 0.6882\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2411 - val_loss: 0.6877\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2308 - val_loss: 0.6867\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2308 - val_loss: 0.6866\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2362 - val_loss: 0.6876\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2278 - val_loss: 0.6868\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.6877\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.6868\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2332 - val_loss: 0.6867\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2229 - val_loss: 0.6863\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2322 - val_loss: 0.6864\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2296 - val_loss: 0.6859\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.6869\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2389 - val_loss: 0.6864\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2376 - val_loss: 0.6865\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.6864\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2288 - val_loss: 0.6868\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2316 - val_loss: 0.6860\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2267 - val_loss: 0.6867\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2346 - val_loss: 0.6865\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2292 - val_loss: 0.6861\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2362 - val_loss: 0.6866\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2296 - val_loss: 0.6860\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2348 - val_loss: 0.6854\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2347 - val_loss: 0.6854\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.6852\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2330 - val_loss: 0.6850\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2318 - val_loss: 0.6855\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2407 - val_loss: 0.6861\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.6858\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2328 - val_loss: 0.6860\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2287 - val_loss: 0.6870\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2215 - val_loss: 0.6860\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2223 - val_loss: 0.6855\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.6863\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2322 - val_loss: 0.6852\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2271 - val_loss: 0.6859\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2298 - val_loss: 0.6859\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2227 - val_loss: 0.6854\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2215 - val_loss: 0.6849\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2199 - val_loss: 0.6845\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2283 - val_loss: 0.6861\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2262 - val_loss: 0.6854\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2253 - val_loss: 0.6857\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2251 - val_loss: 0.6858\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2444 - val_loss: 0.6868\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2243 - val_loss: 0.6853\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2360 - val_loss: 0.6859\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2275 - val_loss: 0.6861\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2165 - val_loss: 0.6873\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2278 - val_loss: 0.6861\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2252 - val_loss: 0.6862\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2343 - val_loss: 0.6850\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2254 - val_loss: 0.6851\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2367 - val_loss: 0.6862\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2240 - val_loss: 0.6863\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2261 - val_loss: 0.6850\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2251 - val_loss: 0.6864\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2178 - val_loss: 0.6863\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2207 - val_loss: 0.6882\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2188 - val_loss: 0.6873\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2191 - val_loss: 0.6857\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.6847\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2277 - val_loss: 0.6860\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2202 - val_loss: 0.6852\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2299 - val_loss: 0.6840\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2310 - val_loss: 0.6859\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.6851\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2221 - val_loss: 0.6856\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2328 - val_loss: 0.6849\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2278 - val_loss: 0.6852\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2401 - val_loss: 0.6853\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2263 - val_loss: 0.6859\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2306 - val_loss: 0.6855\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.6862\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2309 - val_loss: 0.6852\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2228 - val_loss: 0.6851\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2312 - val_loss: 0.6848\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2305 - val_loss: 0.6860\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2289 - val_loss: 0.6863\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2267 - val_loss: 0.6865\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2187 - val_loss: 0.6842\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2428 - val_loss: 0.6840\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2157 - val_loss: 0.6836\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2123 - val_loss: 0.6837\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2350 - val_loss: 0.6841\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.6855\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2262 - val_loss: 0.6855\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2258 - val_loss: 0.6847\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2223 - val_loss: 0.6853\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.6843\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.6844\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.6835\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2315 - val_loss: 0.6843\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2343 - val_loss: 0.6832\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.6840\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2244 - val_loss: 0.6853\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2248 - val_loss: 0.6854\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2235 - val_loss: 0.6865\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2228 - val_loss: 0.6866\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2185 - val_loss: 0.6856\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2253 - val_loss: 0.6861\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2265 - val_loss: 0.6856\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2203 - val_loss: 0.6874\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.6873\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.6863\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2273 - val_loss: 0.6864\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2227 - val_loss: 0.6849\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2170 - val_loss: 0.6850\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2228 - val_loss: 0.6852\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2263 - val_loss: 0.6857\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2290 - val_loss: 0.6854\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2170 - val_loss: 0.6859\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2211 - val_loss: 0.6852\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2343 - val_loss: 0.6845\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2144 - val_loss: 0.6863\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2255 - val_loss: 0.6855\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2121 - val_loss: 0.6852\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2283 - val_loss: 0.6860\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2231 - val_loss: 0.6859\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2090 - val_loss: 0.6875\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2149 - val_loss: 0.6866\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2260 - val_loss: 0.6867\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2238 - val_loss: 0.6867\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.6874\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2165 - val_loss: 0.6877\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2282 - val_loss: 0.6869\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2311 - val_loss: 0.6867\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2216 - val_loss: 0.6886\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.6879\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2086 - val_loss: 0.6876\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2161 - val_loss: 0.6872\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2217 - val_loss: 0.6849\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2145 - val_loss: 0.6851\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2175 - val_loss: 0.6842\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2206 - val_loss: 0.6849\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2142 - val_loss: 0.6848\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2138 - val_loss: 0.6860\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2084 - val_loss: 0.6872\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2228 - val_loss: 0.6860\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2222 - val_loss: 0.6862\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2257 - val_loss: 0.6861\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.6842\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2141 - val_loss: 0.6843\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2111 - val_loss: 0.6854\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2251 - val_loss: 0.6855\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2175 - val_loss: 0.6848\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2213 - val_loss: 0.6849\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2174 - val_loss: 0.6841\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.6849\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2158 - val_loss: 0.6836\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.6850\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.6860\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2197 - val_loss: 0.6863\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2189 - val_loss: 0.6853\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2171 - val_loss: 0.6862\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2164 - val_loss: 0.6865\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2245 - val_loss: 0.6857\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2152 - val_loss: 0.6867\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2259 - val_loss: 0.6873\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.6860\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2173 - val_loss: 0.6859\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2186 - val_loss: 0.6865\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2137 - val_loss: 0.6856\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2276 - val_loss: 0.6858\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2172 - val_loss: 0.6863\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2201 - val_loss: 0.6844\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.6848\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2008 - val_loss: 0.6842\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2255 - val_loss: 0.6837\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2218 - val_loss: 0.6816\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2218 - val_loss: 0.6810\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2165 - val_loss: 0.6820\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2189 - val_loss: 0.6833\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2208 - val_loss: 0.6824\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2105 - val_loss: 0.6835\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2192 - val_loss: 0.6843\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2148 - val_loss: 0.6850\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2133 - val_loss: 0.6837\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2128 - val_loss: 0.6842\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2189 - val_loss: 0.6859\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2172 - val_loss: 0.6841\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2170 - val_loss: 0.6832\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2089 - val_loss: 0.6833\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2071 - val_loss: 0.6838\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2154 - val_loss: 0.6833\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.6831\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.6830\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.6817\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2134 - val_loss: 0.6799\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.6807\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2189 - val_loss: 0.6818\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2029 - val_loss: 0.6830\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2177 - val_loss: 0.6834\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2207 - val_loss: 0.6829\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2156 - val_loss: 0.6817\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2079 - val_loss: 0.6819\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2124 - val_loss: 0.6824\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2072 - val_loss: 0.6825\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2140 - val_loss: 0.6827\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2073 - val_loss: 0.6825\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2053 - val_loss: 0.6829\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2131 - val_loss: 0.6847\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2200 - val_loss: 0.6836\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2210 - val_loss: 0.6834\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2102 - val_loss: 0.6830\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.6843\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2065 - val_loss: 0.6827\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2141 - val_loss: 0.6833\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2110 - val_loss: 0.6833\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2207 - val_loss: 0.6841\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2048 - val_loss: 0.6839\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2123 - val_loss: 0.6835\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.6821\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2077 - val_loss: 0.6840\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2066 - val_loss: 0.6835\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2193 - val_loss: 0.6849\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2037 - val_loss: 0.6823\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2048 - val_loss: 0.6831\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2086 - val_loss: 0.6837\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2275 - val_loss: 0.6830\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2083 - val_loss: 0.6844\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2127 - val_loss: 0.6852\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2166 - val_loss: 0.6839\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2116 - val_loss: 0.6821\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2076 - val_loss: 0.6840\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2118 - val_loss: 0.6842\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2131 - val_loss: 0.6821\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2071 - val_loss: 0.6824\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2149 - val_loss: 0.6827\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2134 - val_loss: 0.6824\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2165 - val_loss: 0.6815\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2102 - val_loss: 0.6830\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2108 - val_loss: 0.6825\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2107 - val_loss: 0.6827\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2094 - val_loss: 0.6814\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2145 - val_loss: 0.6818\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2172 - val_loss: 0.6815\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2026 - val_loss: 0.6813\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2183 - val_loss: 0.6811\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2157 - val_loss: 0.6813\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2177 - val_loss: 0.6809\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2053 - val_loss: 0.6801\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2135 - val_loss: 0.6817\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2103 - val_loss: 0.6817\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2111 - val_loss: 0.6814\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.6803\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2048 - val_loss: 0.6819\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2138 - val_loss: 0.6817\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1992 - val_loss: 0.6824\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2012 - val_loss: 0.6824\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2160 - val_loss: 0.6819\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2028 - val_loss: 0.6812\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2043 - val_loss: 0.6808\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2128 - val_loss: 0.6815\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2048 - val_loss: 0.6809\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2138 - val_loss: 0.6794\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2016 - val_loss: 0.6791\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2040 - val_loss: 0.6821\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1996 - val_loss: 0.6813\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2046 - val_loss: 0.6810\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2133 - val_loss: 0.6826\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2060 - val_loss: 0.6829\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.6816\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2063 - val_loss: 0.6825\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2107 - val_loss: 0.6803\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2082 - val_loss: 0.6790\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2063 - val_loss: 0.6796\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2062 - val_loss: 0.6803\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1942 - val_loss: 0.6782\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2082 - val_loss: 0.6777\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2038 - val_loss: 0.6778\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2106 - val_loss: 0.6778\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2040 - val_loss: 0.6776\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2132 - val_loss: 0.6784\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2032 - val_loss: 0.6794\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2225 - val_loss: 0.6806\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2004 - val_loss: 0.6791\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2037 - val_loss: 0.6785\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1940 - val_loss: 0.6805\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.6808\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.6790\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2017 - val_loss: 0.6807\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1967 - val_loss: 0.6803\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.6792\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2112 - val_loss: 0.6790\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1974 - val_loss: 0.6789\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.6778\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1984 - val_loss: 0.6787\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1971 - val_loss: 0.6800\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2006 - val_loss: 0.6784\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2031 - val_loss: 0.6788\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1961 - val_loss: 0.6791\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2096 - val_loss: 0.6800\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1949 - val_loss: 0.6808\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2067 - val_loss: 0.6809\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2015 - val_loss: 0.6816\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1989 - val_loss: 0.6818\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2075 - val_loss: 0.6805\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2055 - val_loss: 0.6797\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2003 - val_loss: 0.6790\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.6791\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2002 - val_loss: 0.6793\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2112 - val_loss: 0.6803\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.6802\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1950 - val_loss: 0.6790\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2066 - val_loss: 0.6801\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2042 - val_loss: 0.6798\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2151 - val_loss: 0.6785\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2024 - val_loss: 0.6782\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1993 - val_loss: 0.6764\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1987 - val_loss: 0.6785\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2001 - val_loss: 0.6789\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1964 - val_loss: 0.6784\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1890 - val_loss: 0.6787\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2094 - val_loss: 0.6768\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2083 - val_loss: 0.6746\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2050 - val_loss: 0.6759\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2076 - val_loss: 0.6756\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2124 - val_loss: 0.6772\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1976 - val_loss: 0.6754\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2083 - val_loss: 0.6769\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.6770\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1973 - val_loss: 0.6771\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2022 - val_loss: 0.6783\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2086 - val_loss: 0.6759\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1944 - val_loss: 0.6760\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2027 - val_loss: 0.6766\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1966 - val_loss: 0.6772\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2053 - val_loss: 0.6786\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1984 - val_loss: 0.6803\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1880 - val_loss: 0.6791\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1971 - val_loss: 0.6768\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2031 - val_loss: 0.6772\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2062 - val_loss: 0.6758\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1999 - val_loss: 0.6757\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1990 - val_loss: 0.6759\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1916 - val_loss: 0.6768\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2026 - val_loss: 0.6753\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2021 - val_loss: 0.6738\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1968 - val_loss: 0.6744\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1991 - val_loss: 0.6744\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2034 - val_loss: 0.6728\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2001 - val_loss: 0.6730\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1912 - val_loss: 0.6734\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.6725\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2071 - val_loss: 0.6719\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2011 - val_loss: 0.6729\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1971 - val_loss: 0.6720\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2033 - val_loss: 0.6717\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1943 - val_loss: 0.6717\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1982 - val_loss: 0.6726\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2018 - val_loss: 0.6724\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1901 - val_loss: 0.6734\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1987 - val_loss: 0.6733\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2011 - val_loss: 0.6721\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1895 - val_loss: 0.6723\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2024 - val_loss: 0.6737\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1893 - val_loss: 0.6734\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1932 - val_loss: 0.6734\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1938 - val_loss: 0.6731\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2005 - val_loss: 0.6742\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.6743\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2024 - val_loss: 0.6729\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2003 - val_loss: 0.6732\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1975 - val_loss: 0.6732\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1972 - val_loss: 0.6727\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.6728\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2007 - val_loss: 0.6718\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1963 - val_loss: 0.6711\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1994 - val_loss: 0.6721\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1971 - val_loss: 0.6705\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2143 - val_loss: 0.6708\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2019 - val_loss: 0.6719\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1992 - val_loss: 0.6725\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2000 - val_loss: 0.6730\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1932 - val_loss: 0.6715\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1969 - val_loss: 0.6717\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2033 - val_loss: 0.6709\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2004 - val_loss: 0.6680\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2024 - val_loss: 0.6689\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1948 - val_loss: 0.6692\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2074 - val_loss: 0.6705\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2012 - val_loss: 0.6701\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2116 - val_loss: 0.6709\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2036 - val_loss: 0.6692\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1959 - val_loss: 0.6700\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1989 - val_loss: 0.6705\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1874 - val_loss: 0.6711\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1971 - val_loss: 0.6697\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1960 - val_loss: 0.6689\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1818 - val_loss: 0.6687\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2053 - val_loss: 0.6681\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1978 - val_loss: 0.6669\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2012 - val_loss: 0.6676\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1912 - val_loss: 0.6677\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1985 - val_loss: 0.6658\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1928 - val_loss: 0.6650\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1971 - val_loss: 0.6647\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1953 - val_loss: 0.6667\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1996 - val_loss: 0.6658\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1878 - val_loss: 0.6663\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1906 - val_loss: 0.6660\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1915 - val_loss: 0.6649\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1876 - val_loss: 0.6651\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1888 - val_loss: 0.6657\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1829 - val_loss: 0.6661\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1803 - val_loss: 0.6663\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1987 - val_loss: 0.6665\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1976 - val_loss: 0.6658\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1955 - val_loss: 0.6669\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2012 - val_loss: 0.6678\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1997 - val_loss: 0.6668\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1917 - val_loss: 0.6680\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1888 - val_loss: 0.6687\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1998 - val_loss: 0.6679\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1905 - val_loss: 0.6675\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1890 - val_loss: 0.6649\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1901 - val_loss: 0.6662\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1892 - val_loss: 0.6654\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2040 - val_loss: 0.6650\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1867 - val_loss: 0.6660\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1796 - val_loss: 0.6656\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1919 - val_loss: 0.6665\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1820 - val_loss: 0.6664\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1844 - val_loss: 0.6666\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1887 - val_loss: 0.6650\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1927 - val_loss: 0.6657\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2071 - val_loss: 0.6641\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1785 - val_loss: 0.6652\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1976 - val_loss: 0.6650\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1900 - val_loss: 0.6666\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1961 - val_loss: 0.6674\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1803 - val_loss: 0.6661\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1927 - val_loss: 0.6642\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1908 - val_loss: 0.6633\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1871 - val_loss: 0.6637\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1845 - val_loss: 0.6648\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1969 - val_loss: 0.6640\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1917 - val_loss: 0.6634\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1864 - val_loss: 0.6628\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1825 - val_loss: 0.6622\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1921 - val_loss: 0.6621\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1893 - val_loss: 0.6620\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1954 - val_loss: 0.6608\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1836 - val_loss: 0.6609\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1837 - val_loss: 0.6620\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1863 - val_loss: 0.6614\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1803 - val_loss: 0.6597\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1856 - val_loss: 0.6602\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1860 - val_loss: 0.6605\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1846 - val_loss: 0.6610\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1894 - val_loss: 0.6604\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1823 - val_loss: 0.6618\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1927 - val_loss: 0.6619\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1880 - val_loss: 0.6597\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1939 - val_loss: 0.6606\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1857 - val_loss: 0.6597\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1965 - val_loss: 0.6612\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1861 - val_loss: 0.6611\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.6607\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1873 - val_loss: 0.6607\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1866 - val_loss: 0.6603\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1911 - val_loss: 0.6601\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1875 - val_loss: 0.6603\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1885 - val_loss: 0.6600\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1939 - val_loss: 0.6604\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1881 - val_loss: 0.6614\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1835 - val_loss: 0.6619\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1802 - val_loss: 0.6608\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1860 - val_loss: 0.6606\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1840 - val_loss: 0.6610\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1828 - val_loss: 0.6621\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1897 - val_loss: 0.6611\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1784 - val_loss: 0.6600\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1925 - val_loss: 0.6611\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1875 - val_loss: 0.6586\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1888 - val_loss: 0.6593\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1825 - val_loss: 0.6599\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1916 - val_loss: 0.6606\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1823 - val_loss: 0.6598\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1772 - val_loss: 0.6604\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1897 - val_loss: 0.6606\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1829 - val_loss: 0.6591\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1862 - val_loss: 0.6564\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1891 - val_loss: 0.6570\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1943 - val_loss: 0.6571\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1883 - val_loss: 0.6582\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1906 - val_loss: 0.6572\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1833 - val_loss: 0.6578\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1911 - val_loss: 0.6571\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1833 - val_loss: 0.6590\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1846 - val_loss: 0.6593\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1753 - val_loss: 0.6598\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1731 - val_loss: 0.6581\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1864 - val_loss: 0.6580\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1879 - val_loss: 0.6598\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1850 - val_loss: 0.6578\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1845 - val_loss: 0.6580\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1965 - val_loss: 0.6570\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1854 - val_loss: 0.6569\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1793 - val_loss: 0.6575\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1861 - val_loss: 0.6578\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1667 - val_loss: 0.6582\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1782 - val_loss: 0.6580\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1761 - val_loss: 0.6584\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1856 - val_loss: 0.6580\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1790 - val_loss: 0.6581\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1851 - val_loss: 0.6561\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1901 - val_loss: 0.6554\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1868 - val_loss: 0.6557\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1858 - val_loss: 0.6578\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1818 - val_loss: 0.6558\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1735 - val_loss: 0.6571\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1911 - val_loss: 0.6563\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1769 - val_loss: 0.6550\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1850 - val_loss: 0.6545\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1896 - val_loss: 0.6542\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1841 - val_loss: 0.6537\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1700 - val_loss: 0.6550\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1792 - val_loss: 0.6550\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1774 - val_loss: 0.6565\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1817 - val_loss: 0.6556\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1863 - val_loss: 0.6579\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1746 - val_loss: 0.6579\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1755 - val_loss: 0.6576\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1817 - val_loss: 0.6569\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1798 - val_loss: 0.6574\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1788 - val_loss: 0.6565\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1846 - val_loss: 0.6554\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1894 - val_loss: 0.6541\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1905 - val_loss: 0.6536\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1757 - val_loss: 0.6542\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1765 - val_loss: 0.6532\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1747 - val_loss: 0.6519\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1772 - val_loss: 0.6524\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1798 - val_loss: 0.6523\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1873 - val_loss: 0.6514\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1840 - val_loss: 0.6514\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1770 - val_loss: 0.6507\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1790 - val_loss: 0.6530\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1762 - val_loss: 0.6515\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1768 - val_loss: 0.6516\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1806 - val_loss: 0.6513\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1738 - val_loss: 0.6507\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1853 - val_loss: 0.6496\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1772 - val_loss: 0.6501\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1733 - val_loss: 0.6498\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1717 - val_loss: 0.6506\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1721 - val_loss: 0.6501\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1726 - val_loss: 0.6489\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1761 - val_loss: 0.6483\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1686 - val_loss: 0.6481\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1776 - val_loss: 0.6485\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1765 - val_loss: 0.6485\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1730 - val_loss: 0.6475\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1743 - val_loss: 0.6497\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1801 - val_loss: 0.6490\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1929 - val_loss: 0.6482\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1720 - val_loss: 0.6468\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1717 - val_loss: 0.6473\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1695 - val_loss: 0.6481\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1774 - val_loss: 0.6479\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1772 - val_loss: 0.6476\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1729 - val_loss: 0.6481\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1739 - val_loss: 0.6471\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1784 - val_loss: 0.6475\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1765 - val_loss: 0.6468\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1670 - val_loss: 0.6482\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1767 - val_loss: 0.6476\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1700 - val_loss: 0.6481\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1770 - val_loss: 0.6471\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1825 - val_loss: 0.6477\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1694 - val_loss: 0.6481\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1692 - val_loss: 0.6475\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1704 - val_loss: 0.6473\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1702 - val_loss: 0.6483\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1762 - val_loss: 0.6485\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1727 - val_loss: 0.6478\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1763 - val_loss: 0.6482\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1731 - val_loss: 0.6480\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1684 - val_loss: 0.6468\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1798 - val_loss: 0.6474\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1786 - val_loss: 0.6474\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1693 - val_loss: 0.6470\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1766 - val_loss: 0.6461\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1773 - val_loss: 0.6457\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1769 - val_loss: 0.6453\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1690 - val_loss: 0.6443\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1718 - val_loss: 0.6439\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.6439\n",
            "7/7 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbfc4d72800>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number\n",
        "nb_epochs=1000;    # training epochs change\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition4   # choose 4th lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "4qkevaibPMua"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_AV5HY31ii4"
      },
      "source": [
        "#4. Data Splitting Test for Possibilty Distribution#  \n",
        "In thie part, the data will be splitted to:\n",
        "- 4.1 train on one part of the color space and test on an the other part\n",
        "- 4.2 train on data under 3 light and test on 1 light condition \n",
        "  \n",
        "In total, 200 color pairs and 4 lights."
      ],
      "id": "2_AV5HY31ii4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCb-2QO9B5uZ"
      },
      "source": [
        "## 4.1 Possibility Distribution: Train on part of the color space and test on the rest part ##\n"
      ],
      "id": "lCb-2QO9B5uZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IK3zenSIEDL-",
        "outputId": "ee34ae7a-7b0d-4c7e-f792-09423fbe1d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "45/45 [==============================] - 22s 27ms/step - loss: 1.0479 - val_loss: 1.0420\n",
            "Epoch 2/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 1.0391 - val_loss: 1.0338\n",
            "Epoch 3/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 1.0306 - val_loss: 1.0255\n",
            "Epoch 4/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 1.0219 - val_loss: 1.0168\n",
            "Epoch 5/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 1.0132 - val_loss: 1.0079\n",
            "Epoch 6/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 1.0040 - val_loss: 0.9992\n",
            "Epoch 7/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.9948 - val_loss: 0.9898\n",
            "Epoch 8/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.9853 - val_loss: 0.9806\n",
            "Epoch 9/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.9757 - val_loss: 0.9713\n",
            "Epoch 10/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.9659 - val_loss: 0.9615\n",
            "Epoch 11/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.9561 - val_loss: 0.9517\n",
            "Epoch 12/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.9454 - val_loss: 0.9415\n",
            "Epoch 13/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.9351 - val_loss: 0.9316\n",
            "Epoch 14/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.9248 - val_loss: 0.9215\n",
            "Epoch 15/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.9136 - val_loss: 0.9101\n",
            "Epoch 16/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.9026 - val_loss: 0.8991\n",
            "Epoch 17/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.8914 - val_loss: 0.8884\n",
            "Epoch 18/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.8790 - val_loss: 0.8763\n",
            "Epoch 19/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.8667 - val_loss: 0.8654\n",
            "Epoch 20/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.8546 - val_loss: 0.8519\n",
            "Epoch 21/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.8406 - val_loss: 0.8424\n",
            "Epoch 22/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.8277 - val_loss: 0.8266\n",
            "Epoch 23/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.8136 - val_loss: 0.8157\n",
            "Epoch 24/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.7983 - val_loss: 0.7958\n",
            "Epoch 25/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.7800 - val_loss: 0.7825\n",
            "Epoch 26/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.7620 - val_loss: 0.7676\n",
            "Epoch 27/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.7476 - val_loss: 0.7472\n",
            "Epoch 28/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.7242 - val_loss: 0.7279\n",
            "Epoch 29/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.7015 - val_loss: 0.7023\n",
            "Epoch 30/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.6529 - val_loss: 0.6977\n",
            "Epoch 31/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.5802 - val_loss: 0.5681\n",
            "Epoch 32/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.5099 - val_loss: 0.5291\n",
            "Epoch 33/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.4974 - val_loss: 0.5674\n",
            "Epoch 34/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4842 - val_loss: 0.4977\n",
            "Epoch 35/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.4741 - val_loss: 0.4894\n",
            "Epoch 36/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4561 - val_loss: 0.4778\n",
            "Epoch 37/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4405 - val_loss: 0.4726\n",
            "Epoch 38/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4343 - val_loss: 0.4766\n",
            "Epoch 39/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4272 - val_loss: 0.4630\n",
            "Epoch 40/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4230 - val_loss: 0.4680\n",
            "Epoch 41/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4211 - val_loss: 0.4605\n",
            "Epoch 42/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4236 - val_loss: 0.4562\n",
            "Epoch 43/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4143 - val_loss: 0.4709\n",
            "Epoch 44/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.4095 - val_loss: 0.4611\n",
            "Epoch 45/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.4066 - val_loss: 0.4617\n",
            "Epoch 46/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.4102 - val_loss: 0.4555\n",
            "Epoch 47/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4055 - val_loss: 0.4606\n",
            "Epoch 48/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4081 - val_loss: 0.4577\n",
            "Epoch 49/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4000 - val_loss: 0.4532\n",
            "Epoch 50/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3986 - val_loss: 0.4571\n",
            "Epoch 51/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4008 - val_loss: 0.4648\n",
            "Epoch 52/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4020 - val_loss: 0.4425\n",
            "Epoch 53/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3953 - val_loss: 0.4498\n",
            "Epoch 54/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3945 - val_loss: 0.4403\n",
            "Epoch 55/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3927 - val_loss: 0.4528\n",
            "Epoch 56/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3955 - val_loss: 0.4456\n",
            "Epoch 57/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.4027 - val_loss: 0.4488\n",
            "Epoch 58/1000\n",
            "45/45 [==============================] - 1s 26ms/step - loss: 0.3999 - val_loss: 0.4383\n",
            "Epoch 59/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3995 - val_loss: 0.4431\n",
            "Epoch 60/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3927 - val_loss: 0.4375\n",
            "Epoch 61/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3891 - val_loss: 0.4528\n",
            "Epoch 62/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3908 - val_loss: 0.4509\n",
            "Epoch 63/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3854 - val_loss: 0.4556\n",
            "Epoch 64/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3841 - val_loss: 0.4647\n",
            "Epoch 65/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3941 - val_loss: 0.4492\n",
            "Epoch 66/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3835 - val_loss: 0.4439\n",
            "Epoch 67/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3835 - val_loss: 0.4445\n",
            "Epoch 68/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3843 - val_loss: 0.4479\n",
            "Epoch 69/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3842 - val_loss: 0.4343\n",
            "Epoch 70/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3825 - val_loss: 0.4434\n",
            "Epoch 71/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3802 - val_loss: 0.4352\n",
            "Epoch 72/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3846 - val_loss: 0.4653\n",
            "Epoch 73/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3781 - val_loss: 0.4384\n",
            "Epoch 74/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3792 - val_loss: 0.4304\n",
            "Epoch 75/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3825 - val_loss: 0.4452\n",
            "Epoch 76/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3784 - val_loss: 0.4372\n",
            "Epoch 77/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3766 - val_loss: 0.4412\n",
            "Epoch 78/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3784 - val_loss: 0.4371\n",
            "Epoch 79/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3782 - val_loss: 0.4372\n",
            "Epoch 80/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3787 - val_loss: 0.4350\n",
            "Epoch 81/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3754 - val_loss: 0.4567\n",
            "Epoch 82/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3759 - val_loss: 0.4290\n",
            "Epoch 83/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3767 - val_loss: 0.4319\n",
            "Epoch 84/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3774 - val_loss: 0.4632\n",
            "Epoch 85/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3771 - val_loss: 0.4252\n",
            "Epoch 86/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3818 - val_loss: 0.4363\n",
            "Epoch 87/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3715 - val_loss: 0.4409\n",
            "Epoch 88/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3755 - val_loss: 0.4357\n",
            "Epoch 89/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3760 - val_loss: 0.4349\n",
            "Epoch 90/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3730 - val_loss: 0.4325\n",
            "Epoch 91/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3704 - val_loss: 0.4297\n",
            "Epoch 92/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3714 - val_loss: 0.4265\n",
            "Epoch 93/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3765 - val_loss: 0.4512\n",
            "Epoch 94/1000\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.3757 - val_loss: 0.4396\n",
            "Epoch 95/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3706 - val_loss: 0.4594\n",
            "Epoch 96/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3699 - val_loss: 0.4366\n",
            "Epoch 97/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3684 - val_loss: 0.4491\n",
            "Epoch 98/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3727 - val_loss: 0.4273\n",
            "Epoch 99/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3685 - val_loss: 0.4417\n",
            "Epoch 100/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3786 - val_loss: 0.4509\n",
            "Epoch 101/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3696 - val_loss: 0.4450\n",
            "Epoch 102/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3686 - val_loss: 0.4328\n",
            "Epoch 103/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3650 - val_loss: 0.4214\n",
            "Epoch 104/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3661 - val_loss: 0.4553\n",
            "Epoch 105/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3679 - val_loss: 0.4294\n",
            "Epoch 106/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3673 - val_loss: 0.4220\n",
            "Epoch 107/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3627 - val_loss: 0.4349\n",
            "Epoch 108/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3607 - val_loss: 0.4289\n",
            "Epoch 109/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3642 - val_loss: 0.4301\n",
            "Epoch 110/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3622 - val_loss: 0.4216\n",
            "Epoch 111/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3601 - val_loss: 0.4569\n",
            "Epoch 112/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3647 - val_loss: 0.4439\n",
            "Epoch 113/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3618 - val_loss: 0.4319\n",
            "Epoch 114/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3607 - val_loss: 0.4218\n",
            "Epoch 115/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3591 - val_loss: 0.4225\n",
            "Epoch 116/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3579 - val_loss: 0.4161\n",
            "Epoch 117/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3595 - val_loss: 0.4175\n",
            "Epoch 118/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3603 - val_loss: 0.4293\n",
            "Epoch 119/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3609 - val_loss: 0.4273\n",
            "Epoch 120/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3567 - val_loss: 0.4238\n",
            "Epoch 121/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3595 - val_loss: 0.4292\n",
            "Epoch 122/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3618 - val_loss: 0.4243\n",
            "Epoch 123/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3578 - val_loss: 0.4183\n",
            "Epoch 124/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3571 - val_loss: 0.4201\n",
            "Epoch 125/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3578 - val_loss: 0.4393\n",
            "Epoch 126/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3610 - val_loss: 0.4207\n",
            "Epoch 127/1000\n",
            "45/45 [==============================] - 2s 41ms/step - loss: 0.3549 - val_loss: 0.4197\n",
            "Epoch 128/1000\n",
            "45/45 [==============================] - 2s 38ms/step - loss: 0.3592 - val_loss: 0.4239\n",
            "Epoch 129/1000\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.3597 - val_loss: 0.4288\n",
            "Epoch 130/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3585 - val_loss: 0.4241\n",
            "Epoch 131/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3582 - val_loss: 0.4192\n",
            "Epoch 132/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3596 - val_loss: 0.4170\n",
            "Epoch 133/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3540 - val_loss: 0.4138\n",
            "Epoch 134/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3535 - val_loss: 0.4197\n",
            "Epoch 135/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3562 - val_loss: 0.4291\n",
            "Epoch 136/1000\n",
            "45/45 [==============================] - 2s 50ms/step - loss: 0.3548 - val_loss: 0.4182\n",
            "Epoch 137/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3571 - val_loss: 0.4104\n",
            "Epoch 138/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3554 - val_loss: 0.4249\n",
            "Epoch 139/1000\n",
            "45/45 [==============================] - 1s 26ms/step - loss: 0.3545 - val_loss: 0.4222\n",
            "Epoch 140/1000\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.3532 - val_loss: 0.4125\n",
            "Epoch 141/1000\n",
            "45/45 [==============================] - 2s 36ms/step - loss: 0.3563 - val_loss: 0.4114\n",
            "Epoch 142/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3554 - val_loss: 0.4103\n",
            "Epoch 143/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3539 - val_loss: 0.4199\n",
            "Epoch 144/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3580 - val_loss: 0.4200\n",
            "Epoch 145/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3543 - val_loss: 0.4144\n",
            "Epoch 146/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3566 - val_loss: 0.4117\n",
            "Epoch 147/1000\n",
            "45/45 [==============================] - 2s 36ms/step - loss: 0.3556 - val_loss: 0.4123\n",
            "Epoch 148/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3542 - val_loss: 0.4223\n",
            "Epoch 149/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3548 - val_loss: 0.4121\n",
            "Epoch 150/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3553 - val_loss: 0.4066\n",
            "Epoch 151/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3576 - val_loss: 0.4250\n",
            "Epoch 152/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3552 - val_loss: 0.4207\n",
            "Epoch 153/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3522 - val_loss: 0.4212\n",
            "Epoch 154/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3558 - val_loss: 0.4096\n",
            "Epoch 155/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3516 - val_loss: 0.4056\n",
            "Epoch 156/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3528 - val_loss: 0.4083\n",
            "Epoch 157/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3521 - val_loss: 0.4118\n",
            "Epoch 158/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3525 - val_loss: 0.4122\n",
            "Epoch 159/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3530 - val_loss: 0.4180\n",
            "Epoch 160/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3511 - val_loss: 0.4215\n",
            "Epoch 161/1000\n",
            "45/45 [==============================] - 1s 26ms/step - loss: 0.3533 - val_loss: 0.4112\n",
            "Epoch 162/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3496 - val_loss: 0.4205\n",
            "Epoch 163/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3503 - val_loss: 0.4130\n",
            "Epoch 164/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3497 - val_loss: 0.4237\n",
            "Epoch 165/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3508 - val_loss: 0.4143\n",
            "Epoch 166/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3532 - val_loss: 0.4335\n",
            "Epoch 167/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3509 - val_loss: 0.4135\n",
            "Epoch 168/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3533 - val_loss: 0.4118\n",
            "Epoch 169/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3557 - val_loss: 0.4133\n",
            "Epoch 170/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3535 - val_loss: 0.4119\n",
            "Epoch 171/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3533 - val_loss: 0.4088\n",
            "Epoch 172/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3542 - val_loss: 0.4219\n",
            "Epoch 173/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3551 - val_loss: 0.4123\n",
            "Epoch 174/1000\n",
            "45/45 [==============================] - 2s 49ms/step - loss: 0.3517 - val_loss: 0.4084\n",
            "Epoch 175/1000\n",
            "45/45 [==============================] - 2s 42ms/step - loss: 0.3505 - val_loss: 0.4117\n",
            "Epoch 176/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3502 - val_loss: 0.4119\n",
            "Epoch 177/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3506 - val_loss: 0.4101\n",
            "Epoch 178/1000\n",
            "45/45 [==============================] - 1s 26ms/step - loss: 0.3507 - val_loss: 0.4070\n",
            "Epoch 179/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3506 - val_loss: 0.4240\n",
            "Epoch 180/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3516 - val_loss: 0.4121\n",
            "Epoch 181/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3514 - val_loss: 0.4101\n",
            "Epoch 182/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3515 - val_loss: 0.4099\n",
            "Epoch 183/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3494 - val_loss: 0.4079\n",
            "Epoch 184/1000\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.3507 - val_loss: 0.4227\n",
            "Epoch 185/1000\n",
            "45/45 [==============================] - 2s 38ms/step - loss: 0.3490 - val_loss: 0.4098\n",
            "Epoch 186/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3511 - val_loss: 0.4106\n",
            "Epoch 187/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3505 - val_loss: 0.4188\n",
            "Epoch 188/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3495 - val_loss: 0.4097\n",
            "Epoch 189/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3504 - val_loss: 0.4190\n",
            "Epoch 190/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3519 - val_loss: 0.4098\n",
            "Epoch 191/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3488 - val_loss: 0.4063\n",
            "Epoch 192/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3514 - val_loss: 0.4151\n",
            "Epoch 193/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3489 - val_loss: 0.4109\n",
            "Epoch 194/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3491 - val_loss: 0.4102\n",
            "Epoch 195/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3491 - val_loss: 0.4063\n",
            "Epoch 196/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3495 - val_loss: 0.4106\n",
            "Epoch 197/1000\n",
            "45/45 [==============================] - 2s 44ms/step - loss: 0.3497 - val_loss: 0.4128\n",
            "Epoch 198/1000\n",
            "45/45 [==============================] - 2s 46ms/step - loss: 0.3534 - val_loss: 0.4181\n",
            "Epoch 199/1000\n",
            "45/45 [==============================] - 2s 40ms/step - loss: 0.3513 - val_loss: 0.4097\n",
            "Epoch 200/1000\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.3501 - val_loss: 0.4165\n",
            "Epoch 201/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3491 - val_loss: 0.4171\n",
            "Epoch 202/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3505 - val_loss: 0.4147\n",
            "Epoch 203/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3516 - val_loss: 0.4280\n",
            "Epoch 204/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3485 - val_loss: 0.4110\n",
            "Epoch 205/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3478 - val_loss: 0.4104\n",
            "Epoch 206/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3486 - val_loss: 0.4084\n",
            "Epoch 207/1000\n",
            "45/45 [==============================] - 2s 37ms/step - loss: 0.3465 - val_loss: 0.4112\n",
            "Epoch 208/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3473 - val_loss: 0.4111\n",
            "Epoch 209/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3490 - val_loss: 0.4133\n",
            "Epoch 210/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3498 - val_loss: 0.4099\n",
            "Epoch 211/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3484 - val_loss: 0.4102\n",
            "Epoch 212/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3460 - val_loss: 0.4076\n",
            "Epoch 213/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3480 - val_loss: 0.4075\n",
            "Epoch 214/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3483 - val_loss: 0.4311\n",
            "Epoch 215/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3497 - val_loss: 0.4112\n",
            "Epoch 216/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3502 - val_loss: 0.4147\n",
            "Epoch 217/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3498 - val_loss: 0.4108\n",
            "Epoch 218/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3483 - val_loss: 0.4080\n",
            "Epoch 219/1000\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.3477 - val_loss: 0.4055\n",
            "Epoch 220/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3481 - val_loss: 0.4045\n",
            "Epoch 221/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3486 - val_loss: 0.4130\n",
            "Epoch 222/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3490 - val_loss: 0.4130\n",
            "Epoch 223/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3495 - val_loss: 0.4125\n",
            "Epoch 224/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3517 - val_loss: 0.4167\n",
            "Epoch 225/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3567 - val_loss: 0.4293\n",
            "Epoch 226/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3520 - val_loss: 0.4057\n",
            "Epoch 227/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3488 - val_loss: 0.4076\n",
            "Epoch 228/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3485 - val_loss: 0.4107\n",
            "Epoch 229/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3470 - val_loss: 0.4139\n",
            "Epoch 230/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3481 - val_loss: 0.4145\n",
            "Epoch 231/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3488 - val_loss: 0.4160\n",
            "Epoch 232/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3495 - val_loss: 0.4083\n",
            "Epoch 233/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3491 - val_loss: 0.4073\n",
            "Epoch 234/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3462 - val_loss: 0.4098\n",
            "Epoch 235/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3451 - val_loss: 0.3999\n",
            "Epoch 236/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3460 - val_loss: 0.4143\n",
            "Epoch 237/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3476 - val_loss: 0.4058\n",
            "Epoch 238/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3462 - val_loss: 0.4092\n",
            "Epoch 239/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3472 - val_loss: 0.4109\n",
            "Epoch 240/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3481 - val_loss: 0.4185\n",
            "Epoch 241/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3486 - val_loss: 0.4129\n",
            "Epoch 242/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3473 - val_loss: 0.4161\n",
            "Epoch 243/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3454 - val_loss: 0.4101\n",
            "Epoch 244/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3458 - val_loss: 0.4048\n",
            "Epoch 245/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3470 - val_loss: 0.4080\n",
            "Epoch 246/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3490 - val_loss: 0.4080\n",
            "Epoch 247/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3506 - val_loss: 0.4097\n",
            "Epoch 248/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3451 - val_loss: 0.4090\n",
            "Epoch 249/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3494 - val_loss: 0.4123\n",
            "Epoch 250/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3481 - val_loss: 0.4038\n",
            "Epoch 251/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3465 - val_loss: 0.4103\n",
            "Epoch 252/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3525 - val_loss: 0.4058\n",
            "Epoch 253/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3500 - val_loss: 0.4076\n",
            "Epoch 254/1000\n",
            "45/45 [==============================] - 2s 34ms/step - loss: 0.3463 - val_loss: 0.4060\n",
            "Epoch 255/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3474 - val_loss: 0.4058\n",
            "Epoch 256/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3460 - val_loss: 0.4079\n",
            "Epoch 257/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3470 - val_loss: 0.4020\n",
            "Epoch 258/1000\n",
            "45/45 [==============================] - 2s 37ms/step - loss: 0.3478 - val_loss: 0.4147\n",
            "Epoch 259/1000\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.3493 - val_loss: 0.4057\n",
            "Epoch 260/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3464 - val_loss: 0.4095\n",
            "Epoch 261/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3476 - val_loss: 0.4025\n",
            "Epoch 262/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3443 - val_loss: 0.4071\n",
            "Epoch 263/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3512 - val_loss: 0.4104\n",
            "Epoch 264/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3475 - val_loss: 0.4166\n",
            "Epoch 265/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3519 - val_loss: 0.4006\n",
            "Epoch 266/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3464 - val_loss: 0.3999\n",
            "Epoch 267/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3474 - val_loss: 0.4090\n",
            "Epoch 268/1000\n",
            "45/45 [==============================] - 2s 36ms/step - loss: 0.3462 - val_loss: 0.4189\n",
            "Epoch 269/1000\n",
            "45/45 [==============================] - 2s 37ms/step - loss: 0.3496 - val_loss: 0.4120\n",
            "Epoch 270/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3475 - val_loss: 0.4162\n",
            "Epoch 271/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3478 - val_loss: 0.4185\n",
            "Epoch 272/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3471 - val_loss: 0.4125\n",
            "Epoch 273/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3464 - val_loss: 0.4070\n",
            "Epoch 274/1000\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.3450 - val_loss: 0.4087\n",
            "Epoch 275/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3467 - val_loss: 0.4070\n",
            "Epoch 276/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3498 - val_loss: 0.4074\n",
            "Epoch 277/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3495 - val_loss: 0.4137\n",
            "Epoch 278/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3464 - val_loss: 0.4098\n",
            "Epoch 279/1000\n",
            "45/45 [==============================] - 2s 38ms/step - loss: 0.3472 - val_loss: 0.4077\n",
            "Epoch 280/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3455 - val_loss: 0.4097\n",
            "Epoch 281/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3448 - val_loss: 0.4127\n",
            "Epoch 282/1000\n",
            "45/45 [==============================] - 2s 36ms/step - loss: 0.3451 - val_loss: 0.4051\n",
            "Epoch 283/1000\n",
            "45/45 [==============================] - 2s 37ms/step - loss: 0.3453 - val_loss: 0.4080\n",
            "Epoch 284/1000\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.3441 - val_loss: 0.4106\n",
            "Epoch 285/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3463 - val_loss: 0.4220\n",
            "Epoch 286/1000\n",
            "45/45 [==============================] - 2s 39ms/step - loss: 0.3474 - val_loss: 0.4078\n",
            "Epoch 287/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3453 - val_loss: 0.4094\n",
            "Epoch 288/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3451 - val_loss: 0.4122\n",
            "Epoch 289/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3477 - val_loss: 0.4194\n",
            "Epoch 290/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3464 - val_loss: 0.4160\n",
            "Epoch 291/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3468 - val_loss: 0.4046\n",
            "Epoch 292/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3442 - val_loss: 0.4071\n",
            "Epoch 293/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3483 - val_loss: 0.4076\n",
            "Epoch 294/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3445 - val_loss: 0.4048\n",
            "Epoch 295/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3461 - val_loss: 0.4108\n",
            "Epoch 296/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3484 - val_loss: 0.4091\n",
            "Epoch 297/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3458 - val_loss: 0.4055\n",
            "Epoch 298/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3450 - val_loss: 0.4051\n",
            "Epoch 299/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3462 - val_loss: 0.4020\n",
            "Epoch 300/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3442 - val_loss: 0.4038\n",
            "Epoch 301/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3471 - val_loss: 0.4090\n",
            "Epoch 302/1000\n",
            "45/45 [==============================] - 2s 38ms/step - loss: 0.3454 - val_loss: 0.4050\n",
            "Epoch 303/1000\n",
            "45/45 [==============================] - 2s 37ms/step - loss: 0.3499 - val_loss: 0.4056\n",
            "Epoch 304/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3474 - val_loss: 0.4025\n",
            "Epoch 305/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3461 - val_loss: 0.4108\n",
            "Epoch 306/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3457 - val_loss: 0.4188\n",
            "Epoch 307/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3421 - val_loss: 0.4052\n",
            "Epoch 308/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3462 - val_loss: 0.4168\n",
            "Epoch 309/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3477 - val_loss: 0.4115\n",
            "Epoch 310/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3462 - val_loss: 0.4085\n",
            "Epoch 311/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3438 - val_loss: 0.4121\n",
            "Epoch 312/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3445 - val_loss: 0.4066\n",
            "Epoch 313/1000\n",
            "45/45 [==============================] - 2s 50ms/step - loss: 0.3428 - val_loss: 0.4110\n",
            "Epoch 314/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3435 - val_loss: 0.4032\n",
            "Epoch 315/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3493 - val_loss: 0.4131\n",
            "Epoch 316/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3458 - val_loss: 0.4052\n",
            "Epoch 317/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3451 - val_loss: 0.4076\n",
            "Epoch 318/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3451 - val_loss: 0.4063\n",
            "Epoch 319/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3424 - val_loss: 0.4058\n",
            "Epoch 320/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3448 - val_loss: 0.4162\n",
            "Epoch 321/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3452 - val_loss: 0.4100\n",
            "Epoch 322/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3496 - val_loss: 0.4140\n",
            "Epoch 323/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3492 - val_loss: 0.4120\n",
            "Epoch 324/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3456 - val_loss: 0.4059\n",
            "Epoch 325/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3469 - val_loss: 0.4073\n",
            "Epoch 326/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3430 - val_loss: 0.4063\n",
            "Epoch 327/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3451 - val_loss: 0.4058\n",
            "Epoch 328/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3434 - val_loss: 0.4040\n",
            "Epoch 329/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3437 - val_loss: 0.4120\n",
            "Epoch 330/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3451 - val_loss: 0.4097\n",
            "Epoch 331/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3462 - val_loss: 0.4053\n",
            "Epoch 332/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3450 - val_loss: 0.4051\n",
            "Epoch 333/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3426 - val_loss: 0.4074\n",
            "Epoch 334/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3452 - val_loss: 0.4077\n",
            "Epoch 335/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3416 - val_loss: 0.4097\n",
            "Epoch 336/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3416 - val_loss: 0.4072\n",
            "Epoch 337/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3418 - val_loss: 0.4042\n",
            "Epoch 338/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3421 - val_loss: 0.4120\n",
            "Epoch 339/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3461 - val_loss: 0.4144\n",
            "Epoch 340/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3561 - val_loss: 0.4091\n",
            "Epoch 341/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3474 - val_loss: 0.4081\n",
            "Epoch 342/1000\n",
            "45/45 [==============================] - 2s 40ms/step - loss: 0.3447 - val_loss: 0.4045\n",
            "Epoch 343/1000\n",
            "45/45 [==============================] - 2s 41ms/step - loss: 0.3452 - val_loss: 0.4097\n",
            "Epoch 344/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3450 - val_loss: 0.4095\n",
            "Epoch 345/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3426 - val_loss: 0.4134\n",
            "Epoch 346/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3486 - val_loss: 0.4121\n",
            "Epoch 347/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3450 - val_loss: 0.4029\n",
            "Epoch 348/1000\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 0.3429 - val_loss: 0.4008\n",
            "Epoch 349/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3433 - val_loss: 0.4135\n",
            "Epoch 350/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3435 - val_loss: 0.4068\n",
            "Epoch 351/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3426 - val_loss: 0.4065\n",
            "Epoch 352/1000\n",
            "45/45 [==============================] - 2s 45ms/step - loss: 0.3460 - val_loss: 0.4055\n",
            "Epoch 353/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3454 - val_loss: 0.4211\n",
            "Epoch 354/1000\n",
            "45/45 [==============================] - 2s 34ms/step - loss: 0.3436 - val_loss: 0.4081\n",
            "Epoch 355/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3438 - val_loss: 0.4104\n",
            "Epoch 356/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3425 - val_loss: 0.4064\n",
            "Epoch 357/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3424 - val_loss: 0.4064\n",
            "Epoch 358/1000\n",
            "45/45 [==============================] - 2s 38ms/step - loss: 0.3423 - val_loss: 0.4072\n",
            "Epoch 359/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3416 - val_loss: 0.4094\n",
            "Epoch 360/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3431 - val_loss: 0.4074\n",
            "Epoch 361/1000\n",
            "45/45 [==============================] - 2s 38ms/step - loss: 0.3444 - val_loss: 0.4051\n",
            "Epoch 362/1000\n",
            "45/45 [==============================] - 2s 54ms/step - loss: 0.3446 - val_loss: 0.4087\n",
            "Epoch 363/1000\n",
            "45/45 [==============================] - 3s 63ms/step - loss: 0.3442 - val_loss: 0.4136\n",
            "Epoch 364/1000\n",
            "45/45 [==============================] - 3s 64ms/step - loss: 0.3442 - val_loss: 0.4094\n",
            "Epoch 365/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3427 - val_loss: 0.4085\n",
            "Epoch 366/1000\n",
            "45/45 [==============================] - 2s 40ms/step - loss: 0.3434 - val_loss: 0.4063\n",
            "Epoch 367/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3424 - val_loss: 0.4028\n",
            "Epoch 368/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3409 - val_loss: 0.4115\n",
            "Epoch 369/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3450 - val_loss: 0.4125\n",
            "Epoch 370/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3460 - val_loss: 0.4061\n",
            "Epoch 371/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3427 - val_loss: 0.4041\n",
            "Epoch 372/1000\n",
            "45/45 [==============================] - 2s 34ms/step - loss: 0.3437 - val_loss: 0.4052\n",
            "Epoch 373/1000\n",
            "45/45 [==============================] - 2s 37ms/step - loss: 0.3422 - val_loss: 0.4073\n",
            "Epoch 374/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3424 - val_loss: 0.4182\n",
            "Epoch 375/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3448 - val_loss: 0.4031\n",
            "Epoch 376/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3414 - val_loss: 0.4053\n",
            "Epoch 377/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3471 - val_loss: 0.4068\n",
            "Epoch 378/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3424 - val_loss: 0.4086\n",
            "Epoch 379/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3422 - val_loss: 0.4076\n",
            "Epoch 380/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3434 - val_loss: 0.4168\n",
            "Epoch 381/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3423 - val_loss: 0.4022\n",
            "Epoch 382/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3446 - val_loss: 0.4053\n",
            "Epoch 383/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3428 - val_loss: 0.4118\n",
            "Epoch 384/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3413 - val_loss: 0.4040\n",
            "Epoch 385/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3411 - val_loss: 0.4089\n",
            "Epoch 386/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3397 - val_loss: 0.4081\n",
            "Epoch 387/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3412 - val_loss: 0.4068\n",
            "Epoch 388/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3453 - val_loss: 0.4093\n",
            "Epoch 389/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3432 - val_loss: 0.4141\n",
            "Epoch 390/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3479 - val_loss: 0.4083\n",
            "Epoch 391/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3429 - val_loss: 0.4118\n",
            "Epoch 392/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3427 - val_loss: 0.4005\n",
            "Epoch 393/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3422 - val_loss: 0.4030\n",
            "Epoch 394/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3409 - val_loss: 0.4077\n",
            "Epoch 395/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3401 - val_loss: 0.4044\n",
            "Epoch 396/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3415 - val_loss: 0.4085\n",
            "Epoch 397/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3430 - val_loss: 0.4044\n",
            "Epoch 398/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3438 - val_loss: 0.4096\n",
            "Epoch 399/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3437 - val_loss: 0.4105\n",
            "Epoch 400/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3409 - val_loss: 0.4114\n",
            "Epoch 401/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3409 - val_loss: 0.4037\n",
            "Epoch 402/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3404 - val_loss: 0.4072\n",
            "Epoch 403/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3459 - val_loss: 0.4052\n",
            "Epoch 404/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3438 - val_loss: 0.4187\n",
            "Epoch 405/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3481 - val_loss: 0.4077\n",
            "Epoch 406/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3432 - val_loss: 0.4066\n",
            "Epoch 407/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3462 - val_loss: 0.4069\n",
            "Epoch 408/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3470 - val_loss: 0.4132\n",
            "Epoch 409/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3444 - val_loss: 0.4076\n",
            "Epoch 410/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3412 - val_loss: 0.4102\n",
            "Epoch 411/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3400 - val_loss: 0.4086\n",
            "Epoch 412/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3429 - val_loss: 0.4068\n",
            "Epoch 413/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3431 - val_loss: 0.4045\n",
            "Epoch 414/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3420 - val_loss: 0.4061\n",
            "Epoch 415/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3405 - val_loss: 0.4081\n",
            "Epoch 416/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3431 - val_loss: 0.4219\n",
            "Epoch 417/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3441 - val_loss: 0.4022\n",
            "Epoch 418/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3447 - val_loss: 0.4091\n",
            "Epoch 419/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3414 - val_loss: 0.4047\n",
            "Epoch 420/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3430 - val_loss: 0.4050\n",
            "Epoch 421/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3434 - val_loss: 0.4017\n",
            "Epoch 422/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3391 - val_loss: 0.4081\n",
            "Epoch 423/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3392 - val_loss: 0.4024\n",
            "Epoch 424/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3397 - val_loss: 0.4044\n",
            "Epoch 425/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3439 - val_loss: 0.4063\n",
            "Epoch 426/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3408 - val_loss: 0.4147\n",
            "Epoch 427/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3407 - val_loss: 0.4029\n",
            "Epoch 428/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3401 - val_loss: 0.4055\n",
            "Epoch 429/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3413 - val_loss: 0.4074\n",
            "Epoch 430/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3411 - val_loss: 0.4008\n",
            "Epoch 431/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3409 - val_loss: 0.4024\n",
            "Epoch 432/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3417 - val_loss: 0.4073\n",
            "Epoch 433/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3441 - val_loss: 0.4104\n",
            "Epoch 434/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3410 - val_loss: 0.4111\n",
            "Epoch 435/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3390 - val_loss: 0.4107\n",
            "Epoch 436/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3385 - val_loss: 0.4082\n",
            "Epoch 437/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3397 - val_loss: 0.4060\n",
            "Epoch 438/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3390 - val_loss: 0.4066\n",
            "Epoch 439/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3398 - val_loss: 0.4157\n",
            "Epoch 440/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3418 - val_loss: 0.4034\n",
            "Epoch 441/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3409 - val_loss: 0.4055\n",
            "Epoch 442/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3413 - val_loss: 0.4056\n",
            "Epoch 443/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3430 - val_loss: 0.4106\n",
            "Epoch 444/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3422 - val_loss: 0.4069\n",
            "Epoch 445/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3413 - val_loss: 0.4039\n",
            "Epoch 446/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3387 - val_loss: 0.4081\n",
            "Epoch 447/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3382 - val_loss: 0.4073\n",
            "Epoch 448/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3409 - val_loss: 0.4125\n",
            "Epoch 449/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3439 - val_loss: 0.4159\n",
            "Epoch 450/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3395 - val_loss: 0.4136\n",
            "Epoch 451/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3397 - val_loss: 0.4068\n",
            "Epoch 452/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3405 - val_loss: 0.4084\n",
            "Epoch 453/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3412 - val_loss: 0.4138\n",
            "Epoch 454/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3442 - val_loss: 0.4052\n",
            "Epoch 455/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3411 - val_loss: 0.4096\n",
            "Epoch 456/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3403 - val_loss: 0.4223\n",
            "Epoch 457/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3402 - val_loss: 0.4043\n",
            "Epoch 458/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3374 - val_loss: 0.4040\n",
            "Epoch 459/1000\n",
            "45/45 [==============================] - 2s 36ms/step - loss: 0.3386 - val_loss: 0.4076\n",
            "Epoch 460/1000\n",
            "45/45 [==============================] - 2s 38ms/step - loss: 0.3414 - val_loss: 0.4065\n",
            "Epoch 461/1000\n",
            "45/45 [==============================] - 2s 38ms/step - loss: 0.3391 - val_loss: 0.4114\n",
            "Epoch 462/1000\n",
            "45/45 [==============================] - 2s 34ms/step - loss: 0.3398 - val_loss: 0.4066\n",
            "Epoch 463/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3390 - val_loss: 0.4010\n",
            "Epoch 464/1000\n",
            "45/45 [==============================] - 1s 30ms/step - loss: 0.3387 - val_loss: 0.4044\n",
            "Epoch 465/1000\n",
            "45/45 [==============================] - 1s 26ms/step - loss: 0.3374 - val_loss: 0.4077\n",
            "Epoch 466/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3389 - val_loss: 0.4031\n",
            "Epoch 467/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3375 - val_loss: 0.4025\n",
            "Epoch 468/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3395 - val_loss: 0.4087\n",
            "Epoch 469/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3389 - val_loss: 0.4121\n",
            "Epoch 470/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3408 - val_loss: 0.4034\n",
            "Epoch 471/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3400 - val_loss: 0.4101\n",
            "Epoch 472/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3426 - val_loss: 0.4019\n",
            "Epoch 473/1000\n",
            "45/45 [==============================] - 1s 33ms/step - loss: 0.3419 - val_loss: 0.4083\n",
            "Epoch 474/1000\n",
            "45/45 [==============================] - 2s 34ms/step - loss: 0.3396 - val_loss: 0.4042\n",
            "Epoch 475/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3424 - val_loss: 0.4062\n",
            "Epoch 476/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3410 - val_loss: 0.4122\n",
            "Epoch 477/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3400 - val_loss: 0.4027\n",
            "Epoch 478/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3412 - val_loss: 0.4050\n",
            "Epoch 479/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3395 - val_loss: 0.4073\n",
            "Epoch 480/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3394 - val_loss: 0.4113\n",
            "Epoch 481/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3396 - val_loss: 0.4107\n",
            "Epoch 482/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3409 - val_loss: 0.4033\n",
            "Epoch 483/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3396 - val_loss: 0.4082\n",
            "Epoch 484/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3432 - val_loss: 0.4119\n",
            "Epoch 485/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3410 - val_loss: 0.4060\n",
            "Epoch 486/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3434 - val_loss: 0.4088\n",
            "Epoch 487/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3437 - val_loss: 0.4067\n",
            "Epoch 488/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3393 - val_loss: 0.4052\n",
            "Epoch 489/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3395 - val_loss: 0.4090\n",
            "Epoch 490/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3399 - val_loss: 0.4085\n",
            "Epoch 491/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3391 - val_loss: 0.4061\n",
            "Epoch 492/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3401 - val_loss: 0.4038\n",
            "Epoch 493/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3403 - val_loss: 0.4089\n",
            "Epoch 494/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3417 - val_loss: 0.4096\n",
            "Epoch 495/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3394 - val_loss: 0.4069\n",
            "Epoch 496/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3398 - val_loss: 0.4057\n",
            "Epoch 497/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3406 - val_loss: 0.4052\n",
            "Epoch 498/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3385 - val_loss: 0.4088\n",
            "Epoch 499/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3405 - val_loss: 0.4058\n",
            "Epoch 500/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3385 - val_loss: 0.4038\n",
            "Epoch 501/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3392 - val_loss: 0.4110\n",
            "Epoch 502/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3385 - val_loss: 0.4100\n",
            "Epoch 503/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3373 - val_loss: 0.4051\n",
            "Epoch 504/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3373 - val_loss: 0.4030\n",
            "Epoch 505/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3407 - val_loss: 0.4138\n",
            "Epoch 506/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3384 - val_loss: 0.4040\n",
            "Epoch 507/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3377 - val_loss: 0.4065\n",
            "Epoch 508/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3428 - val_loss: 0.4038\n",
            "Epoch 509/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3391 - val_loss: 0.4112\n",
            "Epoch 510/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3369 - val_loss: 0.4120\n",
            "Epoch 511/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3404 - val_loss: 0.4028\n",
            "Epoch 512/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3405 - val_loss: 0.4047\n",
            "Epoch 513/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3382 - val_loss: 0.4047\n",
            "Epoch 514/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3454 - val_loss: 0.4057\n",
            "Epoch 515/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3402 - val_loss: 0.4058\n",
            "Epoch 516/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3382 - val_loss: 0.4093\n",
            "Epoch 517/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3420 - val_loss: 0.4052\n",
            "Epoch 518/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3386 - val_loss: 0.4079\n",
            "Epoch 519/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3373 - val_loss: 0.4073\n",
            "Epoch 520/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3385 - val_loss: 0.4022\n",
            "Epoch 521/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3403 - val_loss: 0.4079\n",
            "Epoch 522/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3388 - val_loss: 0.4208\n",
            "Epoch 523/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3415 - val_loss: 0.4036\n",
            "Epoch 524/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3388 - val_loss: 0.4056\n",
            "Epoch 525/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3374 - val_loss: 0.4120\n",
            "Epoch 526/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3368 - val_loss: 0.4069\n",
            "Epoch 527/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3377 - val_loss: 0.4066\n",
            "Epoch 528/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3375 - val_loss: 0.4094\n",
            "Epoch 529/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3424 - val_loss: 0.4042\n",
            "Epoch 530/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3379 - val_loss: 0.4047\n",
            "Epoch 531/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3382 - val_loss: 0.4090\n",
            "Epoch 532/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3371 - val_loss: 0.4049\n",
            "Epoch 533/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3372 - val_loss: 0.4047\n",
            "Epoch 534/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3386 - val_loss: 0.4091\n",
            "Epoch 535/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3390 - val_loss: 0.4058\n",
            "Epoch 536/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3390 - val_loss: 0.4037\n",
            "Epoch 537/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3406 - val_loss: 0.4019\n",
            "Epoch 538/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3402 - val_loss: 0.4043\n",
            "Epoch 539/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3402 - val_loss: 0.4030\n",
            "Epoch 540/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3399 - val_loss: 0.4315\n",
            "Epoch 541/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3416 - val_loss: 0.4222\n",
            "Epoch 542/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3408 - val_loss: 0.4141\n",
            "Epoch 543/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3408 - val_loss: 0.3996\n",
            "Epoch 544/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3410 - val_loss: 0.4066\n",
            "Epoch 545/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3396 - val_loss: 0.4095\n",
            "Epoch 546/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3393 - val_loss: 0.4060\n",
            "Epoch 547/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3376 - val_loss: 0.4047\n",
            "Epoch 548/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3361 - val_loss: 0.4034\n",
            "Epoch 549/1000\n",
            "45/45 [==============================] - 1s 32ms/step - loss: 0.3365 - val_loss: 0.4052\n",
            "Epoch 550/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3364 - val_loss: 0.4023\n",
            "Epoch 551/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3370 - val_loss: 0.4026\n",
            "Epoch 552/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3385 - val_loss: 0.4209\n",
            "Epoch 553/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3417 - val_loss: 0.4176\n",
            "Epoch 554/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3454 - val_loss: 0.4066\n",
            "Epoch 555/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3443 - val_loss: 0.4065\n",
            "Epoch 556/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3429 - val_loss: 0.4113\n",
            "Epoch 557/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3425 - val_loss: 0.4032\n",
            "Epoch 558/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3386 - val_loss: 0.4031\n",
            "Epoch 559/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3380 - val_loss: 0.4030\n",
            "Epoch 560/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3382 - val_loss: 0.4042\n",
            "Epoch 561/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3379 - val_loss: 0.4084\n",
            "Epoch 562/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3392 - val_loss: 0.4073\n",
            "Epoch 563/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3390 - val_loss: 0.4096\n",
            "Epoch 564/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3372 - val_loss: 0.4070\n",
            "Epoch 565/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3379 - val_loss: 0.4001\n",
            "Epoch 566/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3396 - val_loss: 0.4110\n",
            "Epoch 567/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3393 - val_loss: 0.4161\n",
            "Epoch 568/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3369 - val_loss: 0.4090\n",
            "Epoch 569/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3374 - val_loss: 0.4085\n",
            "Epoch 570/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3384 - val_loss: 0.4036\n",
            "Epoch 571/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3363 - val_loss: 0.4064\n",
            "Epoch 572/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3370 - val_loss: 0.4107\n",
            "Epoch 573/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3373 - val_loss: 0.4082\n",
            "Epoch 574/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3383 - val_loss: 0.4071\n",
            "Epoch 575/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3376 - val_loss: 0.4024\n",
            "Epoch 576/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3405 - val_loss: 0.4078\n",
            "Epoch 577/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3408 - val_loss: 0.4086\n",
            "Epoch 578/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3379 - val_loss: 0.4014\n",
            "Epoch 579/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3383 - val_loss: 0.4038\n",
            "Epoch 580/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3377 - val_loss: 0.4103\n",
            "Epoch 581/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3370 - val_loss: 0.4012\n",
            "Epoch 582/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3379 - val_loss: 0.4002\n",
            "Epoch 583/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3371 - val_loss: 0.4032\n",
            "Epoch 584/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3370 - val_loss: 0.3985\n",
            "Epoch 585/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3357 - val_loss: 0.4029\n",
            "Epoch 586/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3345 - val_loss: 0.4044\n",
            "Epoch 587/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3379 - val_loss: 0.4108\n",
            "Epoch 588/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3384 - val_loss: 0.4067\n",
            "Epoch 589/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3353 - val_loss: 0.4121\n",
            "Epoch 590/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3379 - val_loss: 0.4088\n",
            "Epoch 591/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3384 - val_loss: 0.4096\n",
            "Epoch 592/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3377 - val_loss: 0.4077\n",
            "Epoch 593/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3415 - val_loss: 0.4030\n",
            "Epoch 594/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3380 - val_loss: 0.4032\n",
            "Epoch 595/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3354 - val_loss: 0.4037\n",
            "Epoch 596/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3360 - val_loss: 0.4028\n",
            "Epoch 597/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3375 - val_loss: 0.4008\n",
            "Epoch 598/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3408 - val_loss: 0.4051\n",
            "Epoch 599/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3384 - val_loss: 0.4078\n",
            "Epoch 600/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3376 - val_loss: 0.4066\n",
            "Epoch 601/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3389 - val_loss: 0.4028\n",
            "Epoch 602/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3359 - val_loss: 0.4025\n",
            "Epoch 603/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3375 - val_loss: 0.4000\n",
            "Epoch 604/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3352 - val_loss: 0.4064\n",
            "Epoch 605/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3362 - val_loss: 0.4047\n",
            "Epoch 606/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3373 - val_loss: 0.4035\n",
            "Epoch 607/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3350 - val_loss: 0.4071\n",
            "Epoch 608/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3392 - val_loss: 0.4040\n",
            "Epoch 609/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3361 - val_loss: 0.4009\n",
            "Epoch 610/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3361 - val_loss: 0.4044\n",
            "Epoch 611/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3364 - val_loss: 0.4101\n",
            "Epoch 612/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3388 - val_loss: 0.4040\n",
            "Epoch 613/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3349 - val_loss: 0.4014\n",
            "Epoch 614/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3380 - val_loss: 0.4046\n",
            "Epoch 615/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3375 - val_loss: 0.4042\n",
            "Epoch 616/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3394 - val_loss: 0.4056\n",
            "Epoch 617/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3377 - val_loss: 0.4076\n",
            "Epoch 618/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3381 - val_loss: 0.4080\n",
            "Epoch 619/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3361 - val_loss: 0.4007\n",
            "Epoch 620/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3388 - val_loss: 0.4056\n",
            "Epoch 621/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3404 - val_loss: 0.4089\n",
            "Epoch 622/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3390 - val_loss: 0.4027\n",
            "Epoch 623/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3381 - val_loss: 0.4111\n",
            "Epoch 624/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3390 - val_loss: 0.4042\n",
            "Epoch 625/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3384 - val_loss: 0.4075\n",
            "Epoch 626/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3360 - val_loss: 0.4050\n",
            "Epoch 627/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3418 - val_loss: 0.4029\n",
            "Epoch 628/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3384 - val_loss: 0.4053\n",
            "Epoch 629/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3348 - val_loss: 0.4042\n",
            "Epoch 630/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3441 - val_loss: 0.4171\n",
            "Epoch 631/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3409 - val_loss: 0.4028\n",
            "Epoch 632/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3382 - val_loss: 0.4029\n",
            "Epoch 633/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3367 - val_loss: 0.4089\n",
            "Epoch 634/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3377 - val_loss: 0.4066\n",
            "Epoch 635/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3350 - val_loss: 0.4014\n",
            "Epoch 636/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3367 - val_loss: 0.4071\n",
            "Epoch 637/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3358 - val_loss: 0.4113\n",
            "Epoch 638/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3357 - val_loss: 0.4073\n",
            "Epoch 639/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3374 - val_loss: 0.4166\n",
            "Epoch 640/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3395 - val_loss: 0.4033\n",
            "Epoch 641/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3365 - val_loss: 0.4019\n",
            "Epoch 642/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3367 - val_loss: 0.4080\n",
            "Epoch 643/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3404 - val_loss: 0.4046\n",
            "Epoch 644/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3386 - val_loss: 0.4016\n",
            "Epoch 645/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3383 - val_loss: 0.4124\n",
            "Epoch 646/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3435 - val_loss: 0.4071\n",
            "Epoch 647/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3419 - val_loss: 0.4066\n",
            "Epoch 648/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3357 - val_loss: 0.4091\n",
            "Epoch 649/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3362 - val_loss: 0.3998\n",
            "Epoch 650/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3357 - val_loss: 0.4030\n",
            "Epoch 651/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3361 - val_loss: 0.4046\n",
            "Epoch 652/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3373 - val_loss: 0.4112\n",
            "Epoch 653/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3374 - val_loss: 0.4049\n",
            "Epoch 654/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3369 - val_loss: 0.4109\n",
            "Epoch 655/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3351 - val_loss: 0.4041\n",
            "Epoch 656/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3341 - val_loss: 0.4082\n",
            "Epoch 657/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3364 - val_loss: 0.4067\n",
            "Epoch 658/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3399 - val_loss: 0.4047\n",
            "Epoch 659/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3364 - val_loss: 0.4007\n",
            "Epoch 660/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3360 - val_loss: 0.4049\n",
            "Epoch 661/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3352 - val_loss: 0.4009\n",
            "Epoch 662/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3354 - val_loss: 0.4101\n",
            "Epoch 663/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3359 - val_loss: 0.4058\n",
            "Epoch 664/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3362 - val_loss: 0.4074\n",
            "Epoch 665/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3349 - val_loss: 0.4069\n",
            "Epoch 666/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3388 - val_loss: 0.4127\n",
            "Epoch 667/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3520 - val_loss: 0.4101\n",
            "Epoch 668/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3474 - val_loss: 0.4055\n",
            "Epoch 669/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3395 - val_loss: 0.4069\n",
            "Epoch 670/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3375 - val_loss: 0.4053\n",
            "Epoch 671/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3374 - val_loss: 0.4043\n",
            "Epoch 672/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3380 - val_loss: 0.4048\n",
            "Epoch 673/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3379 - val_loss: 0.4081\n",
            "Epoch 674/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3382 - val_loss: 0.3980\n",
            "Epoch 675/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3424 - val_loss: 0.4089\n",
            "Epoch 676/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3372 - val_loss: 0.4007\n",
            "Epoch 677/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3377 - val_loss: 0.4012\n",
            "Epoch 678/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3370 - val_loss: 0.4125\n",
            "Epoch 679/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3385 - val_loss: 0.4039\n",
            "Epoch 680/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3379 - val_loss: 0.4049\n",
            "Epoch 681/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3358 - val_loss: 0.4050\n",
            "Epoch 682/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3354 - val_loss: 0.4081\n",
            "Epoch 683/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3356 - val_loss: 0.4031\n",
            "Epoch 684/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3356 - val_loss: 0.4053\n",
            "Epoch 685/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3404 - val_loss: 0.4160\n",
            "Epoch 686/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3369 - val_loss: 0.4066\n",
            "Epoch 687/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3368 - val_loss: 0.4120\n",
            "Epoch 688/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3346 - val_loss: 0.4076\n",
            "Epoch 689/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3370 - val_loss: 0.4042\n",
            "Epoch 690/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3360 - val_loss: 0.4074\n",
            "Epoch 691/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3358 - val_loss: 0.4117\n",
            "Epoch 692/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3414 - val_loss: 0.4044\n",
            "Epoch 693/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3374 - val_loss: 0.4057\n",
            "Epoch 694/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3340 - val_loss: 0.4035\n",
            "Epoch 695/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3348 - val_loss: 0.4033\n",
            "Epoch 696/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3393 - val_loss: 0.4041\n",
            "Epoch 697/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3356 - val_loss: 0.4029\n",
            "Epoch 698/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3369 - val_loss: 0.4094\n",
            "Epoch 699/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3356 - val_loss: 0.4051\n",
            "Epoch 700/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3355 - val_loss: 0.4052\n",
            "Epoch 701/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3361 - val_loss: 0.4119\n",
            "Epoch 702/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3348 - val_loss: 0.4088\n",
            "Epoch 703/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3349 - val_loss: 0.4055\n",
            "Epoch 704/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3369 - val_loss: 0.4012\n",
            "Epoch 705/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3383 - val_loss: 0.4102\n",
            "Epoch 706/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3364 - val_loss: 0.4026\n",
            "Epoch 707/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3353 - val_loss: 0.4057\n",
            "Epoch 708/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3367 - val_loss: 0.4017\n",
            "Epoch 709/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3357 - val_loss: 0.4041\n",
            "Epoch 710/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3362 - val_loss: 0.4055\n",
            "Epoch 711/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3366 - val_loss: 0.4134\n",
            "Epoch 712/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3352 - val_loss: 0.4040\n",
            "Epoch 713/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3353 - val_loss: 0.4038\n",
            "Epoch 714/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3357 - val_loss: 0.4006\n",
            "Epoch 715/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3378 - val_loss: 0.4056\n",
            "Epoch 716/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3361 - val_loss: 0.4072\n",
            "Epoch 717/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3363 - val_loss: 0.4016\n",
            "Epoch 718/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3351 - val_loss: 0.4051\n",
            "Epoch 719/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3351 - val_loss: 0.4090\n",
            "Epoch 720/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3363 - val_loss: 0.4086\n",
            "Epoch 721/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3355 - val_loss: 0.4075\n",
            "Epoch 722/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3351 - val_loss: 0.4028\n",
            "Epoch 723/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3389 - val_loss: 0.4107\n",
            "Epoch 724/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3377 - val_loss: 0.4043\n",
            "Epoch 725/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3383 - val_loss: 0.4083\n",
            "Epoch 726/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3353 - val_loss: 0.4035\n",
            "Epoch 727/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3369 - val_loss: 0.4105\n",
            "Epoch 728/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3373 - val_loss: 0.4040\n",
            "Epoch 729/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3366 - val_loss: 0.4119\n",
            "Epoch 730/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3339 - val_loss: 0.4063\n",
            "Epoch 731/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3412 - val_loss: 0.4021\n",
            "Epoch 732/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3368 - val_loss: 0.4000\n",
            "Epoch 733/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3358 - val_loss: 0.4117\n",
            "Epoch 734/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3356 - val_loss: 0.4056\n",
            "Epoch 735/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3351 - val_loss: 0.4030\n",
            "Epoch 736/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3335 - val_loss: 0.4119\n",
            "Epoch 737/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3350 - val_loss: 0.4034\n",
            "Epoch 738/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3341 - val_loss: 0.4001\n",
            "Epoch 739/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3352 - val_loss: 0.4031\n",
            "Epoch 740/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3383 - val_loss: 0.4162\n",
            "Epoch 741/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3349 - val_loss: 0.4055\n",
            "Epoch 742/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3344 - val_loss: 0.4073\n",
            "Epoch 743/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3378 - val_loss: 0.4122\n",
            "Epoch 744/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3358 - val_loss: 0.4047\n",
            "Epoch 745/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3362 - val_loss: 0.4105\n",
            "Epoch 746/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3350 - val_loss: 0.4036\n",
            "Epoch 747/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3342 - val_loss: 0.4081\n",
            "Epoch 748/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3361 - val_loss: 0.4112\n",
            "Epoch 749/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3349 - val_loss: 0.4083\n",
            "Epoch 750/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3363 - val_loss: 0.4149\n",
            "Epoch 751/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3372 - val_loss: 0.4058\n",
            "Epoch 752/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3353 - val_loss: 0.4049\n",
            "Epoch 753/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3355 - val_loss: 0.4116\n",
            "Epoch 754/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3343 - val_loss: 0.4008\n",
            "Epoch 755/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3384 - val_loss: 0.4129\n",
            "Epoch 756/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3357 - val_loss: 0.4062\n",
            "Epoch 757/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3340 - val_loss: 0.4059\n",
            "Epoch 758/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3350 - val_loss: 0.4138\n",
            "Epoch 759/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3348 - val_loss: 0.4056\n",
            "Epoch 760/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3351 - val_loss: 0.4066\n",
            "Epoch 761/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3356 - val_loss: 0.4083\n",
            "Epoch 762/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3362 - val_loss: 0.4072\n",
            "Epoch 763/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3351 - val_loss: 0.4048\n",
            "Epoch 764/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3348 - val_loss: 0.4139\n",
            "Epoch 765/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3356 - val_loss: 0.4049\n",
            "Epoch 766/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3341 - val_loss: 0.4085\n",
            "Epoch 767/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3332 - val_loss: 0.4062\n",
            "Epoch 768/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3335 - val_loss: 0.4044\n",
            "Epoch 769/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3357 - val_loss: 0.4018\n",
            "Epoch 770/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3353 - val_loss: 0.4076\n",
            "Epoch 771/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3338 - val_loss: 0.4096\n",
            "Epoch 772/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3340 - val_loss: 0.4072\n",
            "Epoch 773/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3358 - val_loss: 0.4110\n",
            "Epoch 774/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3383 - val_loss: 0.4061\n",
            "Epoch 775/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3370 - val_loss: 0.4044\n",
            "Epoch 776/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3357 - val_loss: 0.4025\n",
            "Epoch 777/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3353 - val_loss: 0.4041\n",
            "Epoch 778/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3366 - val_loss: 0.4102\n",
            "Epoch 779/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3387 - val_loss: 0.4046\n",
            "Epoch 780/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3353 - val_loss: 0.4262\n",
            "Epoch 781/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3373 - val_loss: 0.4128\n",
            "Epoch 782/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3346 - val_loss: 0.4057\n",
            "Epoch 783/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3346 - val_loss: 0.4021\n",
            "Epoch 784/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3350 - val_loss: 0.4090\n",
            "Epoch 785/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3358 - val_loss: 0.4049\n",
            "Epoch 786/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3342 - val_loss: 0.4086\n",
            "Epoch 787/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3345 - val_loss: 0.4043\n",
            "Epoch 788/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3336 - val_loss: 0.4032\n",
            "Epoch 789/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3348 - val_loss: 0.4031\n",
            "Epoch 790/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3357 - val_loss: 0.4043\n",
            "Epoch 791/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3336 - val_loss: 0.4051\n",
            "Epoch 792/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3337 - val_loss: 0.4093\n",
            "Epoch 793/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3344 - val_loss: 0.4077\n",
            "Epoch 794/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3350 - val_loss: 0.4067\n",
            "Epoch 795/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3366 - val_loss: 0.4013\n",
            "Epoch 796/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3376 - val_loss: 0.4104\n",
            "Epoch 797/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3364 - val_loss: 0.4035\n",
            "Epoch 798/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3365 - val_loss: 0.4066\n",
            "Epoch 799/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3343 - val_loss: 0.4050\n",
            "Epoch 800/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3332 - val_loss: 0.4134\n",
            "Epoch 801/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3337 - val_loss: 0.4131\n",
            "Epoch 802/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3380 - val_loss: 0.4086\n",
            "Epoch 803/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3365 - val_loss: 0.4277\n",
            "Epoch 804/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3480 - val_loss: 0.4262\n",
            "Epoch 805/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3395 - val_loss: 0.4123\n",
            "Epoch 806/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3382 - val_loss: 0.4105\n",
            "Epoch 807/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3360 - val_loss: 0.4085\n",
            "Epoch 808/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3348 - val_loss: 0.4050\n",
            "Epoch 809/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3353 - val_loss: 0.4046\n",
            "Epoch 810/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3341 - val_loss: 0.4075\n",
            "Epoch 811/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3347 - val_loss: 0.4039\n",
            "Epoch 812/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3335 - val_loss: 0.4071\n",
            "Epoch 813/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3343 - val_loss: 0.4039\n",
            "Epoch 814/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3341 - val_loss: 0.4047\n",
            "Epoch 815/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3348 - val_loss: 0.4078\n",
            "Epoch 816/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3357 - val_loss: 0.4037\n",
            "Epoch 817/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3327 - val_loss: 0.4058\n",
            "Epoch 818/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3332 - val_loss: 0.4121\n",
            "Epoch 819/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3327 - val_loss: 0.3989\n",
            "Epoch 820/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3324 - val_loss: 0.4059\n",
            "Epoch 821/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3333 - val_loss: 0.4075\n",
            "Epoch 822/1000\n",
            "45/45 [==============================] - 1s 26ms/step - loss: 0.3334 - val_loss: 0.4053\n",
            "Epoch 823/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3341 - val_loss: 0.4095\n",
            "Epoch 824/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3338 - val_loss: 0.4048\n",
            "Epoch 825/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3327 - val_loss: 0.4067\n",
            "Epoch 826/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3348 - val_loss: 0.4073\n",
            "Epoch 827/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3346 - val_loss: 0.4099\n",
            "Epoch 828/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3336 - val_loss: 0.4082\n",
            "Epoch 829/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3338 - val_loss: 0.4097\n",
            "Epoch 830/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3360 - val_loss: 0.4146\n",
            "Epoch 831/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3329 - val_loss: 0.4036\n",
            "Epoch 832/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3341 - val_loss: 0.4013\n",
            "Epoch 833/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3336 - val_loss: 0.4073\n",
            "Epoch 834/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3334 - val_loss: 0.4082\n",
            "Epoch 835/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3356 - val_loss: 0.4047\n",
            "Epoch 836/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3327 - val_loss: 0.4085\n",
            "Epoch 837/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3329 - val_loss: 0.4062\n",
            "Epoch 838/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3350 - val_loss: 0.4015\n",
            "Epoch 839/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3353 - val_loss: 0.4129\n",
            "Epoch 840/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3357 - val_loss: 0.4054\n",
            "Epoch 841/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3339 - val_loss: 0.4050\n",
            "Epoch 842/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3331 - val_loss: 0.4047\n",
            "Epoch 843/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3323 - val_loss: 0.4043\n",
            "Epoch 844/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3334 - val_loss: 0.4061\n",
            "Epoch 845/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3325 - val_loss: 0.4091\n",
            "Epoch 846/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3345 - val_loss: 0.4046\n",
            "Epoch 847/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3329 - val_loss: 0.4013\n",
            "Epoch 848/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3344 - val_loss: 0.4042\n",
            "Epoch 849/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3357 - val_loss: 0.4052\n",
            "Epoch 850/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3332 - val_loss: 0.4050\n",
            "Epoch 851/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3330 - val_loss: 0.4038\n",
            "Epoch 852/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3322 - val_loss: 0.4098\n",
            "Epoch 853/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3329 - val_loss: 0.4049\n",
            "Epoch 854/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3334 - val_loss: 0.4093\n",
            "Epoch 855/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3380 - val_loss: 0.4056\n",
            "Epoch 856/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3364 - val_loss: 0.4022\n",
            "Epoch 857/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3341 - val_loss: 0.4051\n",
            "Epoch 858/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3349 - val_loss: 0.4024\n",
            "Epoch 859/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3335 - val_loss: 0.4042\n",
            "Epoch 860/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3335 - val_loss: 0.4029\n",
            "Epoch 861/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3332 - val_loss: 0.4051\n",
            "Epoch 862/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3324 - val_loss: 0.4038\n",
            "Epoch 863/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3316 - val_loss: 0.4017\n",
            "Epoch 864/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3327 - val_loss: 0.4096\n",
            "Epoch 865/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3313 - val_loss: 0.4021\n",
            "Epoch 866/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3323 - val_loss: 0.4061\n",
            "Epoch 867/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3336 - val_loss: 0.4129\n",
            "Epoch 868/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3380 - val_loss: 0.4235\n",
            "Epoch 869/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3371 - val_loss: 0.4097\n",
            "Epoch 870/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3388 - val_loss: 0.4095\n",
            "Epoch 871/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3378 - val_loss: 0.4033\n",
            "Epoch 872/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3348 - val_loss: 0.4076\n",
            "Epoch 873/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3379 - val_loss: 0.4015\n",
            "Epoch 874/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3331 - val_loss: 0.4051\n",
            "Epoch 875/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3344 - val_loss: 0.4041\n",
            "Epoch 876/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3320 - val_loss: 0.4004\n",
            "Epoch 877/1000\n",
            "45/45 [==============================] - 1s 16ms/step - loss: 0.3339 - val_loss: 0.4016\n",
            "Epoch 878/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3334 - val_loss: 0.4052\n",
            "Epoch 879/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3331 - val_loss: 0.4013\n",
            "Epoch 880/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3326 - val_loss: 0.4048\n",
            "Epoch 881/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3333 - val_loss: 0.4042\n",
            "Epoch 882/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3336 - val_loss: 0.4021\n",
            "Epoch 883/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3334 - val_loss: 0.4102\n",
            "Epoch 884/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3331 - val_loss: 0.4020\n",
            "Epoch 885/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3351 - val_loss: 0.4107\n",
            "Epoch 886/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3341 - val_loss: 0.4033\n",
            "Epoch 887/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3327 - val_loss: 0.4016\n",
            "Epoch 888/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3315 - val_loss: 0.4044\n",
            "Epoch 889/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3348 - val_loss: 0.4096\n",
            "Epoch 890/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3339 - val_loss: 0.4059\n",
            "Epoch 891/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3378 - val_loss: 0.4020\n",
            "Epoch 892/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3400 - val_loss: 0.4042\n",
            "Epoch 893/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3362 - val_loss: 0.4047\n",
            "Epoch 894/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3363 - val_loss: 0.4022\n",
            "Epoch 895/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3328 - val_loss: 0.4040\n",
            "Epoch 896/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3323 - val_loss: 0.4041\n",
            "Epoch 897/1000\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 0.3356 - val_loss: 0.4027\n",
            "Epoch 898/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3335 - val_loss: 0.4115\n",
            "Epoch 899/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3333 - val_loss: 0.4069\n",
            "Epoch 900/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3348 - val_loss: 0.4087\n",
            "Epoch 901/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3329 - val_loss: 0.4029\n",
            "Epoch 902/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3332 - val_loss: 0.4053\n",
            "Epoch 903/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3325 - val_loss: 0.4049\n",
            "Epoch 904/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3321 - val_loss: 0.4069\n",
            "Epoch 905/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3318 - val_loss: 0.4076\n",
            "Epoch 906/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3329 - val_loss: 0.4028\n",
            "Epoch 907/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3318 - val_loss: 0.4009\n",
            "Epoch 908/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3324 - val_loss: 0.4007\n",
            "Epoch 909/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3346 - val_loss: 0.4048\n",
            "Epoch 910/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3349 - val_loss: 0.4071\n",
            "Epoch 911/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3345 - val_loss: 0.4057\n",
            "Epoch 912/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3328 - val_loss: 0.4034\n",
            "Epoch 913/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3339 - val_loss: 0.4065\n",
            "Epoch 914/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3349 - val_loss: 0.4070\n",
            "Epoch 915/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3322 - val_loss: 0.4018\n",
            "Epoch 916/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3322 - val_loss: 0.4053\n",
            "Epoch 917/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3334 - val_loss: 0.4072\n",
            "Epoch 918/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3355 - val_loss: 0.4061\n",
            "Epoch 919/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3374 - val_loss: 0.4071\n",
            "Epoch 920/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3335 - val_loss: 0.3991\n",
            "Epoch 921/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3315 - val_loss: 0.4019\n",
            "Epoch 922/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3318 - val_loss: 0.4052\n",
            "Epoch 923/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3339 - val_loss: 0.4087\n",
            "Epoch 924/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3350 - val_loss: 0.4035\n",
            "Epoch 925/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3320 - val_loss: 0.4097\n",
            "Epoch 926/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3323 - val_loss: 0.4068\n",
            "Epoch 927/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3354 - val_loss: 0.4113\n",
            "Epoch 928/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3359 - val_loss: 0.4069\n",
            "Epoch 929/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3347 - val_loss: 0.4054\n",
            "Epoch 930/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3345 - val_loss: 0.4061\n",
            "Epoch 931/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3333 - val_loss: 0.4042\n",
            "Epoch 932/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3327 - val_loss: 0.4039\n",
            "Epoch 933/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3335 - val_loss: 0.4050\n",
            "Epoch 934/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3359 - val_loss: 0.4079\n",
            "Epoch 935/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3369 - val_loss: 0.4044\n",
            "Epoch 936/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3349 - val_loss: 0.4052\n",
            "Epoch 937/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3358 - val_loss: 0.4050\n",
            "Epoch 938/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3330 - val_loss: 0.4119\n",
            "Epoch 939/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3320 - val_loss: 0.4042\n",
            "Epoch 940/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3319 - val_loss: 0.4066\n",
            "Epoch 941/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3338 - val_loss: 0.4082\n",
            "Epoch 942/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3386 - val_loss: 0.4110\n",
            "Epoch 943/1000\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.3351 - val_loss: 0.4050\n",
            "Epoch 944/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3324 - val_loss: 0.4097\n",
            "Epoch 945/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3330 - val_loss: 0.4068\n",
            "Epoch 946/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3332 - val_loss: 0.4005\n",
            "Epoch 947/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3315 - val_loss: 0.4037\n",
            "Epoch 948/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3322 - val_loss: 0.4036\n",
            "Epoch 949/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3329 - val_loss: 0.4048\n",
            "Epoch 950/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3324 - val_loss: 0.4107\n",
            "Epoch 951/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3341 - val_loss: 0.4049\n",
            "Epoch 952/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3340 - val_loss: 0.4073\n",
            "Epoch 953/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3318 - val_loss: 0.4073\n",
            "Epoch 954/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3320 - val_loss: 0.4047\n",
            "Epoch 955/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3329 - val_loss: 0.4037\n",
            "Epoch 956/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3328 - val_loss: 0.4023\n",
            "Epoch 957/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3336 - val_loss: 0.4031\n",
            "Epoch 958/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3340 - val_loss: 0.4031\n",
            "Epoch 959/1000\n",
            "45/45 [==============================] - 1s 21ms/step - loss: 0.3318 - val_loss: 0.4075\n",
            "Epoch 960/1000\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.3327 - val_loss: 0.4073\n",
            "Epoch 961/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3330 - val_loss: 0.4043\n",
            "Epoch 962/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3330 - val_loss: 0.4076\n",
            "Epoch 963/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3326 - val_loss: 0.4032\n",
            "Epoch 964/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3319 - val_loss: 0.4175\n",
            "Epoch 965/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3318 - val_loss: 0.4038\n",
            "Epoch 966/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3343 - val_loss: 0.4107\n",
            "Epoch 967/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3325 - val_loss: 0.4059\n",
            "Epoch 968/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3341 - val_loss: 0.4044\n",
            "Epoch 969/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3338 - val_loss: 0.4061\n",
            "Epoch 970/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3392 - val_loss: 0.4050\n",
            "Epoch 971/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3339 - val_loss: 0.4074\n",
            "Epoch 972/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3343 - val_loss: 0.4079\n",
            "Epoch 973/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3333 - val_loss: 0.4018\n",
            "Epoch 974/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3331 - val_loss: 0.4048\n",
            "Epoch 975/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3336 - val_loss: 0.4104\n",
            "Epoch 976/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3372 - val_loss: 0.4050\n",
            "Epoch 977/1000\n",
            "45/45 [==============================] - 1s 28ms/step - loss: 0.3351 - val_loss: 0.4032\n",
            "Epoch 978/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3389 - val_loss: 0.4048\n",
            "Epoch 979/1000\n",
            "45/45 [==============================] - 1s 25ms/step - loss: 0.3371 - val_loss: 0.4027\n",
            "Epoch 980/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3330 - val_loss: 0.4016\n",
            "Epoch 981/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3318 - val_loss: 0.4047\n",
            "Epoch 982/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3323 - val_loss: 0.4013\n",
            "Epoch 983/1000\n",
            "45/45 [==============================] - 1s 24ms/step - loss: 0.3320 - val_loss: 0.4052\n",
            "Epoch 984/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3319 - val_loss: 0.4057\n",
            "Epoch 985/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3331 - val_loss: 0.4027\n",
            "Epoch 986/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3352 - val_loss: 0.4099\n",
            "Epoch 987/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3367 - val_loss: 0.4105\n",
            "Epoch 988/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3360 - val_loss: 0.4069\n",
            "Epoch 989/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3340 - val_loss: 0.4069\n",
            "Epoch 990/1000\n",
            "45/45 [==============================] - 1s 22ms/step - loss: 0.3328 - val_loss: 0.4065\n",
            "Epoch 991/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3353 - val_loss: 0.4014\n",
            "Epoch 992/1000\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.3335 - val_loss: 0.4063\n",
            "Epoch 993/1000\n",
            "45/45 [==============================] - 1s 27ms/step - loss: 0.3335 - val_loss: 0.4010\n",
            "Epoch 994/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3335 - val_loss: 0.4124\n",
            "Epoch 995/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3341 - val_loss: 0.4026\n",
            "Epoch 996/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3311 - val_loss: 0.4003\n",
            "Epoch 997/1000\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.3322 - val_loss: 0.4000\n",
            "Epoch 998/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3325 - val_loss: 0.4073\n",
            "Epoch 999/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3330 - val_loss: 0.4089\n",
            "Epoch 1000/1000\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.3341 - val_loss: 0.4012\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4012\n",
            "24/24 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1f904dec20>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000    # training epochs\n",
        "temperature = 0.1\n",
        "\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "\n",
        "# count occurrences in each row, and make new columns\n",
        "def count_occurrences(row,element):\n",
        "    count = 0\n",
        "    for value in row:\n",
        "        if value == element:\n",
        "            count += 1\n",
        "    return count/20\n",
        "\n",
        "data_df['0occurrences'] = results_subset.apply(count_occurrences, args=(0,), axis=1)\n",
        "data_df['1occurrences'] = results_subset.apply(count_occurrences, args=(1,), axis=1)\n",
        "data_df['2occurrences'] = results_subset.apply(count_occurrences, args=(2,), axis=1)\n",
        "\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "# Randomly choose indices for training and validation\n",
        "train_indices = random.sample(range(200), k=160)\n",
        "val_indices = [i for i in range(200) if i not in train_indices]\n",
        "\n",
        "# Construct the training set\n",
        "train_data = pd.concat([condition1.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition2.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition3.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition4.iloc[train_indices[0]:train_indices[-1]+1, :]])\n",
        "\n",
        "# Construct the test set\n",
        "val_data = pd.concat([condition1.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition2.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition3.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition4.iloc[val_indices[0]:val_indices[-1]+1, :]])\n",
        "\n",
        "# Get the Lab data (X) and probability distribution (Y)\n",
        "train_set = np.asarray(train_data).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6:9]    # probability distribution columns\n",
        "Y_train = Y_train.reshape(-1,3)\n",
        "\n",
        "val_set = np.asarray(val_data).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6:9]\n",
        "Y_val = Y_val.reshape(-1,3)\n",
        "# print(X_val)  # example: output to see the possibility distribution for validation set\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(x)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "IK3zenSIEDL-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfcZbDsXT0f0"
      },
      "source": [
        "## 4.2 Possibility Distribution: Train on data under 3 light and test on 1 light condition ##"
      ],
      "id": "tfcZbDsXT0f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv5UwPEST6xB"
      },
      "source": [
        "### 4.2.1 1st lighting condition as test ###"
      ],
      "id": "Bv5UwPEST6xB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2xD0jyTUDWA",
        "outputId": "ae84c74a-5acc-41e4-a2dd-250830c84c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 18s 36ms/step - loss: 1.1077 - val_loss: 1.0924\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.0876 - val_loss: 1.0813\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 1.0774 - val_loss: 1.0729\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 1.0685 - val_loss: 1.0646\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 1.0596 - val_loss: 1.0553\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 1.0499 - val_loss: 1.0462\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 1.0403 - val_loss: 1.0373\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 1.0307 - val_loss: 1.0310\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 1.0228 - val_loss: 1.0210\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 1.0138 - val_loss: 1.0138\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 1.0059 - val_loss: 1.0050\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 0.9962 - val_loss: 0.9968\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.9868 - val_loss: 0.9874\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.9761 - val_loss: 0.9761\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.9649 - val_loss: 0.9656\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.9525 - val_loss: 0.9532\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.9398 - val_loss: 0.9410\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.9255 - val_loss: 0.9273\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.9108 - val_loss: 0.9136\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.8955 - val_loss: 0.8989\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.8795 - val_loss: 0.8837\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.8629 - val_loss: 0.8685\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.8461 - val_loss: 0.8528\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.8286 - val_loss: 0.8365\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.8111 - val_loss: 0.8203\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 0.7930 - val_loss: 0.8030\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 0.7745 - val_loss: 0.7856\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.7553 - val_loss: 0.7667\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.7354 - val_loss: 0.7482\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.7140 - val_loss: 0.7275\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.6941 - val_loss: 0.7077\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.6729 - val_loss: 0.6876\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.6473 - val_loss: 0.6672\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.6181 - val_loss: 0.6281\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.5490 - val_loss: 0.6077\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.4913 - val_loss: 0.5292\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.4530 - val_loss: 0.5074\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.4269 - val_loss: 0.4870\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.4085 - val_loss: 0.4683\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3975 - val_loss: 0.4586\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3974 - val_loss: 0.4606\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3859 - val_loss: 0.4565\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 0.3833 - val_loss: 0.4691\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.3810 - val_loss: 0.4526\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3811 - val_loss: 0.4666\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3791 - val_loss: 0.4452\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3756 - val_loss: 0.4433\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3716 - val_loss: 0.4478\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 0.3723 - val_loss: 0.4423\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 0.3701 - val_loss: 0.4467\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 0.3677 - val_loss: 0.4547\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 2s 29ms/step - loss: 0.3656 - val_loss: 0.4410\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.3649 - val_loss: 0.4458\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 0.3661 - val_loss: 0.4336\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.3620 - val_loss: 0.4451\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 0.3640 - val_loss: 0.4462\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.3646 - val_loss: 0.4382\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.3591 - val_loss: 0.4343\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 0.3666 - val_loss: 0.4645\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 2s 30ms/step - loss: 0.3623 - val_loss: 0.4327\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 2s 30ms/step - loss: 0.3598 - val_loss: 0.4405\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3607 - val_loss: 0.4316\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3605 - val_loss: 0.4339\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3578 - val_loss: 0.4425\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3592 - val_loss: 0.4317\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.3576 - val_loss: 0.4268\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 3s 41ms/step - loss: 0.3595 - val_loss: 0.4280\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 0.3574 - val_loss: 0.4354\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3553 - val_loss: 0.4472\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.3554 - val_loss: 0.4467\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 0.3561 - val_loss: 0.4298\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.3546 - val_loss: 0.4373\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3539 - val_loss: 0.4307\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 0.3523 - val_loss: 0.4328\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 0.3541 - val_loss: 0.4339\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3526 - val_loss: 0.4288\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3515 - val_loss: 0.4515\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3534 - val_loss: 0.4262\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 0.3497 - val_loss: 0.4245\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 2s 29ms/step - loss: 0.3485 - val_loss: 0.4341\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 0.3486 - val_loss: 0.4215\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 3s 39ms/step - loss: 0.3480 - val_loss: 0.4256\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 0.3457 - val_loss: 0.4144\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.3485 - val_loss: 0.4313\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.3445 - val_loss: 0.4096\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 0.3457 - val_loss: 0.4231\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3489 - val_loss: 0.4104\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 0.3439 - val_loss: 0.4136\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 3s 37ms/step - loss: 0.3445 - val_loss: 0.4109\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 2s 29ms/step - loss: 0.3423 - val_loss: 0.4029\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3398 - val_loss: 0.4110\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3422 - val_loss: 0.4070\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 0.3422 - val_loss: 0.4094\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.3390 - val_loss: 0.4255\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.3423 - val_loss: 0.4203\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 2s 33ms/step - loss: 0.3473 - val_loss: 0.4056\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 2s 29ms/step - loss: 0.3427 - val_loss: 0.4169\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3389 - val_loss: 0.4110\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3375 - val_loss: 0.4119\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3353 - val_loss: 0.4088\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3394 - val_loss: 0.4148\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.3357 - val_loss: 0.4119\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 3s 35ms/step - loss: 0.3357 - val_loss: 0.4026\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 2s 32ms/step - loss: 0.3375 - val_loss: 0.3963\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 0.3380 - val_loss: 0.4021\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3388 - val_loss: 0.4129\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3387 - val_loss: 0.4063\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3356 - val_loss: 0.4055\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3374 - val_loss: 0.4092\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3381 - val_loss: 0.4085\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3348 - val_loss: 0.3972\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 2s 29ms/step - loss: 0.3366 - val_loss: 0.4051\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 0.3353 - val_loss: 0.4019\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3345 - val_loss: 0.4085\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 0.3321 - val_loss: 0.4065\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.3334 - val_loss: 0.4006\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3357 - val_loss: 0.4041\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3364 - val_loss: 0.3982\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3360 - val_loss: 0.3994\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 0.3322 - val_loss: 0.4242\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 0.3332 - val_loss: 0.4048\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3312 - val_loss: 0.3992\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3341 - val_loss: 0.4020\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3306 - val_loss: 0.3952\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3330 - val_loss: 0.3991\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3304 - val_loss: 0.3958\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3339 - val_loss: 0.4023\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3327 - val_loss: 0.4050\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3310 - val_loss: 0.3953\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 0.3332 - val_loss: 0.3986\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3325 - val_loss: 0.4071\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3320 - val_loss: 0.4007\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3324 - val_loss: 0.3996\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3315 - val_loss: 0.4083\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3325 - val_loss: 0.4036\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3317 - val_loss: 0.3936\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3303 - val_loss: 0.4012\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3299 - val_loss: 0.4016\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3284 - val_loss: 0.4004\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3324 - val_loss: 0.3973\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3325 - val_loss: 0.4061\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3305 - val_loss: 0.4027\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3299 - val_loss: 0.4092\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3340 - val_loss: 0.4079\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3315 - val_loss: 0.3923\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3307 - val_loss: 0.4103\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3300 - val_loss: 0.4080\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3310 - val_loss: 0.3976\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3312 - val_loss: 0.4082\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3301 - val_loss: 0.4080\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3311 - val_loss: 0.4016\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3322 - val_loss: 0.4089\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3318 - val_loss: 0.3989\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3324 - val_loss: 0.3977\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.3308 - val_loss: 0.3929\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3309 - val_loss: 0.4093\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3279 - val_loss: 0.4045\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3283 - val_loss: 0.3960\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3286 - val_loss: 0.3968\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3289 - val_loss: 0.3995\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3301 - val_loss: 0.3985\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3277 - val_loss: 0.4013\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3275 - val_loss: 0.3993\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3272 - val_loss: 0.4028\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3286 - val_loss: 0.3924\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3271 - val_loss: 0.4065\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3269 - val_loss: 0.3972\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3309 - val_loss: 0.4031\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 0.3306 - val_loss: 0.3989\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 0.3273 - val_loss: 0.4094\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3272 - val_loss: 0.3987\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3266 - val_loss: 0.4181\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3265 - val_loss: 0.4001\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3270 - val_loss: 0.3978\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3265 - val_loss: 0.3967\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 0.3285 - val_loss: 0.3974\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 0.3273 - val_loss: 0.4025\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 2s 32ms/step - loss: 0.3264 - val_loss: 0.3954\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 2s 30ms/step - loss: 0.3263 - val_loss: 0.4049\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3287 - val_loss: 0.3983\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3271 - val_loss: 0.4062\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3241 - val_loss: 0.3961\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3245 - val_loss: 0.4040\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3298 - val_loss: 0.3941\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3260 - val_loss: 0.4039\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3271 - val_loss: 0.3926\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3295 - val_loss: 0.3977\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3291 - val_loss: 0.3904\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3263 - val_loss: 0.4071\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3255 - val_loss: 0.3944\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3269 - val_loss: 0.3949\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3260 - val_loss: 0.3936\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3256 - val_loss: 0.3979\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3277 - val_loss: 0.3944\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3274 - val_loss: 0.4093\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3258 - val_loss: 0.3942\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3249 - val_loss: 0.3952\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3267 - val_loss: 0.3973\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3275 - val_loss: 0.3941\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3256 - val_loss: 0.4010\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3265 - val_loss: 0.4068\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3259 - val_loss: 0.3936\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3253 - val_loss: 0.3980\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3239 - val_loss: 0.3994\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3244 - val_loss: 0.3918\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3241 - val_loss: 0.4004\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3228 - val_loss: 0.3944\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3265 - val_loss: 0.3994\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3252 - val_loss: 0.3908\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3252 - val_loss: 0.3945\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3251 - val_loss: 0.3904\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3254 - val_loss: 0.3995\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3268 - val_loss: 0.3950\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3250 - val_loss: 0.3991\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3242 - val_loss: 0.3885\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3252 - val_loss: 0.4066\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3279 - val_loss: 0.4086\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3263 - val_loss: 0.3961\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3276 - val_loss: 0.4091\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3279 - val_loss: 0.3896\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3253 - val_loss: 0.3975\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3236 - val_loss: 0.3911\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3234 - val_loss: 0.3950\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3227 - val_loss: 0.3915\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3242 - val_loss: 0.3971\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3236 - val_loss: 0.3956\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3250 - val_loss: 0.3918\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3230 - val_loss: 0.3951\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3238 - val_loss: 0.3995\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3244 - val_loss: 0.3925\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3225 - val_loss: 0.3999\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3226 - val_loss: 0.3995\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3222 - val_loss: 0.3990\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3261 - val_loss: 0.3950\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3237 - val_loss: 0.3963\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3227 - val_loss: 0.3986\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3244 - val_loss: 0.4004\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3255 - val_loss: 0.3933\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3253 - val_loss: 0.3997\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3241 - val_loss: 0.4157\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3232 - val_loss: 0.4003\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3223 - val_loss: 0.4008\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3222 - val_loss: 0.3977\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3234 - val_loss: 0.3965\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3254 - val_loss: 0.4082\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3223 - val_loss: 0.4022\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3219 - val_loss: 0.4085\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3253 - val_loss: 0.3978\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3235 - val_loss: 0.4042\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3236 - val_loss: 0.4083\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3235 - val_loss: 0.3991\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3222 - val_loss: 0.4015\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3253 - val_loss: 0.4014\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3254 - val_loss: 0.3946\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3226 - val_loss: 0.3941\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3224 - val_loss: 0.4014\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3225 - val_loss: 0.3919\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3219 - val_loss: 0.4000\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3226 - val_loss: 0.3989\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3257 - val_loss: 0.4052\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3233 - val_loss: 0.3933\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3231 - val_loss: 0.3983\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3220 - val_loss: 0.4041\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3232 - val_loss: 0.3977\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3213 - val_loss: 0.4008\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3216 - val_loss: 0.4009\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3231 - val_loss: 0.4065\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3217 - val_loss: 0.3961\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3205 - val_loss: 0.3992\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3211 - val_loss: 0.4017\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3200 - val_loss: 0.3966\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3213 - val_loss: 0.4109\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3224 - val_loss: 0.4101\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3226 - val_loss: 0.4032\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3238 - val_loss: 0.3905\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3224 - val_loss: 0.4009\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3219 - val_loss: 0.3918\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3228 - val_loss: 0.3900\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3222 - val_loss: 0.3930\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3209 - val_loss: 0.3980\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3204 - val_loss: 0.3934\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3207 - val_loss: 0.3966\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3217 - val_loss: 0.3983\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3226 - val_loss: 0.3953\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3223 - val_loss: 0.3970\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3215 - val_loss: 0.3943\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3217 - val_loss: 0.3976\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3223 - val_loss: 0.4008\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3223 - val_loss: 0.3917\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3216 - val_loss: 0.4003\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3234 - val_loss: 0.3957\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3218 - val_loss: 0.3957\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3226 - val_loss: 0.4036\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3216 - val_loss: 0.3981\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3224 - val_loss: 0.3856\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3228 - val_loss: 0.3930\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3210 - val_loss: 0.4033\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3228 - val_loss: 0.3965\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3199 - val_loss: 0.4009\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3250 - val_loss: 0.3944\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3213 - val_loss: 0.3926\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3237 - val_loss: 0.3998\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3204 - val_loss: 0.3927\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3204 - val_loss: 0.3976\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3201 - val_loss: 0.3968\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3204 - val_loss: 0.4074\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3215 - val_loss: 0.3948\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3234 - val_loss: 0.3989\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3218 - val_loss: 0.3870\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3210 - val_loss: 0.3975\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3205 - val_loss: 0.4012\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3217 - val_loss: 0.3907\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3220 - val_loss: 0.3986\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3221 - val_loss: 0.4009\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3208 - val_loss: 0.3929\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3204 - val_loss: 0.3965\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3205 - val_loss: 0.3941\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3198 - val_loss: 0.3912\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3193 - val_loss: 0.3932\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3191 - val_loss: 0.3923\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3210 - val_loss: 0.3916\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3202 - val_loss: 0.3908\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3211 - val_loss: 0.3950\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3196 - val_loss: 0.4013\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3209 - val_loss: 0.3956\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3197 - val_loss: 0.4033\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3200 - val_loss: 0.3902\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3206 - val_loss: 0.3972\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3195 - val_loss: 0.4033\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3208 - val_loss: 0.3938\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3196 - val_loss: 0.3972\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3202 - val_loss: 0.3887\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3197 - val_loss: 0.3912\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3203 - val_loss: 0.3899\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3205 - val_loss: 0.3956\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3226 - val_loss: 0.3928\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3217 - val_loss: 0.4015\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3190 - val_loss: 0.3960\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3198 - val_loss: 0.4076\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3214 - val_loss: 0.3980\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3192 - val_loss: 0.3913\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3221 - val_loss: 0.3947\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3214 - val_loss: 0.3958\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3201 - val_loss: 0.4064\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3193 - val_loss: 0.3965\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3191 - val_loss: 0.3943\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3188 - val_loss: 0.3991\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3203 - val_loss: 0.3882\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3218 - val_loss: 0.4061\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3220 - val_loss: 0.3963\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3193 - val_loss: 0.3948\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3205 - val_loss: 0.3938\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3198 - val_loss: 0.3995\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3217 - val_loss: 0.3898\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3213 - val_loss: 0.4024\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3189 - val_loss: 0.4066\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3196 - val_loss: 0.3977\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3184 - val_loss: 0.3952\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3205 - val_loss: 0.3971\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3205 - val_loss: 0.4050\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3200 - val_loss: 0.3962\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3204 - val_loss: 0.4056\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3203 - val_loss: 0.3923\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3194 - val_loss: 0.3906\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3186 - val_loss: 0.3966\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3200 - val_loss: 0.4035\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3187 - val_loss: 0.4045\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3194 - val_loss: 0.3977\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3185 - val_loss: 0.4003\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3191 - val_loss: 0.3980\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3206 - val_loss: 0.3947\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3211 - val_loss: 0.3984\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3210 - val_loss: 0.3917\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3196 - val_loss: 0.3901\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3186 - val_loss: 0.3939\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3203 - val_loss: 0.3999\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3209 - val_loss: 0.3885\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3197 - val_loss: 0.3919\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3204 - val_loss: 0.3908\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3194 - val_loss: 0.3955\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3205 - val_loss: 0.4007\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3192 - val_loss: 0.3920\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3178 - val_loss: 0.3978\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3196 - val_loss: 0.4023\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3182 - val_loss: 0.3911\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3185 - val_loss: 0.3898\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3187 - val_loss: 0.3922\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3186 - val_loss: 0.3977\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3175 - val_loss: 0.4059\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3191 - val_loss: 0.3938\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3222 - val_loss: 0.3965\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3208 - val_loss: 0.3935\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3189 - val_loss: 0.3958\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3194 - val_loss: 0.3971\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3196 - val_loss: 0.3931\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3193 - val_loss: 0.3914\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3186 - val_loss: 0.3971\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3182 - val_loss: 0.3894\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3194 - val_loss: 0.3902\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3196 - val_loss: 0.3972\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3184 - val_loss: 0.3934\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3196 - val_loss: 0.3960\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3190 - val_loss: 0.4036\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3183 - val_loss: 0.4018\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3177 - val_loss: 0.3920\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3180 - val_loss: 0.3908\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3188 - val_loss: 0.4008\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3186 - val_loss: 0.4013\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3186 - val_loss: 0.3931\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3175 - val_loss: 0.3902\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3195 - val_loss: 0.4011\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3192 - val_loss: 0.3915\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3189 - val_loss: 0.3935\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3177 - val_loss: 0.3957\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3171 - val_loss: 0.4072\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3221 - val_loss: 0.4118\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3187 - val_loss: 0.3916\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3181 - val_loss: 0.3959\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3178 - val_loss: 0.3959\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3180 - val_loss: 0.3883\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3178 - val_loss: 0.3900\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3178 - val_loss: 0.3984\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3179 - val_loss: 0.3907\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3175 - val_loss: 0.3873\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3173 - val_loss: 0.3883\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3175 - val_loss: 0.3874\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3188 - val_loss: 0.3890\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3179 - val_loss: 0.3896\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3210 - val_loss: 0.3992\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.3179 - val_loss: 0.3887\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3192 - val_loss: 0.3936\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3181 - val_loss: 0.3949\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3191 - val_loss: 0.3980\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3175 - val_loss: 0.4002\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3179 - val_loss: 0.3952\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3192 - val_loss: 0.3902\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3173 - val_loss: 0.3958\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3170 - val_loss: 0.3885\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3171 - val_loss: 0.4019\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3177 - val_loss: 0.3976\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3192 - val_loss: 0.3939\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3181 - val_loss: 0.3997\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3189 - val_loss: 0.3934\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3189 - val_loss: 0.3886\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3171 - val_loss: 0.3935\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3179 - val_loss: 0.3995\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3186 - val_loss: 0.3968\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3187 - val_loss: 0.3966\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3180 - val_loss: 0.4019\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3184 - val_loss: 0.4003\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3174 - val_loss: 0.3918\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3170 - val_loss: 0.3994\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3184 - val_loss: 0.3913\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3168 - val_loss: 0.3916\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3175 - val_loss: 0.3975\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3176 - val_loss: 0.3928\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3182 - val_loss: 0.3861\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3179 - val_loss: 0.3962\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3170 - val_loss: 0.3936\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3194 - val_loss: 0.3901\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3171 - val_loss: 0.4009\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3203 - val_loss: 0.3991\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3194 - val_loss: 0.3929\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3211 - val_loss: 0.3920\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3176 - val_loss: 0.3919\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3179 - val_loss: 0.3912\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3183 - val_loss: 0.3956\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3182 - val_loss: 0.3924\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3199 - val_loss: 0.3948\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3185 - val_loss: 0.3987\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3179 - val_loss: 0.3945\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3178 - val_loss: 0.3917\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3172 - val_loss: 0.3939\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3205 - val_loss: 0.3939\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3174 - val_loss: 0.3943\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3176 - val_loss: 0.4026\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3191 - val_loss: 0.3933\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3169 - val_loss: 0.4007\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3172 - val_loss: 0.3900\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3171 - val_loss: 0.3888\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3187 - val_loss: 0.3955\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3169 - val_loss: 0.3931\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3180 - val_loss: 0.3992\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3184 - val_loss: 0.3942\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3182 - val_loss: 0.3914\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3197 - val_loss: 0.4058\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3177 - val_loss: 0.3935\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3195 - val_loss: 0.3870\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3182 - val_loss: 0.3975\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3176 - val_loss: 0.3941\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3193 - val_loss: 0.3940\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3177 - val_loss: 0.3939\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3174 - val_loss: 0.3920\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3176 - val_loss: 0.3942\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3168 - val_loss: 0.3946\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3177 - val_loss: 0.3911\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3177 - val_loss: 0.3949\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3180 - val_loss: 0.3889\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3180 - val_loss: 0.3983\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3168 - val_loss: 0.3933\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3171 - val_loss: 0.3936\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3170 - val_loss: 0.4002\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3178 - val_loss: 0.3888\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3184 - val_loss: 0.3910\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3181 - val_loss: 0.4000\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3173 - val_loss: 0.3891\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3166 - val_loss: 0.3931\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3171 - val_loss: 0.4020\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3188 - val_loss: 0.3923\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3163 - val_loss: 0.3995\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3169 - val_loss: 0.3932\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3171 - val_loss: 0.3969\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3158 - val_loss: 0.3917\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3164 - val_loss: 0.3939\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3166 - val_loss: 0.3973\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.3170 - val_loss: 0.3970\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3182 - val_loss: 0.4067\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3199 - val_loss: 0.3943\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3174 - val_loss: 0.3917\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3172 - val_loss: 0.4023\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3166 - val_loss: 0.3964\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3188 - val_loss: 0.3957\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3200 - val_loss: 0.3953\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3173 - val_loss: 0.3938\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3161 - val_loss: 0.3997\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3165 - val_loss: 0.3961\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3167 - val_loss: 0.3932\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3159 - val_loss: 0.3884\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3165 - val_loss: 0.3981\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3169 - val_loss: 0.3977\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3166 - val_loss: 0.3956\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3159 - val_loss: 0.3979\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3995\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3174 - val_loss: 0.3975\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3183 - val_loss: 0.4071\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3186 - val_loss: 0.3952\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3173 - val_loss: 0.3966\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3179 - val_loss: 0.3959\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3182 - val_loss: 0.3965\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3165 - val_loss: 0.3909\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3932\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3159 - val_loss: 0.3939\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3166 - val_loss: 0.4032\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3163 - val_loss: 0.3953\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3176 - val_loss: 0.3942\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3158 - val_loss: 0.3901\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3172 - val_loss: 0.3908\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3161 - val_loss: 0.4021\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3172 - val_loss: 0.3926\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3168 - val_loss: 0.4006\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3162 - val_loss: 0.3979\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3157 - val_loss: 0.3967\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3172 - val_loss: 0.3960\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3165 - val_loss: 0.3908\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3165 - val_loss: 0.3902\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3162 - val_loss: 0.3979\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3156 - val_loss: 0.3951\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3937\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3193 - val_loss: 0.3962\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3168 - val_loss: 0.3918\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3178 - val_loss: 0.3927\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3171 - val_loss: 0.3878\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3158 - val_loss: 0.3991\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3174 - val_loss: 0.3937\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3172 - val_loss: 0.4044\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3173 - val_loss: 0.3915\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.4010\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3192 - val_loss: 0.3925\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3164 - val_loss: 0.3976\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3173 - val_loss: 0.3897\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3156 - val_loss: 0.3939\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3974\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3156 - val_loss: 0.3917\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3185 - val_loss: 0.3938\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3168 - val_loss: 0.3926\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3166 - val_loss: 0.4057\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3168 - val_loss: 0.3940\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3223 - val_loss: 0.3940\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3167 - val_loss: 0.3938\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3152 - val_loss: 0.3959\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3905\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3155 - val_loss: 0.3979\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3176 - val_loss: 0.3986\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3176 - val_loss: 0.3969\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3168 - val_loss: 0.3966\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3161 - val_loss: 0.4036\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3165 - val_loss: 0.3954\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3159 - val_loss: 0.3953\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3169 - val_loss: 0.3904\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3159 - val_loss: 0.3912\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3168 - val_loss: 0.4025\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3182 - val_loss: 0.4030\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3164 - val_loss: 0.3970\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3158 - val_loss: 0.3977\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3159 - val_loss: 0.3930\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3178 - val_loss: 0.4051\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3157 - val_loss: 0.3964\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3157 - val_loss: 0.3915\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3155 - val_loss: 0.3950\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3147 - val_loss: 0.4020\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3187 - val_loss: 0.3944\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3169 - val_loss: 0.3940\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3202 - val_loss: 0.3931\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3189 - val_loss: 0.3955\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3169 - val_loss: 0.3894\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3909\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3158 - val_loss: 0.3905\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3163 - val_loss: 0.3934\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3164 - val_loss: 0.3983\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3159 - val_loss: 0.3969\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3165 - val_loss: 0.4058\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3167 - val_loss: 0.3916\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3156 - val_loss: 0.3912\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3156 - val_loss: 0.4002\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3155 - val_loss: 0.3922\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3157 - val_loss: 0.3952\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3151 - val_loss: 0.3935\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3154 - val_loss: 0.3962\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3154 - val_loss: 0.3943\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3150 - val_loss: 0.3983\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3932\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3148 - val_loss: 0.3988\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3155 - val_loss: 0.3956\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3158 - val_loss: 0.3979\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3149 - val_loss: 0.3969\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.3942\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3166 - val_loss: 0.3993\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3167 - val_loss: 0.3967\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3154 - val_loss: 0.3912\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3155 - val_loss: 0.3977\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3157 - val_loss: 0.3987\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.3959\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3156 - val_loss: 0.3932\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3975\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3154 - val_loss: 0.3999\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3152 - val_loss: 0.3918\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3159 - val_loss: 0.3929\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3153 - val_loss: 0.3970\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3158 - val_loss: 0.4023\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3153 - val_loss: 0.3965\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3148 - val_loss: 0.3973\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3165 - val_loss: 0.3901\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3153 - val_loss: 0.3981\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3157 - val_loss: 0.3953\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3154 - val_loss: 0.3955\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3158 - val_loss: 0.3895\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3153 - val_loss: 0.3987\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3151 - val_loss: 0.3955\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3154 - val_loss: 0.3920\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3161 - val_loss: 0.3945\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3142 - val_loss: 0.3924\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3162 - val_loss: 0.3940\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3158 - val_loss: 0.3974\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3175 - val_loss: 0.3925\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3148 - val_loss: 0.3988\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3155 - val_loss: 0.3943\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3146 - val_loss: 0.4038\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3171 - val_loss: 0.3944\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3145 - val_loss: 0.4057\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3155 - val_loss: 0.3914\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3152 - val_loss: 0.4000\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3150 - val_loss: 0.3952\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3158 - val_loss: 0.3893\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3161 - val_loss: 0.4046\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3189 - val_loss: 0.4047\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3176 - val_loss: 0.3901\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3169 - val_loss: 0.3921\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3145 - val_loss: 0.3994\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3150 - val_loss: 0.3930\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3152 - val_loss: 0.3954\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3157 - val_loss: 0.3904\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3150 - val_loss: 0.3983\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3149 - val_loss: 0.3927\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3150 - val_loss: 0.3905\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3144 - val_loss: 0.3964\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3150 - val_loss: 0.3936\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3144 - val_loss: 0.4070\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3152 - val_loss: 0.3938\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3145 - val_loss: 0.3946\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3150 - val_loss: 0.4012\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3153 - val_loss: 0.3994\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3154 - val_loss: 0.4044\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3151 - val_loss: 0.3968\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3154 - val_loss: 0.3898\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3146 - val_loss: 0.3911\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3159 - val_loss: 0.3990\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3152 - val_loss: 0.3930\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.3890\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3159 - val_loss: 0.3939\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3149 - val_loss: 0.3900\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3880\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3159 - val_loss: 0.3975\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.3978\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3151 - val_loss: 0.3929\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3157 - val_loss: 0.3953\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3150 - val_loss: 0.3977\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3152 - val_loss: 0.3926\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3156 - val_loss: 0.4147\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3155 - val_loss: 0.3915\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3152 - val_loss: 0.3971\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3156 - val_loss: 0.3960\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.3937\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3172 - val_loss: 0.3947\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3161 - val_loss: 0.3939\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.3902\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3144 - val_loss: 0.3895\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3145 - val_loss: 0.3903\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3140 - val_loss: 0.3939\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3142 - val_loss: 0.3925\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3155 - val_loss: 0.3912\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3145 - val_loss: 0.3939\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3149 - val_loss: 0.3933\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3142 - val_loss: 0.3999\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3144 - val_loss: 0.3927\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3149 - val_loss: 0.3962\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3152 - val_loss: 0.3948\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3149 - val_loss: 0.3962\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3149 - val_loss: 0.4025\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3150 - val_loss: 0.3961\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3170 - val_loss: 0.3966\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3155 - val_loss: 0.3965\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3153 - val_loss: 0.3907\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3182 - val_loss: 0.4013\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3161 - val_loss: 0.3988\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3156 - val_loss: 0.3881\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3156 - val_loss: 0.3902\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3143 - val_loss: 0.3994\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3139 - val_loss: 0.3947\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3144 - val_loss: 0.3903\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3143 - val_loss: 0.3954\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3148 - val_loss: 0.4019\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3170 - val_loss: 0.4005\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3263 - val_loss: 0.3947\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3153 - val_loss: 0.3941\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3156 - val_loss: 0.3917\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3148 - val_loss: 0.3948\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3149 - val_loss: 0.4132\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3140 - val_loss: 0.3984\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3141 - val_loss: 0.3979\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3143 - val_loss: 0.3960\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3151 - val_loss: 0.3983\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3141 - val_loss: 0.3977\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3153 - val_loss: 0.3973\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3153 - val_loss: 0.3980\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3162 - val_loss: 0.3901\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3143 - val_loss: 0.3976\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3140 - val_loss: 0.3956\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.3925\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3146 - val_loss: 0.3924\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3146 - val_loss: 0.3898\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3151 - val_loss: 0.4069\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3151 - val_loss: 0.3896\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3157 - val_loss: 0.4018\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3155 - val_loss: 0.4007\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3154 - val_loss: 0.3995\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3137 - val_loss: 0.3979\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3137 - val_loss: 0.3959\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3141 - val_loss: 0.3953\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3139 - val_loss: 0.3908\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3139 - val_loss: 0.3922\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3154 - val_loss: 0.3943\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3150 - val_loss: 0.3888\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3149 - val_loss: 0.3941\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3156 - val_loss: 0.4011\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3152 - val_loss: 0.3980\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3140 - val_loss: 0.3914\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3139 - val_loss: 0.3918\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3146 - val_loss: 0.4008\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3138 - val_loss: 0.3960\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3139 - val_loss: 0.3977\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3145 - val_loss: 0.3916\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3141 - val_loss: 0.3941\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3138 - val_loss: 0.3972\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3136 - val_loss: 0.3927\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3156 - val_loss: 0.3960\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3147 - val_loss: 0.3938\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3136 - val_loss: 0.3932\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3144 - val_loss: 0.3979\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3147 - val_loss: 0.3921\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.3952\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3143 - val_loss: 0.3917\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3144 - val_loss: 0.3979\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3147 - val_loss: 0.3892\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3155 - val_loss: 0.3959\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3145 - val_loss: 0.4062\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3159 - val_loss: 0.3908\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3138 - val_loss: 0.3955\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3149 - val_loss: 0.3927\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3150 - val_loss: 0.3928\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3153 - val_loss: 0.3998\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3155 - val_loss: 0.3986\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3154 - val_loss: 0.3907\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3145 - val_loss: 0.3921\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3142 - val_loss: 0.3874\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3135 - val_loss: 0.4009\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3149 - val_loss: 0.3947\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3124 - val_loss: 0.3958\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3138 - val_loss: 0.3956\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3140 - val_loss: 0.3967\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3139 - val_loss: 0.3950\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3146 - val_loss: 0.3980\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3162 - val_loss: 0.3987\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3153 - val_loss: 0.3950\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3138 - val_loss: 0.3906\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3131 - val_loss: 0.3922\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3138 - val_loss: 0.3880\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3138 - val_loss: 0.3964\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.3884\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3141 - val_loss: 0.3907\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3127 - val_loss: 0.3985\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3137 - val_loss: 0.3909\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3140 - val_loss: 0.3915\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3144 - val_loss: 0.3878\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3146 - val_loss: 0.3918\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.4006\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3144 - val_loss: 0.4034\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3136 - val_loss: 0.3958\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3149 - val_loss: 0.3973\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3140 - val_loss: 0.4009\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3144 - val_loss: 0.4010\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.3981\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3148 - val_loss: 0.3927\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3143 - val_loss: 0.3913\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3129 - val_loss: 0.3993\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.4012\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3131 - val_loss: 0.3969\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.3986\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3138 - val_loss: 0.3996\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3141 - val_loss: 0.3923\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3134 - val_loss: 0.3983\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3147 - val_loss: 0.3938\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3149 - val_loss: 0.3942\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3143 - val_loss: 0.3902\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3132 - val_loss: 0.3956\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3135 - val_loss: 0.3934\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3138 - val_loss: 0.3921\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.4050\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3134 - val_loss: 0.3943\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3149 - val_loss: 0.3951\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3138 - val_loss: 0.4007\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3126 - val_loss: 0.4150\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3149 - val_loss: 0.4005\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3149 - val_loss: 0.4018\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3164 - val_loss: 0.3995\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3143 - val_loss: 0.3925\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3134 - val_loss: 0.3988\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.3958\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3144 - val_loss: 0.3990\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3141 - val_loss: 0.3934\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3133 - val_loss: 0.3980\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3149 - val_loss: 0.3877\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3138 - val_loss: 0.3885\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3131 - val_loss: 0.3908\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3151 - val_loss: 0.3945\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3133 - val_loss: 0.3922\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3132 - val_loss: 0.3947\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3133 - val_loss: 0.3965\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3136 - val_loss: 0.4040\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3139 - val_loss: 0.3938\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3128 - val_loss: 0.3986\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3133 - val_loss: 0.3998\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3142 - val_loss: 0.3951\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3170 - val_loss: 0.3905\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3156 - val_loss: 0.3904\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3139 - val_loss: 0.3964\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3139 - val_loss: 0.3985\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3131 - val_loss: 0.3913\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3133 - val_loss: 0.3888\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3143 - val_loss: 0.3912\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3141 - val_loss: 0.4003\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3126 - val_loss: 0.3923\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3141 - val_loss: 0.3970\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3141 - val_loss: 0.3918\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3120 - val_loss: 0.3938\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3129 - val_loss: 0.3941\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3126 - val_loss: 0.3915\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3140 - val_loss: 0.3978\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3125 - val_loss: 0.4017\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3129 - val_loss: 0.3941\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3132 - val_loss: 0.3903\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.3950\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3139 - val_loss: 0.3946\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3131 - val_loss: 0.3963\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3127 - val_loss: 0.3915\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3127 - val_loss: 0.3961\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3140 - val_loss: 0.3980\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3140 - val_loss: 0.3922\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3140 - val_loss: 0.3958\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3133 - val_loss: 0.3998\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3127 - val_loss: 0.3967\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3136 - val_loss: 0.4003\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3137 - val_loss: 0.3969\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3129 - val_loss: 0.3982\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3127 - val_loss: 0.3952\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3137 - val_loss: 0.3935\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3141 - val_loss: 0.3974\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3148 - val_loss: 0.3936\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3160 - val_loss: 0.4005\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3131 - val_loss: 0.3979\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3141 - val_loss: 0.3946\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3127 - val_loss: 0.3918\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3124 - val_loss: 0.3910\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3137 - val_loss: 0.3963\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3129 - val_loss: 0.3967\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3128 - val_loss: 0.4008\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3129 - val_loss: 0.3919\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3132 - val_loss: 0.3917\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3126 - val_loss: 0.3904\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3136 - val_loss: 0.3944\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3133 - val_loss: 0.3916\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3132 - val_loss: 0.3928\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3133 - val_loss: 0.3909\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3132 - val_loss: 0.3942\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3133 - val_loss: 0.3992\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3141 - val_loss: 0.4041\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3121 - val_loss: 0.3923\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3125 - val_loss: 0.3967\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.3947\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3132 - val_loss: 0.3963\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3133 - val_loss: 0.3945\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3137 - val_loss: 0.3982\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3136 - val_loss: 0.3920\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.4000\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3120 - val_loss: 0.3930\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3129 - val_loss: 0.3981\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3131 - val_loss: 0.3917\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3121 - val_loss: 0.3969\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3122 - val_loss: 0.3925\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3127 - val_loss: 0.3969\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3127 - val_loss: 0.3925\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3121 - val_loss: 0.3936\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3129 - val_loss: 0.3969\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3126 - val_loss: 0.3936\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3129 - val_loss: 0.3961\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3126 - val_loss: 0.3976\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3131 - val_loss: 0.3982\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3136 - val_loss: 0.4007\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3134 - val_loss: 0.3964\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3133 - val_loss: 0.3939\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3148 - val_loss: 0.3934\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3134 - val_loss: 0.3966\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3132 - val_loss: 0.3945\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3129 - val_loss: 0.3980\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3132 - val_loss: 0.3928\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3128 - val_loss: 0.3923\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3134 - val_loss: 0.4000\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3142 - val_loss: 0.3982\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3121 - val_loss: 0.3971\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3133 - val_loss: 0.3959\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3141 - val_loss: 0.3931\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3128 - val_loss: 0.3945\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3126 - val_loss: 0.3929\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3127 - val_loss: 0.3958\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3130 - val_loss: 0.3945\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3120 - val_loss: 0.3983\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3120 - val_loss: 0.3968\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3121 - val_loss: 0.3963\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3120 - val_loss: 0.3952\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3122 - val_loss: 0.4039\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3123 - val_loss: 0.3955\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3119 - val_loss: 0.3985\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3126 - val_loss: 0.3937\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3128 - val_loss: 0.3987\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3136 - val_loss: 0.3928\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3153 - val_loss: 0.4005\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3133 - val_loss: 0.3967\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3134 - val_loss: 0.3952\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3139 - val_loss: 0.3980\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3128 - val_loss: 0.4000\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3125 - val_loss: 0.3908\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3134 - val_loss: 0.3957\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3133 - val_loss: 0.3983\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3140 - val_loss: 0.3986\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3139 - val_loss: 0.4058\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3145 - val_loss: 0.3974\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3124 - val_loss: 0.3958\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3128 - val_loss: 0.3915\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3127 - val_loss: 0.3949\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3119 - val_loss: 0.4006\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3117 - val_loss: 0.3977\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3132 - val_loss: 0.3949\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3147 - val_loss: 0.3964\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.3960\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3129 - val_loss: 0.3936\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3136 - val_loss: 0.3950\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3126 - val_loss: 0.3954\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3115 - val_loss: 0.3971\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3137 - val_loss: 0.4022\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3138 - val_loss: 0.3951\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3132 - val_loss: 0.3962\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3135 - val_loss: 0.3927\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3127 - val_loss: 0.3952\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3129 - val_loss: 0.3913\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3126 - val_loss: 0.3922\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3126 - val_loss: 0.3932\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3112 - val_loss: 0.4012\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3139 - val_loss: 0.3884\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3140 - val_loss: 0.3971\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3126 - val_loss: 0.3995\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.3119 - val_loss: 0.3924\n",
            "24/24 [==============================] - 0s 7ms/step - loss: 0.3924\n",
            "24/24 [==============================] - 1s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1f16da5690>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000    # training epochs\n",
        "temperature = 0.1\n",
        "\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "\n",
        "# count occurrences in each row, and make new columns\n",
        "def count_occurrences(row,element):\n",
        "    count = 0\n",
        "    for value in row:\n",
        "        if value == element:\n",
        "            count += 1\n",
        "    return count/20\n",
        "\n",
        "data_df['0occurrences'] = results_subset.apply(count_occurrences, args=(0,), axis=1)\n",
        "data_df['1occurrences'] = results_subset.apply(count_occurrences, args=(1,), axis=1)\n",
        "data_df['2occurrences'] = results_subset.apply(count_occurrences, args=(2,), axis=1)\n",
        "\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition1   # choose 1st lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Get the Lab data (X) and probability distribution (Y)\n",
        "train_set = np.asarray(train_set).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6:9]    # probability distribution columns\n",
        "Y_train = Y_train.reshape(-1,3)\n",
        "\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6:9]\n",
        "Y_val = Y_val.reshape(-1,3)\n",
        "#print(X_train)  # example: output to see the possibility distribution for validation set\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(x)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "l2xD0jyTUDWA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk656cfOX5lq"
      },
      "source": [
        "### 4.2.2 2nd lighting condition as test ###"
      ],
      "id": "nk656cfOX5lq"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m5PKjq2-X0Pq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3477d69a-25db-49a2-c462-515723987b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 10s 15ms/step - loss: 1.1255 - val_loss: 1.0884\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.0840 - val_loss: 1.0783\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0743 - val_loss: 1.0683\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 1.0647 - val_loss: 1.0582\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0552 - val_loss: 1.0482\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0456 - val_loss: 1.0384\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0360 - val_loss: 1.0284\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0264 - val_loss: 1.0183\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0165 - val_loss: 1.0077\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0062 - val_loss: 0.9958\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9941 - val_loss: 0.9821\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.9807 - val_loss: 0.9675\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.9668 - val_loss: 0.9538\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.9527 - val_loss: 0.9394\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.9385 - val_loss: 0.9237\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9241 - val_loss: 0.9087\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9091 - val_loss: 0.8925\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8938 - val_loss: 0.8760\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8777 - val_loss: 0.8590\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7612 - val_loss: 0.5617\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5418 - val_loss: 0.5248\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5043 - val_loss: 0.4967\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4911 - val_loss: 0.4721\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4855 - val_loss: 0.4668\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4759 - val_loss: 0.4602\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4699 - val_loss: 0.4521\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4642 - val_loss: 0.4490\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4557 - val_loss: 0.4461\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4533 - val_loss: 0.4499\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4490 - val_loss: 0.4544\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4507 - val_loss: 0.4422\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4417 - val_loss: 0.4511\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4375 - val_loss: 0.4241\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4317 - val_loss: 0.4364\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4330 - val_loss: 0.4413\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4347 - val_loss: 0.4359\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4304 - val_loss: 0.4382\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4281 - val_loss: 0.4369\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4321 - val_loss: 0.4456\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4325 - val_loss: 0.4651\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4305 - val_loss: 0.4442\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4322 - val_loss: 0.4349\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4321 - val_loss: 0.4511\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4305 - val_loss: 0.4366\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4269 - val_loss: 0.4349\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4237 - val_loss: 0.4379\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4253 - val_loss: 0.4331\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4260 - val_loss: 0.4368\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4277 - val_loss: 0.4408\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4277 - val_loss: 0.4359\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4250 - val_loss: 0.4386\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4258 - val_loss: 0.4374\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4241 - val_loss: 0.4396\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4226 - val_loss: 0.4407\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4249 - val_loss: 0.4438\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4254 - val_loss: 0.4402\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4261 - val_loss: 0.4351\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4219 - val_loss: 0.4527\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4222 - val_loss: 0.4495\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4206 - val_loss: 0.4319\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4228 - val_loss: 0.4325\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4228 - val_loss: 0.4382\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4252 - val_loss: 0.4348\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4224 - val_loss: 0.4315\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4204 - val_loss: 0.4401\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4219 - val_loss: 0.4329\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4197 - val_loss: 0.4333\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4214 - val_loss: 0.4343\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4226 - val_loss: 0.4445\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4201 - val_loss: 0.4267\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4221 - val_loss: 0.4346\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4258 - val_loss: 0.4356\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4214 - val_loss: 0.4303\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4215 - val_loss: 0.4263\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4195 - val_loss: 0.4324\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4190 - val_loss: 0.4260\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4199 - val_loss: 0.4336\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4204 - val_loss: 0.4268\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4180 - val_loss: 0.4278\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4172 - val_loss: 0.4269\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4169 - val_loss: 0.4260\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4169 - val_loss: 0.4249\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4153 - val_loss: 0.4256\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4188 - val_loss: 0.4256\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4146 - val_loss: 0.4231\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4151 - val_loss: 0.4365\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4189 - val_loss: 0.4329\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4163 - val_loss: 0.4301\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4155 - val_loss: 0.4245\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4155 - val_loss: 0.4226\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4172 - val_loss: 0.4260\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4186 - val_loss: 0.4333\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4137 - val_loss: 0.4259\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4119 - val_loss: 0.4486\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4166 - val_loss: 0.4274\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4175 - val_loss: 0.4278\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4172 - val_loss: 0.4434\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4148 - val_loss: 0.4340\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4116 - val_loss: 0.4446\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4163 - val_loss: 0.4311\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4177 - val_loss: 0.4330\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4129 - val_loss: 0.4305\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4094 - val_loss: 0.4308\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4142 - val_loss: 0.4270\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4141 - val_loss: 0.4276\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4135 - val_loss: 0.4291\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4140 - val_loss: 0.4305\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4125 - val_loss: 0.4290\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4131 - val_loss: 0.4283\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4131 - val_loss: 0.4276\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4104 - val_loss: 0.4305\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4121 - val_loss: 0.4319\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4094 - val_loss: 0.4316\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4100 - val_loss: 0.4239\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4087 - val_loss: 0.4318\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4135 - val_loss: 0.4253\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4099 - val_loss: 0.4266\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4105 - val_loss: 0.4301\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4103 - val_loss: 0.4298\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4110 - val_loss: 0.4267\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4092 - val_loss: 0.4269\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4096 - val_loss: 0.4273\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4107 - val_loss: 0.4350\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4116 - val_loss: 0.4303\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4092 - val_loss: 0.4338\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4091 - val_loss: 0.4253\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4081 - val_loss: 0.4266\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4095 - val_loss: 0.4253\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4058 - val_loss: 0.4337\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4087 - val_loss: 0.4275\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4106 - val_loss: 0.4223\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4105 - val_loss: 0.4190\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4129 - val_loss: 0.4326\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4123 - val_loss: 0.4273\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4142 - val_loss: 0.4299\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4087 - val_loss: 0.4264\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4090 - val_loss: 0.4249\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4069 - val_loss: 0.4242\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4081 - val_loss: 0.4434\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4148 - val_loss: 0.4510\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4108 - val_loss: 0.4304\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4084 - val_loss: 0.4281\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4081 - val_loss: 0.4249\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4083 - val_loss: 0.4253\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4106 - val_loss: 0.4421\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4069 - val_loss: 0.4234\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4073 - val_loss: 0.4260\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4064 - val_loss: 0.4219\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4046 - val_loss: 0.4267\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4046 - val_loss: 0.4235\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4062 - val_loss: 0.4309\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4062 - val_loss: 0.4334\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4073 - val_loss: 0.4277\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4063 - val_loss: 0.4209\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4053 - val_loss: 0.4275\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4046 - val_loss: 0.4218\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4039 - val_loss: 0.4248\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4067 - val_loss: 0.4234\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4058 - val_loss: 0.4282\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4071 - val_loss: 0.4231\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4118 - val_loss: 0.4229\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4051 - val_loss: 0.4237\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4073 - val_loss: 0.4278\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4056 - val_loss: 0.4239\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4031 - val_loss: 0.4273\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4072 - val_loss: 0.4206\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4024 - val_loss: 0.4238\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4059 - val_loss: 0.4224\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4021 - val_loss: 0.4224\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4030 - val_loss: 0.4241\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4029 - val_loss: 0.4307\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4025 - val_loss: 0.4378\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4050 - val_loss: 0.4277\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4028 - val_loss: 0.4263\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4033 - val_loss: 0.4218\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4021 - val_loss: 0.4244\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4028 - val_loss: 0.4216\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4041 - val_loss: 0.4218\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4008 - val_loss: 0.4220\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4044 - val_loss: 0.4405\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4093 - val_loss: 0.4594\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4055 - val_loss: 0.4274\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4049 - val_loss: 0.4229\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4042 - val_loss: 0.4270\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4051 - val_loss: 0.4286\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4024 - val_loss: 0.4264\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4032 - val_loss: 0.4245\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.4031 - val_loss: 0.4253\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4046 - val_loss: 0.4191\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4042 - val_loss: 0.4140\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4028 - val_loss: 0.4214\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4047 - val_loss: 0.4214\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4006 - val_loss: 0.4465\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4035 - val_loss: 0.4272\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4052 - val_loss: 0.4289\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4017 - val_loss: 0.4224\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4017 - val_loss: 0.4215\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4018 - val_loss: 0.4362\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4052 - val_loss: 0.4195\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4008 - val_loss: 0.4159\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4043 - val_loss: 0.4193\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4026 - val_loss: 0.4245\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4059 - val_loss: 0.4216\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3994 - val_loss: 0.4219\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3999 - val_loss: 0.4246\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3996 - val_loss: 0.4193\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3991 - val_loss: 0.4207\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4002 - val_loss: 0.4252\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3975 - val_loss: 0.4155\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4005 - val_loss: 0.4161\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4074 - val_loss: 0.4183\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3987 - val_loss: 0.4165\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4000 - val_loss: 0.4211\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3992 - val_loss: 0.4281\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4010 - val_loss: 0.4206\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4029 - val_loss: 0.4182\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4001 - val_loss: 0.4233\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4133 - val_loss: 0.4308\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4032 - val_loss: 0.4170\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4033 - val_loss: 0.4183\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4004 - val_loss: 0.4377\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4006 - val_loss: 0.4180\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3974 - val_loss: 0.4221\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3992 - val_loss: 0.4197\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3995 - val_loss: 0.4126\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4034 - val_loss: 0.4159\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4014 - val_loss: 0.4186\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3947 - val_loss: 0.4163\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.4165\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3990 - val_loss: 0.4209\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4007 - val_loss: 0.4210\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3965 - val_loss: 0.4151\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3979 - val_loss: 0.4183\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3963 - val_loss: 0.4172\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3954 - val_loss: 0.4143\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3955 - val_loss: 0.4155\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3978 - val_loss: 0.4299\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4028 - val_loss: 0.4096\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3992 - val_loss: 0.4211\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3997 - val_loss: 0.4244\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3990 - val_loss: 0.4123\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3973 - val_loss: 0.4157\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4035 - val_loss: 0.4295\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4040 - val_loss: 0.4179\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4017 - val_loss: 0.4104\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3959 - val_loss: 0.4146\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3983 - val_loss: 0.4136\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3958 - val_loss: 0.4114\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3955 - val_loss: 0.4185\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3953 - val_loss: 0.4151\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4004 - val_loss: 0.4163\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3985 - val_loss: 0.4235\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3979 - val_loss: 0.4129\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3965 - val_loss: 0.4150\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3955 - val_loss: 0.4141\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3987 - val_loss: 0.4179\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3975 - val_loss: 0.4272\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3954 - val_loss: 0.4210\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3953 - val_loss: 0.4134\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3953 - val_loss: 0.4399\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3968 - val_loss: 0.4116\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3953 - val_loss: 0.4092\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4008 - val_loss: 0.4148\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3975 - val_loss: 0.4076\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3955 - val_loss: 0.4123\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3943 - val_loss: 0.4116\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3931 - val_loss: 0.4081\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3978 - val_loss: 0.4082\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3979 - val_loss: 0.4148\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3948 - val_loss: 0.4161\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3964 - val_loss: 0.4147\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3924 - val_loss: 0.4088\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3951 - val_loss: 0.4159\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3957 - val_loss: 0.4126\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3971 - val_loss: 0.4139\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3930 - val_loss: 0.4077\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3952 - val_loss: 0.4156\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3961 - val_loss: 0.4126\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3933 - val_loss: 0.4144\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3934 - val_loss: 0.4104\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3963 - val_loss: 0.4146\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3977 - val_loss: 0.4111\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3923 - val_loss: 0.4096\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4042 - val_loss: 0.4210\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3981 - val_loss: 0.4075\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3957 - val_loss: 0.4058\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3957 - val_loss: 0.4034\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3984 - val_loss: 0.4183\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3981 - val_loss: 0.4442\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4103 - val_loss: 0.4074\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4000 - val_loss: 0.4064\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3978 - val_loss: 0.4059\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.4044\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3954 - val_loss: 0.4037\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3942 - val_loss: 0.4059\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3935 - val_loss: 0.4198\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3979 - val_loss: 0.4035\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3920 - val_loss: 0.4022\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3934 - val_loss: 0.4055\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3931 - val_loss: 0.4093\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3957 - val_loss: 0.4061\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3938 - val_loss: 0.4060\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3936 - val_loss: 0.4055\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3897 - val_loss: 0.4100\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3944 - val_loss: 0.4119\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3950 - val_loss: 0.4155\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3930 - val_loss: 0.4117\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3970 - val_loss: 0.4062\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3920 - val_loss: 0.4074\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3944 - val_loss: 0.4113\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3905 - val_loss: 0.4173\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.4072\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3926 - val_loss: 0.4116\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3918 - val_loss: 0.4053\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.4038\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3925 - val_loss: 0.4022\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3920 - val_loss: 0.4281\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3914 - val_loss: 0.4098\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3910 - val_loss: 0.4108\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3955 - val_loss: 0.4235\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3926 - val_loss: 0.4122\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3904 - val_loss: 0.4093\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3935 - val_loss: 0.4107\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3915 - val_loss: 0.4088\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3911 - val_loss: 0.4064\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3905 - val_loss: 0.4098\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3920 - val_loss: 0.4157\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3897 - val_loss: 0.4096\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3926 - val_loss: 0.4190\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3950 - val_loss: 0.4151\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3920 - val_loss: 0.4108\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3926 - val_loss: 0.4145\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3905 - val_loss: 0.4226\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3950 - val_loss: 0.4085\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3951 - val_loss: 0.4160\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3944 - val_loss: 0.4027\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3930 - val_loss: 0.4083\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3909 - val_loss: 0.4215\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3947 - val_loss: 0.4155\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3912 - val_loss: 0.4090\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3906 - val_loss: 0.4120\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3927 - val_loss: 0.4011\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3906 - val_loss: 0.4155\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3905 - val_loss: 0.4026\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3875 - val_loss: 0.4249\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3905 - val_loss: 0.4066\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3907 - val_loss: 0.4222\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3897 - val_loss: 0.4109\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3927 - val_loss: 0.4105\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3908 - val_loss: 0.4217\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3904 - val_loss: 0.4076\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3874 - val_loss: 0.4122\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3920 - val_loss: 0.4057\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3921 - val_loss: 0.4077\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.4091\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3931 - val_loss: 0.4122\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4026 - val_loss: 0.4188\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4049 - val_loss: 0.4171\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3978 - val_loss: 0.4091\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3960 - val_loss: 0.4122\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3910 - val_loss: 0.4100\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3900 - val_loss: 0.4027\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3911 - val_loss: 0.4086\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3910 - val_loss: 0.4025\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3916 - val_loss: 0.4031\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3910 - val_loss: 0.4023\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3893 - val_loss: 0.4030\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3865 - val_loss: 0.4156\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3891 - val_loss: 0.4049\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3928 - val_loss: 0.4030\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3873 - val_loss: 0.4041\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3996\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.4024\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3886 - val_loss: 0.4027\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3893 - val_loss: 0.4012\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3898 - val_loss: 0.4040\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3901 - val_loss: 0.4036\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3900 - val_loss: 0.4039\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3909 - val_loss: 0.4045\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3870 - val_loss: 0.3998\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3891 - val_loss: 0.3983\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.4031\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3894 - val_loss: 0.4145\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3897 - val_loss: 0.4037\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.4066\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.4101\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3892 - val_loss: 0.4073\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3869 - val_loss: 0.4098\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3899 - val_loss: 0.4117\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.4121\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3888 - val_loss: 0.4005\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3863 - val_loss: 0.4074\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3891 - val_loss: 0.4059\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3912 - val_loss: 0.4296\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4008 - val_loss: 0.4309\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4017 - val_loss: 0.4198\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3952 - val_loss: 0.4178\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3921 - val_loss: 0.4177\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3881 - val_loss: 0.4108\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3875 - val_loss: 0.4053\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3997\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3978\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3968\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3867 - val_loss: 0.4028\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3902 - val_loss: 0.4000\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.4093\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3891 - val_loss: 0.3979\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3877 - val_loss: 0.4063\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3883 - val_loss: 0.3987\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3889 - val_loss: 0.4000\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3899 - val_loss: 0.4110\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3949 - val_loss: 0.4023\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3897 - val_loss: 0.4148\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.4054\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3910 - val_loss: 0.3958\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3857 - val_loss: 0.3994\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3857 - val_loss: 0.4002\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3854 - val_loss: 0.4006\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3897 - val_loss: 0.3983\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3862 - val_loss: 0.4076\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3986\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3920 - val_loss: 0.4073\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.4002\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3881 - val_loss: 0.3982\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3882 - val_loss: 0.4036\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3920 - val_loss: 0.4053\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3883 - val_loss: 0.3976\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3876 - val_loss: 0.3972\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3827 - val_loss: 0.4045\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3936 - val_loss: 0.3976\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3999\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.4014\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3881 - val_loss: 0.4186\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.4047\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3952 - val_loss: 0.3988\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3890 - val_loss: 0.3969\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.3949\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3851 - val_loss: 0.4049\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3899 - val_loss: 0.3959\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3845 - val_loss: 0.4080\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3877 - val_loss: 0.4018\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3880 - val_loss: 0.3950\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3888 - val_loss: 0.4002\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3868 - val_loss: 0.4331\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3909 - val_loss: 0.4038\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3860 - val_loss: 0.4112\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4111 - val_loss: 0.3982\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.4024\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.4017\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3924 - val_loss: 0.3982\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3922 - val_loss: 0.4143\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3945\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4006\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3857 - val_loss: 0.4219\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3855 - val_loss: 0.4027\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3805 - val_loss: 0.3978\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3825 - val_loss: 0.4018\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3844 - val_loss: 0.3945\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.4056\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3859 - val_loss: 0.4000\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3841 - val_loss: 0.3995\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3879 - val_loss: 0.4019\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3834 - val_loss: 0.3938\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3853 - val_loss: 0.3954\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3835 - val_loss: 0.3971\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3858 - val_loss: 0.4151\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.4039\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.4059\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3845 - val_loss: 0.4013\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3837 - val_loss: 0.4028\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3875 - val_loss: 0.4109\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3864 - val_loss: 0.4027\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3905 - val_loss: 0.4012\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3877 - val_loss: 0.4064\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3841 - val_loss: 0.3980\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3842 - val_loss: 0.4068\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3873 - val_loss: 0.4002\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3843 - val_loss: 0.3999\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3862 - val_loss: 0.4144\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3934 - val_loss: 0.4072\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3963 - val_loss: 0.4116\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3914 - val_loss: 0.4048\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3909 - val_loss: 0.4068\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3960 - val_loss: 0.4038\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3869 - val_loss: 0.4074\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3948 - val_loss: 0.4030\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3954 - val_loss: 0.3985\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3870 - val_loss: 0.4115\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3932 - val_loss: 0.3952\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3834 - val_loss: 0.3983\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3835 - val_loss: 0.3973\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3844 - val_loss: 0.3962\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3835 - val_loss: 0.4007\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3836 - val_loss: 0.4006\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3822 - val_loss: 0.4010\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3937 - val_loss: 0.4064\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4048 - val_loss: 0.4094\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3917 - val_loss: 0.4098\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3905 - val_loss: 0.4076\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3884 - val_loss: 0.4046\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3877 - val_loss: 0.4059\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3859 - val_loss: 0.4124\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3905 - val_loss: 0.3992\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3861 - val_loss: 0.4048\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3810 - val_loss: 0.3982\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3824 - val_loss: 0.4031\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3851 - val_loss: 0.4032\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3833 - val_loss: 0.3982\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3838 - val_loss: 0.4033\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3836 - val_loss: 0.4065\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3820 - val_loss: 0.4026\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3863 - val_loss: 0.4099\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3980\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3862 - val_loss: 0.4122\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.4063\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4014\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3979\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3865 - val_loss: 0.4038\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3948 - val_loss: 0.4024\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3873 - val_loss: 0.3988\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.4095\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3871 - val_loss: 0.3991\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3852 - val_loss: 0.3995\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3841 - val_loss: 0.3991\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3874 - val_loss: 0.3979\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3802 - val_loss: 0.3998\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3996\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.4022\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3860 - val_loss: 0.4075\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.4002\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4025\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3798 - val_loss: 0.3967\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.4040\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3999\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3812 - val_loss: 0.4036\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.4039\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.4069\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.3979\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3819 - val_loss: 0.4048\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3833 - val_loss: 0.4007\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3839 - val_loss: 0.3995\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3816 - val_loss: 0.3974\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3824 - val_loss: 0.3999\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3992\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.4006\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.3963\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3834 - val_loss: 0.3950\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3838 - val_loss: 0.4002\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3964\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3961\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3977\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.4018\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4059\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3997\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4079\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3790 - val_loss: 0.4078\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3834 - val_loss: 0.3993\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3803 - val_loss: 0.3964\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3811 - val_loss: 0.4156\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3923 - val_loss: 0.4183\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3875 - val_loss: 0.4035\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.4180\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.4003\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3815 - val_loss: 0.3976\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.4011\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.4042\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3949\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3948\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.4051\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.4066\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3938\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3902 - val_loss: 0.4052\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3821 - val_loss: 0.3972\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3817 - val_loss: 0.4004\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3833 - val_loss: 0.3972\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3862 - val_loss: 0.3994\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3851 - val_loss: 0.4040\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3844 - val_loss: 0.4042\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3989\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.3978\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.4178\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3880 - val_loss: 0.3989\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3988\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.4171\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.4039\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3776 - val_loss: 0.3990\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3926\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3911\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3974\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3803 - val_loss: 0.4003\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3946\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3796 - val_loss: 0.4030\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3773 - val_loss: 0.4115\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3801 - val_loss: 0.4052\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3797 - val_loss: 0.4004\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3966\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4007\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3842 - val_loss: 0.3995\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.4178\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3896 - val_loss: 0.3981\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.3959\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3920\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.3994\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3973\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3947\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3959\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.4053\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.4015\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3787 - val_loss: 0.4041\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3882 - val_loss: 0.4075\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3860 - val_loss: 0.3970\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3811 - val_loss: 0.4008\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.4006\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4024\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.4059\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.4237\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3979 - val_loss: 0.4107\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3893 - val_loss: 0.4022\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3888 - val_loss: 0.4015\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.4000\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3908 - val_loss: 0.4063\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.4006\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3984\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3812 - val_loss: 0.4036\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3787 - val_loss: 0.4060\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3776 - val_loss: 0.4009\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3776 - val_loss: 0.4214\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3860 - val_loss: 0.4088\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3789 - val_loss: 0.4012\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.4072\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3997\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.4073\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.4020\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3786 - val_loss: 0.4019\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.4121\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3875 - val_loss: 0.3952\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3956\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3981\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3979\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.4025\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3952\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3783 - val_loss: 0.4006\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3768 - val_loss: 0.4005\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3792 - val_loss: 0.3998\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3788 - val_loss: 0.3945\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3979\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3779 - val_loss: 0.3965\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.3982\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.4085\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3996\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3992\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.4121\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3990\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.3980\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.4072\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.3993\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3964\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3945\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3808 - val_loss: 0.4012\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3799 - val_loss: 0.3935\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3830 - val_loss: 0.4016\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3845 - val_loss: 0.3988\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3896 - val_loss: 0.4062\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.4031\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4015\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.4005\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4154\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4095 - val_loss: 0.5534\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5171 - val_loss: 0.4842\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5072 - val_loss: 0.4743\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4986 - val_loss: 0.4781\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4926 - val_loss: 0.4816\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4931 - val_loss: 0.4648\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4745 - val_loss: 0.4533\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4368 - val_loss: 0.4316\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4135 - val_loss: 0.4179\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3996 - val_loss: 0.4111\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3931 - val_loss: 0.4086\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3900 - val_loss: 0.4090\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3901 - val_loss: 0.4186\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3858 - val_loss: 0.4107\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3866 - val_loss: 0.4114\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3825 - val_loss: 0.4076\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3819 - val_loss: 0.4171\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3872 - val_loss: 0.4093\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.4070\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3830 - val_loss: 0.4180\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3833 - val_loss: 0.4110\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3836 - val_loss: 0.3992\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3803 - val_loss: 0.4005\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3847 - val_loss: 0.4119\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3799 - val_loss: 0.4010\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3828 - val_loss: 0.4088\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3823 - val_loss: 0.4066\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4095\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4006\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3792 - val_loss: 0.4047\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3978\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.4028\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3829 - val_loss: 0.4108\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3786 - val_loss: 0.3995\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.4097\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3957\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3815 - val_loss: 0.3992\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3941\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3786 - val_loss: 0.3933\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3763 - val_loss: 0.3980\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3787 - val_loss: 0.3957\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3783 - val_loss: 0.3994\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3788 - val_loss: 0.4036\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3760 - val_loss: 0.3996\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3770 - val_loss: 0.4033\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.4017\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.4018\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.4109\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3760 - val_loss: 0.4040\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.4042\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3787 - val_loss: 0.4021\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.4009\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3778 - val_loss: 0.4094\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4021 - val_loss: 0.4348\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4109 - val_loss: 0.4020\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3818 - val_loss: 0.3927\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3790 - val_loss: 0.3979\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3768 - val_loss: 0.3996\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3752 - val_loss: 0.3950\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3977\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3786 - val_loss: 0.3978\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3775 - val_loss: 0.3983\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.4119\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3760 - val_loss: 0.3980\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.4009\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.4017\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.4054\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3766 - val_loss: 0.4039\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.4135\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.4110\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.4011\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3769 - val_loss: 0.4048\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3801 - val_loss: 0.3949\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3744 - val_loss: 0.4005\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3740 - val_loss: 0.3949\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3913\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4082\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.4021\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3966\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3844 - val_loss: 0.4144\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3843 - val_loss: 0.3990\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.4001\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3979\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.3946\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.3892\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3921\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3919\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3885 - val_loss: 0.4011\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3789 - val_loss: 0.3917\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3741 - val_loss: 0.3964\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3754 - val_loss: 0.4015\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.4072\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3766 - val_loss: 0.3998\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.4019\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3805 - val_loss: 0.4020\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3939\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3996\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3775 - val_loss: 0.3950\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3983\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.3947\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3977\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.4046\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3989\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3779 - val_loss: 0.3919\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3798 - val_loss: 0.4025\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3839 - val_loss: 0.3964\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3816 - val_loss: 0.3977\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3781 - val_loss: 0.3988\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3959\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3939\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.4002\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3932\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3916\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3846\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3766 - val_loss: 0.4007\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3732 - val_loss: 0.4099\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3945\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.3962\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3885\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3880\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3732 - val_loss: 0.3960\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3690 - val_loss: 0.4059\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3800 - val_loss: 0.3958\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3771 - val_loss: 0.3926\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3719 - val_loss: 0.3965\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.3930\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3883\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3896\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3884\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3957\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.4010\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3924\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4039\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.4047\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3926\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3731 - val_loss: 0.3918\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.4145\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3736 - val_loss: 0.3956\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3712 - val_loss: 0.4034\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3759 - val_loss: 0.4271\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3734 - val_loss: 0.4093\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3737 - val_loss: 0.3925\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.3998\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.4008\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4045\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.4051\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.4014\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4062\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.4050\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.4040\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.3984\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4053\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.3967\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3994\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3974\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3818 - val_loss: 0.3893\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3768 - val_loss: 0.3948\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3747 - val_loss: 0.3929\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3728 - val_loss: 0.3996\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3855\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3766 - val_loss: 0.3993\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3918\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3979\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3748 - val_loss: 0.4055\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3914\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3719 - val_loss: 0.3883\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3706 - val_loss: 0.3937\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3904\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.3991\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.3929\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3817 - val_loss: 0.4021\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3781 - val_loss: 0.3992\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3722 - val_loss: 0.3891\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3692 - val_loss: 0.4004\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3959\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3936\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3937\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.4023\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.3978\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.3988\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.4020\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3954\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.4015\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3897\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.3913\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3980\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3686 - val_loss: 0.3934\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3688 - val_loss: 0.4028\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3686 - val_loss: 0.3940\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3708 - val_loss: 0.4101\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3870 - val_loss: 0.4276\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4010\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3811 - val_loss: 0.3926\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.4030\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3788 - val_loss: 0.4014\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3735 - val_loss: 0.4250\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3798 - val_loss: 0.4003\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3857 - val_loss: 0.4085\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3852 - val_loss: 0.3901\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3778 - val_loss: 0.4044\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3772 - val_loss: 0.4101\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3756 - val_loss: 0.4016\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3766 - val_loss: 0.3998\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3712 - val_loss: 0.4004\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3715 - val_loss: 0.3939\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3735 - val_loss: 0.3976\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4009\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3964\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3940\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3979\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.4062\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3923\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3870\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.4123\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.3971\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3894\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4001\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4001\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3712 - val_loss: 0.4010\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3801 - val_loss: 0.3874\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3763 - val_loss: 0.3932\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3732 - val_loss: 0.4089\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3748 - val_loss: 0.3960\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.3942\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3733 - val_loss: 0.4049\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3946\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3940\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4017\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.3953\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3922\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3970\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.3970\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3924\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.3977\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3719 - val_loss: 0.3959\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3778 - val_loss: 0.4001\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3740 - val_loss: 0.4019\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3714 - val_loss: 0.3970\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3706 - val_loss: 0.4086\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.4027\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.3946\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3966\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.3852\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3889\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3833\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3952\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.4048\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3866\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.3980\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3982\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.3916\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3680 - val_loss: 0.4048\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3697 - val_loss: 0.3925\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3691 - val_loss: 0.3921\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3738 - val_loss: 0.3996\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3765 - val_loss: 0.3895\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3860\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3866\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3861\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.4044\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3899\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.3834\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3719 - val_loss: 0.3882\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.4053\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3928\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3947\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3847\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.3907\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3718 - val_loss: 0.3971\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3766 - val_loss: 0.3830\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3717 - val_loss: 0.4060\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3712 - val_loss: 0.3851\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.3868\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3740 - val_loss: 0.3918\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3890\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3917\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.3924\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3898\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.4069\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3867\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3880\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.3892\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4059\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.3924\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.4047\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3741 - val_loss: 0.3887\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3778 - val_loss: 0.3936\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3727 - val_loss: 0.4132\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3739 - val_loss: 0.3848\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.4037\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3733 - val_loss: 0.3882\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3814\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.4076\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3947\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3894\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.3907\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.3883\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.3814\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3953\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.3736 - val_loss: 0.3935\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4047\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3715 - val_loss: 0.3980\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3697 - val_loss: 0.3968\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3712 - val_loss: 0.4088\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3679 - val_loss: 0.3869\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.3843\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.3867\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4006\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4006\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3953\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.3890\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3658 - val_loss: 0.3853\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.3824\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3662 - val_loss: 0.3924\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3852\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3947\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.4003\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3852\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3720 - val_loss: 0.3896\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.3712 - val_loss: 0.3905\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.3725 - val_loss: 0.4024\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3736 - val_loss: 0.3992\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.3969\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.3951\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.3923\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3676 - val_loss: 0.3917\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3895\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.3874\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.3943\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.3963\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.3871\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.3949\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4030\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4030\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0756c51ab0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000    # training epochs\n",
        "temperature = 0.1\n",
        "\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "\n",
        "# count occurrences in each row, and make new columns\n",
        "def count_occurrences(row,element):\n",
        "    count = 0\n",
        "    for value in row:\n",
        "        if value == element:\n",
        "            count += 1\n",
        "    return count/20\n",
        "\n",
        "data_df['0occurrences'] = results_subset.apply(count_occurrences, args=(0,), axis=1)\n",
        "data_df['1occurrences'] = results_subset.apply(count_occurrences, args=(1,), axis=1)\n",
        "data_df['2occurrences'] = results_subset.apply(count_occurrences, args=(2,), axis=1)\n",
        "\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition2   # choose lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Get the Lab data (X) and probability distribution (Y)\n",
        "train_set = np.asarray(train_set).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6:9]    # probability distribution columns\n",
        "Y_train = Y_train.reshape(-1,3)\n",
        "\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6:9]\n",
        "Y_val = Y_val.reshape(-1,3)\n",
        "#print(X_train)  # example: output to see the possibility distribution for validation set\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(x)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "m5PKjq2-X0Pq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWWOSi2FX7hG"
      },
      "source": [
        "### 4.2.3 3rd lighting condition as test ###"
      ],
      "id": "dWWOSi2FX7hG"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6edKHaXaYASc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc5b8380-da7d-439f-e437-f1f96af4a7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 10s 15ms/step - loss: 1.0976 - val_loss: 1.0915\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.0883 - val_loss: 1.0842\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0814 - val_loss: 1.0768\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.0746 - val_loss: 1.0693\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0679 - val_loss: 1.0622\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0612 - val_loss: 1.0550\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0546 - val_loss: 1.0480\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.0481 - val_loss: 1.0409\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0416 - val_loss: 1.0339\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0352 - val_loss: 1.0270\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0289 - val_loss: 1.0202\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0226 - val_loss: 1.0134\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.0164 - val_loss: 1.0067\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 1.0103 - val_loss: 1.0001\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 1.0042 - val_loss: 0.9936\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.9982 - val_loss: 0.9870\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.9922 - val_loss: 0.9806\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9863 - val_loss: 0.9743\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9805 - val_loss: 0.9680\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9748 - val_loss: 0.9618\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9690 - val_loss: 0.9556\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9634 - val_loss: 0.9494\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9578 - val_loss: 0.9434\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9523 - val_loss: 0.9374\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9468 - val_loss: 0.9316\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9415 - val_loss: 0.9256\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.9361 - val_loss: 0.9198\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.9308 - val_loss: 0.9141\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.9256 - val_loss: 0.9084\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.9204 - val_loss: 0.9028\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.9154 - val_loss: 0.8971\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.9103 - val_loss: 0.8917\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.9053 - val_loss: 0.8862\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.9004 - val_loss: 0.8808\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8955 - val_loss: 0.8755\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8907 - val_loss: 0.8703\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8859 - val_loss: 0.8650\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8812 - val_loss: 0.8598\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8765 - val_loss: 0.8547\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8719 - val_loss: 0.8497\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8674 - val_loss: 0.8447\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8629 - val_loss: 0.8397\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8584 - val_loss: 0.8348\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8540 - val_loss: 0.8299\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8497 - val_loss: 0.8252\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8454 - val_loss: 0.8204\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8411 - val_loss: 0.8157\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.8370 - val_loss: 0.8111\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.8328 - val_loss: 0.8065\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.8287 - val_loss: 0.8020\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.8247 - val_loss: 0.7975\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8207 - val_loss: 0.7931\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8167 - val_loss: 0.7887\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8128 - val_loss: 0.7844\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.8090 - val_loss: 0.7801\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8052 - val_loss: 0.7759\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8014 - val_loss: 0.7717\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7978 - val_loss: 0.7675\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7941 - val_loss: 0.7635\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.7905 - val_loss: 0.7594\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7869 - val_loss: 0.7554\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7834 - val_loss: 0.7516\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7799 - val_loss: 0.7477\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7765 - val_loss: 0.7439\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.7731 - val_loss: 0.7399\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.7698 - val_loss: 0.7362\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.7665 - val_loss: 0.7325\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.7633 - val_loss: 0.7289\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7601 - val_loss: 0.7253\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7569 - val_loss: 0.7217\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7538 - val_loss: 0.7181\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7507 - val_loss: 0.7147\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7477 - val_loss: 0.7112\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7447 - val_loss: 0.7078\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7417 - val_loss: 0.7045\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7389 - val_loss: 0.7012\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7360 - val_loss: 0.6979\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7332 - val_loss: 0.6947\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7304 - val_loss: 0.6915\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7277 - val_loss: 0.6884\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7250 - val_loss: 0.6853\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7223 - val_loss: 0.6823\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.7197 - val_loss: 0.6793\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.7171 - val_loss: 0.6763\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.7146 - val_loss: 0.6734\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.7121 - val_loss: 0.6704\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7096 - val_loss: 0.6676\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7072 - val_loss: 0.6649\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7048 - val_loss: 0.6621\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7025 - val_loss: 0.6592\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7002 - val_loss: 0.6566\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6979 - val_loss: 0.6540\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6957 - val_loss: 0.6514\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6935 - val_loss: 0.6488\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6913 - val_loss: 0.6464\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6892 - val_loss: 0.6439\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6871 - val_loss: 0.6413\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6851 - val_loss: 0.6389\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6830 - val_loss: 0.6366\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6811 - val_loss: 0.6342\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.6791 - val_loss: 0.6318\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.6772 - val_loss: 0.6296\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.6753 - val_loss: 0.6274\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.6734 - val_loss: 0.6251\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6716 - val_loss: 0.6230\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6698 - val_loss: 0.6209\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6681 - val_loss: 0.6187\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6663 - val_loss: 0.6166\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6646 - val_loss: 0.6145\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6630 - val_loss: 0.6126\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6613 - val_loss: 0.6107\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6597 - val_loss: 0.6087\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6581 - val_loss: 0.6068\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6566 - val_loss: 0.6049\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6551 - val_loss: 0.6031\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6536 - val_loss: 0.6012\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6521 - val_loss: 0.5995\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6507 - val_loss: 0.5977\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.6493 - val_loss: 0.5959\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.6479 - val_loss: 0.5943\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.6465 - val_loss: 0.5926\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.6452 - val_loss: 0.5909\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.6439 - val_loss: 0.5893\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.6426 - val_loss: 0.5877\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6413 - val_loss: 0.5862\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6401 - val_loss: 0.5846\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6389 - val_loss: 0.5831\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6377 - val_loss: 0.5816\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6365 - val_loss: 0.5802\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6354 - val_loss: 0.5787\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6343 - val_loss: 0.5773\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6332 - val_loss: 0.5759\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6321 - val_loss: 0.5745\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6310 - val_loss: 0.5732\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.6300 - val_loss: 0.5719\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.6290 - val_loss: 0.5706\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.6280 - val_loss: 0.5693\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.6270 - val_loss: 0.5681\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6261 - val_loss: 0.5669\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6251 - val_loss: 0.5657\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6242 - val_loss: 0.5645\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6233 - val_loss: 0.5633\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6224 - val_loss: 0.5622\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6216 - val_loss: 0.5611\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6207 - val_loss: 0.5600\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6199 - val_loss: 0.5590\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6191 - val_loss: 0.5579\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6183 - val_loss: 0.5568\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6175 - val_loss: 0.5558\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.6167 - val_loss: 0.5548\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6160 - val_loss: 0.5538\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6152 - val_loss: 0.5528\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6145 - val_loss: 0.5519\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.6138 - val_loss: 0.5509\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6131 - val_loss: 0.5501\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6125 - val_loss: 0.5491\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6118 - val_loss: 0.5483\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6112 - val_loss: 0.5474\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6106 - val_loss: 0.5466\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6099 - val_loss: 0.5457\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6093 - val_loss: 0.5450\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6087 - val_loss: 0.5442\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6082 - val_loss: 0.5433\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6076 - val_loss: 0.5426\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.6071 - val_loss: 0.5418\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6065 - val_loss: 0.5411\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.6060 - val_loss: 0.5404\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.6055 - val_loss: 0.5397\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.6050 - val_loss: 0.5390\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.6045 - val_loss: 0.5383\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6040 - val_loss: 0.5377\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6035 - val_loss: 0.5370\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6031 - val_loss: 0.5364\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6026 - val_loss: 0.5358\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6022 - val_loss: 0.5351\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6017 - val_loss: 0.5346\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6013 - val_loss: 0.5339\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6009 - val_loss: 0.5334\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6005 - val_loss: 0.5328\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.6001 - val_loss: 0.5323\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5997 - val_loss: 0.5317\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5994 - val_loss: 0.5312\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5990 - val_loss: 0.5307\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5986 - val_loss: 0.5301\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5983 - val_loss: 0.5296\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5979 - val_loss: 0.5292\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5976 - val_loss: 0.5287\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5973 - val_loss: 0.5282\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5969 - val_loss: 0.5278\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5966 - val_loss: 0.5273\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5963 - val_loss: 0.5269\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5960 - val_loss: 0.5264\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5957 - val_loss: 0.5259\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5954 - val_loss: 0.5255\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5952 - val_loss: 0.5251\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5949 - val_loss: 0.5248\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5946 - val_loss: 0.5244\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5943 - val_loss: 0.5239\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5941 - val_loss: 0.5236\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5938 - val_loss: 0.5232\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5936 - val_loss: 0.5229\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5933 - val_loss: 0.5225\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5931 - val_loss: 0.5221\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5929 - val_loss: 0.5218\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5926 - val_loss: 0.5215\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5924 - val_loss: 0.5211\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5922 - val_loss: 0.5208\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5920 - val_loss: 0.5205\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5918 - val_loss: 0.5201\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5916 - val_loss: 0.5198\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5914 - val_loss: 0.5195\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5912 - val_loss: 0.5193\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5910 - val_loss: 0.5190\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5908 - val_loss: 0.5187\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5906 - val_loss: 0.5184\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5904 - val_loss: 0.5181\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5903 - val_loss: 0.5178\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5901 - val_loss: 0.5176\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5899 - val_loss: 0.5174\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5898 - val_loss: 0.5171\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5896 - val_loss: 0.5169\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5895 - val_loss: 0.5166\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5893 - val_loss: 0.5164\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5892 - val_loss: 0.5162\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5890 - val_loss: 0.5159\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5889 - val_loss: 0.5158\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5887 - val_loss: 0.5155\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5886 - val_loss: 0.5153\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5885 - val_loss: 0.5151\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5883 - val_loss: 0.5149\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5882 - val_loss: 0.5147\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5881 - val_loss: 0.5145\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5880 - val_loss: 0.5143\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5878 - val_loss: 0.5141\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5877 - val_loss: 0.5139\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5876 - val_loss: 0.5138\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5875 - val_loss: 0.5136\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5874 - val_loss: 0.5134\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5873 - val_loss: 0.5133\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5872 - val_loss: 0.5131\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5871 - val_loss: 0.5129\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5870 - val_loss: 0.5128\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5869 - val_loss: 0.5126\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5868 - val_loss: 0.5125\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5867 - val_loss: 0.5123\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5866 - val_loss: 0.5122\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5865 - val_loss: 0.5121\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5864 - val_loss: 0.5119\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5864 - val_loss: 0.5118\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5863 - val_loss: 0.5116\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5862 - val_loss: 0.5115\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5861 - val_loss: 0.5114\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5860 - val_loss: 0.5113\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5860 - val_loss: 0.5111\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5859 - val_loss: 0.5110\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5858 - val_loss: 0.5109\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5858 - val_loss: 0.5108\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5857 - val_loss: 0.5107\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5856 - val_loss: 0.5106\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5856 - val_loss: 0.5105\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5855 - val_loss: 0.5104\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5854 - val_loss: 0.5103\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5854 - val_loss: 0.5102\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5853 - val_loss: 0.5101\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5853 - val_loss: 0.5100\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5852 - val_loss: 0.5098\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5851 - val_loss: 0.5098\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5851 - val_loss: 0.5097\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5850 - val_loss: 0.5096\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5850 - val_loss: 0.5095\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5849 - val_loss: 0.5094\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5849 - val_loss: 0.5093\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5848 - val_loss: 0.5093\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5848 - val_loss: 0.5092\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5847 - val_loss: 0.5091\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5847 - val_loss: 0.5090\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5846 - val_loss: 0.5089\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5846 - val_loss: 0.5089\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5846 - val_loss: 0.5088\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5845 - val_loss: 0.5087\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5845 - val_loss: 0.5086\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5844 - val_loss: 0.5086\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5844 - val_loss: 0.5085\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5844 - val_loss: 0.5085\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5843 - val_loss: 0.5084\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5843 - val_loss: 0.5083\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5843 - val_loss: 0.5082\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5842 - val_loss: 0.5082\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5842 - val_loss: 0.5081\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5842 - val_loss: 0.5081\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5841 - val_loss: 0.5080\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5841 - val_loss: 0.5080\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5841 - val_loss: 0.5079\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5840 - val_loss: 0.5079\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5840 - val_loss: 0.5078\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5840 - val_loss: 0.5078\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5839 - val_loss: 0.5077\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5839 - val_loss: 0.5076\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5839 - val_loss: 0.5076\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5839 - val_loss: 0.5076\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5838 - val_loss: 0.5075\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5838 - val_loss: 0.5075\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5838 - val_loss: 0.5074\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5838 - val_loss: 0.5074\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5837 - val_loss: 0.5073\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5837 - val_loss: 0.5073\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5837 - val_loss: 0.5072\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5837 - val_loss: 0.5072\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5837 - val_loss: 0.5072\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5836 - val_loss: 0.5071\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5836 - val_loss: 0.5071\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5836 - val_loss: 0.5071\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5836 - val_loss: 0.5070\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5836 - val_loss: 0.5070\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5835 - val_loss: 0.5070\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5835 - val_loss: 0.5069\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5835 - val_loss: 0.5069\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5835 - val_loss: 0.5069\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5835 - val_loss: 0.5068\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5834 - val_loss: 0.5068\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5834 - val_loss: 0.5067\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5834 - val_loss: 0.5067\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5834 - val_loss: 0.5067\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5834 - val_loss: 0.5067\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5834 - val_loss: 0.5066\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5834 - val_loss: 0.5066\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5833 - val_loss: 0.5066\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5833 - val_loss: 0.5066\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5833 - val_loss: 0.5065\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5833 - val_loss: 0.5065\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5833 - val_loss: 0.5065\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5833 - val_loss: 0.5064\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5833 - val_loss: 0.5064\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5832 - val_loss: 0.5064\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5832 - val_loss: 0.5064\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5832 - val_loss: 0.5063\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5832 - val_loss: 0.5063\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5832 - val_loss: 0.5063\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5832 - val_loss: 0.5062\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5832 - val_loss: 0.5062\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5832 - val_loss: 0.5062\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5832 - val_loss: 0.5062\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5831 - val_loss: 0.5062\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5831 - val_loss: 0.5062\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5831 - val_loss: 0.5061\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5831 - val_loss: 0.5061\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5831 - val_loss: 0.5061\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5831 - val_loss: 0.5061\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5831 - val_loss: 0.5060\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5831 - val_loss: 0.5060\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5831 - val_loss: 0.5060\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5831 - val_loss: 0.5060\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5831 - val_loss: 0.5060\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5831 - val_loss: 0.5060\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5831 - val_loss: 0.5060\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5830 - val_loss: 0.5059\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5830 - val_loss: 0.5059\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5830 - val_loss: 0.5059\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5830 - val_loss: 0.5059\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5830 - val_loss: 0.5059\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5830 - val_loss: 0.5058\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5830 - val_loss: 0.5058\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5830 - val_loss: 0.5058\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5830 - val_loss: 0.5058\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5830 - val_loss: 0.5058\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5830 - val_loss: 0.5058\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5830 - val_loss: 0.5058\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5830 - val_loss: 0.5057\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5830 - val_loss: 0.5057\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5830 - val_loss: 0.5057\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5057\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5057\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5057\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5829 - val_loss: 0.5057\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5829 - val_loss: 0.5056\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5829 - val_loss: 0.5056\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5829 - val_loss: 0.5056\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5829 - val_loss: 0.5056\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5056\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5056\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5056\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5829 - val_loss: 0.5056\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5829 - val_loss: 0.5055\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5829 - val_loss: 0.5054\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5054\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5053\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5828 - val_loss: 0.5052\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.5051\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5051\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5050\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5049\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5048\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5047\n",
            "5/5 [==============================] - 1s 8ms/step - loss: 0.5047\n",
            "5/5 [==============================] - 1s 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f07444458d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHMCAYAAAAzqWlnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABor0lEQVR4nO3deXxU1f3/8dedzEz2MAkhJBBICGGXTQQVqIC4IFBX3FsXLO5t1aptUSpal1Lr+lO/2ioodQOpKKDgCm4gAooIsi+yJSQhmYTsmcz9/THJQEiAhGQWJu/n45FHcs9d5jOfJOTDOeeea5imaSIiIiISoiyBDkBERETEl1TsiIiISEhTsSMiIiIhTcWOiIiIhDQVOyIiIhLSVOyIiIhISFOxIyIiIiFNxY6IiIiENBU7IiIiEtJU7IgEkGEYjBw5stnXGTlyJIZhND+gENNS+RWRE5uKHWnVDMNo0serr74a6JDFB4Lh5+DVV1897mvXxiUiDbMGOgCRQHrggQfqtT399NMUFhbyxz/+EYfDUWffgAEDWvT1169fT1RUVLOvM3PmTEpLS1sgotYp0D8HIuJbhh4EKlJXeno6v/zyC9u3byc9PT3Q4UgzGIbBiBEjWLJkSZPP9ffPwauvvsr111/PjBkzuO6665p0bm2vjv45F2mYhrFEGql2XkxlZSUPPfQQPXr0IDw83PuHqbCwkMcff5wzzzyT1NRU7HY77dq14/zzz2fZsmUNXrOhOSVTp07FMAyWLFnCnDlzGDJkCFFRUSQkJHDFFVewZ8+eI8Z2qCVLlmAYBlOnTmX16tWMGzcOh8NBVFQUI0aMYOnSpQ3GlJWVxfXXX09SUhKRkZEMGDCA1157rc71GqM5+cjLy+PGG28kJSWF8PBw+vTpw4wZMxo8p7Kykr///e907dqV8PBwunTpwv33309FRUWj4jwey5cvZ8KECSQnJ2O32+nUqRM33XQTe/furXfstm3buPHGG8nMzCQyMpKEhAT69u3LzTffzP79+wHP9+/6668H4Prrr68zZLZjx44Wjb2iooJ//OMf9O3bl6ioKOLi4vjVr37F7NmzGzx+3rx5jB492vu96NChAyNGjOCFF15o8vs81FtvvcWoUaNwOBxERETQq1cvHn744Qa/b1999RW//vWvSU1NJTw8nOTkZE477TQefPDBlkmKhDwNY4k00SWXXMKKFSs477zzuPDCC0lKSgI8Q1L33XcfZ5xxBuPGjSM+Pp6dO3cyb948Fi5cyPz58xkzZkyjX+eFF15g3rx5nH/++YwYMYLly5cza9YsfvzxR1avXk14eHijrrNy5Ur++c9/cvrpp/O73/2OnTt38r///Y/Ro0ezevVqevTo4T02JyeH008/nV9++YUzzjiDoUOHkp2dza233so555zTpDwdbz6cTifDhg3DbrczYcIEKioqeOedd5g4cSIWi4Vrr73We6xpmlx22WW8//77dO3aldtvv53KykqmT5/OTz/91KR4G2v69OnceOONhIeHc/7559OpUyc2b97Myy+/zPz58/n222/p3Lkz4CkcBw8eTFFREWPHjuWSSy6hvLyc7du389///pfbb7+dtm3bct111+FwOHj//fe54IIL6gyTHT6E1hyVlZWce+65fPHFF/Ts2ZPbbruN0tJS5syZw+WXX87q1at59NFHvcf/+9//5qabbiI5OZlf//rXJCYmkpOTw5o1a5gxYwa33nprk95nrYkTJzJjxgxSU1O55JJLcDgcfPvtt0yZMoXPPvuMTz75BKvV8+dp0aJFjBs3jri4OM4//3w6duxIfn4+69ev54UXXmhwCFKkHlNE6khLSzMBc/v27XXaR4wYYQJm3759zdzc3HrnOZ3OBtt37dplpqSkmD179qy3DzBHjBhRp+2BBx4wATM2NtZcs2ZNnX1XXnmlCZizZs1qMLZDLV682ARMwJwxY0adfS+++KIJmLfcckud9okTJ5qAee+999ZpX716tWm3203AfOCBB+q9j4Ycbz4A84YbbjBdLpe3fd26dWZYWJjZq1evOse/8cYbJmCedtppZllZmbd9//79ZkZGRoP5bayGfg42btxo2mw2s2vXrubu3bvrHP/pp5+aFovFvPDCC71tzz77rAmYTz/9dL3rFxcXm6Wlpd7tGTNmNPi9aozavB3Lo48+agLmeeedZ1ZVVXnb9+3b532/33zzjbf95JNPNu12u7lv37561zr0e3s87/Oiiy6q026aB3/2D73OxRdfbALm6tWrjxqDyNFoGEukif7+97+TmJhYr71NmzYNtqempjJhwgQ2bNjAzp07G/06f/jDH+jbt2+dtkmTJgHw3XffNfo6w4YNqzcHZOLEiVit1jrXqays5K233qJNmzbcf//9dY7v378/11xzTaNfE44/H1FRUTz55JOEhYV523r37s2wYcNYv349xcXF3vbaoa1HH32UiIgIb3tCQgJTpkxpUryN8X//939UVVXxzDPP0LFjxzr7Ro8ezfnnn8/8+fM5cOBAnX2RkZH1rhUdHd1guy9Nnz4dwzB48sknvT0nAElJSd58vfzyy3XOsVqt2Gy2etdq6HvbmPf5zDPPYLVamT59er3jp0yZQtu2bXnjjTcade2GYhBpiIaxRJpoyJAhR9z3zTff8Mwzz7Bs2TJycnKorKyss3/Pnj3eIY5jOeWUU+q1derUCYCCgoJGx9vQdWw2G+3bt69znY0bN1JWVsYpp5xCbGxsvXOGDx9e7w/hsRxPPrp160ZcXFy9ax363mNiYgD4/vvvsVgsDB8+vN7xvlhfp3au0RdffMGKFSvq7c/JyaG6uppNmzYxaNAgzj//fCZPnsxtt93GRx99xLnnnsuwYcPo3bu3328VP3DgAFu2bKFjx4707Nmz3v4zzzwTgB9++MHbdvXVV/OnP/2J3r17c8UVVzBixAiGDRtGu3bt6pzb2PdZWlrKjz/+SGJiIk8//XSDcYaHh7N+/fo6Mbz77ruceuqpXH755YwaNYphw4aRmpranHRIK6NiR6SJkpOTG2yfO3cuEyZMICIigrPPPpuuXbsSHR2NxWJhyZIlfPHFF02aNNvQXI3a/41XV1c36zq11zr0OoWFhQC0b9++weOP1H4kx5uPo8UL1Is5ISGhwZ6HI32fmqN2ou3jjz9+1ONqe5/S0tL47rvvmDp1KosWLeLdd98FPIXb3XffzR/+8IcWj/FIar+/KSkpDe6vbXc6nd62u+66i8TERF544QWeffZZnn76ae8dbo8//ri3kG7s+ywoKMA0TXJzcxs9ufjiiy9mwYIFPPHEE0yfPp2XXnoJgEGDBvHYY49x9tlnNz0Z0uqo2BFpoiP9j3zKlCnY7XZWrlxJr1696uy76aab+OKLL/wR3nGr7U3Zt29fg/uP1H4k/shHmzZtyM/Pp6qqql7Bk52d3ezrN/R64CkcGup9akivXr2YNWsWLpeLH3/8kU8//ZT/9//+H3/84x+Jjo7mhhtuaPE4G1Ib+5HykpWVVee4Wtdccw3XXHMNTqeTpUuXMnfuXKZPn865557Lhg0bvL08jXmftdceOHAg33//faNjHzduHOPGjaOkpITly5ezYMEC/u///o/x48fzww8/0Lt37ybnQ1oXzdkRaSFbtmyhd+/e9f6wu91uvv766wBF1Xg9e/YkMjKSNWvW1JtzAjT5PfgjHyeffPIRr3c8a+scy2mnnQZ4boVuKqvVyqBBg/jzn//MW2+9BcB7773n3V87R6kpvXZNERsbS9euXdmzZw+bN2+ut3/x4sWAJ6cNcTgcjB07lv/85z9cd9115Ofn8+WXX9Y77mjvMyYmhj59+rBu3Try8/Ob/B6io6M588wzefLJJ5k8eTKVlZUsXLiwydeR1kfFjkgLSU9PZ/PmzXXWWjFNk6lTp/Lzzz8HMLLGsdvtXH755RQWFvLwww/X2ffjjz8yc+bMJl3PH/moXZvmvvvuo7y83Nuen59f7z20hNtvvx2bzcadd97Jpk2b6u2vrKysUwitWrXKO3x0qNpeskNXz669Nbspk9ibauLEiZimyT333FOnqMrLy+Pvf/+795haixcvbnChwpycHOBg/E15n3fddReVlZVMnDixzpBZrYKCgjq9Pl9++SUul6tR1xY5Eg1jibSQO++8k5tvvpmBAwdyySWXYLPZ+Oabb/j555/59a9/zfz58wMd4jH94x//4PPPP+ef//wny5cvZ+jQoWRlZTF79mzGjh3Le++9h8XSuP8j+SMfV155JbNmzWLevHmcdNJJXHDBBVRVVTFnzhwGDx7M1q1bm/0ah+rZsyfTp09n4sSJ9OnThzFjxtC9e3eqqqrYuXMnX331Fe3atWPDhg0A/Pe//+Wll15i+PDhdO3alfj4eLZu3cr8+fMJDw/njjvu8F779NNPJyoqiqeffpr9+/d75xz9/ve/rze0dCRHW3n5hRde4O6772bhwoW8//779O/fn7Fjx1JaWso777xDTk4O9957b53J3hdddBExMTGcdtpppKenY5omX331FStWrGDQoEGcddZZTX6fEydOZNWqVbzwwgt07dqVc889l86dO5Ofn8/27dv58ssvuf7663nxxRcBz12Je/bsYdiwYaSnp2O321m1ahWff/45aWlpXHHFFY3KjbRygbzvXSQYHWudnaOZMWOG2b9/fzMqKsps27ateeGFF5pr1qzxrh+yePHiOsdzlHV2Dj/WNE1z+/btJmBee+21x4ytdp2dI62Lk5aWZqalpdVr3717t3nNNdeYiYmJZkREhNm/f3/z1VdfNd955x0TMJ966qmj5uBQLZGPWtdee22D35eKigrzwQcfNLt06WLa7XYzLS3NnDx5slleXt7i6+zUWrNmjXnttdeanTt3Nu12uxkfH2/26dPHvPHGG83PPvvMe9y3335r3nzzzWa/fv3M+Ph4MyIiwuzatat53XXXmT/99FO96y5cuNA87bTTzOjoaO/aOQ29/uFqjz3aR0FBgWmapllWVmY+8sgjZp8+fcyIiAgzJibGHDZsmPnmm2/Wu+7//d//mRdeeKHZpUsXMzIy0oyPjzcHDBhgTps2zSwqKjru92mapjl//nxz3LhxZrt27UybzWa2b9/eHDx4sHnfffeZ69ev9x43a9Ys84orrjAzMzPN6OhoMzY21uzTp485efJkMycn55i5ETFN09SzsUSkUe677z4effRRFi1axLnnnhvocEREGk3FjojUsXfvXjp06FCn7aeffmLo0KHY7Xb27NlTZwE/EZFgpzk7IlLHKaecQmZmJieddBLR0dFs3ryZDz74ALfbzUsvvaRCR0ROOOrZEZE6HnzwQd577z127NjBgQMHcDgcnHbaadx9990+WZVYRMTXVOyIiIhISNM6OyIiIhLSVOyIiIhISFOxIyIiIiFNxY6IiIiENN16XqOgoKDB5680V7t27cjNzW3x60pdyrN/KM/+o1z7h/LsH77Is9VqJT4+vnHHtugrn8BcLhdVVVUtek3DMLzX1k1vvqM8+4fy7D/KtX8oz/4RDHnWMJaIiIiENBU7IiIiEtJU7IiIiEhIU7EjIiIiIU0TlEVEJOS4XC5KS0uPeVxZWRmVlZV+iKh1O548m6aJ1WolOjq62a+vYkdEREKKy+WipKSE2NhYLJajD2DYbLYWvxNX6jvePJeUlFBRUUF4eHizXl/DWCIiElJKS0sbVehI8IuKiqKioqLZ19FPgoiIhBwVOqGhdo2e5gqqYayff/6ZefPmsX37dgoKCrj77rsZMmTIEY8vKChg5syZbNu2jezsbM477zyuu+46/wUsIiIiQS+oSt+KigrS09O54YYbGnV8VVUVcXFxXHzxxaSlpfk4OhERETkRBVWxM3DgQK644oqj9uYcKikpieuvv54RI0YQFRXl4+hERERODKeeeir/+c9/WuRaS5cupWPHjhQWFrbI9QIhqIaxREREWqsJEybQu3dvHnrooWZf68MPP1QnwCFaXbFTVVVV5/Y3wzCIjIz0ft1STLcbiouoqq7AsEa02HWlvtrvW0t+/6Q+5dl/lGtpiGmaVFdXY7Ue+09327Zt/RCR/zT3d6HVFTtz585lzpw53u0uXbowbdo02rVr16Kv48raTdadv2FfeASp737doteWhiUnJwc6hFZBefYf5fr4lJWVYbPZGn18U471ld///vcsW7aMZcuW8corrwDw7LPP8oc//IG33nqLxx57jPXr1zN79mw6dOjAAw88wMqVKyktLaV79+7cd999jBgxwnu9QYMGceONN3LTTTcBnmkfTz75JJ988glLliwhOTmZBx98kDFjxhwzttriymazeXM1f/58/vnPf7J9+3bat2/PDTfcwK233uo9Z/r06bz00kvs3buX2NhYTjvtNKZPn+4991//+hfbt28nMjKSk046iZkzZx5x8UC73U5KSspxZPWQ99Css09AF110EePHj/du11aLubm5uFyuFnsds9yzLoBZUU7WLzvA3rwFkeTIDMMgOTmZ7OxsTNMMdDghS3n2H+W6eSorK+v04JumCZUNr9Xi80UF7eGN6pWYOnUqW7ZsoWfPntx9990AbNy4EYCHHnqIv/3tb3Tu3Jk2bdqwd+9eRo4cyT333IPdbmfOnDn89re/5csvv6Rjx47AwV6gQ9/b448/zv333899993HjBkzuOWWW1i+fDnx8fFHja32b2PtyMiaNWuYNGkSd911F+effz4rV65k8uTJxMXFcfnll/Pjjz9y33338eyzz3LKKafgdDpZuXIlVVVV7Nu3j5tuuon77ruP8847j+LiYpYvX15v1OVQlZWVZGVl1Wu3Wq2N7qhodcXOoZXp4VryHxXTHgE2O1RVYhYWQGL7Fru2NMw0Tf1h8APl2X+U6xZSWYH79ssa3NX85eqOzvLcbAg/9lSGuLg47HY7ERERJCUlAbBlyxYA7rnnHs444wzvsfHx8fTp08e7fe+997Jo0SI+/vhjrr/++iO+xmWXXcaFF14IwF/+8hdeeeUVVq9ezahRo5r0nv79738zfPhw7rzzTgC6du3K5s2befHFF7n88svZs2cPUVFRnHXWWcTExJCamsrAgQOpqqoiJycHl8vF2LFjSU1NBaBXr17HfM3m/h4EVbFTXl5Odna2dzsnJ4cdO3YQExNDYmIib775Jvn5+dx+++3eY3bs2OE9t6ioiB07dmC1Wr1JDJSC8mpm9rqcCpebPx8oVLEjIiLHpV+/fnW2S0pKeOKJJ/jss8+8xUN5eTl79uw56nUOLSqioqKIjY0lLy+vyfFs3ryZc889t07b4MGDefnll6muruaMM84gNTWV008/nZEjRzJq1Ch+/etfY7PZ6N27N8OHD2f06NGMGDGCESNGMG7cOBwOR5PjaIqgKna2bt3Kgw8+6N2eOXMmACNGjOC2226joKCg3jfm3nvv9X69bds2vv76a9q1a8fzzz/vn6CPwAIsTuiLYbqpKiwk8CPCIiKtlD3c08PSAH8MYzXX4XdVPfTQQ3z11VdMmTKF9PR0IiIiuPHGG4/5oM3DRzUMw8Dtdjc7vsPFxMSwaNEili5dypdffsm//vUvnnzyST744APatGnD22+/zcqVK/niiy+YMWMG06ZNY8GCBXTu3LnFY6kVVMVOnz59mD274R9IgNtuu61e29GOD6S4iDCsZjUuI4wC5wGSAh2QiEgrZRjGEYeSDJsNwxLm54gaZrPZGlV8rFy5kksvvZTzzjsP8PT07N6929fheXXr1o0VK1bUaVuxYgUZGRmEhXlyabVaOeOMMzjjjDO466676NWrF9988w1jx47FMAwGDx7M4MGDufPOOxkyZAgLFy70Tqb2haAqdkKJxTBoSwX7iCLPWaJiR0REjqpTp0788MMP7Nq1i+jo6CMWPl26dGHhwoWcffbZGIbB448/7pMemiO56aabGDt2LE899RTnn38+q1atYsaMGTz66KMAfPLJJ+zcuZNTTz0Vh8PBZ599htvtpmvXrnz//fd8/fXXjBgxgsTERL7//nvy8/Pp1q2bT2NWseNDba1u9lVD3oHyQIciIiJB7qabbuKOO+5g5MiRlJeX8+STTzZ43AMPPMBdd93FBRdcQEJCArfddhvFxcV+i7Nv3768+OKL/Otf/+KZZ54hKSmJe+65h8svvxyANm3asHDhQp588knKy8vp0qULL730Ej169GDz5s0sX76cl19+meLiYjp27Mjf/vY3zjzzTJ/GbJia6g94bj1v6XHbJ977ni9Lori2+AcuvunKFr22HGQYBikpKWRlZenOFR9Snv1HuW6eoqIi4uLiGnWsz+fsCNC8PB/p+2mz2Rp963lQPRsr1LSN8UxMyzv6nDERERHxIQ1j+VA7RzTsK2e/acc0TS39LiIiQefPf/4z7777boP7Lr74YqZNm+bniFqeih0fapvogI3Z7LfFQnERxLYJdEgiIiJ13HPPPdx8880N7ouNjfVzNL6hYseH2sV5bnXMC3fA/hwVOyIiEnQSExNJTEwMdBg+pTk7PtQ2yrOAk9Meg2t/boCjERERaZ1U7PhQm4gwrKYb07CwP7cg0OGIiIi0Sip2fMhiGCSGeZ4Wu9/pvzUQRERE5CAVOz6WVPNYlLwDvn62roiIiDRExY6PJdWutVNWHeBIREREWicVOz6WHB8DwH5XcDxoTkREpCG7du2iY8eOrF27NtChtDgVOz6WnBQPwH5LJGaFnpElIiINmzBhAn/7299a7Hp33HEHEydObLHrnchU7PhY+0TP8zzywttAfl6AoxEREWl9VOz4WPtYz8KC+2sXFhQRETnMHXfcwbJly3jllVfo2LEjHTt2ZNeuXWzYsIHf/OY3dOvWjf79+/P73/+e/Px873kLFixg9OjRdO3alT59+nD55ZdTWlrKE088wTvvvMNHH33kvd7SpUubHNeyZcsYN24cXbp0YeDAgTz66KO4XK5jvj7A0qVLGTduHJmZmWRmZnLBBRewe/fu5ifrOGgFZR+rLXac9hgq9+8iPMDxiIi0NqZpUlHd8NPjq3FT5XL77LXDw4xGPRfxoYceYtu2bfTs2ZO7774bAKvVyrhx47jyyiuZOnUq5eXlPPLII9x0002888477Nu3j9tuu4377ruP8847j+LiYpYvX45pmtx8881s3ryZ4uJinnzySQAcDkeTYs/KyuK3v/0tl112Gc888wxbtmzhnnvuITw8nD/96U9HfX2Xy8UNN9zAVVddxfPPP49pmqxYsSJgz4hUseNjjigbVty4DAsFeYUkBzogEZFWpqLa5PJZmwLy2rMu706E9dh/4OPi4rDb7URERJCUlATA008/zUknncRf//pX73FPPPEEgwcPZuvWrZSWluJyuRg7diypqakA9OrVy3tsREQElZWV3us11WuvvUaHDh145JFHMAyDzMxMsrOzefTRR7nzzjvJyck54usXFBRQVFTEWWedRXp6OjabjS5duhxXHC1BxY6PWQyDthYX+9x28gpLVOyIiEij/PzzzyxdupRu3brV2/fLL78wYsQIhg8fzujRoxkxYgQjRoxg3LhxTe7BOZItW7YwaNCgOr0xgwcPpqSkhKysLHr37n3E14+Pj+eyyy7j6quv5le/+hUjR45k7NixtG/fvkViayoVO36QGA77ymB/SWWgQxERaXXCwwxmXd69wX02q40qV5VPX/t4lZaWcvbZZzN58uR6+9q3b09YWBhvv/02K1eu5IsvvmDGjBlMmzaNBQsW0Llz5+aE3SjHev2nnnqKG264gcWLF/Pee+/x2GOP8dZbbzFo0CCfx3Y4TVD2g9oHguZpEWUREb8zDIMIq6XhD9sR2lvooylzVGw2G273wflDJ510Ehs3bqRTp0506dKlzkdUVJT3vQ0ePJi7776bjz76CJvNxsKFCwGw2+1UVx//graZmZmsWrUK0zw432nFihXExMSQkpJyzNevfQ+///3v+fDDD+nRowfvvffeccfTHCp2/CCxjeeHcr/biunWSsoiIlJfp06d+OGHH9i1axf5+flcd911OJ1Obr31VlavXs2OHTtYsmQJd955J9XV1Xz//fc8++yz/Pjjj+zZs4cPP/yQ/Px877BXamoq69evZ8uWLeTn51NV1bQerGuvvZa9e/dy//33s2XLFj766COeeOIJbrzxRiwWy1Fff+fOnTz22GOsXLmS3bt3s3jxYrZv305mZqYvUndMGsbyg8T4GKCMPHscOPMhoV2gQxIRkSBz0003cccddzBy5EjKy8v59ttvee+993j00Ue56qqrqKioIDU1lZEjR2KxWIiNjWX58uW8/PLLFBcX07FjR/72t79x5plnAnD11VezbNkyxo4dS0lJCe+88w5Dhw5tdDwpKSn897//5eGHH+bss8/G4XBw5ZVX8sc//hHgqK+fm5vLli1beOeddygoKKB9+/Zcd911/Pa3v/VJ7o7FMA/tn2rFcnNzm1z1HothGKSkpDD3u008+sVuMot28q+zUzEye7fo67R2tXnOyspCP86+ozz7j3LdPEVFRcTFxTXqWJvN1uL/9kt9zcnzkb6fNpuNdu0a13mgYSw/SIzydKDlhTsw9+cGOBoREZHWRcNYfpAY7ZmgXGiPoSpvpxYWFBERv3v22Wf5f//v/zW479RTT+X111/3c0T+o2LHD+LCww4uLJhfpLV2RETE737729/y61//usF9ERERfo7Gv1Ts+IHFMGhrrWafy6KFBUVEJCDi4+OJj48PdBgBoTk7fpIYEQZAXqnrGEeKiIhIS1Kx4yeJsZ6ZOvsrwXT77qFzIiKtne5gk8Op2PGTto4YAPJssVBYEOBoRERCl9VqpaSkREVPCKisrGyRJ6Vrzo6fJEbbAdgf3gb274P4tgGOSEQkNEVHR1NRUcGBAweOeazdbqeyUs8t9LXjzbNhGMTExDT79VXs+EmdtXby9mlhQRERHwoPDyc8/OgLfWjxRv8IhjxrGMtPatfa2R/eBvL2BTgaERGR1kPFjp+0renZ8SwsmBPgaERERFoPFTt+0iY8DJthYhoW8gpKAh2OiIhIq6Fix08Mw6BdzfBxXrEmw4mIiPiLih0/ahfjuSMrt8rArK4OcDQiIiKtg4odP2oXFwlArt0B+Xr6uYiIiD+o2PGjpNqenQiH7sgSERHxExU7ftSu5vbz3Jq1dkRERMT3VOz4Ubtoz+3nuRHxoNvPRURE/ELFjh8l1fTs5EU4cKtnR0RExC9U7PhR2ygbFkyqLDacBUWBDkdERKRVULHjR1aLQbzd8/RWrbUjIiLiHyp2/KxdTM0kZZcFs7IiwNGIiIiEvqB66vnPP//MvHnz2L59OwUFBdx9990MGTLkqOesW7eOmTNnsmvXLtq2bcsll1zCyJEj/RPwcWgXG8GG/CpyIuJhfy6kpAY6JBERkZAWVD07FRUVpKenc8MNNzTq+JycHP7xj3/Qp08f/vnPfzJu3DhefPFFVq9e7dtAmyHJe/t5vNbaERER8YOg6tkZOHAgAwcObPTxH3/8MUlJSVxzzTUApKamsmHDBj744AMGDBjgoyibp90hd2SZefswAhyPiIhIqAuqYqepNm/eTN++feu09e/fn1dfffWI51RVVVFVVeXdNgyDyMhI79ctqfZ6h1734CrK8bD/lxZ/zdaooTxLy1Oe/Ue59g/l2T+CIc8ndLHjdDpp06ZNnbY2bdpQVlZGZWUldru93jlz585lzpw53u0uXbowbdo02rVr57M4k5OTvV/3thUDu8gNdxBR/BOJKSk+e93W5tA8i+8oz/6jXPuH8uwfgczzCV3sHI+LLrqI8ePHe7drK83c3FxcLleLvpZhGCQnJ5OdnY1pmp62KjcAJbYo9m/fQ1VWVou+ZmvUUJ6l5SnP/qNc+4fy7B++yrPVam10R8UJXew4HA4KCwvrtBUWFhIZGdlgrw6AzWbDZrM1uM9XP+ymaXqvHWE1iLXCARfkHignxu1WF2oLOTTP4jvKs/8o1/6hPPtHIPMcVHdjNVW3bt346aef6rStWbOG7t27ByiixkmsnbdDBBQfCHA0IiIioS2oip3y8nJ27NjBjh07AM+t5Tt27CAvLw+AN998k+eee857/DnnnENOTg6vv/46e/bs4aOPPmLZsmWMGzcuEOE3Wp1JyrkaxhIREfGloBrG2rp1Kw8++KB3e+bMmQCMGDGC2267jYKCAm/hA5CUlMRf/vIXXnvtNT788EPatm3LzTffHLS3ndeqvf08N8KBmZOFkdEjwBGJiIiErqAqdvr06cPs2bOPuP+2225r8Jx//vOfvgyrxXmffh4eD7nZAY5GREQktAXVMFZrkRjtqTFzIxwaxhIREfExFTsB4H1kREQ8Zo6KHREREV9SsRMAtXN2CuyxVOXlBDgaERGR0KZiJwDahIdhDzMwDQv7K8AsLw10SCIiIiFLxU4AGIZxyB1Z8ZCrp5+LiIj4ioqdAGlfU+zsi0jQJGUREREfUrETIO1jPMVOTmSCJimLiIj4kIqdAEmKObRnR2vtiIiI+IqKnQDx9uzo9nMRERGfUrETIO2jPc/HUs+OiIiIb6nYCZDanh1neBwVTidmVVWAIxIREQlNKnYCJMZuIcrmSX9uuAP26/ZzERERX1CxEyCGYXh7dzSUJSIi4jsqdgKo9hlZmqQsIiLiOyp2AsjbsxOpnh0RERFfUbETQAdvP9fCgiIiIr6iYieA6t5+rmJHRETEF1TsBNChCwuSuw/TXR3giEREREKPip0Aqn1kRIktihLDCnk5AY5IREQk9KjYCaAIq4U24WFAzVDWvr0BjkhERCT0qNgJsKRDJynv2xPgaEREREKPip0Aq7OwoHp2REREWpyKnQBrX7uwYGS8enZERER8QMVOgLWPOeT2c/XsiIiItDgVOwFWZxgrPxezoiLAEYmIiIQWFTsBVlvs5EYmYALkqndHRESkJanYCbDEKBsGUGmxUWCP1VCWiIhIC1OxE2C2MIO2UVag5vbzbE1SFhERaUkqdoKAbj8XERHxHRU7QSAltuaOrMi2uv1cRESkhanYCQLJNT07WZFt1bMjIiLSwlTsBIHanp3syLZQcgCzuCjAEYmIiIQOFTtBwFvsRLXzNKh3R0REpMWo2AkCtcNYhbZoSsPCNW9HRESkBanYCQLR9jDahIcBNUNZ6tkRERFpMSp2gkTyIfN2tNaOiIhIy1GxEyRSYmvvyEoEDWOJiIi0GBU7QaLOHVk5WZhud4AjEhERCQ0qdoKEd62dqHZQVQn5uQGOSEREJDSo2AkS9W4/z9odwGhERERCh4qdIFFb7OTbYqiw2DCzdgU4IhERkdCgYidIxNotRNs9347syATIVs+OiIhIS1CxEyQMwyAlpnaSciLm3p0BjkhERCQ0qNgJIgdvP28LWbsxTTPAEYmIiJz4VOwEkYO3nydCaTEccAY2IBERkRCgYieIeIuduBRPw15NUhYREWkua6ADaMiiRYuYP38+TqeTtLQ0Jk6cSGZmZoPHulwu3nvvPb744gvy8/Pp0KEDV199NQMGDPBv0C2gdq2d7MhEAMys3Rg9+wUyJBERkRNe0PXsLF26lJkzZzJhwgSmTZtGWloajzzyCIWFhQ0e//bbb/PJJ59w/fXX8+STT3L22Wfz+OOPs337dj9H3ny1PTt5YVFUGWGg289FRESaLeiKnQULFjB69GhGjRpFamoqkyZNwm63s3jx4gaP/+qrr7jooos4+eSTad++Peeccw4DBw5k/vz5fo68+RwRYURYDdwY5EQkaK0dERGRFhBUw1gul4tt27Zx4YUXetssFgt9+/Zl06ZNDZ5TVVWF3W6v02a329m4ceMRj6+qqvJuG4ZBZGSk9+uWVHu9xl7XMAxSYu1sL6ggK7ItHbN2t3hMoaipeZbjozz7j3LtH8qzfwRDnoOq2CkqKsLtduNwOOq0OxwO9u7d2+A5/fv3Z8GCBfTq1Yv27duzdu1avvvuO9xHeJDm3LlzmTNnjne7S5cuTJs2jXbt2rXY+zhccnJyo4/t0i6P7QW5ngeC7tlA+9gYLDGxPostlDQlz3L8lGf/Ua79Q3n2j0DmOaiKneNx/fXX8+KLL3LHHXdgGAbt27dn5MiRRxz2uuiiixg/frx3u7bSzM3NxeVytWhshmGQnJxMdnZ2o9fMibd5irSs+FTYA9mrV2J07dmicYWa48mzNJ3y7D/KtX8oz/7hqzxbrdZGd1QEVbETFxeHxWLB6XTWaXc6nfV6ew49595776WyspLi4mLi4+N54403aN++fYPH22w2bDZbg/t89cNummajr+29IyvWc/u5e+9OLBk9fBJXqGlKnuX4Kc/+o1z7h/LsH4HMc1BNULZarWRkZLB27Vpvm9vtZu3atXTv3v2o59rtdhISEqiurmb58uWccsopvg7XJ2pXUc4Oj/c06OnnIiIizRJUPTsA48eP5/nnnycjI4PMzEw+/PBDKioqGDlyJADPPfccCQkJXHXVVQBs3ryZ/Px80tPTyc/P55133sE0TS644IIAvovjl1zzfKwcIqg2LITpjiwREZFmCbpiZ+jQoRQVFTF79mycTifp6elMnjzZO4yVl5dXZ0Z3VVUVb7/9Njk5OURERDBw4EBuv/12oqOjA/QOmqdtlBWbxaDKDbnhDpJV7IiIiDRL0BU7AGPGjGHMmDEN7ps6dWqd7d69e/PUU0/5ISr/sBgGybE2dhVWkh3ZluT9WzArKzDs4YEOTURE5IQUVHN2xKN2JeUsRyqYJmRr3o6IiMjxUrEThDrUFDt726YBYO7ZGchwRERETmgqdoJQx7iaYiemZgGmPb8EMBoREZETm4qdIFRb7OwOiwPAVLEjIiJy3FTsBKHaYievOowKi1U9OyIiIs2gYicItQkPI9puwcQgKzIRCvIwS4oDHZaIiMgJScVOEDIMg461k5STunoa1bsjIiJyXFTsBKnaoaw97TIAMPeq2BERETkeKnaClPeOrJoHgqpnR0RE5Pio2AlS3p4da80dWbtV7IiIiBwPFTtBqmOc5/EQe112TIA9v2CaZkBjEhERORGp2AlSKbE2DKC0GpwRbaCsBAryAh2WiIjICUfFTpCyh1loH2MDYG+Hnp5GPTZCRESkyVTsBDHvJOX2mQCYe3YEMBoREZETk4qdINahdpJyXAdPg+7IEhERaTIVO0HMu7CgzQHojiwREZHjoWIniHkfCFrtuTOL7F2YLlcAIxIRETnxqNgJYrXFTk65m6qIaHC5IDcrwFGJiIicWFTsBLGESCsRVgtuE7I79wE0lCUiItJUKnaCmGEYh9yR1c3TuHt7ACMSERE58ajYCXLex0Y4UgEwd24LZDgiIiInHBU7QS61ttgJb+tp2KWeHRERkaZQsRPkOrWpvSPLDoYBhfmYRQUBjkpEROTEoWInyHVq47ntfNcBF2ZSzeKCO9W7IyIi0lgqdoJcSqydMAPKXW7y03oDYGooS0REpNFU7AQ5q8XwPjZiV1LNHVm7NElZRESksVTsnABqh7J2x6UA6tkRERFpChU7J4DaScq7rA5Pw749mBXlgQtIRETkBKJi5wTQKa5mknIp0CYeTBN27whoTCIiIicKFTsnAG/PTlEFZqcMQENZIiIijaVi5wTQMc6OxYCSSjfO1O6eRk1SFhERaRQVOycAW5iF5JiaxQUT1bMjIiLSFCp2ThDeoayo9p6G3Tswq6sDGJGIiMiJQcXOCcJ7+7k7HMIjoaoSsncHOCoREZHgZ23OyXl5eeTl5dGzZ09v244dO1iwYAFVVVUMGzaMIUOGNDtIOdizs7OwEtIyYNM6zB1bMDqmBTgyERGR4Nasnp3p06fzzjvveLedTicPPvggy5cvZ/369TzxxBMsX7682UEKdK59RlZhBaTVrKT8y+YARiQiInJiaFaxs3XrVvr27evd/vLLL6msrOTxxx/nxRdfpG/fvsyfP7/ZQYrnjiwDOFDppjDVU+yYO7YENigREZETQLOKneLiYtq0aePdXrVqFb179yY5ORmLxcKQIUPYs2dPs4MUCLdaaB9jA2B3Qs3Q1a7tmK6qAEYlIiIS/JpV7MTFxZGbmwtASUkJmzdvpn///t79brcbt9vdvAjFy3tHlhEDUdHgqoK9OwMclYiISHBr1gTlvn37snDhQqKioli3bh2madaZkLx7927atm3b7CDFo1ObcFbsKWF3USWkZcL6Hz2TlDt3DXRoIiIiQatZPTtXXXUVqamp/Pe//2XNmjX89re/JSkpCYCqqiqWLVvGSSed1CKBysHbz3cWVmKkZ3oad2iSsoiIyNE0q2fH4XDw97//ndLSUux2O1brwcuZpsmUKVNITExsdpDiUXtH1i/OCuhSM0n5F01SFhEROZpmFTu1oqKi6rXZ7XbS09Nb4vJSo1MbzzOyDlRUU5DcFQfAnl8wqyoxbPYARyciIhKcmjWM9dNPPzFv3rw6bZ9//jm33HILkyZN4tVXX9UE5RYUbrWQEuspan4xoiG2DVRXg56TJSIickTNKnbeeecdduzY4d3euXMn//nPf4iLi6N3794sXLiwXjEkzZPmqJ23UwHpGsoSERE5lmYNY+3Zs4dTTz3Vu/3ll18SGRnJQw89RHh4OP/+97/58ssvufDCC5t03UWLFjF//nycTidpaWlMnDiRzMzMIx7/wQcf8PHHH5OXl0dcXBynnnoqV111FXZ76A3tpDnCWbrzADsKKjDSMjF/WglaXFBEROSImtWzU15eTmRkpHd79erVDBgwgPBwT+9DZmamdx2exlq6dCkzZ85kwoQJTJs2jbS0NB555BEKCwsbPP7rr7/mzTff5NJLL+Wpp57i5ptvZtmyZbz11lvH/8aCWLrj4CTl2juyTN2RJSIickTNKnYSExPZunUrANnZ2ezatYt+/fp59xcXF2Oz2Zp0zQULFjB69GhGjRpFamoqkyZNwm63s3jx4gaP37hxIz169GD48OEkJSXRv39/hg0bxpYtodnbUTuMtauwkupONevrZO3GLC8LYFQiIiLBq1nDWMOHD2fOnDnk5+eze/duoqOjGTx4sHf/tm3bSElJafT1XC4X27ZtqzPsZbFY6Nu3L5s2bWrwnB49evDVV1+xZcsWMjMz2bdvHz/88AO/+tWvjvt9BbP2MTYirAblLpOssBg6ONqCcz/s3ArdtaaRiIjI4ZpV7Fx88cW4XC5++OEHEhMTufXWW4mOjgY8vTrr1q1j7Nixjb5eUVERbrcbh8NRp93hcLB3794Gzxk+fDhFRUVMmTIFgOrqas4++2wuvvjiBo+vqqqiqurg86QMw/AOxRmG0ehYG6P2ei153TDDoLMjnE155ewsrKRjRnfM75fBtk0YPfoe+wIhyBd5lvqUZ/9Rrv1DefaPYMhzs4qdsLAwrrzySq688sp6+2JiYvjPf/7TnMs3yrp165g7dy6/+93v6NatG9nZ2cyYMYM5c+YwYcKEesfPnTuXOXPmeLe7dOnCtGnTaNeunc9iTE5ObtHr9ergZFNeFnlVVuIGDKHw+2WE791BYhN60UJRS+dZGqY8+49y7R/Ks38EMs8tsqggeCYr5+XlAZ65PBEREU2+RlxcHBaLBafTWafd6XTW6+2pNWvWLM444wxGjx4NQOfOnSkvL+ff//43F198MRZL3WlJF110EePHj/du11aaubm5uFyuJsd8NIZhkJycTHZ2NqZptth1k+yetYvW7t5PcccOAJStW83evXtb5f9QfJVnqUt59h/l2j+UZ//wVZ6tVmujOyqaXexs2bKFN954gw0bNngXELRYLPTs2ZPf/OY3dO3a+IdUWq1WMjIyWLt2rfeBom63m7Vr1zJmzJgGz6moqKj3B/7wAudQNpvtiJOmffXDbppmi147zVGzsKCzAnNoVwizQpETMzcb2rXe/6G0dJ6lYcqz/yjX/qE8+0cg89ysYmfz5s1MnToVq9XKmWeeSceOHQHP+jvffPMNDzzwAFOnTj3qGjmHGz9+PM8//zwZGRlkZmby4YcfUlFRwciRIwF47rnnSEhI4KqrrgJg0KBBfPDBB3Tp0sU7jDVr1iwGDRp01KLnRJbm8PSa7SuuoowwwjtnwPZNmFs3YLTiYkdERKQhzSp23n77bRISEvj73/9eb5jp0ksvZcqUKbz11lveycONMXToUIqKipg9ezZOp5P09HQmT57svX5eXl6dnpxLLrkEwzB4++23yc/PJy4ujkGDBjU4jyhUxIWHER9ppaDMxU5nJd279sTcvgm2bYDTRgY6PBERkaDS7J6dCRMmNDifxuFwcNZZZ/G///2vydcdM2bMEYetpk6dWmc7LCyMSy+9lEsvvbTJr3MiS3OEU1DmYoeznO4ZPYF5mFs3BjosERGRoNOscR7DMKiurj7ifrfb3SonzPpDl5rFBXcUVGB07elp3L0ds6I8gFGJiIgEn2YVOz169OCjjz5q8JEQeXl5fPzxx/Ts2bM5LyFHkJHgmbezraAcIyER4hPB7QY9OkJERKSOZg1jXXnllTzwwAPccccdDBkyxLta8t69e1m5ciUWiyWk584EUkb8wZ6dareJkdEDc1WeZ5JyK11cUEREpCHNKna6dOnCo48+yltvvcXKlSuprKwEwG63M2DAAC699FJiY2NbJFCpKyXWTniYQUW1SdaBSjp07QmrvsHcpnk7IiIih2r2Ojupqancc889uN1uioqKgIOLA7777rvMmjWLWbNmNTtQqSvMYpAeH87GvHK2FVTQsWtPTICtGzBNU3OlREREarTYQjQWiwWHw4HD4QjZ9W2CTUZ8zbyd/HLonAFWGxQXQU5WgCMTEREJHqpKTmBdaoqd7QXlGFYbpHlWqza3rg9kWCIiIkFFxc4JLCPBM0l5W0GFZ+gqs5dnx+afAxiViIhIcFGxcwJLc4RjMaCoopr9ZS6M7icBYG5aG+DIREREgkeTJyhv27at0cfm5+c39fLSBPYwC53iwvmlsIJt+eW0zewNhgVysjCd+zEcbQMdooiISMA1udj561//6os45Dh1ifcUO9sLKhiSmgidusDOrZib1mEMOSPQ4YmIiARck4udW265xRdxyHHKSIhgyY4ithV4HhNhdD8Jc+dW2LQWVOyIiIg0vdgZOXKkD8KQ49WlZiXlbfkVABjd+2B++j7mpnWBDEtERCRoaILyCa52rZ2ckiqKK6uhW2/PjqxdmEXOwAUmIiISJFTsnOBiwsNIivZ00G0vKMeIiYOOaZ6dugVdRERExU4o6OJdSbl2KEu3oIuIiNRSsRMCMhIOeWwEYPRQsSMiIlJLxU4IyKwpdrbUFDveeTt7fsEsORCgqERERIKDip0QkNnWU+zsKaqktKoaIy4eklPBNGGz7soSEZHWTcVOCHBEWGkXZcUEtuYfXG8HwNyooSwREWndVOyEiMy2kQBs3l8zlNWzHwDm+h8DFZKIiEhQULETIrrVDGVtqSl2jJ79wDA883YKCwIZmoiISECp2AkRtfN2aicpG7Fx0CkDAHP96kCFJSIiEnAqdkJE15o7svYVV1FU7gLA6D3As/Pn1YEJSkREJAio2AkRMfYwOsTagUN6d2qKHXP9j5imGajQREREAkrFTgg5fN4Omb3AZgdnPmTtCmBkIiIigaNiJ4TUztvZXNuzY7N7Fxg0NZQlIiKtlIqdENKtZt6O9/ZzDhnKUrEjIiKtlIqdEJKREIHFgIIyF/tLqwAweg3w7Ny0FtNVFbjgREREAkTFTggJt1ro3CYcOKR3JzUdYttARTls2xi44ERERAJExU6I6Z7oGcramFcGgGGxHBzK+mlVoMISEREJGBU7IaZHouexEbXFDgB9TwHA/GllIEISEREJKBU7Iaa22Nm8vxyX27O2jnHSyWBYPI+O2J8TyPBERET8TsVOiOkYZyfabqGy2uQXZwUARnQsdO0JgLlGvTsiItK6qNgJMRbDoHvNE9A35B4cyjL6aShLRERaJxU7IahnA/N2jH6DPV9sWINZURGIsERERAJCxU4I6tGugUnKHTpDQjuoqoSNawIUmYiIiP+p2AlBtc/Iyi6uwln7BHTDODiUtWZFwGITERHxNxU7ISjGHkanNp4noNcZyjrkFnQ9BV1ERFoLFTshyrveziGTlOnZD+zhkJ8HO7cFKDIRERH/UrETohpaXNCwh8NJgwAwv18WkLhERET8TcVOiOrZwOKCAMbJpwNgfr80IHGJiIj4m4qdEJXaxk6M3UJFtcn2gnJvu9H3FAizQvZuzKxdAYxQRETEP1TshCiLYdCr5hb0n3MOGcqKioZe/QENZYmISOugYieE9U6KAmBdTmmd9oNDWSp2REQk9KnYCWF9aoqdn3NKcR9yq7kx4FTPg0F3bsXMzQ5UeCIiIn5hDXQADVm0aBHz58/H6XSSlpbGxIkTyczMbPDYqVOn8vPPP9drHzhwIH/96199HWpQy4iPIDzM4EClm92FlXR2hANgxLaB7n1g40+YP3yLcc6FgQ1URETEh4Ku2Fm6dCkzZ85k0qRJdOvWjQ8++IBHHnmEp59+mjZt2tQ7/u6778blcnm3Dxw4wD333MPpp5/uz7CDki3MoEdiJGv2lbIup9Rb7AAYA0/H3PgT5sqvQcWOiIiEsKAbxlqwYAGjR49m1KhRpKamMmnSJOx2O4sXL27w+JiYGBwOh/djzZo1hIeHc9ppp/k58uB0cCirrE67ccowz1DW9k2YOVmBCE1ERMQvgqpnx+VysW3bNi688EJvm8VioW/fvmzatKlR1/j8888ZOnQoERERDe6vqqqiqqrKu20YBpGRkd6vW1Lt9Vr6uk3Rp30U/ATrckvrxuRIwOzVD/Pn1bDiK4zxlwcsxuYKhjy3Bsqz/yjX/qE8+0cw5Dmoip2ioiLcbjcOh6NOu8PhYO/evcc8f8uWLezatYtbbrnliMfMnTuXOXPmeLe7dOnCtGnTaNeu3XHHfSzJyck+u/axxCdWY/18F/tLXZhR8XRwRHr3lZxzAfk/r8ay8muSf/fHE/4XPpB5bk2UZ/9Rrv1DefaPQOY5qIqd5vr888/p3LnzESczA1x00UWMHz/eu137Bz43N7fO3J+WYBgGycnJZGdnB/TBm5kJEWzIK2Px2h2M7urwtptde4PVhmv3DrK++wajc9eAxdgcwZLnUKc8+49y7R/Ks3/4Ks9Wq7XRHRVBVezExcVhsVhwOp112p1OZ73ensOVl5fzzTffcPnlRx+Osdls2Gy2Bvf56ofdNM2A/iL1TopkQ14Z63JKOTPjkEneEZHQfzCsWor72yVYOmUELMaWEOg8txbKs/8o1/6hPPtHIPMcVBOUrVYrGRkZrF271tvmdrtZu3Yt3bt3P+q53377LS6Xi1/96le+DvOEc+h6O4eznDoSAPO7LzHd1f4MS0RExC+CqtgBGD9+PJ999hlLlixh9+7dvPzyy1RUVDBy5EgAnnvuOd588816533++ecMHjyY2NhYP0cc/Hq2i8QA9h6ooqDssKG6kwZBVDQ482Hj2gbPFxEROZEF1TAWwNChQykqKmL27Nk4nU7S09OZPHmydxgrLy+v3kTavXv3smHDBu6///4ARBz8YuxhpMeHs72ggp/2lXJGepx3n2GzYQwahvnVx5hLP8eoeW6WiIhIqAi6YgdgzJgxjBkzpsF9U6dOrdfWoUMHZs+e7eOoTmz9k6PZXlDB6qySOsUOgDH8bE+xs+obzCsnYUTFBChKERGRlhd0w1jiGwNSogH4Mbuk/gSxLt2hYxpUVWIu/zIA0YmIiPiOip1Wone7SKwWg7xSF3sOVNbZZxgGxq/OAcD8+uNAhCciIuIzKnZaiXCrhV7tPAsK/phV/64s47SRYLXBzm2Yv2z1c3QiIiK+o2KnFRmQfHAo63BGdCzGyZ6Hp6p3R0REQomKnVakf4pnvZ2f9pVS7a6/sJMx/GwAzOVfYFaU+zU2ERERX1Gx04pkxEcQY7dQWuVmS34DxUyPvpCUAmWlmMuX+D0+ERERX1Cx04qEWQz6tq8ZyspqYCjLYsEYNRYA87MFWj5dRERCgoqdVqZ/smcoa3UD83YAjKFnQXgE7N0JG9b4MzQRERGfULHTytSut7Mxr4yyKne9/UZUNMbpZwLg/vwDv8YmIiLiCyp2WpnkGBtJ0TZc7oYfDApgnDnO88WP32HmZvsxOhERkZanYqeVMQzj2ENZKZ2g90Aw3ZifzvNneCIiIi1OxU4r1L9mvZ3VDUxSrmU59yLAs+aOeaDQL3GJiIj4goqdVmhASjQWA3YWVrKvuLLhg3r1h/RuUFmJ+el8/wYoIiLSglTstEKx4WHeR0es3HOEoSzDwHLeBADMxR9gljU8v0dERCTYqdhppU7pGAPAyj3FRz5owKmQ0gnKSjCXLPRTZCIiIi1LxU4rNbim2Fmzr7TBW9ChZpHBMZcAYH76PmZFhd/iExERaSkqdlqp1Dg7yTE2XG6TNUe4KwvAGHIGtE2CIifmEq27IyIiJx4VO62UYRjeoawVRxnKMqxWjPOvBMBc+D/M0iMXRiIiIsFIxU4rVjuUtXJvCe6jPAfLOG2kZ+5OyQHMT973U3QiIiItQ8VOK9YnKZIIq4WCMhfb8o88H8ewhGG54GoAzE/e17o7IiJyQlGx04rZwiwMTPGspnzUu7IATj4d0jKhogxz3pt+iE5ERKRlqNhp5Wrn7Xx3jGLHMAwsl04EwPziI8xd230em4iISEtQsdPKndIhBgPYml9ObknVUY81epyEMWgYmG7cb/8H8yjzfERERIKFip1WzhFppXeSZzXlpTsPHPN449LrwWaHTWvh+6W+Dk9ERKTZVOwIwzrHAfDNzqJjHmu0TcIYczEA7tnTMSvKfRqbiIhIc6nYEU7vHIsBbMw79lAWgHHuJZDQDvJzMd97w/cBioiINIOKHSEh0up9MGijhrLCw7H89lYAzM/mY27f5NP4REREmkPFjgAwLC0WgG8aUewAGCcN8iw2aLpxv/b/MF3H7hESEREJBBU7AsDpnWqHssoaNZQFYFz2O4iJgz2/aO0dEREJWip2BIC2UTbvUNayXY3s3YmNOzictehdzA1rfBafiIjI8VKxI15DO9cMZf3SuGIHwDh5KMavzgHTxP3KU5jFx76jS0RExJ9U7IjX6TXFzoa8MvaXNn4OjnH576B9R3Duxz3zOS02KCIiQUXFjnglRtnomdj4u7JqGeERWCb9CcKs8MO3mIv+56sQRUREmkzFjtQxvOaurC92NG04ykjLxLhiEgDm3P9i/rSqxWMTERE5Hip2pI5fpcdhMWDz/nJ2F1U06VzLyPMwzjjXM3/nP//CzN7joyhFREQaT8WO1OGIsDIwJRqAJduaPtnYuPJGyOwFZSW4/99DmEUFLR2iiIhIk6jYkXpGdWkDwBc7CnE3cbKxYbVhueUvkNgecrJwP/MgZlmpL8IUERFpFBU7Us+Q1BiibBZySlyszylr8vlGXDyWOx6E2Dawcxvu5x/BrKr0QaQiIiLHpmJH6gm3Wrxr7izeXnhc1zDad8Dyx6kQEQkbf/IUPJVNmwMkIiLSElTsSINqh7K+/uUAZVXu47qGkdYVy+1TwB4O637A/exDmOVN7ykSERFpDhU70qA+SZGkxNooc7n5+pfjXxXZ6HGSZ0irtofn6Qe0yrKIiPiVih1pkGEYnNPVAcBHW5zNu1a33lju+jtERcPWDbgfuwcza3fzgxQREWkEFTtyRGd2bYPV4llzZ3tBebOuZXTpjuXeadA2yXOX1mP3YP68umUCFREROQoVO3JEjggrp6Z6Jip/3MzeHQCjY2csk/8FXXt61uF5ZiruD9/BdB/fnCAREZHGULEjR3VOpgOAJduLKHc1vygx4hxY/vQwxumjwO3GnPtfzzweZ36zry0iItIQFTtyVP2So0iOsVFa5WbxtuO7Df1whs2Ocf0dGNf+3nOn1vofcT/4e9zLFuuJ6SIi0uKsgQ6gIYsWLWL+/Pk4nU7S0tKYOHEimZmZRzy+pKSEt956i++++47i4mLatWvHtddey8knn+zHqEOTxTAY3yOel1flsGBjAed2c2AxjGZf1zAMjOFnY3bthfs/j8Ou7ZjTn8L8djGWq2/BSEppgehFRESCsGdn6dKlzJw5kwkTJjBt2jTS0tJ45JFHKCxsuFfB5XLx8MMPk5uby1133cXTTz/NTTfdREJCgp8jD12ju7Yh0mphd1Elq7NKWvTaRkoqlslPYFx8DVht8PNq3H+7DfesVzBLDrToa4mISOsUdMXOggULGD16NKNGjSI1NZVJkyZht9tZvHhxg8d//vnnFBcXc88999CzZ0+SkpLo3bs36enp/g08hEXZwhjd1bPI4IKNLf9gT8NqxXLeBCwPPAu9B0K1C/PT93FPvgn3wv/p2VoiItIsQTWM5XK52LZtGxdeeKG3zWKx0LdvXzZt2tTgOatWraJbt2688sorrFy5kri4OIYNG8aFF16IxVK/lquqqqKqqsq7bRgGkZGR3q9bUu31Wvq6gTC+ZwIfbCxg1d4S9hRVktomvMVfw0hJxXLXQ7jXfo/7nRmwZwfmu69hLpyDMWosltG/xmgTX/+8EMpzMFOe/Ue59g/l2T+CIc9BVewUFRXhdrtxOBx12h0OB3v37m3wnH379pGbm8vw4cP561//SnZ2Ni+//DLV1dVceuml9Y6fO3cuc+bM8W536dKFadOm0a5duxZ9L4dKTk722bX9JSUFhnV18vXW/SzeXcG9PdN9+GLjMM8cQ+mShRTNfhXX7h2YH75D9cfvETV8NDHnXYK9z4B6vzihkOcTgfLsP8q1fyjP/hHIPAdVsXM8TNMkLi6Om266CYvFQkZGBvn5+cybN6/BYueiiy5i/Pjx3u3aP5i5ubm4XK4Wjc0wDJKTk8nOzg6Ju4zOTY/m6637eX/NXs7vGkWbCB//+PQehPm3gVhWL8e9cA5s30TpkkWULlkEyalYTj0DY/CvsKR0Cqk8B6tQ+3kOZsq1fyjP/uGrPFut1kZ3VARVsRMXF4fFYsHpdNZpdzqd9Xp7ajkcDqxWa50hq44dO+J0OnG5XFitdd+izWbDZrM1eC1f/bCbphkSv0h920fSNSGCrfnlzN+Qz9X9fdcb5mUYGANPI2zgaZg7NmN++RHm8i8gezfu99+E99+kulMGRaPG4E7vjtkxHaOB4UtpOaHy83wiUK79Q3n2j0DmOaj+KlitVjIyMli7dq23ze12s3btWrp3797gOT169CA7Oxv3IavwZmVlER8fX6/QkeYxDINL+7QF4IONBZRWVfv39dO7YbnmdiyPv4px3R/hpJMhLAx2baNw5gtUP3QH7ruvxf3yE7iXfo7p3O/X+EREJDgFXTUwfvx4nn/+eTIyMsjMzOTDDz+koqKCkSNHAvDcc8+RkJDAVVddBcA555zDRx99xKuvvsqYMWPIzs5m7ty5nHfeeQF8F6Hr1E4xpMbZ2V1UycJNTi6pKX78yYiKxhg2GoaN9jxB/YdvsW/4kfIfV8KBQk/Pz/IvMAES2mFk9ICMHhhdukGHNIyoaL/HLCIigRN0xc7QoUMpKipi9uzZOJ1O0tPTmTx5sncYKy8vr87E1MTERO677z5ee+017rnnHhISEjjvvPPq3NElLcdiGFzSpy3PLMvi/Q35jOsRT4Q1cB2ERkwcxhnn0u7y69i7ayfm1g2Y637AXPcD7NwG+bmY+bmw8mu8nacJiZ6ip2Nnz+f2HaBdMsS20V0ZIiIhyDA1UAl4Jigfekt6SzAMg5SUFLKyskJqPNjlNrl1/jb2FVfx2wHtmBCA3p1DHSnPZnkp7NiCuW0j5raN8MtWONrQVkQkJCZDQiKGoy04EsCRgOFIgDYJEN8WomNb7ZygUP15DkbKtX8oz/7hqzzbbLYTc4KynBisFoMr+yby9LIs3v15P2MyHcSEhwU6rHqMiCjo2Q+jZz9vm1lSDHt3Yu75Bfb+grlnJ+RmQ0EelJfB7u2wezuH/jrW+9WMiITIaM/nqGiIjMKI9HwmIsrzuWbbiKr5urY9IhJsds9q0VarepJERPxAxY4clzPS45j7cz6/FFbw7s/7uWZgUqBDahQjOga69cbo1rtOu1lVCXn7IDfbM7HZmQ/OfM/T2Gu3D9Q8sqS8zPNx6PlHeL1j/h/GagOb7bDPds/E6zDrwc8WS51tIyzM02ZYwDDAYhz82rDUbB+tjbr7GnENw2LhgMOBu+iA530dWqjVq9kO3XeUgu7wfXW2D993xA3fxBJAhmFQ6nDgdjpbtschiN5jMDAwKI134C5wYh77t1WOk4FBWfv2kNbwjUb+oGJHjkuYxeDqAYk8+sUe5m8sYHzPBBIiT9wfJ8Nmh5ROkNKp/t/KGqarCkpLoKwUymo/l3oeZ1FWAuWl3jZKSzzDaLXbZSVQVgYVdYskXFWejyYKxD/LJuAMwOu2Riagewl9T3n2DxPIT0jE8s8ZAYvhxP3rJAE3pGMMPRIj2ZhXxuyf8rh5SGivQmpYbRDn8Hwc2t6Ea5imCS6Xp8Cpqqz5XFV/u9oF1dWe54TVfObwz24TTBNMt+ezu+bzoW2Hbh9+fJ1zDmvDbPD4iPBwystKMQ9Z6qFu5WUe/oaPlIhjH1Nv31GufaQYGnvtIJuvYRgGdrudyspKzSXxIQMO5jnQwYQwAwhvm0jLzoptGhU7ctwMw+CaAe2479OdfLzFya97JtAxzh7osIKaYRie4SqbzTOHpzHn+DimxjIMg0RN5vQLwzBIUq59Tnn2j0P/7QiU1nlbibSYk9pHMahDNNUmTF+1L9DhiIiI1KNiR5pt4qAkwgxYubeElXuKAx2OiIhIHSp2pNlS48L5dc8EAF5ZtY+qavcxzhAREfEfFTvSIi7v2xZHRBh7D1Tx3vr8QIcjIiLipWJHWkSULYzrT/astTPrp/3sKaoMcEQiIiIeKnakxYxIj2NASjRVbpMXlmfh1t0NIiISBFTsSIsxDINbh7QnPMxgbU4Zn24tDHRIIiIiKnakZbWPsXN1f8+D2V79PofckkAuIyUiIqJiR3xgfI94urWNoKTKzVNL91Lt1nCWiIgEjoodaXFhFoM/DetAhNXCupwy/rdOT58REZHAUbEjPpESa+emwe0BeOunPDbklh3jDBEREd9QsSM+M6pLHGekx+E24V9f76Go3BXokEREpBVSsSM+YxgGtwxpT0qsjdxSF49/rfk7IiLifyp2xKeibGFMPiOVCKvBmn2lvPZDTqBDEhGRVkbFjvhcZ0c4fzw9BYD3NxSweJvW3xEREf9RsSN+MbRzHBP6tAXgueVZrM4qCXBEIiLSWqjYEb+5un8iw9NicbnhsS/3sC2/PNAhiYhIK6BiR/zGYhjccXoKfdtHUe5yM/XzXex0VgQ6LBERCXEqdsSvbGEW/npGR7omRFBYUc39n+1kZ6EKHhER8R0VO+J30fYwHjyzE13iwyksr2bKpyp4RETEd1TsSEDEhofx0OjOdIkPx1lezV8//oX1OaWBDktEREKQih0JmLiagqdHYgTFlW7+9vkuvt11INBhiYhIiFGxIwEVFx7G30d3ZnDHGCqrTf7x5R7e/ikPt6mVlkVEpGWo2JGAC7d6Ji2f182BCby1Jo8HF++mUM/SEhGRFqBiR4JCmMXg5iHJ/PH0FOxhBquzSrjzwx2sz9U8HhERaR4VOxJUzsxow+PnptExzs7+Mhf3fbKTOev26wGiIiJy3FTsSNBJj4/gX2PSGJ4WS7UJ/12dy58W7WDLfq24LCIiTadiR4JSlC2Mu4d14I+npxBjt7C9oIJ7PtrBjO9zKK6sDnR4IiJyAlGxI0HLMAzOzGjD87/O4Iy0ONwmvLc+nxvf38rsn/IorVLRIyIix6ZiR4KeI8LKn4Z3YMrIVDq1sVNS6eaNNXnc+N5W5qzdT1mVO9AhiohIELMGOgCRxjqlYwwDU6L5ZucB3v4pjz1Flfz3x1ze35DPdae5GJ5iJTzMCHSYIiISZFTsyAklzGJwRnocwzrH8tUvRbz9Ux5ZB6p49outzAgPY1SXOM7u6qCzIzzQoYqISJBQsSMnpDCLwcgubfhVWhxf7Chizs8F7CksZ96GAuZtKKBHYgRnd3UwPC2OSJtGa0VEWjMVO3JCC7MYjO7q4MqhPfnw+y18vKWAFbuL2ZhXzsa8bF5etY/TO8UyJDWGASnRRNnCAh2yiIj4mYodCQlhFoNTOsYwqEM0BWUuFm8r5JOtTvYeqGLx9iIWby/CajE4qX0UgzpE0yMxkoz4cGxh6vUREQl1KnYk5MRHWrm4T1su6p3Az7llfLvrAN/tLia7uIrVWSWszioBwGYxyEiIoEdiBD0TI+meGEm7aFuAoxcRkZamYkdClmEY9EmKok9SFBNPTmJ3USUrdhezLqeUjfvLOVBRzca8MjbmlTGPAgDaRlrpnhhJj8QIuidG0jHOTpvwMAxDd3mJiJyoVOxIq2AYBp3ahNOpTTgX92mLaZpkHajyFjsb88rY4axgf5mLZbsOsGzXAe+5EVaD9jF2kmNsJMfYDn4daycp2qqhMBGRIKdiR1olwzDoEGenQ5ydURltACh3udm6v5wNNcXP1vxy9pe6KHeZ/OKs4BdnRf3rAG2jrCRF24iPtOKItBIfEeb5OsLzERceRpTNQqTNQphFPUQiIv6mYkekRoTVQp/2UfRpH+Vtq6p2s6+kin0HqsgurmJfcSXZxQe/LneZ5JW6yCt1Neo1wsOMmsLHUwDVFkGRNV9H2cIO+drTHmG1YA8zCA/zfLaHWQi3ej7bwgzCDDTMJiJyFEFZ7CxatIj58+fjdDpJS0tj4sSJZGZmNnjskiVLeOGFF+q02Ww23njjDX+EKiHOFmYhNS6c1Lj6ixSapklhRTX7iqvYV1xFYbkLZ3k1BWUunOWej4Kyaoorq6msNgGoqDapqK6moLxln+sVZnjuSAszDKyWg1+HWTzbFsPAajm4XbsvzGJgrT3XYmA1DMIsDV/LYjGI21JKSXExlpoCy2KABQPDwPNB7WfDu80R2iyGZxsOPe9g4XakNovh+cLzyfOFhbpth8ZCzfmHXq82JguHNBzGOOwz3lgN7/kNHl+v/eDxh7/Uoe//UBaLQYm1mNyCckzTxGzgdSyHXtCkzjGHH3tUDcRw+Hv2auhFGvmiRuOi8SvDgMrwUnKLKjAb896a+3q+f4mgZBgG7siygD6fKuiKnaVLlzJz5kwmTZpEt27d+OCDD3jkkUd4+umnadOmTYPnREZG8swzz/g5UmntDMPwDlX1SIw86rFV1SZlLjdlVdWUVrkprXJTVvO5tKbt4Hbd4ypdJhXVbiqqTSqrPdtV7rr/MlebUF1t0ri/Rs2x38fXl4O2BzqAVmJboANoFRKjdzH9oq4Be/2gK3YWLFjA6NGjGTVqFACTJk3i+++/Z/HixVx44YUNnmMYBg6Hw39BijSRLczAFhZGXHjLLGroNk2qqj0fLtOk2m1S7YZq08Tl9my7arY9X5uegqh2n1lzvNs85Bxqjjt4vUO3TQwio6I4UFyCaZq4TU8ctZ9N8PYwmODtkaj9H3NDbe6ak8xDeibctV/XHltzfJ0287B2TGrrv4P7DvaIHN5W9/WPxKz31eH/+6/fbtbZNg9tbajtkIMPb7dYLLjd7preKjzdEIfkBBPcNNADdch1jtaT0Ng4Dr/Q8fRO+KHT5LhZDAtu0w8PEw7mJPhBlD2w5UZQFTsul4tt27bVKWosFgt9+/Zl06ZNRzyvvLycW2+9FdM06dKlC1deeSWdOnXyQ8QigWExDMKtBuF+/A02DIOUlBSysrIw/dHn34op1/6hPPvHoXkOlKAqdoqKinC73fV6aRwOB3v37m3wnA4dOnDLLbeQlpZGaWkp8+bN4/777+fJJ5+kbdu29Y6vqqqiqqrKu20YBpGRkd6vW5J3noEmj/qU8uwfyrP/KNf+oTz7RzDkOaiKnePRvXt3unfvXmf7zjvv5JNPPuGKK66od/zcuXOZM2eOd7tLly5MmzaNdu3a+SzG5ORkn11bDlKe/UN59h/l2j+UZ/8IZJ6DqtiJi4vDYrHgdDrrtDudzkbPybFarXTp0oXs7OwG91900UWMHz/eu11baebm5uJyNe724cYyDIPk5GSys7PVRepDyrN/KM/+o1z7h/LsH77Ks9VqbXRHRVAVO1arlYyMDNauXcuQIUMAcLvdrF27ljFjxjTqGm63m507dzJw4MAG99tsNmy2hp9/5KsfdtM09YvkB8qzfyjP/qNc+4fy7B+BzHNQFTsA48eP5/nnnycjI4PMzEw+/PBDKioqGDlyJADPPfccCQkJXHXVVQDMmTOHbt26kZycTElJCfPmzSM3N5fRo0cH8F2IiIhIsAi6Ymfo0KEUFRUxe/ZsnE4n6enpTJ482TuMlZeXV2eSU3FxMS+99BJOp5Po6GgyMjJ4+OGHSU1NDdA7EBERkWBimOq7Azxzdg69S6sl6LZG/1Ce/UN59h/l2j+UZ//wVZ5tNluj5+zocc0iIiIS0lTsiIiISEhTsSMiIiIhTcWOiIiIhDQVOyIiIhLSVOyIiIhISFOxIyIiIiEt6BYVDBSr1Xep8OW15SDl2T+UZ/9Rrv1DefaPls5zU66nRQVFREQkpGkYy4fKysr485//TFlZWaBDCWnKs38oz/6jXPuH8uwfwZBnFTs+ZJom27dv1zLkPqY8+4fy7D/KtX8oz/4RDHlWsSMiIiIhTcWOiIiIhDQVOz5ks9mYMGECNpst0KGENOXZP5Rn/1Gu/UN59o9gyLPuxhIREZGQpp4dERERCWkqdkRERCSkqdgRERGRkKZiR0REREKaHgjiI4sWLWL+/Pk4nU7S0tKYOHEimZmZgQ7rhDF37ly+++479uzZg91up3v37vzmN7+hQ4cO3mMqKyuZOXMmS5cupaqqiv79+/O73/0Oh8PhPSYvL4///Oc/rFu3joiICEaMGMFVV11FWFhYAN5V8Hvvvfd48803GTt2LNdddx2gPLek/Px8Xn/9dVavXk1FRQXJycnceuutdO3aFfAsvjZ79mw+++wzSkpK6NmzJ7/73e9ISUnxXqO4uJjp06ezatUqDMPg1FNP5frrryciIiJQbyuouN1uZs+ezVdffYXT6SQhIYERI0ZwySWXYBgGoDwfj59//pl58+axfft2CgoKuPvuuxkyZIh3f0vl9JdffuGVV15h69atxMXFMWbMGC644IJmx6+eHR9YunQpM2fOZMKECUybNo20tDQeeeQRCgsLAx3aCePnn3/m3HPP5ZFHHuH++++nurqahx9+mPLycu8xr732GqtWreKuu+7iwQcfpKCggCeeeMK73+1289hjj+FyuXj44Ye57bbbWLJkCbNmzQrEWwp6W7Zs4ZNPPiEtLa1Ou/LcMoqLi5kyZQpWq5XJkyfz1FNPcc011xAdHe095v3332fhwoVMmjSJRx99lPDwcB555BEqKyu9xzz77LPs2rWL+++/n7/85S+sX7+el156KRBvKSi99957fPLJJ9xwww089dRTXH311cybN4+FCxd6j1Gem66iooL09HRuuOGGBve3RE5LS0t5+OGHSUxM5B//+Ae/+c1veOedd/j000+b/wZMaXF//etfzZdfftm7XV1dbd54443m3LlzAxfUCa6wsNC89NJLzXXr1pmmaZolJSXmFVdcYS5btsx7zO7du81LL73U3Lhxo2mapvn999+bl112mVlQUOA95qOPPjKvueYas6qqyq/xB7uysjLzD3/4g/njjz+aDzzwgDljxgzTNJXnlvT666+bU6ZMOeJ+t9ttTpo0yXz//fe9bSUlJeZVV11lfv3116ZpmuauXbvMSy+91NyyZYv3mB9++MG87LLLzP379/su+BPIY489Zr7wwgt12h5//HHzmWeeMU1TeW4Jl156qbl8+XLvdkvl9KOPPjKvu+66Ov9uvP766+Yf//jHZsesnp0W5nK52LZtG3379vW2WSwW+vbty6ZNmwIY2YmttLQUgJiYGAC2bdtGdXV1nTx37NiRxMREb543bdpE586d6wy3DBgwgLKyMnbt2uW/4E8AL7/8MgMHDqRfv3512pXnlrNy5UoyMjJ48skn+d3vfse9995b53+sOTk5OJ3OOt+DqKgoMjMz6+Q6OjraO+wF0LdvXwzDYMuWLf57M0Gse/furF27lr179wKwY8cONm7cyMCBAwHl2RdaKqebNm2iV69eWK0HZ9j079+fvXv3Ulxc3KwYNWenhRUVFeF2u+v8ww/gcDi8v3zSNG63m1dffZUePXrQuXNnAJxOJ1artc4QAECbNm1wOp3eYw7/PrRp08a7Tzy++eYbtm/fzmOPPVZvn/LccnJycvjkk08YN24cF110EVu3bmXGjBlYrVZGjhzpzVVt7modnuu4uLg6+8PCwoiJiVGua1x44YWUlZVx5513YrFYcLvdXHHFFfzqV78CUJ59oKVy6nQ6SUpKqnNM7b8tTqfT+5/d46FiR4LeK6+8wq5du3jooYcCHUrIycvL49VXX+X+++/HbrcHOpyQ5na76dq1K1dddRUAXbp0YefOnXzyySeMHDkysMGFkGXLlvH111/zhz/8gU6dOrFjxw5effVV4uPjledWTMVOC4uLi8NisdSr/hv6368c2yuvvML333/Pgw8+SNu2bb3tDocDl8tFSUlJnV6HwsJCb54dDke9LufaSeL6Xnhs27aNwsJC/vznP3vb3G4369evZ9GiRdx3333KcwuJj48nNTW1TltqairLly8HDuaqsLCQ+Ph47zGFhYWkp6d7jykqKqpzjerqaoqLi5XrGq+//joXXHABw4YNA6Bz587k5uby3nvvMXLkSOXZB1oqpw6Ho8G/nYe+xvHSnJ0WZrVaycjIYO3atd42t9vN2rVr6d69ewAjO7GYpskrr7zCd999x9/+9rd6XZsZGRmEhYXx008/edv27t1LXl6eN8/du3dn586dde6CW7NmDZGRkfX+6LRWffv25V//+hf//Oc/vR9du3Zl+PDh3q+V55bRo0ePekPZe/fupV27dgAkJSXhcDjq5Lq0tJQtW7bUyXVJSQnbtm3zHrN27VpM09TSFjUqKiqwWOr+abNYLJg1j4FUnlteS+W0e/furF+/HpfL5T1mzZo1dOjQoVlDWKCeHZ8YP348zz//PBkZGWRmZvLhhx9SUVGhLtQmeOWVV/j666+59957iYyM9Fb3UVFR2O12oqKiOPPMM5k5cyYxMTFERUUxffp0unfv7v3l6t+/P6mpqTz33HNcffXVOJ1O3n77bc4991w95bhGZGSkdx5UrfDwcGJjY73tynPLGDduHFOmTOHdd99l6NChbNmyhc8++4wbb7wRAMMwGDt2LO+++y4pKSkkJSXx9ttvEx8fz+DBgwFPT9CAAQN46aWXmDRpEi6Xi+nTpzN06FASEhIC+faCxqBBg3j33XdJTEwkNTWVHTt2sGDBAkaNGgUoz8ervLyc7Oxs73ZOTg47duwgJiaGxMTEFsnp8OHDeeedd3jxxRe54IIL2LVrFwsXLuTaa69tdvx66rmPLFq0iHnz5uF0OklPT+f666+nW7dugQ7rhHHZZZc12H7rrbd6i8baxe6++eYbXC5Xg4vd5ebm8vLLL7Nu3TrCw8MZMWIEV199tRa7O4qpU6eSnp5eb1FB5bn5Vq1axZtvvkl2djZJSUmMGzeOs846y7vfrFmY7dNPP6W0tJSePXtyww031FlMs7i4mFdeeaXOwmwTJ05stYvdHa6srIxZs2bx3XffUVhYSEJCAsOGDWPChAneu3yU56Zbt24dDz74YL32ESNGcNttt7VYTg9dVDA2NpYxY8Zw4YUXNjt+FTsiIiIS0jRnR0REREKaih0REREJaSp2REREJKSp2BEREZGQpmJHREREQpqKHREREQlpKnZEREQkpKnYEZFWa8mSJVx22WVs3bo10KGIiA/pcREi4jNLlizhhRdeOOL+hx9+OKSeGbdixQqeeOIJXn31VSIiIpgxYwa//PILU6dODXRoIq2aih0R8bnLLrus3sNcAZKTkwMQje9s3ryZzp07e5e/37RpEyeddFKAoxIRFTsi4nMDBw6ka9eugQ7D57Zu3ep9Bl5lZSU7duzgoosuCnBUIqJiR0QCLicnh9tvv53f/OY3WCwWPvzwQwoLC8nMzOSGG26o92T2tWvXMnv2bLZv305YWBi9e/fmqquuIjU1tc5x+fn5zJo1i9WrV3PgwAHi4+MZMGAA119/vfehkABVVVW89tprfPnll1RWVtKvXz9uuukm4uLijhl7UVGR9+utW7dyyimnUFRUxNatW6murqZ9+/YUFRURHh5OeHh4MzMlIsdDDwIVEZ+pnbMzZcoU0tLS6uwzDIPY2FjgYLHTuXNnysrKOOecc6iqquLDDz/EYrHwr3/9y/uU9TVr1vDYY4+RlJTE6NGjqaysZOHChbjdbqZNm+YdLsvPz+evf/0rpaWljB49mo4dO5Kfn8+3337Lww8/THR0tDe+Ll26EB0dzZAhQ8jJyeHDDz/k1FNP5c477zzme7zssssalYsJEyY0+lgRaVnq2RERn/v73/9er81ms/HGG2/UacvOzubZZ58lISEBgAEDBjB58mTef/99rr32WgBef/11YmJieOSRR4iJiQFg8ODB3HvvvcyePZvbb78dgDfffBOn08mjjz5aZwjt8ssv5/D/48XExHD//fdjGAYApmmycOFCSktLiYqKOup7u//++wH49ttvWbFiBb///e8BeOONN4iPj2fs2LEAtG/fvhGZEhFfULEjIj53ww03kJKSUqfNYqm/8sXgwYO9hQ5AZmYm3bp144cffuDaa6+loKCAHTt2cP7553sLHYC0tDT69evHDz/8AIDb7WbFihUMGjSowblCtUVNrbPOOqtOW69evfjggw/Izc2t1yN1uH79+gHw8ccfc9JJJ9GvXz/cbjfZ2dmcd9553v0iEjgqdkTE5zIzMxs1Qfnwgqi2bdmyZQDk5uYC0KFDh3rHdezYkR9//JHy8nLKy8spKyurN9fnSBITE+tsR0dHA1BSUnLU84qLi3G73QD8/PPPXHzxxRQVFbFz507v6xcVFWG32713aImI/6nYEZFWr6FeJqDecNfh/vznP3sLMICZM2cyc+ZM7/Zf/vIXAEaMGMFtt93WApGKyPFQsSMiQSMrK6vBtnbt2gF4P+/du7fecXv37iU2NpaIiAjsdjuRkZHs3LnTp/H+/ve/p7KykhUrVrBs2TL+8Ic/APD2228TGxvLuHHjAOoMzYmI/+lxESISNFasWEF+fr53e8uWLWzevJkBAwYAEB8fT3p6Ol988UWdIaadO3fy448/MnDgQMDTUzN48GBWrVrV4KMgWuom1J49e9KvXz/Kysro3r07/fr1o1+/fuTl5TFo0CDv9uG3xIuIf6lnR0R87ocffmDPnj312nv06FHnLqXk5GSmTJlS59bz2NhYLrjgAu8xv/nNb3jssce4//77GTVqFJWVlSxatIioqKg6t3ZfddVVrFmzhqlTpzJ69GhSU1MpKCjg22+/5aGHHvLOy2kJGzdu5KyzzgJg3759OJ1OevTo0WLXF5HmUbEjIj43e/bsBttvvfXWOsXOGWecgcVi4YMPPqCoqIjMzEwmTpxIfHy895h+/foxefJkZs+ezezZs72LCl599dV1HkmRkJDAo48+yttvv83XX39NWVkZCQkJDBgwoEUX93M6nezbt89b3GzatInIyEg6derUYq8hIs2jRQVFJOAOXUH5/PPPD3Q4IhJiNGdHREREQpqKHREREQlpKnZEREQkpGnOjoiIiIQ09eyIiIhISFOxIyIiIiFNxY6IiIiENBU7IiIiEtJU7IiIiEhIU7EjIiIiIU3FjoiIiIQ0FTsiIiIS0lTsiIiISEj7//VWKztUtbBmAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000    # training epochs\n",
        "temperature = 0.1\n",
        "\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "\n",
        "# count occurrences in each row, and make new columns\n",
        "def count_occurrences(row,element):\n",
        "    count = 0\n",
        "    for value in row:\n",
        "        if value == element:\n",
        "            count += 1\n",
        "    return count/20\n",
        "\n",
        "data_df['0occurrences'] = results_subset.apply(count_occurrences, args=(0,), axis=1)\n",
        "data_df['1occurrences'] = results_subset.apply(count_occurrences, args=(1,), axis=1)\n",
        "data_df['2occurrences'] = results_subset.apply(count_occurrences, args=(2,), axis=1)\n",
        "\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition3   # choose lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Get the Lab data (X) and probability distribution (Y)\n",
        "train_set = np.asarray(train_set).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6:9]    # probability distribution columns\n",
        "Y_train = Y_train.reshape(-1,3)\n",
        "\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6:9]\n",
        "Y_val = Y_val.reshape(-1,3)\n",
        "#print(X_train)  # example: output to see the possibility distribution for validation set\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(x)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "6edKHaXaYASc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTIeFXfKYBMQ"
      },
      "source": [
        "### 4.2.4 4th lighting condition as test ###"
      ],
      "id": "hTIeFXfKYBMQ"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "raahBxPnYI1Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "166e7128-e02b-4259-f48c-8bbe2084e2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 9s 14ms/step - loss: 1.0304 - val_loss: 0.9865\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.9568 - val_loss: 0.9150\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.8983 - val_loss: 0.8498\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.8445 - val_loss: 0.7859\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.7920 - val_loss: 0.7182\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.7303 - val_loss: 0.5901\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.6118 - val_loss: 0.5065\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5678 - val_loss: 0.4657\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5108 - val_loss: 0.4322\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4848 - val_loss: 0.4203\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4815 - val_loss: 0.4194\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4758 - val_loss: 0.4011\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4700 - val_loss: 0.4002\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4696 - val_loss: 0.4319\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4713 - val_loss: 0.4078\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4681 - val_loss: 0.3955\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4660 - val_loss: 0.3945\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4654 - val_loss: 0.4000\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4688 - val_loss: 0.3964\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4654 - val_loss: 0.3906\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4628 - val_loss: 0.3888\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4592 - val_loss: 0.3912\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4587 - val_loss: 0.3993\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4596 - val_loss: 0.3975\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4575 - val_loss: 0.3924\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4582 - val_loss: 0.3992\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4599 - val_loss: 0.3916\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4598 - val_loss: 0.3937\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4571 - val_loss: 0.3979\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4591 - val_loss: 0.4009\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4569 - val_loss: 0.3975\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4550 - val_loss: 0.3913\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4542 - val_loss: 0.3978\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4559 - val_loss: 0.3910\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4550 - val_loss: 0.3929\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4549 - val_loss: 0.3863\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4543 - val_loss: 0.4043\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4567 - val_loss: 0.3918\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4551 - val_loss: 0.3883\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4592 - val_loss: 0.3875\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4538 - val_loss: 0.3947\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4526 - val_loss: 0.3858\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4505 - val_loss: 0.4099\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4531 - val_loss: 0.4027\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4560 - val_loss: 0.4008\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4550 - val_loss: 0.3909\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4542 - val_loss: 0.3892\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4517 - val_loss: 0.3950\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4512 - val_loss: 0.3958\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4509 - val_loss: 0.3892\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4507 - val_loss: 0.3914\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4487 - val_loss: 0.4018\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4499 - val_loss: 0.4001\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4517 - val_loss: 0.3907\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4473 - val_loss: 0.3898\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4474 - val_loss: 0.3865\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4504 - val_loss: 0.3836\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4471 - val_loss: 0.3879\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4462 - val_loss: 0.3943\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4460 - val_loss: 0.4009\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4536 - val_loss: 0.3844\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4483 - val_loss: 0.3875\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4470 - val_loss: 0.3897\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4448 - val_loss: 0.3877\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4448 - val_loss: 0.3917\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4470 - val_loss: 0.3916\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4459 - val_loss: 0.3861\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4454 - val_loss: 0.3878\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4454 - val_loss: 0.3851\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4466 - val_loss: 0.3990\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4448 - val_loss: 0.3865\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4486 - val_loss: 0.3985\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4451 - val_loss: 0.3843\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4418 - val_loss: 0.3902\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4423 - val_loss: 0.3839\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4420 - val_loss: 0.3834\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4415 - val_loss: 0.3842\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4416 - val_loss: 0.3855\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4420 - val_loss: 0.3853\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4425 - val_loss: 0.3882\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4407 - val_loss: 0.3837\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4413 - val_loss: 0.3881\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4413 - val_loss: 0.3907\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4451 - val_loss: 0.3828\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4458 - val_loss: 0.3885\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4433 - val_loss: 0.3897\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4405 - val_loss: 0.3907\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4406 - val_loss: 0.3907\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4384 - val_loss: 0.3899\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4416 - val_loss: 0.3844\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4408 - val_loss: 0.3865\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4408 - val_loss: 0.3938\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4389 - val_loss: 0.3882\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4397 - val_loss: 0.3867\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4398 - val_loss: 0.3860\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4391 - val_loss: 0.3799\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4384 - val_loss: 0.3855\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4384 - val_loss: 0.3830\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4395 - val_loss: 0.3819\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4376 - val_loss: 0.3825\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4387 - val_loss: 0.3830\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4386 - val_loss: 0.3816\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4381 - val_loss: 0.3894\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4376 - val_loss: 0.3935\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4364 - val_loss: 0.3822\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4390 - val_loss: 0.3844\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4389 - val_loss: 0.3865\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4389 - val_loss: 0.3907\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4372 - val_loss: 0.4045\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4379 - val_loss: 0.3858\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4384 - val_loss: 0.3828\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4393 - val_loss: 0.3935\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4387 - val_loss: 0.3868\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4376 - val_loss: 0.3898\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4380 - val_loss: 0.3884\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4368 - val_loss: 0.3890\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4374 - val_loss: 0.3853\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4348 - val_loss: 0.3870\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4346 - val_loss: 0.3949\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4434 - val_loss: 0.3904\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4362 - val_loss: 0.3903\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4381 - val_loss: 0.3836\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4364 - val_loss: 0.3843\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4347 - val_loss: 0.3882\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4374 - val_loss: 0.3983\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4424 - val_loss: 0.3902\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4396 - val_loss: 0.3885\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4362 - val_loss: 0.3871\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4361 - val_loss: 0.3904\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4359 - val_loss: 0.3995\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4357 - val_loss: 0.3917\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4371 - val_loss: 0.3851\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4335 - val_loss: 0.3897\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4342 - val_loss: 0.3962\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4363 - val_loss: 0.3885\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4347 - val_loss: 0.3878\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4359 - val_loss: 0.3991\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4353 - val_loss: 0.3885\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4350 - val_loss: 0.3895\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4370 - val_loss: 0.3878\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4348 - val_loss: 0.3850\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4334 - val_loss: 0.3852\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4342 - val_loss: 0.3813\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4351 - val_loss: 0.3860\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4347 - val_loss: 0.3813\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4336 - val_loss: 0.3859\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4344 - val_loss: 0.3899\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4337 - val_loss: 0.3865\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4334 - val_loss: 0.3862\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4334 - val_loss: 0.3881\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4337 - val_loss: 0.3889\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4322 - val_loss: 0.3866\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4335 - val_loss: 0.3892\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4335 - val_loss: 0.3866\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4334 - val_loss: 0.3916\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4342 - val_loss: 0.4012\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4341 - val_loss: 0.3833\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4350 - val_loss: 0.3865\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4336 - val_loss: 0.4075\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4349 - val_loss: 0.3953\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4330 - val_loss: 0.3903\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4332 - val_loss: 0.3838\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4329 - val_loss: 0.3909\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4321 - val_loss: 0.3856\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4307 - val_loss: 0.3897\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4315 - val_loss: 0.3867\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4325 - val_loss: 0.3898\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4319 - val_loss: 0.3870\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4327 - val_loss: 0.3928\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4318 - val_loss: 0.3935\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4311 - val_loss: 0.3931\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4314 - val_loss: 0.3998\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4346 - val_loss: 0.4054\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4323 - val_loss: 0.3860\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4311 - val_loss: 0.3932\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4326 - val_loss: 0.3981\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4311 - val_loss: 0.3972\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4305 - val_loss: 0.3934\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4304 - val_loss: 0.3939\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4301 - val_loss: 0.3933\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4328 - val_loss: 0.3916\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4314 - val_loss: 0.3944\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4308 - val_loss: 0.3896\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4302 - val_loss: 0.3891\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4320 - val_loss: 0.3875\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4318 - val_loss: 0.3969\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4320 - val_loss: 0.3892\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4341 - val_loss: 0.3956\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4334 - val_loss: 0.3916\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4363 - val_loss: 0.3810\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4315 - val_loss: 0.3909\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4316 - val_loss: 0.3891\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4303 - val_loss: 0.3935\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4384 - val_loss: 0.3868\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4337 - val_loss: 0.4032\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4368 - val_loss: 0.3840\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4312 - val_loss: 0.3954\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4312 - val_loss: 0.3887\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4326 - val_loss: 0.3905\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4310 - val_loss: 0.3974\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4312 - val_loss: 0.3986\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4304 - val_loss: 0.3999\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4321 - val_loss: 0.3937\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4315 - val_loss: 0.3866\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4306 - val_loss: 0.3869\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4321 - val_loss: 0.3960\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4292 - val_loss: 0.3917\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4287 - val_loss: 0.3858\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4290 - val_loss: 0.3924\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4294 - val_loss: 0.3972\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4296 - val_loss: 0.4026\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4301 - val_loss: 0.3918\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4314 - val_loss: 0.3962\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4296 - val_loss: 0.3938\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4290 - val_loss: 0.3906\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4303 - val_loss: 0.3936\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4297 - val_loss: 0.3949\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4298 - val_loss: 0.3887\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4325 - val_loss: 0.4114\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4339 - val_loss: 0.3886\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4325 - val_loss: 0.3911\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4294 - val_loss: 0.3898\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4297 - val_loss: 0.3957\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4292 - val_loss: 0.3866\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4287 - val_loss: 0.3879\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4301 - val_loss: 0.3931\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4314 - val_loss: 0.3927\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4319 - val_loss: 0.3911\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4297 - val_loss: 0.3920\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4302 - val_loss: 0.3947\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4291 - val_loss: 0.3881\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4296 - val_loss: 0.3929\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4296 - val_loss: 0.3918\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4299 - val_loss: 0.3893\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4302 - val_loss: 0.3987\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4279 - val_loss: 0.3934\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4296 - val_loss: 0.3913\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4291 - val_loss: 0.3985\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4305 - val_loss: 0.3927\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4276 - val_loss: 0.3930\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4294 - val_loss: 0.3911\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4284 - val_loss: 0.3934\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4289 - val_loss: 0.3978\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4290 - val_loss: 0.3936\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4322 - val_loss: 0.3973\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4306 - val_loss: 0.3953\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4274 - val_loss: 0.3900\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4268 - val_loss: 0.3962\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4294 - val_loss: 0.3870\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4276 - val_loss: 0.3948\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4283 - val_loss: 0.3991\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4291 - val_loss: 0.3998\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4297 - val_loss: 0.3883\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4317 - val_loss: 0.3897\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4286 - val_loss: 0.3962\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4287 - val_loss: 0.3918\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4272 - val_loss: 0.3957\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4290 - val_loss: 0.3985\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4294 - val_loss: 0.3901\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4286 - val_loss: 0.3963\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4304 - val_loss: 0.3942\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4275 - val_loss: 0.3976\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4284 - val_loss: 0.3985\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4298 - val_loss: 0.3964\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4271 - val_loss: 0.3944\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4277 - val_loss: 0.3897\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4287 - val_loss: 0.3915\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4283 - val_loss: 0.4067\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4285 - val_loss: 0.3959\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4303 - val_loss: 0.3948\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4281 - val_loss: 0.3927\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4275 - val_loss: 0.3943\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4284 - val_loss: 0.3937\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4280 - val_loss: 0.3943\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4279 - val_loss: 0.3948\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4273 - val_loss: 0.3986\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4287 - val_loss: 0.4024\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4282 - val_loss: 0.3995\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4278 - val_loss: 0.3950\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4278 - val_loss: 0.3936\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4272 - val_loss: 0.3929\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4278 - val_loss: 0.3912\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4325 - val_loss: 0.3958\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4258 - val_loss: 0.3914\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4270 - val_loss: 0.3946\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4321 - val_loss: 0.3973\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4268 - val_loss: 0.3977\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4283 - val_loss: 0.3986\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4277 - val_loss: 0.4058\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4279 - val_loss: 0.3980\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4260 - val_loss: 0.4006\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4288 - val_loss: 0.3947\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4277 - val_loss: 0.3955\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4259 - val_loss: 0.3956\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4280 - val_loss: 0.3951\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4270 - val_loss: 0.3959\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4271 - val_loss: 0.4004\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4254 - val_loss: 0.3981\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4258 - val_loss: 0.3974\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4265 - val_loss: 0.4075\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4287 - val_loss: 0.3942\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4257 - val_loss: 0.4005\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4266 - val_loss: 0.3983\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4259 - val_loss: 0.3988\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4265 - val_loss: 0.3997\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4274 - val_loss: 0.4027\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4275 - val_loss: 0.3982\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4254 - val_loss: 0.4059\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4274 - val_loss: 0.3959\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4273 - val_loss: 0.3940\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4263 - val_loss: 0.3967\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4270 - val_loss: 0.3963\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4281 - val_loss: 0.3902\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4284 - val_loss: 0.4028\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4284 - val_loss: 0.3947\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4273 - val_loss: 0.4012\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4286 - val_loss: 0.4083\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4274 - val_loss: 0.4013\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4270 - val_loss: 0.4001\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4278 - val_loss: 0.4109\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4266 - val_loss: 0.4091\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4269 - val_loss: 0.4084\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4270 - val_loss: 0.3986\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4262 - val_loss: 0.3997\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4259 - val_loss: 0.4003\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4253 - val_loss: 0.3958\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4260 - val_loss: 0.4055\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4256 - val_loss: 0.4029\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4255 - val_loss: 0.4011\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4253 - val_loss: 0.4086\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4257 - val_loss: 0.4006\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4291 - val_loss: 0.4015\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4276 - val_loss: 0.4026\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4269 - val_loss: 0.4007\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4270 - val_loss: 0.4028\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4307 - val_loss: 0.3985\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4291 - val_loss: 0.3975\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4278 - val_loss: 0.4010\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4254 - val_loss: 0.4014\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4268 - val_loss: 0.4031\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4268 - val_loss: 0.3967\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4258 - val_loss: 0.4039\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4258 - val_loss: 0.4013\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4262 - val_loss: 0.3970\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4246 - val_loss: 0.3935\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4259 - val_loss: 0.4060\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4250 - val_loss: 0.3912\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4283 - val_loss: 0.4026\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4262 - val_loss: 0.4046\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4241 - val_loss: 0.3997\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4245 - val_loss: 0.4053\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4264 - val_loss: 0.3996\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4267 - val_loss: 0.3979\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4243 - val_loss: 0.3993\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4247 - val_loss: 0.3988\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4253 - val_loss: 0.4064\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4247 - val_loss: 0.4059\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4259 - val_loss: 0.4062\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4240 - val_loss: 0.3997\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4268 - val_loss: 0.4052\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4265 - val_loss: 0.3995\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4243 - val_loss: 0.3985\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4257 - val_loss: 0.3981\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4256 - val_loss: 0.3986\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4269 - val_loss: 0.4029\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4256 - val_loss: 0.4112\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4272 - val_loss: 0.4035\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4261 - val_loss: 0.4027\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4254 - val_loss: 0.3973\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4246 - val_loss: 0.4043\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4244 - val_loss: 0.3973\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4248 - val_loss: 0.4021\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4248 - val_loss: 0.4083\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4251 - val_loss: 0.4016\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4263 - val_loss: 0.4108\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4235 - val_loss: 0.4015\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4247 - val_loss: 0.4062\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4266 - val_loss: 0.4026\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4257 - val_loss: 0.4005\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4241 - val_loss: 0.3995\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4234 - val_loss: 0.4024\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4237 - val_loss: 0.4007\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4230 - val_loss: 0.4017\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4306 - val_loss: 0.3951\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4317 - val_loss: 0.3911\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4318 - val_loss: 0.3920\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4267 - val_loss: 0.3975\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4277 - val_loss: 0.3966\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4276 - val_loss: 0.3946\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4283 - val_loss: 0.3974\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4247 - val_loss: 0.3974\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4256 - val_loss: 0.3970\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4262 - val_loss: 0.3965\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4256 - val_loss: 0.3889\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4287 - val_loss: 0.3954\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4263 - val_loss: 0.3950\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4270 - val_loss: 0.4096\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4285 - val_loss: 0.3949\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4245 - val_loss: 0.3937\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4248 - val_loss: 0.4002\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4234 - val_loss: 0.3973\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4234 - val_loss: 0.4009\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4261 - val_loss: 0.3984\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4271 - val_loss: 0.4080\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4269 - val_loss: 0.3966\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4253 - val_loss: 0.3926\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4256 - val_loss: 0.3971\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4246 - val_loss: 0.3964\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4250 - val_loss: 0.4002\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4249 - val_loss: 0.3941\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4252 - val_loss: 0.3965\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4245 - val_loss: 0.3939\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4235 - val_loss: 0.3965\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4244 - val_loss: 0.4009\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4226 - val_loss: 0.3948\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4244 - val_loss: 0.4000\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4240 - val_loss: 0.4079\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4246 - val_loss: 0.3882\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4244 - val_loss: 0.3919\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4254 - val_loss: 0.3965\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4241 - val_loss: 0.3914\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4247 - val_loss: 0.4026\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4263 - val_loss: 0.3962\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4260 - val_loss: 0.3964\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4241 - val_loss: 0.3942\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4237 - val_loss: 0.3974\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4240 - val_loss: 0.3993\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4246 - val_loss: 0.3975\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4264 - val_loss: 0.3987\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4235 - val_loss: 0.3919\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4235 - val_loss: 0.3965\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4250 - val_loss: 0.3968\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4230 - val_loss: 0.3950\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4232 - val_loss: 0.3976\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4262 - val_loss: 0.4082\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4239 - val_loss: 0.3918\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4245 - val_loss: 0.3995\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4241 - val_loss: 0.3908\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4236 - val_loss: 0.3907\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4231 - val_loss: 0.3930\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4234 - val_loss: 0.3938\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4239 - val_loss: 0.3957\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4229 - val_loss: 0.4035\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4236 - val_loss: 0.4014\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4234 - val_loss: 0.4013\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4242 - val_loss: 0.4025\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4249 - val_loss: 0.3994\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4249 - val_loss: 0.3986\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4240 - val_loss: 0.3940\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4271 - val_loss: 0.3968\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4237 - val_loss: 0.3939\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4229 - val_loss: 0.3991\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4228 - val_loss: 0.3960\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4237 - val_loss: 0.3983\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4244 - val_loss: 0.3970\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4243 - val_loss: 0.3988\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4227 - val_loss: 0.3945\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4228 - val_loss: 0.3961\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4244 - val_loss: 0.3914\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4245 - val_loss: 0.3985\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4231 - val_loss: 0.3993\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4225 - val_loss: 0.3939\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4223 - val_loss: 0.3954\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4224 - val_loss: 0.3952\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4227 - val_loss: 0.4094\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4277 - val_loss: 0.3913\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4242 - val_loss: 0.3942\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4245 - val_loss: 0.3989\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4244 - val_loss: 0.4031\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4225 - val_loss: 0.3995\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4234 - val_loss: 0.3968\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4242 - val_loss: 0.3986\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4252 - val_loss: 0.3993\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4240 - val_loss: 0.3958\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4237 - val_loss: 0.3994\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4251 - val_loss: 0.3989\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4225 - val_loss: 0.3956\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4226 - val_loss: 0.3977\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4237 - val_loss: 0.4021\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4231 - val_loss: 0.4021\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4257 - val_loss: 0.3979\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4245 - val_loss: 0.4007\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4232 - val_loss: 0.3911\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4241 - val_loss: 0.4054\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4219 - val_loss: 0.3970\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4225 - val_loss: 0.4006\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4252 - val_loss: 0.4020\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4248 - val_loss: 0.3987\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4230 - val_loss: 0.4000\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4227 - val_loss: 0.4013\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4241 - val_loss: 0.4071\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4222 - val_loss: 0.3987\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4250 - val_loss: 0.3998\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4232 - val_loss: 0.3997\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4225 - val_loss: 0.3985\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4223 - val_loss: 0.4000\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4228 - val_loss: 0.3966\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4224 - val_loss: 0.4054\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4227 - val_loss: 0.4069\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4237 - val_loss: 0.3925\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4236 - val_loss: 0.4001\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4222 - val_loss: 0.4006\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4239 - val_loss: 0.4015\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4231 - val_loss: 0.4031\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4224 - val_loss: 0.4071\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4231 - val_loss: 0.4034\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4207 - val_loss: 0.4011\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4229 - val_loss: 0.3974\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4225 - val_loss: 0.4054\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4303 - val_loss: 0.4039\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4224 - val_loss: 0.4044\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4232 - val_loss: 0.4103\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4228 - val_loss: 0.3999\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4217 - val_loss: 0.4054\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4234 - val_loss: 0.3997\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4216 - val_loss: 0.3964\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4216 - val_loss: 0.4000\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4214 - val_loss: 0.3985\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4228 - val_loss: 0.4000\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4226 - val_loss: 0.4010\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4257 - val_loss: 0.4049\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4226 - val_loss: 0.4026\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4232 - val_loss: 0.4037\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4237 - val_loss: 0.3989\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4251 - val_loss: 0.4051\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4236 - val_loss: 0.4101\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4241 - val_loss: 0.3988\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4247 - val_loss: 0.4027\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4245 - val_loss: 0.4019\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4363 - val_loss: 0.3971\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4261 - val_loss: 0.4111\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4247 - val_loss: 0.3968\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4230 - val_loss: 0.4022\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4225 - val_loss: 0.4000\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4215 - val_loss: 0.4119\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4217 - val_loss: 0.4030\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4222 - val_loss: 0.4047\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4243 - val_loss: 0.4053\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4235 - val_loss: 0.4126\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4218 - val_loss: 0.4006\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4218 - val_loss: 0.3981\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4211 - val_loss: 0.3999\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4212 - val_loss: 0.4060\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4232 - val_loss: 0.4075\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4219 - val_loss: 0.4052\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4218 - val_loss: 0.4029\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4234 - val_loss: 0.4044\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4223 - val_loss: 0.3990\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4222 - val_loss: 0.4021\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4212 - val_loss: 0.3974\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4217 - val_loss: 0.3985\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4223 - val_loss: 0.4010\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4215 - val_loss: 0.4010\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4228 - val_loss: 0.4052\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4232 - val_loss: 0.4048\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4214 - val_loss: 0.3997\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4214 - val_loss: 0.4034\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4214 - val_loss: 0.4053\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4224 - val_loss: 0.3972\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4204 - val_loss: 0.3990\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4219 - val_loss: 0.4008\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4215 - val_loss: 0.4064\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4224 - val_loss: 0.4026\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4218 - val_loss: 0.4070\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4218 - val_loss: 0.4007\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4220 - val_loss: 0.3989\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4215 - val_loss: 0.3967\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4236 - val_loss: 0.4030\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4235 - val_loss: 0.4089\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4221 - val_loss: 0.4017\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4228 - val_loss: 0.3933\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4246 - val_loss: 0.4027\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4207 - val_loss: 0.4060\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4225 - val_loss: 0.3999\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4208 - val_loss: 0.4053\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4217 - val_loss: 0.4044\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4208 - val_loss: 0.3999\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4216 - val_loss: 0.4034\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4209 - val_loss: 0.3978\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4225 - val_loss: 0.4016\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4208 - val_loss: 0.4003\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4213 - val_loss: 0.4049\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4219 - val_loss: 0.4054\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4216 - val_loss: 0.4031\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4223 - val_loss: 0.4043\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4217 - val_loss: 0.4017\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4210 - val_loss: 0.4030\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4217 - val_loss: 0.4055\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4231 - val_loss: 0.4019\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4219 - val_loss: 0.3986\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4206 - val_loss: 0.4005\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4214 - val_loss: 0.4060\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4231 - val_loss: 0.4045\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4205 - val_loss: 0.4067\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4202 - val_loss: 0.3961\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4217 - val_loss: 0.3990\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4217 - val_loss: 0.4101\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4271 - val_loss: 0.4050\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4211 - val_loss: 0.4029\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4217 - val_loss: 0.4154\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4225 - val_loss: 0.3981\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4210 - val_loss: 0.4047\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4225 - val_loss: 0.4001\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4206 - val_loss: 0.3996\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4207 - val_loss: 0.3962\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4218 - val_loss: 0.4010\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4225 - val_loss: 0.3997\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4213 - val_loss: 0.4025\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4215 - val_loss: 0.4066\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4212 - val_loss: 0.4016\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4206 - val_loss: 0.4009\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4207 - val_loss: 0.4007\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4215 - val_loss: 0.3985\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4204 - val_loss: 0.4048\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4217 - val_loss: 0.4057\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4219 - val_loss: 0.4044\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4200 - val_loss: 0.4055\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4204 - val_loss: 0.4045\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4210 - val_loss: 0.4062\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4225 - val_loss: 0.4044\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4220 - val_loss: 0.4075\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4201 - val_loss: 0.4020\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4206 - val_loss: 0.4041\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4206 - val_loss: 0.4005\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4217 - val_loss: 0.4068\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4219 - val_loss: 0.4033\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4228 - val_loss: 0.4030\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4216 - val_loss: 0.4070\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4209 - val_loss: 0.4088\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4212 - val_loss: 0.3998\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4205 - val_loss: 0.4018\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4212 - val_loss: 0.4023\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4207 - val_loss: 0.4038\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4212 - val_loss: 0.4069\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4208 - val_loss: 0.4085\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4212 - val_loss: 0.4038\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4211 - val_loss: 0.4052\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4198 - val_loss: 0.4085\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4212 - val_loss: 0.4057\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4200 - val_loss: 0.4085\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4212 - val_loss: 0.4103\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4204 - val_loss: 0.4078\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4214 - val_loss: 0.4068\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4197 - val_loss: 0.4084\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4207 - val_loss: 0.4176\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4210 - val_loss: 0.4084\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4200 - val_loss: 0.4125\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4205 - val_loss: 0.4127\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4199 - val_loss: 0.4118\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4213 - val_loss: 0.4038\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4202 - val_loss: 0.4107\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4211 - val_loss: 0.4144\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4213 - val_loss: 0.4006\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4197 - val_loss: 0.4098\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4209 - val_loss: 0.4131\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4210 - val_loss: 0.4127\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4209 - val_loss: 0.4104\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4196 - val_loss: 0.4098\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4213 - val_loss: 0.4065\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4206 - val_loss: 0.4049\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4203 - val_loss: 0.4043\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4199 - val_loss: 0.4045\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4200 - val_loss: 0.4005\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4210 - val_loss: 0.4029\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4216 - val_loss: 0.4062\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4203 - val_loss: 0.4049\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4207 - val_loss: 0.4089\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4218 - val_loss: 0.4183\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4232 - val_loss: 0.4019\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4211 - val_loss: 0.4074\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4228 - val_loss: 0.4091\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4207 - val_loss: 0.4051\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4200 - val_loss: 0.4058\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4199 - val_loss: 0.4051\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4215 - val_loss: 0.4158\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4222 - val_loss: 0.4089\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4202 - val_loss: 0.4082\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4197 - val_loss: 0.4122\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4199 - val_loss: 0.4081\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4210 - val_loss: 0.4017\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4208 - val_loss: 0.4086\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4197 - val_loss: 0.4071\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4196 - val_loss: 0.4011\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4198 - val_loss: 0.4137\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4215 - val_loss: 0.4024\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4208 - val_loss: 0.4214\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4215 - val_loss: 0.4018\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4204 - val_loss: 0.4098\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4196 - val_loss: 0.4097\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4198 - val_loss: 0.4070\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4193 - val_loss: 0.4056\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4198 - val_loss: 0.4065\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4207 - val_loss: 0.4072\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4215 - val_loss: 0.4105\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4205 - val_loss: 0.4115\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4205 - val_loss: 0.4142\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4203 - val_loss: 0.4180\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4199 - val_loss: 0.4160\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4194 - val_loss: 0.4078\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4200 - val_loss: 0.4096\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4197 - val_loss: 0.4077\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4217 - val_loss: 0.4144\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4213 - val_loss: 0.4096\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4210 - val_loss: 0.4112\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4195 - val_loss: 0.4098\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4201 - val_loss: 0.4119\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4205 - val_loss: 0.4126\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4205 - val_loss: 0.4101\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4196 - val_loss: 0.4082\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4199 - val_loss: 0.4080\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4226 - val_loss: 0.4042\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4208 - val_loss: 0.4132\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4196 - val_loss: 0.4063\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4216 - val_loss: 0.4050\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4209 - val_loss: 0.4127\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4197 - val_loss: 0.4033\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4203 - val_loss: 0.4026\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4194 - val_loss: 0.4023\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4189 - val_loss: 0.4080\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4208 - val_loss: 0.4031\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4211 - val_loss: 0.4138\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4201 - val_loss: 0.4061\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4187 - val_loss: 0.4040\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4199 - val_loss: 0.4016\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4191 - val_loss: 0.4094\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4205 - val_loss: 0.4118\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4207 - val_loss: 0.4088\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4194 - val_loss: 0.4110\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4191 - val_loss: 0.4128\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4210 - val_loss: 0.4063\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4193 - val_loss: 0.4103\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4190 - val_loss: 0.4163\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4191 - val_loss: 0.4090\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4210 - val_loss: 0.4207\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4256 - val_loss: 0.4098\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4197 - val_loss: 0.4191\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4200 - val_loss: 0.4108\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4196 - val_loss: 0.4096\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4195 - val_loss: 0.4095\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4227 - val_loss: 0.4152\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4210 - val_loss: 0.4102\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4201 - val_loss: 0.4147\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4196 - val_loss: 0.4147\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4185 - val_loss: 0.4160\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4202 - val_loss: 0.4091\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4203 - val_loss: 0.4196\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4190 - val_loss: 0.4125\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4199 - val_loss: 0.4058\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4188 - val_loss: 0.4086\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4192 - val_loss: 0.4104\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4201 - val_loss: 0.4142\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4202 - val_loss: 0.4237\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4214 - val_loss: 0.4089\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4203 - val_loss: 0.4063\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4186 - val_loss: 0.4081\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4192 - val_loss: 0.4093\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4188 - val_loss: 0.4104\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4200 - val_loss: 0.4150\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4196 - val_loss: 0.4096\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4194 - val_loss: 0.4033\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4190 - val_loss: 0.4129\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4200 - val_loss: 0.4105\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4187 - val_loss: 0.4062\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4194 - val_loss: 0.4098\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4189 - val_loss: 0.4069\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4212 - val_loss: 0.4131\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4198 - val_loss: 0.4096\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4186 - val_loss: 0.4185\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4205 - val_loss: 0.4102\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4222 - val_loss: 0.4143\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4193 - val_loss: 0.4155\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4218 - val_loss: 0.4150\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4196 - val_loss: 0.4102\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4201 - val_loss: 0.4039\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4199 - val_loss: 0.4121\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4200 - val_loss: 0.4156\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4204 - val_loss: 0.4114\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4193 - val_loss: 0.4132\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4186 - val_loss: 0.4131\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4183 - val_loss: 0.4149\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4190 - val_loss: 0.4168\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4203 - val_loss: 0.4253\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4199 - val_loss: 0.4090\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4180 - val_loss: 0.4091\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4188 - val_loss: 0.4123\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4190 - val_loss: 0.4166\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4191 - val_loss: 0.4134\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4206 - val_loss: 0.4097\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4185 - val_loss: 0.4083\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4199 - val_loss: 0.4115\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4194 - val_loss: 0.4154\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4191 - val_loss: 0.4121\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4195 - val_loss: 0.4141\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4194 - val_loss: 0.4198\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4203 - val_loss: 0.4142\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4210 - val_loss: 0.4087\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4196 - val_loss: 0.4186\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4193 - val_loss: 0.4164\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4192 - val_loss: 0.4103\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4195 - val_loss: 0.4108\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4182 - val_loss: 0.4120\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4187 - val_loss: 0.4187\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4196 - val_loss: 0.4145\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4182 - val_loss: 0.4144\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4209 - val_loss: 0.4143\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4214 - val_loss: 0.4112\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4193 - val_loss: 0.4100\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4194 - val_loss: 0.4135\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4194 - val_loss: 0.4107\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4183 - val_loss: 0.4159\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4179 - val_loss: 0.4147\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4184 - val_loss: 0.4111\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4182 - val_loss: 0.4102\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4191 - val_loss: 0.4140\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4179 - val_loss: 0.4140\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4215 - val_loss: 0.4107\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4189 - val_loss: 0.4073\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4192 - val_loss: 0.4109\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4187 - val_loss: 0.4092\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4181 - val_loss: 0.4143\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4190 - val_loss: 0.4123\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4186 - val_loss: 0.4097\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4187 - val_loss: 0.4091\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4181 - val_loss: 0.4069\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4188 - val_loss: 0.4094\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4191 - val_loss: 0.4127\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4181 - val_loss: 0.4171\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4182 - val_loss: 0.4138\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4207 - val_loss: 0.4182\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4189 - val_loss: 0.4128\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4197 - val_loss: 0.4099\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4188 - val_loss: 0.4077\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4226 - val_loss: 0.4161\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4199 - val_loss: 0.4186\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4189 - val_loss: 0.4168\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4185 - val_loss: 0.4163\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4194 - val_loss: 0.4127\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4192 - val_loss: 0.4123\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4180 - val_loss: 0.4158\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4200 - val_loss: 0.4162\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4194 - val_loss: 0.4133\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4189 - val_loss: 0.4133\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4180 - val_loss: 0.4116\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4184 - val_loss: 0.4190\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4196 - val_loss: 0.4162\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4193 - val_loss: 0.4162\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4183 - val_loss: 0.4126\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4186 - val_loss: 0.4216\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4179 - val_loss: 0.4190\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4186 - val_loss: 0.4106\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4186 - val_loss: 0.4125\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4182 - val_loss: 0.4152\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4186 - val_loss: 0.4137\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4181 - val_loss: 0.4155\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4200 - val_loss: 0.4073\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4196 - val_loss: 0.4152\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4187 - val_loss: 0.4175\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4181 - val_loss: 0.4202\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4182 - val_loss: 0.4188\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4185 - val_loss: 0.4166\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4189 - val_loss: 0.4151\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4182 - val_loss: 0.4150\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4175 - val_loss: 0.4226\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4199 - val_loss: 0.4151\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4178 - val_loss: 0.4120\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4180 - val_loss: 0.4168\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4178 - val_loss: 0.4172\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4193 - val_loss: 0.4115\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4182 - val_loss: 0.4131\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4184 - val_loss: 0.4119\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4179 - val_loss: 0.4167\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4203 - val_loss: 0.4148\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4176 - val_loss: 0.4136\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4186 - val_loss: 0.4160\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4194 - val_loss: 0.4120\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4181 - val_loss: 0.4133\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4177 - val_loss: 0.4110\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4188 - val_loss: 0.4073\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4179 - val_loss: 0.4151\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4198 - val_loss: 0.4083\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4191 - val_loss: 0.4192\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4192 - val_loss: 0.4156\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4183 - val_loss: 0.4175\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4189 - val_loss: 0.4166\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4199 - val_loss: 0.4152\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4196 - val_loss: 0.4196\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4199 - val_loss: 0.4156\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4197 - val_loss: 0.4160\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4189 - val_loss: 0.4145\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4186 - val_loss: 0.4179\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4176 - val_loss: 0.4187\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4175 - val_loss: 0.4211\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4179 - val_loss: 0.4207\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4174 - val_loss: 0.4122\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4177 - val_loss: 0.4155\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4176 - val_loss: 0.4155\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4177 - val_loss: 0.4187\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4187 - val_loss: 0.4186\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4189 - val_loss: 0.4205\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4184 - val_loss: 0.4215\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4173 - val_loss: 0.4143\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4180 - val_loss: 0.4198\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4191 - val_loss: 0.4196\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4213 - val_loss: 0.4210\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4186 - val_loss: 0.4202\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4182 - val_loss: 0.4162\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4175 - val_loss: 0.4188\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4187 - val_loss: 0.4175\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4180 - val_loss: 0.4226\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4180 - val_loss: 0.4138\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4188 - val_loss: 0.4122\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4181 - val_loss: 0.4130\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4182 - val_loss: 0.4167\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4185 - val_loss: 0.4153\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4180 - val_loss: 0.4186\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4170 - val_loss: 0.4214\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4186 - val_loss: 0.4241\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4187 - val_loss: 0.4259\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4184 - val_loss: 0.4217\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4179 - val_loss: 0.4175\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4187 - val_loss: 0.4179\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4203 - val_loss: 0.4155\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4175 - val_loss: 0.4180\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4185 - val_loss: 0.4183\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4197 - val_loss: 0.4221\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4189 - val_loss: 0.4224\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4191 - val_loss: 0.4148\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4185 - val_loss: 0.4145\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4186 - val_loss: 0.4189\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4176 - val_loss: 0.4217\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4183 - val_loss: 0.4163\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4191 - val_loss: 0.4170\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4175 - val_loss: 0.4132\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4191 - val_loss: 0.4100\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4191 - val_loss: 0.4233\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4171 - val_loss: 0.4081\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4177 - val_loss: 0.4177\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4174 - val_loss: 0.4089\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4193 - val_loss: 0.4147\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4182 - val_loss: 0.4194\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4177 - val_loss: 0.4195\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4181 - val_loss: 0.4187\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4183 - val_loss: 0.4181\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4181 - val_loss: 0.4179\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4179 - val_loss: 0.4172\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4181 - val_loss: 0.4159\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4172 - val_loss: 0.4172\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4174 - val_loss: 0.4198\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4175 - val_loss: 0.4206\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4181 - val_loss: 0.4188\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4197 - val_loss: 0.4199\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4196 - val_loss: 0.4189\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4179 - val_loss: 0.4177\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4193 - val_loss: 0.4190\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4198 - val_loss: 0.4242\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4174 - val_loss: 0.4236\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4174 - val_loss: 0.4138\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4177 - val_loss: 0.4223\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4178 - val_loss: 0.4164\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4176 - val_loss: 0.4232\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4189 - val_loss: 0.4150\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4179 - val_loss: 0.4170\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4178 - val_loss: 0.4180\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4181 - val_loss: 0.4150\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4184 - val_loss: 0.4138\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4176 - val_loss: 0.4232\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4186 - val_loss: 0.4205\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4174 - val_loss: 0.4162\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4180 - val_loss: 0.4193\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4185 - val_loss: 0.4193\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4183 - val_loss: 0.4204\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 0.4183 - val_loss: 0.4187\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4183 - val_loss: 0.4121\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4173 - val_loss: 0.4186\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4187 - val_loss: 0.4110\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4178 - val_loss: 0.4147\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4173 - val_loss: 0.4165\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4184 - val_loss: 0.4218\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4176 - val_loss: 0.4132\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4182 - val_loss: 0.4185\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4180 - val_loss: 0.4232\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4178 - val_loss: 0.4189\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4170 - val_loss: 0.4193\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4179 - val_loss: 0.4143\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.4192 - val_loss: 0.4184\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4172 - val_loss: 0.4151\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4178 - val_loss: 0.4210\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4191 - val_loss: 0.4150\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.4196 - val_loss: 0.4215\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4164 - val_loss: 0.4214\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4179 - val_loss: 0.4207\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4183 - val_loss: 0.4172\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4194 - val_loss: 0.4188\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.4176 - val_loss: 0.4117\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4179 - val_loss: 0.4173\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4170 - val_loss: 0.4200\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4183 - val_loss: 0.4147\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4173 - val_loss: 0.4263\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4206 - val_loss: 0.4163\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.4198 - val_loss: 0.4249\n",
            "5/5 [==============================] - 1s 7ms/step - loss: 0.4249\n",
            "5/5 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f073819b9a0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHMCAYAAAAzqWlnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACBsklEQVR4nO3dd3xT5eIG8OecjO5JJy10s8uQKSBTBQEZylCcgIh7/RQVQXGA4kD0KtfBEhfrCrKHAqLsjWWXVaCbNt1t1vv7I03a0BRa2iSlPN/Px3vJOScn73mbNk/edSQhhAARERFRPSU7uwBERERE9sSwQ0RERPUaww4RERHVaww7REREVK8x7BAREVG9xrBDRERE9RrDDhEREdVrDDtERERUrzHsEBERUb3GsEPkRJIkoVevXjU+T69evSBJUs0LVM/UVv0S0c2NYYduaZIkVeu/BQsWOLvIZAd14X2wYMGCGz63uVxEZJvS2QUgcqZ33nmnwrZZs2YhJycHL774Inx9fa32tW3btlZf//jx43B3d6/xeRYuXIjCwsJaKNGtydnvAyKyL4k3AiWyFhkZiQsXLuDcuXOIjIx0dnGoBiRJQs+ePbF169ZqP9fR74MFCxZgzJgxmD9/Ph5//PFqPdfcqsM/50S2sRuLqIrM42K0Wi3ee+89NG3aFC4uLpYPppycHHzyySfo06cPwsPDoVarERgYiMGDB2Pnzp02z2lrTMnUqVMhSRK2bt2KZcuWoVOnTnB3d4e/vz8eeOABXL58udKylbd161ZIkoSpU6fi0KFDGDhwIHx9feHu7o6ePXtix44dNsuUkpKCMWPGICgoCG5ubmjbti1++OEHq/NVRU3qIzMzE08++SRCQ0Ph4uKCli1bYv78+Tafo9Vq8f777yMmJgYuLi6IiorC5MmTUVJSUqVy3ojdu3dj+PDhCAkJgVqtRqNGjTBhwgQkJydXOPbs2bN48sknERsbCzc3N/j7+yM+Ph5PPfUUrly5AsD08xszZgwAYMyYMVZdZufPn6/VspeUlOCjjz5CfHw83N3d4e3tjTvuuANLliyxefzKlSvRt29fy8+iYcOG6NmzJ2bPnl3t6yzv119/Re/eveHr6wtXV1c0b94cH3zwgc2f299//417770X4eHhcHFxQUhICLp06YJ33323diqF6j12YxFV0/3334+9e/finnvuwdChQxEUFATA1CX11ltvoUePHhg4cCD8/PyQlJSElStXYt26dVi1ahX69+9f5deZPXs2Vq5cicGDB6Nnz57YvXs3Fi9ejMOHD+PQoUNwcXGp0nn27duHjz/+GLfffjueeOIJJCUl4X//+x/69u2LQ4cOoWnTppZj09PTcfvtt+PChQvo0aMHunbtitTUVDzzzDO4++67q1VPN1ofGo0G3bp1g1qtxvDhw1FSUoKlS5di7NixkGUZjz32mOVYIQRGjhyJ33//HTExMXjuueeg1Woxb948/Pvvv9Uqb1XNmzcPTz75JFxcXDB48GA0atQIp0+fxpw5c7Bq1Srs2rULjRs3BmAKjh07dkRubi4GDBiA+++/H8XFxTh37hx+/PFHPPfcc2jQoAEef/xx+Pr64vfff8eQIUOsusmu7kKrCa1Wi379+uGvv/5Cs2bN8Oyzz6KwsBDLli3DqFGjcOjQIUyfPt1y/HfffYcJEyYgJCQE9957LwICApCeno4jR45g/vz5eOaZZ6p1nWZjx47F/PnzER4ejvvvvx++vr7YtWsXpkyZgj///BObNm2CUmn6eFq/fj0GDhwIb29vDB48GGFhYcjKysLx48cxe/Zsm12QRBUIIrISEREhAIhz585Zbe/Zs6cAIOLj40VGRkaF52k0GpvbL168KEJDQ0WzZs0q7AMgevbsabXtnXfeEQCEl5eXOHLkiNW+Bx98UAAQixcvtlm28rZs2SIACABi/vz5Vvu++eYbAUA8/fTTVtvHjh0rAIiJEydabT906JBQq9UCgHjnnXcqXIctN1ofAMS4ceOEXq+3bD969KhQKBSiefPmVsf//PPPAoDo0qWLKCoqsmy/cuWKiI6Otlm/VWXrfXDy5EmhUqlETEyMuHTpktXxf/zxh5BlWQwdOtSy7csvvxQAxKxZsyqcPz8/XxQWFloez58/3+bPqirM9XY906dPFwDEPffcI3Q6nWV7Wlqa5Xq3b99u2X7bbbcJtVot0tLSKpyr/M/2Rq5z2LBhVtuFKHvvlz/PfffdJwCIQ4cOXbMMRNfCbiyianr//fcREBBQYbuPj4/N7eHh4Rg+fDhOnDiBpKSkKr/OCy+8gPj4eKtt48ePBwDs2bOnyufp1q1bhTEgY8eOhVKptDqPVqvFr7/+Ch8fH0yePNnq+DZt2uDRRx+t8msCN14f7u7umDlzJhQKhWVbixYt0K1bNxw/fhz5+fmW7eaurenTp8PV1dWy3d/fH1OmTKlWeaviv//9L3Q6Hb744guEhYVZ7evbty8GDx6MVatWIS8vz2qfm5tbhXN5eHjY3G5P8+bNgyRJmDlzpqXlBACCgoIs9TVnzhyr5yiVSqhUqgrnsvWzrcp1fvHFF1AqlZg3b16F46dMmYIGDRrg559/rtK5bZWByBZ2YxFVU6dOnSrdt337dnzxxRfYuXMn0tPTodVqrfZfvnzZ0sVxPR06dKiwrVGjRgCA7OzsKpfX1nlUKhWCg4OtznPy5EkUFRWhQ4cO8PLyqvCc7t27V/ggvJ4bqY+4uDh4e3tXOFf5a/f09AQAHDhwALIso3v37hWOt8f6OuaxRn/99Rf27t1bYX96ejoMBgNOnTqF9u3bY/DgwZg0aRKeffZZbNiwAf369UO3bt3QokULh08Vz8vLQ2JiIsLCwtCsWbMK+/v06QMAOHjwoGXbQw89hP/7v/9DixYt8MADD6Bnz57o1q0bAgMDrZ5b1essLCzE4cOHERAQgFmzZtksp4uLC44fP25Vht9++w2dO3fGqFGj0Lt3b3Tr1g3h4eE1qQ66xTDsEFVTSEiIze3Lly/H8OHD4erqirvuugsxMTHw8PCALMvYunUr/vrrr2oNmrU1VsP8bdxgMNToPOZzlT9PTk4OACA4ONjm8ZVtr8yN1se1ygugQpn9/f1ttjxU9nOqCfNA208++eSax5lbnyIiIrBnzx5MnToV69evx2+//QbAFNxeffVVvPDCC7VexsqYf76hoaE295u3azQay7ZXXnkFAQEBmD17Nr788kvMmjXLMsPtk08+sQTpql5ndnY2hBDIyMio8uDi++67D6tXr8Znn32GefPm4dtvvwUAtG/fHh9++CHuuuuu6lcG3XIYdoiqqbJv5FOmTIFarca+ffvQvHlzq30TJkzAX3/95Yji3TBza0paWprN/ZVtr4wj6sPHxwdZWVnQ6XQVAk9qamqNz2/r9QBTcLDV+mRL8+bNsXjxYuj1ehw+fBh//PEH/vOf/+DFF1+Eh4cHxo0bV+vltMVc9srqJSUlxeo4s0cffRSPPvooNBoNduzYgeXLl2PevHno168fTpw4YWnlqcp1ms/drl07HDhwoMplHzhwIAYOHIiCggLs3r0bq1evxn//+18MGjQIBw8eRIsWLapdH3Rr4ZgdolqSmJiIFi1aVPhgNxqN+Oeff5xUqqpr1qwZ3NzccOTIkQpjTgBU+xocUR+33XZbpee7kbV1rqdLly4ATFOhq0upVKJ9+/Z4/fXX8euvvwIAVqxYYdlvHqNUnVa76vDy8kJMTAwuX76M06dPV9i/ZcsWAKY6tcXX1xcDBgzA999/j8cffxxZWVnYtm1bheOudZ2enp5o2bIljh49iqysrGpfg4eHB/r06YOZM2di0qRJ0Gq1WLduXbXPQ7cehh2iWhIZGYnTp09brbUihMDUqVNx7NgxJ5asatRqNUaNGoWcnBx88MEHVvsOHz6MhQsXVut8jqgP89o0b731FoqLiy3bs7KyKlxDbXjuueegUqnw8ssv49SpUxX2a7VaqyC0f/9+S/dReeZWsvKrZ5unZldnEHt1jR07FkIIvPbaa1ahKjMzE++//77lGLMtW7bYXKgwPT0dQFn5q3Odr7zyCrRaLcaOHWvVZWaWnZ1t1eqzbds26PX6Kp2bqDLsxiKqJS+//DKeeuoptGvXDvfffz9UKhW2b9+OY8eO4d5778WqVaucXcTr+uijj7B582Z8/PHH2L17N7p27YqUlBQsWbIEAwYMwIoVKyDLVfuO5Ij6ePDBB7F48WKsXLkSrVq1wpAhQ6DT6bBs2TJ07NgRZ86cqfFrlNesWTPMmzcPY8eORcuWLdG/f380adIEOp0OSUlJ+PvvvxEYGIgTJ04AAH788Ud8++236N69O2JiYuDn54czZ85g1apVcHFxwUsvvWQ59+233w53d3fMmjULV65csYw5ev755yt0LVXmWisvz549G6+++irWrVuH33//HW3atMGAAQNQWFiIpUuXIj09HRMnTrQa7D1s2DB4enqiS5cuiIyMhBACf//9N/bu3Yv27dvjzjvvrPZ1jh07Fvv378fs2bMRExODfv36oXHjxsjKysK5c+ewbds2jBkzBt988w0A06zEy5cvo1u3boiMjIRarcb+/fuxefNmRERE4IEHHqhS3dAtzpnz3onqouuts3Mt8+fPF23atBHu7u6iQYMGYujQoeLIkSOW9UO2bNlidTyusc7O1ccKIcS5c+cEAPHYY49dt2zmdXYqWxcnIiJCREREVNh+6dIl8eijj4qAgADh6uoq2rRpIxYsWCCWLl0qAIjPP//8mnVQXm3Uh9ljjz1m8+dSUlIi3n33XREVFSXUarWIiIgQkyZNEsXFxbW+zo7ZkSNHxGOPPSYaN24s1Gq18PPzEy1bthRPPvmk+PPPPy3H7dq1Szz11FOidevWws/PT7i6uoqYmBjx+OOPi3///bfCedetWye6dOkiPDw8LGvn2Hr9q5mPvdZ/2dnZQgghioqKxLRp00TLli2Fq6ur8PT0FN26dRO//PJLhfP+97//FUOHDhVRUVHCzc1N+Pn5ibZt24oZM2aI3NzcG75OIYRYtWqVGDhwoAgMDBQqlUoEBweLjh07irfeekscP37cctzixYvFAw88IGJjY4WHh4fw8vISLVu2FJMmTRLp6enXrRsiIYTgvbGIqEreeustTJ8+HevXr0e/fv2cXRwioipj2CEiK8nJyWjYsKHVtn///Rddu3aFWq3G5cuXrRbwIyKq6zhmh4isdOjQAbGxsWjVqhU8PDxw+vRprFmzBkajEd9++y2DDhHddNiyQ0RW3n33XaxYsQLnz59HXl4efH190aVLF7z66qt2WZWYiMjeGHaIiIioXuM6O0RERFSvMewQERFRvcawQ0RERPUaww4RERHVa5x6Xio7O9vm/VdqKjAwEBkZGbV+XrLGenYM1rPjsK4dg/XsGPaoZ6VSCT8/v6odW6uvfBPT6/XQ6XS1ek5Jkizn5qQ3+2E9Owbr2XFY147BenaMulDP7MYiIiKieo1hh4iIiOo1hh0iIiKq1xh2iIiIqF7jAGUiIqp39Ho9CgsLr3tcUVERtFqtA0p0a7uRehZCQKlUwsPDo8avz7BDRET1il6vR0FBAby8vCDL1+7AUKlUtT4Tlyq60XouKChASUkJXFxcavT67MYiIqJ6pbCwsEpBh+o+d3d3lJSU1Pg8fCcQEVG9w6BTP5jX6KkpvhuIiIioXmPYISIionqNYYeIiKie6dy5M77//vtaOdeOHTsQFhaGnJycWjmfM3A2FhERUR0wfPhwtGjRAu+9916Nz7V27Vq4u7vXQqnqB4YdOxE6HZCngV5ZO4OriIjo1iaEgMFggFJ5/Y/uBg0aOKBENw92Y9nLhUQYXh+HjDefcnZJiIiojnvppZewc+dOzJ07F2FhYQgLC8PixYsRFhaGzZs3o3///oiKisKePXtw/vx5jBkzBm3atEFcXBwGDBiAbdu2WZ3v6m6ssLAw/PLLLxg3bhxiYmLQrVs3bNy48YbLu2bNGvTu3RtRUVHo3LkzvvnmG6v9CxYsQLdu3RAdHY02bdpg7Nixln2rV69G3759ERMTg5YtW2LUqFFVWgCyJtiyYy8KBQBAGI1g2w4RkfMIIQCt7bVahNFgaom3F7VLlaZPv/feezh79iyaNWuGV199FQBw8uRJAMD06dPx9ttvo3HjxvDx8UFycjL69OmD119/HWq1GsuWLcOYMWOwbds2hIWFVfoaM2fOxOTJkzF58mTMnz8fzz33HHbv3g0/P79qXdKRI0fw1FNP4ZVXXsHgwYOxb98+TJo0CX5+fhg1ahQOHz6Mt99+G19++SU6dOgAjUaDffv2AQDS0tLw7LPP4q233sI999yD/Px87N692/QzsiOGHXsxr/FgMDi3HEREtzptCYzPjbS5q+bL1V2b/NUSwMX1usd5e3tDrVbD1dUVQUFBAIDExEQAwGuvvYYePXpYjvXz80PLli0tjydOnIj169dj48aNGDNmTKWvMXLkSAwdOhQA8MYbb2Du3Lk4dOgQevfuXa1r+u6779C9e3e8/PLLAICYmBicPn0a33zzDUaNGoXLly/D3d0dd955Jzw9PREeHo527dpBp9MhPT0der0eAwYMQHh4OACgefPm1Xr9G8FuLHuRTFUrjAw7RER041q3bm31uKCgAO+99x569uyJ5s2bIy4uDqdPn8bly5eveZ7yocLd3R1eXl7IzMysdnlOnz6Njh07Wm3r2LEjzp07B4PBgB49eiA8PBy33347nn/+efz222+WbqoWLVqge/fu6Nu3L5588kn8/PPP0Gg01S5DdbFlx15Ku7FgNDq3HEREtzq1i6mFxQa73xtLXbN7OgGoMKvqvffew99//40pU6YgMjISrq6uePLJJ697o02VSmX1WJIkGO3wGeXp6Yn169djx44d2LZtGz799FPMnDkTa9asgY+PDxYtWoR9+/bhr7/+wvz58zFjxgysXr0ajRs3rvWymDHs2Au7sYiI6gRJkirtSpJUKkiywsElsk2lUlUpfOzbtw8jRozAPffcA8DU0nPp0iV7F88iLi4Oe/futdq2d+9eREdHQ1H6RV+pVKJHjx7o0aMHXnnlFTRv3hzbt2/HgAEDIEkSOnbsiI4dO+Lll19Gp06dsG7dOkyYMMFuZWbYsRd2YxERUTU0atQIBw8exMWLF+Hh4VFp8ImKisK6detw1113QZIkfPLJJ3ZpoanMhAkTMGDAAHz++ecYPHgw9u/fj/nz52P69OkAgE2bNiEpKQmdO3eGr68v/vzzTxiNRsTExODAgQP4559/0LNnTwQEBODAgQPIyspCXFycXcvMsGMv5m4stuwQEVEVTJgwAS+99BJ69eqF4uJizJw50+Zx77zzDl555RUMGTIE/v7+ePbZZ5Gfn++wcsbHx+Obb77Bp59+ii+++AJBQUF47bXXMGrUKACAj48P1q1bh5kzZ6K4uBhRUVH49ttv0bRpU5w+fRq7d+/GnDlzkJ+fj7CwMLz99tvo06ePXcssCXvP97pJZGRk1Gq/rbiSDuMbT0BSu0Axe5ndp9XdyiRJQmhoKFJSUljPdsR6dhzWdc3k5ubC29u7SsfafcwOAahZPVf281SpVAgMDKzSOTgby15k8zo7bNkhIiJyJnZj2QsHKBMR0U3g9ddfx2+//WZz33333YcZM2Y4uES1j2HHXsxhRwg2QxMRUZ312muv4amnbN/ayMvLy8GlsY86FXaOHTuGlStX4ty5c8jOzsarr76KTp06XfM5R48excKFC3Hx4kU0aNAA999/P3r16uWYAl9L+amMRmNZ+CEiIqpDAgICEBAQ4Oxi2FWd+gQuKSlBZGQkxo0bV6Xj09PT8dFHH6Fly5b4+OOPMXDgQHzzzTc4dOiQfQtaFeXDDcftEBEROU2datlp164d2rVrV+XjN27ciKCgIDz66KMAgPDwcJw4cQJr1qxB27Zt7VTKKrq6ZYeIiIicok617FTX6dOnER8fb7WtTZs2OHXqlJNKVE75lh0OUiYiInKaOtWyU10ajQY+Pj5W23x8fFBUVAStVgu1Wl3hOTqdzmquvyRJcHNzs/y71ijKWnYkIYDaPDdZMf/cavXnRxWwnh2HdU1kraa/Czd12LkRy5cvx7JlyyyPo6KiMGPGjCovTFRVQgiY71QSFBgAhY9frZ6fKgoJCXF2EW4JrGfHYV3fmKKiogo3vbyW6hxLN+5G61mtViM0NLRGr31Thx1fX1/k5ORYbcvJyYGbm5vNVh0AGDZsGAYNGmR5bE6LGRkZ0Ov1tVtASQaEEekpKUBhce2emywkSUJISAhSU1M5zd+OWM+Ow7quGa1WW+XVermCcpmLFy+iS5cu2LBhA1q1alWr565JPWu1WqSkpFTYrlQqq9xQcVOHnbi4OBw8eNBq25EjR9CkSZNKn6NSqSpNl7X+R0WWAYMRwmAA+AfL7gTXNHII1rPjsK5vLcOHD0eLFi3w3nvv1cr5XnrpJeTm5mLevHm1cj5nqunvQZ0aoFxcXIzz58/j/PnzAExTy8+fP4/MzEwAwC+//IKvvvrKcvzdd9+N9PR0/PTTT7h8+TI2bNiAnTt3YuDAgc4ofkUK88KCnI1FRETkLHUq7Jw5cwYTJ07ExIkTAQALFy7ExIkTsXjxYgBAdna2JfgAQFBQEN544w0cOXIEr732GlavXo2nnnrK+dPOzSTe+ZyIiK7vpZdews6dOzF37lyEhYUhLCwMFy9exIkTJ/Dwww8jLi4Obdq0wfPPP4+srCzL81avXo2+ffsiJiYGLVu2xKhRo1BYWIjPPvsMS5cuxYYNGyzn27FjR7XLZW5AiIqKQrt27TB9+nSrIR+VvT4A7NixAwMHDkRsbCxiY2MxZMgQXLp0qbKXsqs61Y3VsmVLLFmypNL9zz77rM3nfPzxx/Ys1g3JLtJjR2gnuBbloS/X2SEichohBEoMtrtBDDBCp7ff32gXhVSlmUTvvfcezp49i2bNmuHVV18FYBqTMnDgQDz44IOYOnUqiouLMW3aNEyYMAFLly5FWloann32Wbz11lu45557kJ+fj927d0MIgaeeegqnT59Gfn4+Zs6cCcA0zrU6UlJS8Mgjj2DkyJH44osvkJiYiNdeew0uLi74v//7v2u+vl6vx7hx4zB69Gh8/fXXEEJg7969TpthWKfCTn2Slq/Dd5EDEVx0BX25gjIRkdOUGARGLXbO+muLRzWBq/L6H/De3t5Qq9VwdXVFUFAQAGDWrFlo1aoV3nzzTctxn332GTp27IgzZ86gsLAQer0eAwYMQHh4OACgefPmlmNdXV2h1Wot56uuH374AQ0bNsS0adMgSRJiY2ORmpqK6dOn4+WXX0Z6enqlr5+dnY3c3FzceeediIyMhEqlQlRU1A2VozYw7NiJXPreNkoSV1AmIqJqO3bsGHbs2IG4uLgK+y5cuICePXuie/fu6Nu3L3r27ImePXti4MCB1W7BqUxiYiLat29v1RrTsWNHFBQUICUlBS1atKj09f38/DBy5Eg89NBDuOOOO9CrVy8MGDAAwcHBtVK26mLYsRNFadoxSDLDDhGRE7koJCweZXuWrkqpgk5vv6nnLoob77YpLCzEXXfdhUmTJlXYFxwcDIVCgUWLFmHfvn3466+/MH/+fMyYMQOrV69G48aNa1LsKrne63/++ecYN24ctmzZghUrVuDDDz/Er7/+ivbt29u9bFerUwOU6xNLyw5k3giUiMiJJEmCq1K2/Z+qku219F91xqioVCoYy305btWqFU6ePIlGjRohKirK6j93d3fLtXXs2BGvvvoqNmzYAJVKhXXr1gEwLcZnqMEEmdjYWOzfv99q2vfevXvh6elpWeTvWq9vvobnn38ea9euRdOmTbFixYobLk9NMOzYiVz6BjeyZYeIiKqgUaNGOHjwIC5evIisrCw8/vjj0Gg0eOaZZ3Do0CGcP38eW7duxcsvvwyDwYADBw7gyy+/xOHDh3H58mWsXbsWWVlZlm6v8PBwHD9+HImJicjKyqr2on6PPfYYkpOTMXnyZCQmJmLDhg347LPP8OSTT0KW5Wu+flJSEj788EPs27cPly5dwpYtW3Du3DnExsbao+qui91YdqKwjNmROfWciIiua8KECXjppZfQq1cvFBcXY9euXVixYgWmT5+O0aNHo6SkBOHh4ejVqxdkWYaXlxd2796NOXPmID8/H2FhYXj77bfRp08fAMBDDz2EnTt3YsCAASgoKMDSpUvRtWvXKpcnNDQUP/74Iz744APcdddd8PX1xYMPPogXX3wRAK75+hkZGUhMTMTSpUuRnZ2N4OBgPP7443jkkUfsUnfXIwkuzwnAdLuI2lwyPDlXi6dXnYW7vgi/dlUDcS1r7dxkTZIkhIaGIiUlhavN2hHr2XFY1zWTm5sLb2/vKh3L20U4Rk3qubKfp0qlqvLtItiNZSdyuZYdwW4sIiIip2E3lp1YzcZiNxYRETnZl19+if/85z8293Xu3Bk//fSTg0vkOAw7dmI1G0uwiZSIiJzrkUcewb333mtzn6urq4NL41gMO3ZSNhuLiwoSEZHz+fn5wc/Pz9nFcAqO2bET82wsIckw6tmNRURE5CwMO3Yil1tIysiWHSIih+EMNroaw46dyOVq1sgBykREDqNUKlFQUMDQUw9otdpauVM6x+zYiaLcD8dg5C8cEZGjeHh4oKSkBHl5edc9Vq1WQ6vVOqBUt7YbrWdJkuDp6Vnj12fYsRO5XBA1GtiNRUTkSC4uLnBxcbnmMVy80THqQj2zG8tOrMfssBuLiIjIWRh27KR8yw67sYiIiJyHYcdOJEmCXNpcZ2TYISIichqGHTuSYQ477MYiIiJyFoYdO5JhGphsMLBlh4iIyFkYduzIXLmcjUVEROQ8DDt2ZOnG4pRGIiIip2HYsSMFOECZiIjI2Rh27MjcsmPgvbGIiIichmHHjixjdtiNRURE5DQMO3akkMwtOww7REREzsKwY0ecjUVEROR8DDt2xNlYREREzsewY0eK0vtjsRuLiIjIeRh27Ejm1HMiIiKnY9ixI/Odz9mNRURE5DwMO3akKP1/A8MOERGR0zDs2JGlZYfdWERERE7DsGNHZYsKOrUYREREtzSGHTvibCwiIiLnY9ixIw5QJiIicj6GHTsqCzvOLQcREdGtjGHHjsxhh7OxiIiInIdhx47MU8/ZskNEROQ8DDt2JEumph2O2SEiInIepbMLcLX169dj1apV0Gg0iIiIwNixYxEbG2vzWL1ejxUrVuCvv/5CVlYWGjZsiIceeght27Z1bKErwTE7REREzlenWnZ27NiBhQsXYvjw4ZgxYwYiIiIwbdo05OTk2Dx+0aJF2LRpE8aMGYOZM2firrvuwieffIJz5845uOS2MewQERE5X50KO6tXr0bfvn3Ru3dvhIeHY/z48VCr1diyZYvN4//++28MGzYMt912G4KDg3H33XejXbt2WLVqlYNLbptcmnaMRicXhIiI6BZWZ7qx9Ho9zp49i6FDh1q2ybKM+Ph4nDp1yuZzdDod1Gq11Ta1Wo2TJ09W+jo6nQ46nc7yWJIkuLm5Wf5dm5Tm2Vh2ODeVMdct69i+WM+Ow7p2DNazY9SFeq4zYSc3NxdGoxG+vr5W2319fZGcnGzzOW3atMHq1avRvHlzBAcHIyEhAXv27IHxGk0py5cvx7JlyyyPo6KiMGPGDAQGBtbKdZTn4nIMyAMkWYHQ0NBaPz9ZCwkJcXYRbgmsZ8dhXTsG69kxnFnPdSbs3IgxY8bgm2++wUsvvQRJkhAcHIxevXpV2u0FAMOGDcOgQYMsj81JMyMjA3q9vlbLZ9DrAKih0+uRkpJSq+emMpIkISQkBKmpqRCc+WY3rGfHYV07BuvZMexVz0qlssoNFXUm7Hh7e0OWZWg0GqvtGo2mQmtP+edMnDgRWq0W+fn58PPzw88//4zg4OBKX0elUkGlUtncV9tvdvMAZb2o/XNTRUII1rMDsJ4dh3XtGKxnx3BmPdeZAcpKpRLR0dFISEiwbDMajUhISECTJk2u+Vy1Wg1/f38YDAbs3r0bHTp0sHdxq8QyQJm/Q0RERE5TZ1p2AGDQoEH4+uuvER0djdjYWKxduxYlJSXo1asXAOCrr76Cv78/Ro8eDQA4ffo0srKyEBkZiaysLCxduhRCCAwZMsSJV1FGlkxZkpOxiIiInKdOhZ2uXbsiNzcXS5YsgUajQWRkJCZNmmTpxsrMzLQaza3T6bBo0SKkp6fD1dUV7dq1w3PPPQcPDw8nXYE1BVt2iIiInK5OhR0A6N+/P/r3729z39SpU60et2jRAp9//rkDSnVj2I1FRETkfHVmzE59pChthTKAazgQERE5C8OOHckKtuwQERE5G8OOHXGAMhERkfMx7NiRecyOQbAbi4iIyFkYduxIIbNlh4iIyNkYduzIMhvLyeUgIiK6lTHs2JGsMFUvZ2MRERE5D8OOHbEbi4iIyPkYduxINocdDlAmIiJyGoYdO7LMxpJkCCPbd4iIiJyBYceOFKVjdoySBDDsEBEROQXDjh3JCgUAwAgZMBqcXBoiIqJbE8OOHVnG7EgMO0RERM7CsGNHZd1YMmBgNxYREZEzMOzYUVnLjsSWHSIiIidh2LEjRbnZWDAw7BARETkDw44dmcMOx+wQERE5D8OOHZVmHdNsLLbsEBEROQXDjh3JUrluLK6zQ0RE5BQMO3akMLfscIAyERGR0zDs2JG5ZcfIAcpEREROw7BjR+YxOwYOUCYiInIahh07ksvPxuKigkRERE7BsGNHljE74JgdIiIiZ2HYsaOyMTsKjtkhIiJyEoYdOyoLO2zZISIichaGHTtScIAyERGR0zHs2JFlBWUOUCYiInIahh07knlvLCIiIqdj2LEjhXnMDiQOUCYiInIShh074qKCREREzsewY0flu7EEW3aIiIicgmHHjqwGKPOu50RERE7BsGNH5jE77MYiIiJyHoYdOypr2eEAZSIiImdh2LGj8reL4JgdIiIi52DYsSPzCsoAYOSigkRERE7BsGNH5tlYAGDkmB0iIiKnYNixI7l8yw5nYxERETkFw44dmWdjAezGIiIichaGHTuSy4cdtuwQERE5hdLZBbja+vXrsWrVKmg0GkRERGDs2LGIjY2t9Pg1a9Zg48aNyMzMhLe3Nzp37ozRo0dDrVY7sNS2le/GMrBlh4iIyCnqVMvOjh07sHDhQgwfPhwzZsxAREQEpk2bhpycHJvH//PPP/jll18wYsQIfP7553jqqaewc+dO/Prrrw4uuW0cs0NEROR8dSrsrF69Gn379kXv3r0RHh6O8ePHQ61WY8uWLTaPP3nyJJo2bYru3bsjKCgIbdq0Qbdu3ZCYmOjgktsmSRJkCAAcs0NEROQsdaYbS6/X4+zZsxg6dKhlmyzLiI+Px6lTp2w+p2nTpvj777+RmJiI2NhYpKWl4eDBg7jjjjsqfR2dTgedTmd5LEkS3NzcLP+uTZIkQQEBIyQYjaLWz08m5npl/doX69lxWNeOwXp2jLpQz3Um7OTm5sJoNMLX19dqu6+vL5KTk20+p3v37sjNzcWUKVMAAAaDAXfddRfuu+++Sl9n+fLlWLZsmeVxVFQUZsyYgcDAwJpfhA0yEgAAaldXhIaG2uU1yCQkJMTZRbglsJ4dh3XtGKxnx3BmPdeZsHMjjh49iuXLl+OJJ55AXFwcUlNTMX/+fCxbtgzDhw+3+Zxhw4Zh0KBBlsfmpJmRkQG9Xl+r5ZMkCQpJAAIoyC9ASkpKrZ6fTCRJQkhICFJTUyGEcHZx6i3Ws+Owrh2D9ewY9qpnpVJZ5YaKOhN2vL29IcsyNBqN1XaNRlOhtcds8eLF6NGjB/r27QsAaNy4MYqLi/Hdd9/hvvvugyxXHJKkUqmgUqlsns8eb3ZzCYxGI3+Z7EwIwTp2ANaz47CuHYP17BjOrOc6M0BZqVQiOjoaCQkJlm1GoxEJCQlo0qSJzeeUlJRU6AO0FXCcyTwji1PPiYiInKPOtOwAwKBBg/D1118jOjoasbGxWLt2LUpKStCrVy8AwFdffQV/f3+MHj0aANC+fXusWbMGUVFRlm6sxYsXo3379nUm9JhvBmrktwYiIiKnqFNhp2vXrsjNzcWSJUug0WgQGRmJSZMmWbqxMjMzrVpy7r//fkiShEWLFiErKwve3t5o3749HnzwQSddQUWWbiwDww4REZEz1KmwAwD9+/dH//79be6bOnWq1WOFQoERI0ZgxIgRDijZjTG37BjYskNEROQUdaOvpx4rCzscs0NEROQMDDt2Zh6gzG4sIiIi52DYsTPznc+NzDpEREROwbBjZ5bZWEw7RERETsGwY2eK0pYdjtkhIiJyDoYdOzMv98OGHSIiIudg2LEzc8sOu7GIiIicg2HHzixhh+vsEBEROQXDjp2Zu7E485yIiMg5GHbsjC07REREzsWwY2eyzDE7REREzsSwY2cK2Tz1XLrOkURERGQPDDt2pigdtGMEW3aIiIicgWHHzsoWFXRyQYiIiG5RDDt2Zhmzw7BDRETkFAw7dqZg2CEiInIqhh07s4zZYdghIiJyCoYdO7N0Yzm5HERERLcqhh07UyhMVcwBykRERM7BsGNnZVPPiYiIyBkYduysbDYWFxUkIiJyBoYdO1MqFAAAIyQII9t3iIiIHI1hx87k0m4sgyQDDDtEREQOx7BjZ+YBykZJAowGJ5eGiIjo1sOwY2dlYUdm2CEiInIChh07U5TvxjKwG4uIiMjRGHbsTDa37IAtO0RERM6grMmTMzMzkZmZiWbNmlm2nT9/HqtXr4ZOp0O3bt3QqVOnGhfyZqY0Tz2XJMDAsENERORoNWrZmTdvHpYuXWp5rNFo8O6772L37t04fvw4PvvsM+zevbvGhbyZyZIp7Ji6sRh2iIiIHK1GYefMmTOIj4+3PN62bRu0Wi0++eQTfPPNN4iPj8eqVatqXMibmWVRQUkGBMfsEBEROVqNwk5+fj58fHwsj/fv348WLVogJCQEsiyjU6dOuHz5co0LeTNTWoUd3iCLiIjI0WoUdry9vZGRkQEAKCgowOnTp9GmTRvLfqPRCOMtvpCeuRuLYYeIiMg5ajRAOT4+HuvWrYO7uzuOHj0KIYTVgORLly6hQYMGNS7kzcwSdiAx7BARETlBjcLO6NGjkZKSgh9//BFKpRKPPPIIgoKCAAA6nQ47d+5Et27daqWgNysFx+wQERE5VY3Cjq+vL95//30UFhZCrVZDqSw7nRACU6ZMQUBAQI0LeTMrXWbHNBuLLTtEREQOV6OwY+bu7l5hm1qtRmRkZG2c/qZmNWbHyLBDRETkaDUKO//++y/OnTuHwYMHW7Zt3rwZS5cuhV6vR7du3fDoo49a7vx9KzKHHQGwG4uIiMgJapRCli5divPnz1seJyUl4fvvv4e3tzdatGiBdevWYeXKlTUt401NKv1/IXGAMhERkTPUKOxcvnwZMTExlsfbtm2Dm5sb3nvvPbz88svo27cvtm3bVuNC3sxKG3YgILFlh4iIyAlqFHaKi4vh5uZmeXzo0CG0bdsWLi4uAIDY2FjLOjy3qrJuLLbsEBEROUONxuwEBATgzJkz6NOnD1JTU3Hx4kUMGjTIsj8/Px8qlara512/fj1WrVoFjUaDiIgIjB07FrGxsTaPnTp1Ko4dO1Zhe7t27fDmm29W+7Vrm7llx8huLCIiIqeoUdjp3r07li1bhqysLFy6dAkeHh7o2LGjZf/Zs2cRGhparXPu2LEDCxcuxPjx4xEXF4c1a9Zg2rRpmDVrltWtKcxeffVV6PV6y+O8vDy89tpruP3222/8wmqRxJYdIiIip6pRN9Z9992HoUOH4sqVKwgICMBrr70GDw8PAKZWnaNHj6JDhw7VOufq1avRt29f9O7dG+Hh4Rg/fjzUajW2bNli83hPT0/4+vpa/jty5AhcXFzQpUuXmlxarTFXsJAk4Ba/dQYREZEz1KhlR6FQ4MEHH8SDDz5YYZ+npye+//77ap1Pr9fj7NmzGDp0qGWbLMuIj4/HqVOnqnSOzZs3o2vXrnB1da3Wa9sLW3aIiIicq1YWFQRMg5UzMzMBmMby3EjYyM3NhdFohK+vr9V2X19fJCcnX/f5iYmJuHjxIp5++ulKj9HpdNDpdJbHkiRZBlmbg0ltkSQJsnk2liRBgqj116Cynxvr1r5Yz47DunYM1rNj1IV6rnHYSUxMxM8//4wTJ05Y7nAuyzKaNWuGhx9+2Gpqur1t3rwZjRs3rnQwMwAsX74cy5YtszyOiorCjBkzEBgYaJcySdmpAEw3AvX394drNccwUdWFhIQ4uwi3BNaz47CuHYP17BjOrOcahZ3Tp09j6tSpUCqV6NOnD8LCwgCY1t/Zvn073nnnHUydOvWa4aM8b29vyLIMjUZjtV2j0VRo7blacXExtm/fjlGjRl3zuGHDhlnNGDMnzYyMDKuBzrVBkqRySVbClYxMyCkptfoaZKrnkJAQpKamQrCr0G5Yz47DunYM1rNj2KuelUpllRsqahR2Fi1aBH9/f7z//vsVwsiIESMwZcoU/Prrr5gyZUrVCqNUIjo6GgkJCejUqRMAwGg0IiEhAf3797/mc3ft2gW9Xo877rjjmsepVKpKp8Pb480uS6YhykZJAowG/kLZkRCC9esArGfHYV07BuvZMZxZzzWajXX69GncddddNltdfH19ceedd+L06dPVOuegQYPw559/YuvWrbh06RLmzJmDkpIS9OrVCwDw1Vdf4ZdffqnwvM2bN6Njx47w8vK6kUuxG8tsLA5QJiIicooatexIkgSDwVDpfqPRWO0BSV27dkVubi6WLFkCjUaDyMhITJo0yRKoMjMzK5wzOTkZJ06cwOTJk6t9DXbHRQWJiIicqkZhp2nTptiwYQO6d+9eod8sMzMTGzduRLNmzap93v79+1fabTV16tQK2xo2bIglS5ZU+3UcQS4fzBh2iIiIHK5GYefBBx/EO++8g5deegmdOnWyrJacnJyMffv2QZZlm2vw3Ep4uwgiIiLnqlHYiYqKwvTp0/Hrr79i37590Gq1AAC1Wo22bdtixIgRdW4MjaNZ3wiUKygTERE5Wo3X2QkPD8drr70Go9GI3NxcAGVTyH/77TcsXrwYixcvrnFBb1bmTiwhyWzZISIicoJaW0FZluXrroVzKyq7XQTYskNEROQENZp6TtdnuV0EJMDIlh0iIiJHY9ixM/OYHQ5QJiIicg6GHTsrm3nOAcpERETOUO0xO2fPnq3ysVlZWdU9fb1jzjpGSYIQAry3LhERkWNVO+y8+eab9ihHvWU99ZzdWERERI5W7bDz9NNP26Mc9Za5G0twzA4REZFTVDvsmG/ISVVj3bJT+X3EiIiIyD44QNnOLIsKcuo5ERGRUzDs2JnEqedEREROxbBjZ1aLCnLqORERkcMx7NiZ5XYREtiyQ0RE5AQMO3YmsWWHiIjIqRh27MzqdhEcoExERORwDDt2VjYbS2Y3FhERkRMw7NhZ2To7Zf9LREREjsOwY2dWKyizG4uIiMjhGHbsjAOUiYiInIthx85kLipIRETkVAw7dmZu2QFbdoiIiJyCYcfOJLBlh4iIyJkYduzM6nYRHKBMRETkcAw7dma5XQS7sYiIiJyCYcfO5PJTz9mNRURE5HAMO3ZWtoIyW3aIiIicgWHHziSrqedOLgwREdEtiGHHzuSyueds2SEiInIChh07M4/Z4dRzIiIi52DYcRBOPSciInIOhh07s9z1XJLZjUVEROQEDDt2JlsN2WHLDhERkaMx7NiZVG6AspEtO0RERA7HsGNnVpOx2LBDRETkcAw7dlZ+6jm7sYiIiByPYcfOrFp2uKogERGRwzHs2JmMci07zDpEREQOx7BjZ+VbdoxGDlAmIiJyNIYdO7PqxjLonVcQIiKiW5TS2QW42vr167Fq1SpoNBpERERg7NixiI2NrfT4goIC/Prrr9izZw/y8/MRGBiIxx57DLfddpsDS105qwHKBrbsEBEROVqdCjs7duzAwoULMX78eMTFxWHNmjWYNm0aZs2aBR8fnwrH6/V6fPDBB/D29sYrr7wCf39/ZGZmwt3d3Qmlt40tO0RERM5Vp8LO6tWr0bdvX/Tu3RsAMH78eBw4cABbtmzB0KFDKxy/efNm5Ofn4/3334dSabqUoKAgRxb5uqRyA5SNbNkhIiJyuDoTdvR6Pc6ePWsVamRZRnx8PE6dOmXzOfv370dcXBzmzp2Lffv2wdvbG926dcPQoUMhy7aHI+l0Ouh0OstjSZLg5uZm+XdtkiTpqttFGGv9Najs58a6tS/Ws+Owrh2D9ewYdaGe60zYyc3NhdFohK+vr9V2X19fJCcn23xOWloaMjIy0L17d7z55ptITU3FnDlzYDAYMGLECJvPWb58OZYtW2Z5HBUVhRkzZiAwMLDWrqUyaqUSoaGhdn+dW1VISIizi3BLYD07DuvaMVjPjuHMeq4zYedGCCHg7e2NCRMmQJZlREdHIysrCytXrqw07AwbNgyDBg2yPDYnzYyMDOj1tTumRpIkhISEQIaAERKKC4uQkpJSq69BZfWcmpoKwcWM7Ib17Disa8dgPTuGvepZqVRWuaGizoQdb29vyLIMjUZjtV2j0VRo7THz9fWFUqm06rIKCwuDRqOBXq+3jOMpT6VSQaVS2Tyfvd/sRqOBv1B2JIRg/ToA69lxWNeOwXp2DGfWc51ZZ0epVCI6OhoJCQmWbUajEQkJCWjSpInN5zRt2hSpqalWi/WlpKTAz8/PZtBxFnMlC4PBqeUgIiK6FdWZsAMAgwYNwp9//omtW7fi0qVLmDNnDkpKStCrVy8AwFdffYVffvnFcvzdd9+N/Px8LFiwAMnJyThw4ACWL1+Ofv36OekKbDOPyWLYISIicry60/wBoGvXrsjNzcWSJUug0WgQGRmJSZMmWbqxMjMzrUZzBwQE4K233sIPP/yA1157Df7+/rjnnntsTlN3JnOJebsIIiIix6tTYQcA+vfvj/79+9vcN3Xq1ArbmjRpgmnTptm5VDUjSwAE19khIiJyhjrVjVVfmdfaYcsOERGR4zHsOIBlgDLDDhERkcMx7DiAeZyR0cipjURERI7GsOMAZd1YDDtERESOxrDjALKlZYdTz4mIiByNYccB2LJDRETkPAw7DmBp2eFy5ERERA7HsOMA5pYdzsYiIiJyPIYdByg/G4s3myMiInIshh0HkEubdoySBLB1h4iIyKEYdhzAMmZHkgHOyCIiInIohh0HsIzZgQTwzudEREQOxbDjAOZuLANbdoiIiByOYccByrqxJIB3PiciInIohh0HMIcdAbbsEBERORrDjgNYVlCWOGaHiIjI0Rh2HEDibCwiIiKnYdhxAKvZWAw7REREDsWw4wAcoExEROQ8DDsOUDZmh91YREREjsaw4wCWsMNFBYmIiByOYccBeLsIIiIi52HYcQCp/NRz3giUiIjIoRh2HKBsUUHOxiIiInI0hh0HsBqgzNlYREREDsWw4wCcjUVEROQ8DDsOYBmgzNlYREREDsew4wBW98Ziyw4REZFDMew4gAROPSciInIWhh0HKFtUkAOUiYiIHI1hxwHk0rQjJAmCLTtEREQOxbDjAOZKNt0IlGGHiIjIkRh2HMCqG4stO0RERA7FsOMAZffGYssOERGRozHsOIBktaggBygTERE5EsOOA1gtKshuLCIiIodi2HEARWnLjmA3FhERkcMx7DiAJHFRQSIiImdh2HEA69lYHLNDRETkSAw7DmA1G0uvd3JpiIiIbi0MOw4glb8RqE7n3MIQERHdYpTOLoAt69evx6pVq6DRaBAREYGxY8ciNjbW5rFbt27F7NmzrbapVCr8/PPPjihqlSjKd2PpSpxbGCIioltMnQs7O3bswMKFCzF+/HjExcVhzZo1mDZtGmbNmgUfHx+bz3Fzc8MXX3zh4JJWnbJ00I5BVgC6YieXhoiI6NZS57qxVq9ejb59+6J3794IDw/H+PHjoVarsWXLlkqfI0kSfH19rf6rS8xhRy8p2LJDRETkYHWqZUev1+Ps2bMYOnSoZZssy4iPj8epU6cqfV5xcTGeeeYZCCEQFRWFBx98EI0aNbJ5rE6ng67cuBlJkuDm5mb5d20yn0+lMGVKvawAdLpaf51bnbk+Wa/2xXp2HNa1Y7CeHaMu1HOdCju5ubkwGo0VWmZ8fX2RnJxs8zkNGzbE008/jYiICBQWFmLlypWYPHkyZs6ciQYNGlQ4fvny5Vi2bJnlcVRUFGbMmIHAwMBavZby/H19AKRBLyngqpAREBpqt9e6lYWEhDi7CLcE1rPjsK4dg/XsGM6s5zoVdm5EkyZN0KRJE6vHL7/8MjZt2oQHHnigwvHDhg3DoEGDLI/NSTMjIwP6Wp4WLkkSQkJCUJifBwDQyUoU5+YiJSWlVl/nVmeu59TUVAghnF2ceov17Disa8dgPTuGvepZqVRWuaGiToUdb29vyLIMjUZjtV2j0VR5HI5SqURUVBRSU1Nt7lepVFCpVDb32evNriwdGaWXFBBaLX+p7EQIwbp1ANaz47CuHYP17BjOrOc6NUBZqVQiOjoaCQkJlm1GoxEJCQlWrTfXYjQakZSUBD8/P3sVs9osA5RlBaDTOrk0REREt5Y61bIDAIMGDcLXX3+N6OhoxMbGYu3atSgpKUGvXr0AAF999RX8/f0xevRoAMCyZcsQFxeHkJAQFBQUYOXKlcjIyEDfvn2deBXWymZjKRl2iIiIHKzOhZ2uXbsiNzcXS5YsgUajQWRkJCZNmmTpxsrMzLQa0Z2fn49vv/0WGo0GHh4eiI6OxgcffIDw8HAnXUFFKkW5lh0tww4REZEj1bmwAwD9+/dH//79be6bOnWq1ePHH38cjz/+uP0LVQNW6+zoGXaIiIgcqU6N2amvrMbsFBVxIBwREZEDMew4QFnYUQJFBUDKRSeXiIiI6NbBsOMAqtKwo1ObVmoWp485szhERES3FIYdB1CWDlA2KErX90m97MTSEBER3VoYdhzA3I2lkxUAAJHGsENEROQoDDsOYBmzY67u1EtOLA0REdGthWHHAcrCTun6QBmpEElnnVgiIiKiWwfDjgNYwo4REArT0kbG91+CSLd9J3ciIiKqPQw7DmCejQUAhsg4y7+Nbz0Fw1cfcN0dIiIiO2LYcQDzbCwAMD72EqBWl+08vIezs4iIiOyIYccBlOVbdvyDIH84x2q/OHbQ0UUiIiK6ZTDsOEC5hh3ojQKSty/kZ9+ybBNHGXaIiIjshWHHASRJKltrx2ganyO17Qz57S9MB/y7D+LsSRj/2QRx5oSziklERFQv1cm7ntdHSlmC3iigN5YbjBwWAXj7ArkaGD98DQAgACCqCeQ3P4EkSTDu2Azx67eQX3wHUmwLZxSdiIjopsaWHQdRldZ0+bAjyTKkPoMqHnzuFMT/foA4fhhi/iyguAjGebNg3P0XRFamYwpMRERUT7Blx0HK1tqxnmYuDxwJ0TgGxjWLgYJ8y+rKYsNvEBt+KzswIxVizmcQXj6mVp/AEKvziKJCQKWCpFTZ90KIiIhuMgw7DlJZ2AEAKb49FPHtAQAi9RKMU18ADHrbJ8rLgXHZAiiefgMiNxtw9wR0OhjfHA8EBEMxeabdroGIiOhmxLDjIOa1dvSGay8gKIWEQ35jBsTZk5C8fQFZhnHJPOBKetlBB3bA8MojQF4O4BcAKb49UJAHFORBFORD8vC045UQERHdXBh2HOTq2VjXIkXGQSq30rKk1ULMnQl4+UBq2Q5i11ZT0AGA7EyIbRvKnpxyEYhtDgAwblkLKTAEUqvbau06iIiIbjYMOw5yrW6s65E69wQkCVJ4FNCwEdCsDcSCL2wea/z4DdPxRiPEnm0QAOT//gZJWfajFgV5gJs7JFlxQ9dCRER0M2HYcRBVTcKOJJkCjPlxt74Qfv4wLp4LJCdZHyyEqeWnvAuJQEwzAIBx1xaI+V9A6nAHpPH/V+2yEBER3Ww49dxBatKyY4vUoh3kd76E/OkPkAaMAJrGA0rb2dX420KIjFTT1PUfvy5t9fkLojC/2jchFWdPwvDuixDHDkFosiBOH6uNyyEiIrIbtuw4iGWAsrH2zinJMuDjB2nYIwAAkZ4CcWQvpFbtAVmCce7nwNmTwKkEGCc9WeH5xhdHAw2CII8cC7TpDElx/W4t4zczgOxMGD9/G4iIBS4kQn7hbUjxHWrvwoiIiGoRw46D1KQbq6qkoFBIdw62PJafmwzjrHeApLOVP+lKOoz//cj078AQoKgA0sBRgI8f8O9+wMsHyMkC3D0h3fsAkF1uUcMLiQAA4+bVUDg47IhL5wEX1wrrDREREV2NYcdBarsbqyokLx8opsyCMBiA4iKIY4cAXQmkZq1hfH1cxSdkpAIAxOI5FfcBEFvW2H6hsydhXPc/06DnZvGAhxfE+v9B6nkPpKDQWrqa0jIU5gNZmTC++wIAQJ4802rmGlFNCSFMY+GCw6wG9hPRzYu/yQ5imXp+nXV27EFSKAAPT0gdu1u2ydO+hVj+I6Se/SFyNaYVmlf8dGMvUFgA8dsPAErv7VVKbFwBqfdASE1bAa3aQ3JxNW2/kgHxz0agIB/SgBGQfP3LnqMtAU4lAC3aVpgtJoxGGD9+E7h8wbLN+MErkOesurFyE9kg/t4A8eNsSN36Qnr8RWcXh4hqAcOOgzijZedapKBQSBMmmv5t3jhwpGlaulFA/PqtKaA0CAQiYoDiYhjfehLQaiuezMUNKCmy+TpiyxpTi5CHl2nhQxv75eemAH7+EGnJwLFDEP9sAlq2g/z4C0BuDlBcCKlJKxg/sQ46lnOcOgoRHAwhBIReV+GWGeLcKRj/8z6k3gMh3/vAdetGaEtM9aJ2ue6xlZ4jPxcoLgJ8/CGpeAuPm4lYucj0/9v/BBh2iOoFhh0HMYedy7laPLXyDO5t6o+BTf2cXKqKJA8v0/8/+Zr1Dld3yJ8sgFjxM+DtAym+A8SebZAGjwZUaoh5n0Ps/sv03K59IQ7tAgoLyp5vI+iYGb96v+LGowdhnPKMKTBch+HjN3Dp47LH8hsfA67uEKsXQep7L4w/zQbyciBW/gLRsTvEqkUQly9AfuZNSEENIYwGQKcDsjIB/wDT67q4QJ48C1CrIRZ8CRiNkMa8aDpOWwzJywcATF2EuhJIru7W1/T2s5aFH+VnJkFq1+W611EdIj0ZMBghhYbX3jmFgFg2H2LfP5BHjIXUofv1n3QTEoUFkNw9Kj9A5iRVovpGEtWde1xPZWRkQKfT1eo5JUlCaGgoUlJS8N3eVKw+mW21//eHmtXq6zmT0GlNrS6NYyDJsilAnD1pmrGVfQVIToLxh/+YZn+NGAM0ioLY+48pFCUccF7BJQmo7FdAqYLUvqslxKF5G+D4YdO/Xd0gDR4Nsf0PICMF8tOTTCtXnz0JBDeE8Y0nrE4lf/g9kH0FUlwLAKUtP1mZkBpHWx0nCvOBvFygMB+IjIMkSdb7L52DSDwBsfxHoLgQ8jtfQmrYuGy/wVClWXW2iGMHYfz8HdODFu1MYdDFFeLMCcDXH5KrGxrGNUVKSkq1lyyoK8ShXTB+PR3SqCcglxvMX57hjScst2dRfL/SkcWzKP+342at65sB69kx7FXPKpUKgYGBVSsDw46JvcPOr4cz8Ou/mVb761PYuVFCpzWNFQoOg9SmE8TevyH5B0Ds3wmRkgQkXzR90/bxgzx8DNCuC8Qfv0Ps3gbp7qFAZhpwIRGKK+nQX2vWWR0idbwDIjkJuHwB0qAHgIwUiBP/AgFBwJkT1gc3bwPpjn6QOnSDJEkwTHkaSL1ctl+lhvzKe0BMc9NYk5++gTzhNcAvAGjYCJKrO8SFM4BKBSiUpi7CxtGQO94BoDSkFhcBkCB2boZYOq9cQSVId9xddjsSH3+EzfkNKSsWmX5ezdvYvD4hBCCMkGSFqUtQkq/ZlScKCyB2/wXp9t6QXN2s9xXkwzh/FuQuvWqlpcnwzHBAZ+qKrSzIGCY9aRms7+iwI9KTIU4dhdytLxqGhfND2M4YduxPZzBCpZDRsGFDp4YddmM5iJcLb81gi6RSQxoxtuxx6bdt6baulT/n7mHA3cPKHpf+wUpOToZx43LgwhmgdUdIDQIhEg5AahoPxDSD2LfdNPg5NBxS644Qe/4GAoIhfvwKMBjKXqBxDJB0puxx6XpCtUXs/bvs36sXle3Iyap48PHDEMcPQ3wHwD8QyMqw3q/TwjjjDatNxm9mXLcMxssXTN2P5kHpbh7AVa1MEML6vms5WUgZNxTGXI3psdoFUr/7IMU0M7VqaUsgtqyF+HsjkJsNacBIiN1bgfw8SH3vhfhrHaT23SCNnmBpsRIZqWVrQJ0/DTzyDADJ1AXo6gaxbhlweA+Mh/dAbt0R4uAuUwhTuwB+DWBctQjy4AcBWQFxIRFShzuAy+cBDy9IwQ0rXriouNCVSL0M4/8WQO7ZH6Ig3xJ0gCp0eVWByEwD/AOqdHsW43svAyVFEJIEjHi0Rq97MzHu/RtITzFNWLiqNfNGJWlKsOtSHoY084eL8tbomrxSqMOXu1LRM9IbfaJ9bvg8Qoha+TnkFuvxyP9Mfzu/HO6CSNcan/KGsWWnlL1bdv46l4PPtidb7WfLTu2o6bczoS0xtR6lXAKCQiG5uMK4cwtw7CCknv0hxbaAyEiFOLADUq8BwOljpm4oSYLU8Q7Th2NWBqDXAS5uEDlZEJvXAImlq0vHtQCquNK01KE7pF4DIP7dC7FhOaBQAgZ9xeO69IY4f8q6lae2+AWYBpyXH3NVm4LDAHcP4NypivvKD3b38Qd8fK+9TlRl3Nwhj3kJ4thBiD3bABc3SO26QGxebTlE6ncfpNYdYPxkUqWnkQaMBIoKII7shTz2JaCoCCLtsmnc1l3DAIUMsX8HpNBGQHikqQtXrwf0OohF30Mc3m0Ke/c+AHnwaAi9DtDrIbm6md6rmWlA6iXTulYGA4zTXzW9btvOaPjmR0jNzYMoKbbMZDQTRoNpzJZKZRo3lpECKaTm47dEVqbpfd40HlKjqJqdS68HCvMhefte+ziDAcanTF9e5P/7AGgQZLV+ls4g8O3eVLQN9UD3CO8qv/6Qn02tpKPiG2B0a9vf/m/0b4cwGICCvOte29XySgxwUUpQK2ovfOVrDfBQyZAkCW//mYTDqYUAbH++ZBTosPtSHu6M8YVrJQHwzzMazD2QjodaB8JNJSM+2B0qWcKh1ALcEeENhWwdgi5oSpBRoEOHMM8K51pzMhvf7UsDADQP8cKMO8PZjeVs9g47B5LzMXXzRav9DDu1oy42RQshgPQUU3iSJIjiIsBoNI0POnvC9KGuUADunoCHJ8SxQ5A63mFzvI3ITIPYsdl0vEoF6fa+kLy8IfJyTM8Lj4Lxl/8Cp44CLq5ASbHtQoU2gjRwJJCZBrFlrSlwpFyscJj83BSgRVsYv/sEUMiQe/QDZAWMn002HeDlA+m22yESj9ucHUdVIMk2W5kq1eo2SM3bQGxaCQQGA5os05ivjt0hzp40vdfCIyFFNzW9B3z9Ta2V+bmmGZSxzSGFNoI4fRRISzb9DBvHQGRnQgqPNL0HN/1uapUzu60rcOk8pMEPmlrSImLxz+8bIQwG3HFff1O36JY1kNp1gRQRC3F4D8S/+yB16Q0EN4RYMg9i1xYgphnkIQ8BcS2Bf/cB4ZGAXwPThABtCZCbjfTZM+FXkguFjy+Qkw1p3MuQu/QGAKw7lY1v9po+MFeMirW+qXHSGSAjFcZNvwNnTkAa8yLkrn0BlIWd9g09MKVrkKlr1dXNFBQTjwMRcZBdXREaGorL63+HyM+FdHsf0++rXg+x6XdIbTpajYkDAJGTbaqrjcshv/QupBZtrfbvupiH5FwthrXwt2odyS8x4LHfEhHmpcaXg6oeJM9nF6NYL9As0A2aYj3Wn9Kgq58RBQmHYWjTCZO3pGBYC3+Mbh2A4YvKvkDMvy8WH/51Ccl5Wgxu5g8JwIZEDTIL9RjY1A9Pdgi2ep0SvRFqhYTXNlzA6Stlf0MauCsR7eeKvZfzMbCJL8Z3CIYkSdAZBGbvScXms6aJGDPujoBSlvDHGQ2CPFUY1NQPmxJzLGGna1QDvNEtiGHH2ewddk5nFuH/1p+32s+wUzvqYthxFiEEoNdBUqnLHhfkQfK0/Y1YXDoHFBQAPr4Qa5YAbh6QRoy1PcYmMw0eR/ejILKJqWsPprWPxMblEKsWQeozyHTrEp3WFMzOJwLePhBH9kEc2g2pURSkbndC/LvP1KXo6gqpdSfTuKCE/RDbNkDyLV2CwBwOM1KBiBhIfQdB/P6L6T5wkgSxdL6p1aBLr7Ib3/r4ATnWkwCgVAL6ii1jNjVvA+m22yG1aAcEhkD8NNu6G+8WVyyrMLrHNADAT39PgTv01t2/N+iwXyzebfMkvLX5+O/uGXAzmJZ++CewDbJdvHHFxRsrG5luhPzb1olAw8amAfwaU7evACAgQTav8uXiBvg1wH3NTAuPdkc6Xtn3HZCfi5K4eMz1vA0dk/bgNnUBFG07o+T0SbgkHjE9N6oJ5KfegHHx98CBnQAA+bUPAVc3U7jbuhZi61oAgF6SIQkBZac7IDJSUXjHPUhOy8HEEtMkhP9r74fu2cdwcc8+fBw2ECHeLtiXYfqMWRpfAJW2CMYta4C8XEj974PUIAgntu+D0dcf6oxkHDZ6o5d3CZ5WdIPWIPBK11DM3JFSaT2+06sh3t1a1nswOG0XVgbbngXqpwbu8cqHR2QU+jfxx/YLudc8d3lqhYT7WvjDUzJizpGy37dwN+BSucmz7V0KsL+krAu4m6cWrw9tw7DjbPYOO4VaAx7732mUlFtUkGGndjDsOIYz6rkqYweETgtosixdH8JoKG05EWXdSrIE8edqiGOHID/xf6bupz3bAJ0ecHOzfKO/+rVxcJfpQXikaeZdThYQ1xJSeBTEgR2mQBYUCmSmQ6xdAgCQ7hoChISZlnFoEg+xdC7Ezi1W55a63wUoVUBhPoQmyzSWDDBti21uWqrh4jkgINh0Gxe9zrorVO1iahUBTAtwNmxsaq0z6AGV2jIIGwCuqL2xoWEX9E/eBX9tbqX1qJWVmNzuacQUZ2DC0UUV9mtUnhjb7W0AwJwd78NfmwcB4MfoAVAa9Rh9fmOF51TF220mIMEvBgDQKTMBbyQshABwfy/TehJNc87jpE8kgNKwU44A8Ga7Z6GTlfh4/5dQlAYenaTAqJ4fAgD6puzBsyeXAQBWNOqBhTGDAACBxVm47cpJbGzYGVMPf494zRlcz8bQzljRuCfuv7AZC2IGoaXmLN44uhAAMKv5A9gWfJvl2J6p+/HiicX4rMVobA9qa3WexxNX4Y70g3DXl8DFaPrc0UsyRvb8qIq1VlG/yzuxIez2aj+vcXEGklyrFhhqokvGv3jzmcGm924tYdi5AfYOO0IIfLTtEnZezLfsXz66KeRaGox3K2PYcQzW8/XVZGCnKCkBFApcKjDgUq4W93euOM1fGEu7plQupnFmBj0k94pjJURRoWlMVFgEXt+WjpO5As29JXzYwR3w9jUtodA4BuLMCUhNWgFX0rBD2RAf7zRNuV8xpCE2HzyPYndvDBCXgMBQpJw4haczTd0vs90TEIpiXOpwF57fZvp2/6H6GJrf0Qni6CFIYY0hrqRDCgwxhbm8HNNNir19TSFRq4XUewByEs/g8RjrW9e8HaPFiaNnsMS1OQDAU1eAfJWpheDXXe8hReWDRj4uUFw6C42HP8Z2NA3QH6A/j2EhRuzW+aDhv9vwXuOySQzBRVcw5chcvNzxFejkivNyAouz8G3GCqzQhQDCiCEXt+Hqn+KugFb4uFXFQeMR+SlQGXVI9Lbu7vIvyUH/yzvxS3T/Cs8pz11fhHGnVyKyIBn/1+Hlax5bFa2zT+OIX927hU7fxh54sUdjtuw4myPCzn92peCPMzmW/UtGNbllZgnYEz+EHYP17BjmsSbfPNAODZXFNa5r8/kA4D8Do/D+1ku4K8YHvaN9EOhR1l25MVGDr3ebZqIte6Aphi86CQD4bkg0gj3V+O3YFfxw0DQbcNaASET5uWJTogZflT5HrZDQyEeNqX0aw7t09umJjCI09FbDQ2X6O7fo30w0D3TDbQ1NAW3hwXT875iNWYiVuLeZH1adyMbDbQIwrEUDnMwowqQ/km60aqy0b+iB/cmmQfneSqBjAwk9WjTED/tSML59MObsT8OZvGqMs7IDGQLG0hgW6aPC+ZyKn1k/BCbisYzYKp1vdIN87C90xckiJR7GWQzqFI3dqSX4PKn2Wl/MfnmsEzwNeZx6fiu4evR7iUHAhT8Bu7tSqMPlXC1ah9RsCrGj6Y0CElBh9sONEkIgKUeLcG+15ZwGo6i189cneSUGGIwCGxM1UMgSVp/MxohWDTCgieNWPT+ZloeGYbV7q5Hn15wDAPx8JBO/HMnEmz3C0LmRadX0En3ZB3l6QdmH6P+tO48vBkZZgg4AFJceaw4HAKA1CJzJKsEjy04jyEOJ7hHe+K00yAR7qvBAfACWJFwBALxxRxgW/ZuJ85qSapV/1QlTK9JPhzORkFaIQ6Uzj2pD+WvJ1QN/pgn8mWaa7fjm1hTE+rsCqGTw/3W0b+iBCF8XS31cz50xPpYvxq2C3JCQbhoM80znUKw4noV8rQEf9ouE1iDwwupzyCkxjZ26r4U/fNsNws9aA9af0qBLI098uO0ycksMCPVS4WSmdfnbdWiJe33USLxSjPjgppAkCb3igIW/JeJKkR4+rgoIAeSWGOCllvHfwTF4eNnpCuVt6KVCcp7pPdPYR40JtzXAW1vKxgC9f2djxAV5IiWl8pX07Y0ftQ5UIezojUAV1t8xCoEVx7LQPMgNzQPdr3s8WRu3/AwEgPf7NrppAo+mSI9nVp1Fq2B3vNkjrFbWvFh/WoNv9qZZZmKczCzC5D+S8HCbQAxp7l/p84p0RmxPykXncC/U7j3sry+vxIDkPC2aBrhd/+AqOp5eiJ+OZGLcbUGI9i+b0q0zCHy3LxVtQzzw8T/JFZ737d60aoUdIQS0BlHl1luDUeDb0llHQPVCrvnbcvn3iXnNlUqfA2D+wXS0DfWAi1JGoa4s7JzMLBtpmqc14oud1oNXFx3JxAOtA7D7ku0Pr/QCvdUHe1q+Dn+fLxsv9NHfNV8yobaCjiwBVbllYWJWxaDTNMDNqq4qo1bIeKxdEDo09KxSS9TzXULxfJdQaA1GFOuMeOy3RHioZNwZ44Mekd4wCAF3lQLuKuD+lg0w70A62jf0wKNtTa0cnmoFhrdqAAD4qnTm14ZETYWw08jHBW4qucLfxY/7R+BoWiG6l041P55eCDeVDC8XBR5qE4CfD5ctkNs7yhtPdwqBziDgoTZNgdcajABM75mF98fC18359wdk2HEgt6v+6GmreAf0nUl5+OGQ6VtVfRnUnJavhabYUKsfYpUx1/Lh1MI6E3YS0grR0FsNf7eKv4InMorw+kbTlO7dl/KxMTEH/eJ8seG0BqtOZmFKr3AEe6qr/ZoLDprGY6w5mY3RrQMwcYPpNeYdSL9m2Jm7Pw2bzuRgU0AO5jRqiAUH0tEpzAPNg8qCd4HWgPWnNege4WWzbIU6A77bm4Y7IrzR3sZ6HJV5ce05XCnU490+jdA29Po/O3PrhDlgGIWALEkwGAWK9UZ4qBV4Y5Ppw+az7cn4+t6yhRTXn87GxsQcbEzMqXjiUqevFMFdpUCYd+X1rzMIyJLp/PuTCzD73ig0cL/+H/sDyQXYkKixPFZe5x5dJzKK0MhHDQ+1AgsOZuDPszn4pF8Efj2Sid2X8lCsv/7fl5Q8Hdaf1mBIc39kF5XNWrs63By+KlgcSi3EoVRTPUoo+x27lgMpdlq3yYbBzfzQLtQD7265VGGfn5vScq0SgPcGtsC2E8nYWK7uKxPkoUTbUA/Le+TtXuEo0BnQwF2FredyEOqpRpMAV7yxMQlFeiMu55oGipszaMtgd3zcLwKXckpwLKMId8f6ItRTZVl472pqhQy1QsZ3Q2LgopAgSRJclNYheEhzf3QO90SQp8rmlyLztnY2fn/cVLbfYwHuKvSMKluUsPzv+v0tTCFq+4U8vNw1FJF+pi8M5Xsp1AoZU/s0gt4g4ONaN2JG3SjFVdavX49Vq1ZBo9EgIiICY8eORWzs9fsgt2/fji+++AIdOnTAxIkTr3u8o7mqrN+I5ZuNryWzsOyPUG2tbOlsT/5uWijuq0FRaORT1j+sMwicyy5GjL9rne9e2ZiowfJjVzClVyM0rOTDTwiBredyEeHrYmlFOJJagCl/XoS7SsavI5sgPV+HnRfzcHesL9xUMiZtsl675o8zGrQL9cDsPaZv6T8eysCr3cOsjtEU6bHmVDbujPGBUQA+rgosP5aFYxlFuC3UA/c284NU7mPpt6NXrJ6flq+tNED9VfqN/ERmEX7ZdxG/HbuC345dsQre3+xNw7bzufjjTA7+Ozi6wjkW/3sFW87lYsu53GoF9iul7/1dF/MqDTtJmhL4uirgrlbgmVVnoZQlzL43GgdTCvDZ9mQ83i4IF3NKsPZUNqb0bmR53qVcrdXv09ns63dRTNxwAV5qBb4fGgMXpQyjEMgtMcBNKSNPa8Cak9n47VgW4hq4WtYq+XDbZXSP8ELfaF94uSigNwpcyilBY18XnMkqhptKxsztyTiTZd2lozcaAchYdSIL/1zIw1u9wpGkKcG57GLM2W8Kru4q6xaZz7YnW62Rci3mFo0DKQXoHO6Jdac1VXre1R5uE4gfD5d1cfWJ9sGlnBKcqmI5zO5v4Y9CnRG+bkp0DPPEW5tMgaEq/q9bQ2w6o8GR0lA25rYgFOuNiPV3hSQBYd5qbD2XizG3BSI+2AOTNiXhzhgfDG3eAG3iQtDS24jbQj3QMsgNLkoZiVeKbbbANAlwQ9fG3paw46GW4VnaOn9njK/luE/7R8AggPt/NY17auBe9lHbNMANTQPc0Lfc8bc38rRMXrkrpuLKx+XHVtkS4nX9Lz/BnmqM7xCE7/eZ3jsPtwm47nNsUcgSRrYKwMhW136+rXDlTHUu7OzYsQMLFy7E+PHjERcXhzVr1mDatGmYNWsWfHwqX/46PT0dP/74I5o3b+7A0lbP1S07+dprr1Gx5mQ2sor0Vt8i80oM8HZQUr6cq8X3+9IwslUDtAiyT/fZycwiq7Dzzd5U/HEmB4+1DcR9LRtU+jxR+iHjzG8N5sGc3+9Lwzt9Gtk85nBqIWaVfks2f8jvu2z6o2b+kHpz0wVkFuqRUajDE+2DcXWDX3qBDi+uPWd5nKct+wAo0Rux62IefjmSidR8nWVMRJSfC85lmz48E9IKsT0pF+Uz8tWDQt/+8yK+HRJj8xrKjyc8mWa72+KfC6ZAlJyntbn/Uo71B3mRzoi0fK3lW+H1yLJkaaW5+rzPrzkHd5WMj+6OsHwx0BTr8f5W07f6JQmZlu1XL+z594U8eLkoKmyvjFEAOSUGjFx8CotHNcG6U9lYUG4si1n5wHH6SjFOXynG/AMZ+OiuxliccAUHq9DK8emfp3H7g80swebtP5MsP1Oz8kHn6tc1kyXg3qZ+yNMasPlsWVfSPU38sOZkNg6lFGDCSturVHuoZBToKg8c4d5q3NvMDwEeSuy6mI9hLfzRNMANOcV6PFpJa4XZvU39sOpkNnxcFFg4vOLsobnDYjB6qWl8yPNdQvCfXaloF+pRoe6e7BCMHpHelveg6ZoluKsU+OyeSACmvxdDmvkjwtcFClnCLyPioJAlS9BVyBJub+xleX7LYHc82SEYF3NKML5DMJYkZGJjYg4ebB2AMC81nu4UjAB32y0pgKk1RSkBr3QNxeZzudcNBs90DoUkpSLK1wXDWlTeylpTg5r6o5GPC4I8VAitQkCqT+pc2Fm9ejX69u2L3r1Nq2eOHz8eBw4cwJYtWzB06FCbzzEajfjPf/6DkSNH4vjx4ygocFxzaXW4XtVkmHeNsCOEsKw8eW+zsnECaQU6eLsq8W9aAQLc7fuG/fjvyzivKcHBlIJa7T4r1JVd99VdeeZBef87duWaYWfegXSsPJGNN3uE4fbG1gvmpeRpMXt3Ku5r2cAh3y7MP8czWcVYeyobj7QNhG9pCDt1paw/39yKoC83QOBERpHlg3j/5QIYjRXHWGiKrd8nh1IKMOTnExh7WxDmHUi3WaarPxTPZJVArai8pSw1X4dpf11Cn2gf3N7Iq9LjDJUMbii/+YOtF9Es0B3Dy/38dFc9z9w1BgBv9ghDl2u8JmAK/geS8/HFgCi4KGXklRiw4ngWjmeYvskX6oxYeLCsLq6Uaw0t3zJ6tUX/ZlZ6Tdcz+Y+kKrfOmpm70Kpqy7myLrWrf6ZVMe3Oxmga4AaVwrSybfmwE+fvinvifK/ZovNUp5AKt7kxG9rcH/e3bAAXpYxeUT7oVa7bw8dViQZuSlwpqrzumwa4YWBTv0q7UjzUCkzqEQZZktAx3BPRfq4I9VLjgSWmVYKVMjBnaCz8SruCWwa5Y/elfJvnkiTJanxWVVqNBzYt+7v7YOtAPFjulhP946o2dqtnlI9Vd1BlvF0UeP2OsOseVxva1JGufEerU/Oe9Xo9zp49i/j4eMs2WZYRHx+PU6ds3Een1LJly+Dt7Y0+ffo4opg37OqWnaUJV1CkM6JIZ4TeKFCoM2DKn0n4z64Uqw+H8n+4dyTlISVPi8l/XMRTK8+iWG/Enkt5mPD7Gcsf/tpi7m++ngKtAbnFZWW81odHWr4WDy4pG82vq2Tcku91WmxWls7K+OFgOrQGIzadSLO0lM3cnowjaYWYuvniDX+QmV3O1V53AKL5Gt75Mwl/nMnBl+XGO5R//U1ncnBBU4I1pzSWbeaxOQCgUkhW+66nsqBTmeuNEdtzKR8fbbuM01cqv96tiWUDE/88o8E/F3KxLMG6S2zv5QL8eCgDJXqjJdiW/zkbjMISdADgy122V27VXPVBmZKnw55L+cgo0OH341lYdvQKjqaXlXVfudk05i6/67mcq0Vq/o0tOXH6SjGScqr2O3KjvrJRN32jfTD9rsY2jjZp4K7EfS388cN9sWgV7A5VacjtHeWDR9qWfWCrlRImdAy2eY7Bzfzw+0PN0CPS9srbTQPcMOa2IMsUc1s+vScSU3qF4/N7ImFrjLa/mxKhXupr/q53buSFjuGmMV7R/q5wU8mY0iscTRq4YtaAKEvQAUzh5JE2gZhZ2ppDVF6datnJzc2F0WiEr6+v1XZfX18kJ9v+dnHixAls3rwZH3/8cZVeQ6fTWa2nI0kS3NzcLP+uTebzmf8/toH1YNxz2SWYuOECkvNKcPUXxLhyxxaU67Y4kVGEVsFlyfxgSgE+2maa2TBrRwq+GxoLoxCY8kcS/k0rxPNdQnFXrO8Nld9Qrv/i6roxGAWWJGSiRZA7Zmy7hHytEYtHNcX8A2nYkZSH/wyKtvpDZHb1h7nWWDZmonwwuJSrxakrxdcdwCxBwtTNF5GQVoj7WzbAY+2CrMYKaI3lr6HidfybWoAfDqVDhoTBzf0R6+8KnVGgoZcaSTkleHHNOcgS8P3QWPx+PAtNA9yQXqCzap3SGwWMoqx7aX9ygen+OkLggqbsw/Dr3amI8K18/YqaBrPaciy9CKFeLijQGuDvXvmfiGvN9AGAp1edxZVCPe6I8LYK79+Um3EEmN7fM/6+jB1Jpi6yFkFuCPZUY8vZigOFP62kleFqV7eCVHXGjS2Ptg3EwkNlXVU9I70t45huVNfGXgj1UuN/V42dqop+cX5oFuiGd/o0wrubL0IC8HTnEMwu7VadeU+Uzd89pULCiFYBWHEsC3laA1oGeUCWZbzRIwwfbbuMu2N9MaipH3ZezMN9LRpYflem39UYvx7JROKVYoR5qxHbwBWj4gOu+/eygbvKMjD72yGxuJhTgtYhHvjrXA7OZZegRbD7Df3N7RjuhY7hFVsCVQoJI+KrNw7l6r/RZB91oZ7rVNiprqKiIvznP//BhAkT4O1dtbvhLl++HMuWLbM8joqKwowZM6q8MNGNCAkxLWMfCmDpOH88s/ggMvJNH4JJObabpv9b7ptpfrkvuNklAoczyzZolWVjaYQkIzQ0FEnZhfg3zbSQ2H92peDRO25sHJNRHLf8OzTUetLxhuOp+PVIptW2Zafysb60SXx7qh7ju1Ycx6JQW3+AKVzcLee+rLFuUXht/XksHdcZkf4eWJWQggW7LuCde5qjdZgPAFPZLudpcbl0nMiGxBzcFhVifX6PsubmQqGC5OGHEO+y5uxhv2yxhIwTV02H9VArSusBGLe88vEHQpJRqLL+46t18cHKf5Ox86L1GJcL11hX5NJVLWmv9o3Dqn9TcDK9rGl+6bjOcFUqcO+3Oyo9T3mdI/yw+0LZ/Wsa+brhYrl63v1qbxRqDZi+8QQ2nTC1FM07kG5pNfJzV1Xogqoqc4vk3xesg0H5WS8dG/thb1K2JegAprB1LP3603mr49FOETh8OQcHL5le+4nbIzFn53nL/qn3NMfnWxORU1SxleeJni1QgET879BlxAV64sU7W+CvObss+6cNaom3Vh+1PP5ieBu8uOyw1Tl83VTQlDv3+0PawttVhTuaZeKl/x1BgIcamQWmn39jPzckZVtf//+e6IL75+yCm0qBjk0bw9NFiUGhQPvYcBgFEOzlgjyjGt1jGqBFo2t3sax8KhAFWj2CvUy/B/eFhKBVZENENfCAWinj9hbWx4eGAne1rdoidZUJBdC69N8PhzWs0bnswfw3muzLmfVcp8KOt7c3ZFmGRqOx2q7RaCq09gBAWloaMjIyMGPGDMs283oTDzzwAGbNmlWhcocNG4ZBgwZZHpuTZkZGBvRVvWFgFUmShJCQEKSmplrK5QKgX4wPfjpccVBjZc5kljXPp+QW4/d/y5q2k9LKBpp6qiSkpKTg8lWzSpKTkytN1JdySuChVlh9EzQKUWHw9MXLyVDKEn48lI7UPJ3NqbdLD5aFBU1OLlJSKjbBZ+daj6dKuZJjOW6/jTU7Jq04jNYhHpZvwON+2Y8vB9q+Y3B+id7qQwcARszbbfn3mqOpWHM0FUsfaIp/LuTieEbRNVtTCq4zgNzsck4xHlm412rb8LllH4YhnqpKu0q81ArcHedr8xt+j1AlWvuF4tHSRbwifF3gUpILUQJM6hmOJf9mIiVfa9XyBwDv9G6EAyn5iPZzRZ9oH6B7MHZfykd2kR7dI7zx+obzuJSrxX0tGiAt1RSqn+/QABGeEubss251yS6s3VXFy3NTyhjaxAt7k7Kvf3AVvNUzHO3DPJFZqMO6U9lYXm4QdoiLHt06NMCMwmLc28wfvaPdsCZBhZQ8HUa2aoDbGgBzhkRDloBdF/PxyT+m9/KY24KguZKOEU084IZA9I32gbLYNOPs03+SMbCpH+J9jfjv4Gi8tv487m3mjyhXLdqEeOBwatl7/YUuIWgd4o6FBzPQpZEXCrIzUQAg2g34rH8kPNQKPLXSdG+md3uHYcs5U/dgoc6AfrG+UBXn4PuhMdAZBPKyMlD+N0UGkFEEjGjqAaDY5u+dLSnlhrd4A7iSYXu8S31m62801T571bNSqbw5V1BWKpWIjo5GQkICOnXqBMA0+DghIQH9+1e8v0jDhg3x6aefWm1btGgRiouL8fjjjyMgoGKTpkqlgsrWHZ0Bu73ZhRBW576toUe1ws61XM4taynwclEgo0CLlKtmxBTpjDYHAWaXLlynVkhY+kBTy/b/7k7FpjMaq2NfXXcOn/aPxNLS8RltQq49O8sobNdnsd46QKTkmab/aorKZs+UZ57JUt4La85VOK46RpQug1+brtX4MePuCHzw1yXLddzXwt+y4NrjtwWiZ6RPhbBzd6wPhBDwcVHg60FRWPRvJka0CrDUaedwT3QO90SJ3oiz2cV4Y2PZwNcgDyWeaG89FqNzeNnaNuXXlin/Mwq/xtoxZoNahWB7Ygayi6sWBK+lbag7Iv2su/XahLhXWNPF7O1e4SgxGLEhMQcFWgOGNfe3LP4X4eOCTqXXGOyhwuOlC7i9VTp92FutQJCHymp2zgd3Nsa5rBJ0DPeEEMIygLt7hBdub9QUWUV6BLgrIYSAm0rGiNIB10KYujlnljtXQy81fhoeZ+m+fKtnGFLytHhx7XkAgEICVLKEce2DLM8xi21gamF5pWsoPNUK+LspMbxlAzx0exMs2nUK/eN8IYRAUOn0Y34o176r/0aTfTiznutU2AGAQYMG4euvv0Z0dDRiY2Oxdu1alJSUoFevXgCAr776Cv7+/hg9ejTUajUaN7YeqOfhYRrPcvX2uiTG3xUz74nEK+vO1/hch1IKy/27AOOWV7xz75msYrQKdofBKPDLkUz4uylxJqsYiaUfvlqDQF6JAV6lgw032Fhc62x2Cc6UW0G0sg8ks/KzVJI0JQjwUMJdpaiw0Nm/aYVYcCC9SouS1XVjbwtCtwivCj8DXzclJvcKR3bpMgJqhYyujb1wLL0IfaJ9IEsSpt3Z2PLBDADjO5SFlXAflwrr6pi5KGU0D3S3mpLrqb7+qty2xJWbreLjosDbvRvh/9afBwAEuisR5KnG6PaNEeoq8P2+NIR7q+HtYmoV7BHpjQ+3lbXsqWQJT3QIwpJ/r+BKkR5BHkqkF1i3nI5uEwhPtQLPdwnBDwcz8GDrAPSP88WwX8rCaPmpz21CPaCUJXQtP/vOxkrHZi2DysZ7hdoIcgHuKgRUstifQpauu7bJ1cq3nrooZUT6uaJzuCcu5pSgeeD1F8+8etaOv4caw1o04IcwUS2oc2Gna9euyM3NxZIlS6DRaBAZGYlJkyZZurEyMzPrxWCyGH9X9Ij0xrbSgY6Rvi7ILtJjbPsgfL6jas3QAKq06NZbfyRh2QNN8OG2y1b3fynvcq4WzQLdkHWNqaKvbbhQ6b6rrTqZjftaNkBanhZvbEqCh1qu0N1itvx4WXdDtJ8LzpYOLv3wrsb4fl+a5bEjeKhlPNQ60DLt35bRrQOwIVFjGZPycb8INPJRw11lChnv922EKX+a1m2Z2N00PsHXVWk16ySugZvVIPRWwe7oF+uLDYkaqGQJakX1JkpO6RWOF0oHU3u73ljY8XRRoEWgGxKzivF6jzBLiwNg+iB+tF0QQoM84aH3Q6inCk0D3SzBqvyAbQD48O7GiGvghiYN3PC/Y1fwcJtA7Lucb1kzZmATXzQuXV/pzhhfqwXZ3ukdjo//TsZTnYIR7eeKV9efR784XyhtTBc23xjy0XYVm7IlScLH/SJQojfaXKnaESb1DLe5PhARORbvel7KEXc9v9qJjCK8sfECgj1VmH1vtNXNGe/79dpdLRE+LvBQyziWUbWBnOVvLGfL/S388Wi7ICSkFVq1MNTEwKZ+OJJagIvVmJ7bK8obW8+ZAuDcYTHwc1Xikf+drjQomTUJ8sSpcgN5+8f54rF2gZi7P93mdbcN9cCh0paQaD8XfNo/EkZhmlmlVkg26/+9vo2w+2IeHmsXhH/TCvHT4Qw80ykETWzMGNMZhGXKb1UV6gxYlnAFd0R6I6qKi+2VVxs3DtUZjCjSC8uU4mdWncXlXC0+vycSMQ3cKn0/l3/PfjEgstLFAtPytTiVWYyO4Z4V7hVXXvmAcK26FEIgp8Rw3aUKbja8w7xjsJ4dw171zLue3ySaBbrhk/4R8HFRWn1AKWQJ3SO88M8F6wG7vaO88WTHYChKPwS2nMupcti5+gP/6vvZ/O9YFpYfz7KMKbhRLgoJJaVrqqw5Wf2Bp+0belrCjp+rqV5mD4rGpD+SIKHijCUAmH9fLNafL7YKO55qBdxVCjzfJRS3N/LCpdwSXMzR4o8zOQjzVuOVrqFYeCgDfaN90DzQDZIkQQFYPlQ/7heBiRsuoIGbEr2jfXBnjA9CvdSWBbk6hHmiwzXu8VTdoAMA7ioFHm134/Vvq+WjulQKGapyDUMf3x2BzELddVc6VsgSnu0cghK98ZrHBnuqq3Rfr/ItIdeqS0mS6l3QIaLax78SThbXwHZf/lMdQ+DnpsSd0T6lC9Fl46E2gZauEsD2vUdm3xuNZ1bZXvq9vNhy9+5RypJlrRjzfVNs6dDQA8cyiuCqlC3dXT8Oj0N+iQFPrzoLWQJ+HdkEWoOwrHJq1tDLdGO5/BIDVtkIQUOa+eHuWF+EeauRkOaLUC+VJQD6uinx9aAo5GuNeLh0ZpK/m9JShgbuKjzRNRQNVHokaUpwPKMIg8utOm0OJrnFesT4u6JnpDc81KYgVJmmAW74z8Aoy1ijW5Wni8Jy75/rufsG13MiIrI3hp06ystFYZlRE+nnanPJ8WBPNbo19kJKnha9onwQ6qVCmLcaK0Y3xaVcLXKKDZYuqUhfF6gUkiXg9Iv1xYAmppkjuy7lY08ly6zH+LviTFYxhjX3xyNtA1FiMOKfC3n4bm8aHm4bAG8XBbxdFPh+SAxUCgkKWYKbLOHhNgH46XDZOjyzSpf5B4CsIj22J+XhsbaB2HQmB093Cra6G/kznSuuxSBJEjzUZd0eD7cJwO5L+WhZes8uXzcV+sf5XbOJ1NtViQFNqrbMOwA0vsYCgEREdPNg2LnJTbRxPxVJktDIxwWNfEw3n8wtMcBLLUOSJKTla3E8owg9Ir0tXQUJNhZwC/FU4eE2gegY7ontF3LRLcIbClmCu6zA3bG+6BvtY9X1FuRpPXOlf5yfJey0b+hhCToA8Eq3hniigwH+bspr3v/qauW7NpoFulvdNZiIiKgyDDu3gPL3r7E1ZuKB+AY4nlGIAU38MLiZPwxGYRVkbIWK6w2C9XJR4MfhcSjSVRw8qpSlG54d892QaGiKDTYXNSQiIrKFYYcQ7KnGN4NjLI9rMpunPHMXV20yhbVaPSUREdVzdequ50RERES1jWGHiIiI6jWGHSIiIqrXGHaIiIioXmPYISIionqNYYeIiIjqNYYdIiIiqtcYdoiIiKheY9ghIiKieo1hh4iIiOo1hh0iIiKq1xh2iIiIqF5j2CEiIqJ6jWGHiIiI6jWlswtQVyiV9qsKe56byrCeHYP17Disa8dgPTtGbddzdc4nCSFErb46ERERUR3Cbiw7Kioqwuuvv46ioiJnF6VeYz07BuvZcVjXjsF6doy6UM8MO3YkhMC5c+fAxjP7Yj07BuvZcVjXjsF6doy6UM8MO0RERFSvMewQERFRvcawY0cqlQrDhw+HSqVydlHqNdazY7CeHYd17RisZ8eoC/XM2VhERERUr7Flh4iIiOo1hh0iIiKq1xh2iIiIqF5j2CEiIqJ6jTcEsZP169dj1apV0Gg0iIiIwNixYxEbG+vsYt00li9fjj179uDy5ctQq9Vo0qQJHn74YTRs2NByjFarxcKFC7Fjxw7odDq0adMGTzzxBHx9fS3HZGZm4vvvv8fRo0fh6uqKnj17YvTo0VAoFE64qrpvxYoV+OWXXzBgwAA8/vjjAFjPtSUrKws//fQTDh06hJKSEoSEhOCZZ55BTEwMANPCa0uWLMGff/6JgoICNGvWDE888QRCQ0Mt58jPz8e8efOwf/9+SJKEzp07Y8yYMXB1dXXWZdU5RqMRS5Yswd9//w2NRgN/f3/07NkT999/PyRJAsC6vhHHjh3DypUrce7cOWRnZ+PVV19Fp06dLPtrq04vXLiAuXPn4syZM/D29kb//v0xZMiQGpefLTt2sGPHDixcuBDDhw/HjBkzEBERgWnTpiEnJ8fZRbtpHDt2DP369cO0adMwefJkGAwGfPDBByguLrYc88MPP2D//v145ZVX8O677yI7OxufffaZZb/RaMSHH34IvV6PDz74AM8++yy2bt2KxYsXO+OS6rzExERs2rQJERERVttZzzWXn5+PKVOmQKlUYtKkSfj888/x6KOPwsPDw3LM77//jnXr1mH8+PGYPn06XFxcMG3aNGi1WssxX375JS5evIjJkyfjjTfewPHjx/Htt98645LqrBUrVmDTpk0YN24cPv/8czz00ENYuXIl1q1bZzmGdV19JSUliIyMxLhx42zur406LSwsxAcffICAgAB89NFHePjhh7F06VL88ccfNb8AQbXuzTffFHPmzLE8NhgM4sknnxTLly93XqFucjk5OWLEiBHi6NGjQgghCgoKxAMPPCB27txpOebSpUtixIgR4uTJk0IIIQ4cOCBGjhwpsrOzLcds2LBBPProo0Kn0zm0/HVdUVGReOGFF8Thw4fFO++8I+bPny+EYD3Xlp9++klMmTKl0v1Go1GMHz9e/P7775ZtBQUFYvTo0eKff/4RQghx8eJFMWLECJGYmGg55uDBg2LkyJHiypUr9iv8TebDDz8Us2fPttr2ySefiC+++EIIwbquDSNGjBC7d++2PK6tOt2wYYN4/PHHrf5u/PTTT+LFF1+scZnZslPL9Ho9zp49i/j4eMs2WZYRHx+PU6dOObFkN7fCwkIAgKenJwDg7NmzMBgMVvUcFhaGgIAASz2fOnUKjRs3tupuadu2LYqKinDx4kXHFf4mMGfOHLRr1w6tW7e22s56rh379u1DdHQ0Zs6ciSeeeAITJ060+raanp4OjUZjVf/u7u6IjY21qmcPDw9LtxcAxMfHQ5IkJCYmOu5i6rgmTZogISEBycnJAIDz58/j5MmTaNeuHQDWtT3UVp2eOnUKzZs3h1JZNsKmTZs2SE5ORn5+fo3KyDE7tSw3NxdGo9HqDz8A+Pr6Wn75qHqMRiMWLFiApk2bonHjxgAAjUYDpVJp1Q0AAD4+PtBoNJZjrv45+Pj4WPaRyfbt23Hu3Dl8+OGHFfaxnmtHeno6Nm3ahIEDB2LYsGE4c+YM5s+fD6VSiV69elnqyVxvZlfXs7e3t9V+hUIBT09P1nM5Q4cORVFREV5++WXIsgyj0YgHHngAd9xxBwCwru2gtupUo9EgKCjI6hjz3xaNRmP5snsjGHaozps7dy4uXryI9957z9lFqXcyMzOxYMECTJ48GWq12tnFqbeMRiNiYmIwevRoAEBUVBSSkpKwadMm9OrVy7mFq2d27tyJf/75By+88AIaNWqE8+fPY8GCBfDz82Nd38IYdmqZt7c3ZFmukP5tfful65s7dy4OHDiAd999Fw0aNLBs9/X1hV6vR0FBgVWrQ05OjqWefX19KzQ5mweJ82dhcvbsWeTk5OD111+3bDMajTh+/DjWr1+Pt956i/VcC/z8/BAeHm61LTw8HLt37wZQVk85OTnw8/OzHJOTk4PIyEjLMbm5uVbnMBgMyM/PZz2X89NPP2HIkCHo1q0bAKBx48bIyMjAihUr0KtXL9a1HdRWnfr6+tr87Cz/GjeKY3ZqmVKpRHR0NBISEizbjEYjEhIS0KRJEyeW7OYihMDcuXOxZ88evP322xWaNqOjo6FQKPDvv/9atiUnJyMzM9NSz02aNEFSUpLVLLgjR47Azc2twgfPrSo+Ph6ffvopPv74Y8t/MTEx6N69u+XfrOeaa9q0aYVu7OTkZAQGBgIAgoKC4Ovra1XPhYWFSExMtKrngoICnD171nJMQkIChBBc1qKckpISyLL1R5ssyxClt4FkXde+2qrTJk2a4Pjx49Dr9ZZjjhw5goYNG9aoCwtgy45dDBo0CF9//TWio6MRGxuLtWvXoqSkhE2o1TB37lz8888/mDhxItzc3Czp3t3dHWq1Gu7u7ujTpw8WLlwIT09PuLu7Y968eWjSpInll6tNmzYIDw/HV199hYceeggajQaLFi1Cv379eJfjUm5ubpZxUGYuLi7w8vKybGc919zAgQMxZcoU/Pbbb+jatSsSExPx559/4sknnwQASJKEAQMG4LfffkNoaCiCgoKwaNEi+Pn5oWPHjgBMLUFt27bFt99+i/Hjx0Ov12PevHno2rUr/P39nXl5dUr79u3x22+/ISAgAOHh4Th//jxWr16N3r17A2Bd36ji4mKkpqZaHqenp+P8+fPw9PREQEBArdRp9+7dsXTpUnzzzTcYMmQILl68iHXr1uGxxx6rcfl513M7Wb9+PVauXAmNRoPIyEiMGTMGcXFxzi7WTWPkyJE2tz/zzDOW0Ghe7G779u3Q6/U2F7vLyMjAnDlzcPToUbi4uKBnz5546KGHuNjdNUydOhWRkZEVFhVkPdfM/v378csvvyA1NRVBQUEYOHAg7rzzTst+Uboo2x9//IHCwkI0a9YM48aNs1pIMz8/H3PnzrValG3s2LG37EJ3thQVFWHx4sXYs2cPcnJy4O/vj27dumH48OGWWT6s6+o7evQo3n333Qrbe/bsiWeffbbW6rT8ooJeXl7o378/hg4dWuPyM+wQERFRvcYxO0RERFSvMewQERFRvcawQ0RERPUaww4RERHVaww7REREVK8x7BAREVG9xrBDRERE9RrDDhHdkrZu3YqRI0fizJkzzi4KEdkZbxdBRHaxdetWzJ49u9L9H3zwQb26X9zevXvx2WefYcGCBXB1dcX8+fNx4cIFTJ061dlFI7rlMewQkV2NHDmywo1cASAkJMQJpbGf06dPo3Hjxpal70+dOoVWrVo5uVREBDDsEJGdtWvXDjExMc4uht2dOXPGcv87rVaL8+fPY9iwYU4uFREBDDtE5GTp6el47rnn8PDDD0OWZaxduxY5OTmIjY3FuHHjKtyVPSEhAUuWLMG5c+egUCjQokULjB49GuHh4VbHZWVlYfHixTh06BDy8vLg5+eHtm3bYsyYMZYbQgKATqfDDz/8gG3btkGr1aJ169aYMGECvL29r1v23Nxcy7/PnDmDDh06IDc3F2fOnIHBYEBwcDByc3Ph4uICFxeXGtYUEd0o3giUiOzCPGZnypQpiIiIsNonSRK8vLwAlIWdxo0bo6ioCHfffTd0Oh3Wrl0LWZbx6aefWu6wfuTIEXz44YcICgpC3759odVqsW7dOhiNRsyYMcPSXZaVlYU333wThYWF6Nu3L8LCwpCVlYVdu3bhgw8+gIeHh6V8UVFR8PDwQKdOnZCeno61a9eic+fOePnll697jSNHjqxSXQwfPrzKxxJR7WPLDhHZ1fvvv19hm0qlws8//2y1LTU1FV9++SX8/f0BAG3btsWkSZPw+++/47HHHgMA/PTTT/D09MS0adPg6ekJAOjYsSMmTpyIJUuW4LnnngMA/PLLL9BoNJg+fbpVF9qoUaNw9fc7T09PTJ48GZIkAQCEEFi3bh0KCwvh7u5+zWubPHkyAGDXrl3Yu3cvnn/+eQDAzz//DD8/PwwYMAAAEBwcXIWaIiJ7YdghIrsaN24cQkNDrbbJcsVVLzp27GgJOgAQGxuLuLg4HDx4EI899hiys7Nx/vx5DB482BJ0ACAiIgKtW7fGwYMHAQBGoxF79+5F+/btbY4VMocaszvvvNNqW/PmzbFmzRpkZGRUaJG6WuvWrQEAGzduRKtWrdC6dWsYjUakpqbinnvusewnIudi2CEiu4qNja3SAOWrA5F5286dOwEAGRkZAICGDRtWOC4sLAyHDx9GcXExiouLUVRUVGGsT2UCAgKsHnt4eAAACgoKrvm8/Px8GI1GAMCxY8dw3333ITc3F0lJSZbXz83NhVqttszQIiLnYNgholuarVYmABW6u672+uuvWwIYACxcuBALFy60PH7jjTcAAD179sSzzz5bCyUlohvFsENEdUJKSorNbYGBgQBg+f/k5OQKxyUnJ8PLywuurq5Qq9Vwc3NDUlKSXcv7/PPPQ6vVYu/evdi5cydeeOEFAMCiRYvg5eWFgQMHAoBV1xwROQdvF0FEdcLevXuRlZVleZyYmIjTp0+jbdu2AAA/Pz9ERkbir7/+supiSkpKwuHDh9GuXTsAppaajh07Yv/+/TZvBVFbE1CbNWuG1q1bo6ioCE2aNEHr1q3RunVrZGZmon379pbHV0+JJyLHY8sOEdnVwYMHcfny5QrbmzZtajVLKSQkBFOmTLGaeu7l5YUhQ4ZYjnn44Yfx4YcfYvLkyejduze0Wi3Wr18Pd3d3q6ndo0ePxpEjRzB16lT07dsX4eHhyM7Oxq5du/Dee+9ZxuXUhpMnT+LOO+8EAKSlpUGj0aBp06a1dn4iqjmGHSKyqyVLltjc/swzz1iFnR49ekCWZaxZswa5ubmIjY3F2LFj4efnZzmmdevWmDRpEpYsWYIlS5ZYFhV86KGHrG5J4e/vj+nTp2PRokX4559/UFRUBH9/f7Rt27ZWF/fTaDRIS0uzhJtTp07Bzc0NjRo1qrXXIKKa46KCRORU5VdQHjx4sLOLQ0T1EMfsEBERUb3GsENERET1GsMOERER1Wscs0NERET1Glt2iIiIqF5j2CEiIqJ6jWGHiIiI6jWGHSIiIqrXGHaIiIioXmPYISIionqNYYeIiIjqNYYdIiIiqtcYdoiIiKhe+3/CUus4HAV2JQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000    # training epochs\n",
        "temperature = 0.1\n",
        "\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "\n",
        "# count occurrences in each row, and make new columns\n",
        "def count_occurrences(row,element):\n",
        "    count = 0\n",
        "    for value in row:\n",
        "        if value == element:\n",
        "            count += 1\n",
        "    return count/20\n",
        "\n",
        "data_df['0occurrences'] = results_subset.apply(count_occurrences, args=(0,), axis=1)\n",
        "data_df['1occurrences'] = results_subset.apply(count_occurrences, args=(1,), axis=1)\n",
        "data_df['2occurrences'] = results_subset.apply(count_occurrences, args=(2,), axis=1)\n",
        "\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition4   # choose lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Get the Lab data (X) and probability distribution (Y)\n",
        "train_set = np.asarray(train_set).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6:9]    # probability distribution columns\n",
        "Y_train = Y_train.reshape(-1,3)\n",
        "\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6:9]\n",
        "Y_val = Y_val.reshape(-1,3)\n",
        "#print(X_train)  # example: output to see the possibility distribution for validation set\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(x)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "raahBxPnYI1Q"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0KweRiGk9SoV",
        "F-OQiTO_KaOV",
        "Q2Quhtsl9cU9",
        "JrEgpgcmMEWp",
        "gqOqgMkNmUzr",
        "Yxm_PyRkFQCm",
        "aZHdyLOfNdHT",
        "Xw3rR3HbKElf",
        "Pl0GYtnNXFJC",
        "2uYwG8d50qQf",
        "ok-LvipnKoD0",
        "oWLs2EQZi7OW",
        "4vqXNWwoTkqf",
        "9uHGpIjBAGC8",
        "0NraLzXmAPKk",
        "OsUDTN5OO9sH",
        "cNKobK2ePIbf",
        "Do9GnXSdPMkJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}