{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jianfeng0Liu/Color-Difference_Visually-Impaired-People/blob/main/Color_Difference_Visually_Impaired_People.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8XduP_K8BC2",
        "outputId": "c3e85aad-c4ee-4888-f248-803297298c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This is for google colab, which used for training now.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')    "
      ],
      "id": "v8XduP_K8BC2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KweRiGk9SoV"
      },
      "source": [
        "#Packages#"
      ],
      "id": "0KweRiGk9SoV"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W5ubR-gR6bV6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras import layers, Model, Input\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "id": "W5ubR-gR6bV6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-OQiTO_KaOV"
      },
      "source": [
        "#GPU Training Setting (Selected)#\n",
        "Setting to use GPU for Accelerating training.  \n",
        "  \n",
        "CPU training is also available, the program of this module is not required when using CPU training."
      ],
      "id": "F-OQiTO_KaOV"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F7Ki2VAWIqbn"
      },
      "outputs": [],
      "source": [
        "# Setting to use GPU for training\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.backend import set_session\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Assign which GPU for training\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.allow_soft_placement=True \n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.4 # 40% GPU memory for training\n",
        "config.gpu_options.allow_growth=True   # GPU memory not fully occupied during initialization, allocated on demand\n",
        "sess = tf.compat.v1.Session(config = config)\n",
        "set_session(sess)"
      ],
      "id": "F7Ki2VAWIqbn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Quhtsl9cU9"
      },
      "source": [
        "#1. Preliminary Try#\n",
        "Based on the preliminary study code (Given by Prof. Damien).\n",
        "\n",
        "**Changing Parameters:**  \n",
        "Layer: 10  \n",
        "Unit(nn): 256  \n",
        "Epochs: 1000  \n",
        "Learning Rate: 1e-4\n",
        "\n",
        "**Result**  \n",
        "Around Train_loss=0.03 and Validation_loss=0.04"
      ],
      "id": "Q2Quhtsl9cU9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsLDEkmiQdeD",
        "outputId": "a95e67d6-dd14-43b9-9834-a8f4b2830dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "80/80 [==============================] - 9s 12ms/step - loss: 0.6690 - val_loss: 0.4341\n",
            "Epoch 2/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.4106 - val_loss: 0.3608\n",
            "Epoch 3/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3920 - val_loss: 0.4178\n",
            "Epoch 4/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3722 - val_loss: 0.3690\n",
            "Epoch 5/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.3721 - val_loss: 0.3600\n",
            "Epoch 6/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3452 - val_loss: 0.3954\n",
            "Epoch 7/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3532 - val_loss: 0.4108\n",
            "Epoch 8/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3171 - val_loss: 0.3110\n",
            "Epoch 9/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3173 - val_loss: 0.3648\n",
            "Epoch 10/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.3061 - val_loss: 0.3195\n",
            "Epoch 11/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.3212 - val_loss: 0.3524\n",
            "Epoch 12/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.2963 - val_loss: 0.2918\n",
            "Epoch 13/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.2951 - val_loss: 0.2804\n",
            "Epoch 14/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.2852 - val_loss: 0.2969\n",
            "Epoch 15/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.2759 - val_loss: 0.2903\n",
            "Epoch 16/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2848 - val_loss: 0.2927\n",
            "Epoch 17/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2784 - val_loss: 0.2689\n",
            "Epoch 18/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2564 - val_loss: 0.2658\n",
            "Epoch 19/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2413 - val_loss: 0.2595\n",
            "Epoch 20/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2352 - val_loss: 0.2690\n",
            "Epoch 21/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2411 - val_loss: 0.3340\n",
            "Epoch 22/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2391 - val_loss: 0.2357\n",
            "Epoch 23/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2328 - val_loss: 0.2368\n",
            "Epoch 24/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2230 - val_loss: 0.2347\n",
            "Epoch 25/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2259 - val_loss: 0.2326\n",
            "Epoch 26/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2180 - val_loss: 0.2414\n",
            "Epoch 27/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.2075 - val_loss: 0.2327\n",
            "Epoch 28/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.2040 - val_loss: 0.2174\n",
            "Epoch 29/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1933 - val_loss: 0.2119\n",
            "Epoch 30/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1904 - val_loss: 0.2057\n",
            "Epoch 31/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1956 - val_loss: 0.1820\n",
            "Epoch 32/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1847 - val_loss: 0.1831\n",
            "Epoch 33/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.1752 - val_loss: 0.1953\n",
            "Epoch 34/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1730 - val_loss: 0.2006\n",
            "Epoch 35/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1999 - val_loss: 0.1978\n",
            "Epoch 36/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1814 - val_loss: 0.1703\n",
            "Epoch 37/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1699 - val_loss: 0.1790\n",
            "Epoch 38/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1648 - val_loss: 0.2024\n",
            "Epoch 39/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1760 - val_loss: 0.1873\n",
            "Epoch 40/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1706 - val_loss: 0.1978\n",
            "Epoch 41/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1599 - val_loss: 0.1794\n",
            "Epoch 42/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1445 - val_loss: 0.1879\n",
            "Epoch 43/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1552 - val_loss: 0.1566\n",
            "Epoch 44/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1466 - val_loss: 0.1845\n",
            "Epoch 45/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1551 - val_loss: 0.1662\n",
            "Epoch 46/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1480 - val_loss: 0.1795\n",
            "Epoch 47/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1370 - val_loss: 0.1692\n",
            "Epoch 48/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1470 - val_loss: 0.1587\n",
            "Epoch 49/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1387 - val_loss: 0.1640\n",
            "Epoch 50/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1380 - val_loss: 0.1557\n",
            "Epoch 51/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1327 - val_loss: 0.1581\n",
            "Epoch 52/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1481 - val_loss: 0.1667\n",
            "Epoch 53/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1525 - val_loss: 0.1569\n",
            "Epoch 54/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1350 - val_loss: 0.1543\n",
            "Epoch 55/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1282 - val_loss: 0.1492\n",
            "Epoch 56/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1141 - val_loss: 0.1533\n",
            "Epoch 57/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1291 - val_loss: 0.1451\n",
            "Epoch 58/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1225 - val_loss: 0.1413\n",
            "Epoch 59/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1200 - val_loss: 0.1523\n",
            "Epoch 60/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1367 - val_loss: 0.1363\n",
            "Epoch 61/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1231 - val_loss: 0.1452\n",
            "Epoch 62/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1170 - val_loss: 0.1438\n",
            "Epoch 63/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1284 - val_loss: 0.1390\n",
            "Epoch 64/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1092 - val_loss: 0.1245\n",
            "Epoch 65/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1242 - val_loss: 0.1437\n",
            "Epoch 66/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1305 - val_loss: 0.1464\n",
            "Epoch 67/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1314 - val_loss: 0.1316\n",
            "Epoch 68/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1459 - val_loss: 0.1396\n",
            "Epoch 69/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1282 - val_loss: 0.1294\n",
            "Epoch 70/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1315 - val_loss: 0.1305\n",
            "Epoch 71/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1165 - val_loss: 0.1276\n",
            "Epoch 72/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1051 - val_loss: 0.1232\n",
            "Epoch 73/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.1276\n",
            "Epoch 74/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.1511\n",
            "Epoch 75/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1338 - val_loss: 0.1467\n",
            "Epoch 76/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1418 - val_loss: 0.1433\n",
            "Epoch 77/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.1266 - val_loss: 0.1282\n",
            "Epoch 78/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.1115 - val_loss: 0.1295\n",
            "Epoch 79/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1054 - val_loss: 0.1399\n",
            "Epoch 80/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1201 - val_loss: 0.1219\n",
            "Epoch 81/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1159 - val_loss: 0.1211\n",
            "Epoch 82/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.1027\n",
            "Epoch 83/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.1229\n",
            "Epoch 84/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1146 - val_loss: 0.1085\n",
            "Epoch 85/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0965 - val_loss: 0.1137\n",
            "Epoch 86/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1019 - val_loss: 0.1182\n",
            "Epoch 87/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.1192\n",
            "Epoch 88/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1201 - val_loss: 0.1222\n",
            "Epoch 89/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.1083\n",
            "Epoch 90/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0887 - val_loss: 0.1221\n",
            "Epoch 91/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0907 - val_loss: 0.1037\n",
            "Epoch 92/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0865 - val_loss: 0.0955\n",
            "Epoch 93/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1119 - val_loss: 0.1151\n",
            "Epoch 94/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1018 - val_loss: 0.1025\n",
            "Epoch 95/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0888 - val_loss: 0.1168\n",
            "Epoch 96/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0859 - val_loss: 0.1193\n",
            "Epoch 97/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.1035 - val_loss: 0.1111\n",
            "Epoch 98/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0995 - val_loss: 0.1162\n",
            "Epoch 99/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0914 - val_loss: 0.0989\n",
            "Epoch 100/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1017 - val_loss: 0.0977\n",
            "Epoch 101/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0930 - val_loss: 0.1168\n",
            "Epoch 102/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0868 - val_loss: 0.1039\n",
            "Epoch 103/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0849 - val_loss: 0.0959\n",
            "Epoch 104/1000\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.0895 - val_loss: 0.0923\n",
            "Epoch 105/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0883 - val_loss: 0.0956\n",
            "Epoch 106/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.1067 - val_loss: 0.0993\n",
            "Epoch 107/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0793 - val_loss: 0.1124\n",
            "Epoch 108/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0918 - val_loss: 0.0946\n",
            "Epoch 109/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0942 - val_loss: 0.0998\n",
            "Epoch 110/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0919 - val_loss: 0.1021\n",
            "Epoch 111/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0805 - val_loss: 0.0956\n",
            "Epoch 112/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.0894\n",
            "Epoch 113/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0804 - val_loss: 0.1001\n",
            "Epoch 114/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0940 - val_loss: 0.0951\n",
            "Epoch 115/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0916 - val_loss: 0.1082\n",
            "Epoch 116/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0935 - val_loss: 0.0971\n",
            "Epoch 117/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0819 - val_loss: 0.1012\n",
            "Epoch 118/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0836 - val_loss: 0.0899\n",
            "Epoch 119/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0877 - val_loss: 0.1013\n",
            "Epoch 120/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0932 - val_loss: 0.1054\n",
            "Epoch 121/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0874 - val_loss: 0.0939\n",
            "Epoch 122/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0907 - val_loss: 0.0948\n",
            "Epoch 123/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0870 - val_loss: 0.1010\n",
            "Epoch 124/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0827 - val_loss: 0.1077\n",
            "Epoch 125/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0774 - val_loss: 0.0943\n",
            "Epoch 126/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0851 - val_loss: 0.1155\n",
            "Epoch 127/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0907 - val_loss: 0.1101\n",
            "Epoch 128/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0873 - val_loss: 0.1012\n",
            "Epoch 129/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0863 - val_loss: 0.0919\n",
            "Epoch 130/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0853 - val_loss: 0.1042\n",
            "Epoch 131/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0822 - val_loss: 0.1027\n",
            "Epoch 132/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0785 - val_loss: 0.1008\n",
            "Epoch 133/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0813 - val_loss: 0.0848\n",
            "Epoch 134/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0706 - val_loss: 0.0885\n",
            "Epoch 135/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.0825\n",
            "Epoch 136/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0774 - val_loss: 0.0940\n",
            "Epoch 137/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0800 - val_loss: 0.0902\n",
            "Epoch 138/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0719 - val_loss: 0.1010\n",
            "Epoch 139/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0849 - val_loss: 0.1031\n",
            "Epoch 140/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0861 - val_loss: 0.1009\n",
            "Epoch 141/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0841 - val_loss: 0.1124\n",
            "Epoch 142/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.1087\n",
            "Epoch 143/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0826 - val_loss: 0.1013\n",
            "Epoch 144/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.1028\n",
            "Epoch 145/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0745 - val_loss: 0.1002\n",
            "Epoch 146/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0767 - val_loss: 0.1060\n",
            "Epoch 147/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0703 - val_loss: 0.1033\n",
            "Epoch 148/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0792 - val_loss: 0.0949\n",
            "Epoch 149/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0842 - val_loss: 0.0979\n",
            "Epoch 150/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.0993\n",
            "Epoch 151/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.0911\n",
            "Epoch 152/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0762 - val_loss: 0.0840\n",
            "Epoch 153/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0808 - val_loss: 0.0968\n",
            "Epoch 154/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0788 - val_loss: 0.0987\n",
            "Epoch 155/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0820 - val_loss: 0.0854\n",
            "Epoch 156/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0751 - val_loss: 0.0831\n",
            "Epoch 157/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0661 - val_loss: 0.0874\n",
            "Epoch 158/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0750 - val_loss: 0.0857\n",
            "Epoch 159/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0683 - val_loss: 0.0806\n",
            "Epoch 160/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0653 - val_loss: 0.1048\n",
            "Epoch 161/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0740 - val_loss: 0.0813\n",
            "Epoch 162/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0810 - val_loss: 0.0882\n",
            "Epoch 163/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0834 - val_loss: 0.0944\n",
            "Epoch 164/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0810 - val_loss: 0.0903\n",
            "Epoch 165/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0714 - val_loss: 0.0776\n",
            "Epoch 166/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0744 - val_loss: 0.0827\n",
            "Epoch 167/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0642 - val_loss: 0.0679\n",
            "Epoch 168/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0658 - val_loss: 0.0967\n",
            "Epoch 169/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0697 - val_loss: 0.0878\n",
            "Epoch 170/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.0848\n",
            "Epoch 171/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0822 - val_loss: 0.0804\n",
            "Epoch 172/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0725 - val_loss: 0.0821\n",
            "Epoch 173/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0704 - val_loss: 0.0886\n",
            "Epoch 174/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0771 - val_loss: 0.1019\n",
            "Epoch 175/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0844 - val_loss: 0.0864\n",
            "Epoch 176/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0755 - val_loss: 0.0947\n",
            "Epoch 177/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0668 - val_loss: 0.0924\n",
            "Epoch 178/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0801 - val_loss: 0.1077\n",
            "Epoch 179/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0786 - val_loss: 0.0814\n",
            "Epoch 180/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0675 - val_loss: 0.0826\n",
            "Epoch 181/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0901 - val_loss: 0.0916\n",
            "Epoch 182/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0725 - val_loss: 0.0708\n",
            "Epoch 183/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0727 - val_loss: 0.0951\n",
            "Epoch 184/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0843 - val_loss: 0.0888\n",
            "Epoch 185/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0690 - val_loss: 0.0849\n",
            "Epoch 186/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0639 - val_loss: 0.0746\n",
            "Epoch 187/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0647 - val_loss: 0.0739\n",
            "Epoch 188/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0662 - val_loss: 0.0793\n",
            "Epoch 189/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0628 - val_loss: 0.0838\n",
            "Epoch 190/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0638 - val_loss: 0.0904\n",
            "Epoch 191/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0675 - val_loss: 0.0757\n",
            "Epoch 192/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0826 - val_loss: 0.0933\n",
            "Epoch 193/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0773 - val_loss: 0.0922\n",
            "Epoch 194/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0674 - val_loss: 0.0840\n",
            "Epoch 195/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0672 - val_loss: 0.0783\n",
            "Epoch 196/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0722 - val_loss: 0.0853\n",
            "Epoch 197/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0588 - val_loss: 0.0807\n",
            "Epoch 198/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0596 - val_loss: 0.0825\n",
            "Epoch 199/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0715 - val_loss: 0.1015\n",
            "Epoch 200/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0678 - val_loss: 0.0841\n",
            "Epoch 201/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0709 - val_loss: 0.1002\n",
            "Epoch 202/1000\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0725 - val_loss: 0.0857\n",
            "Epoch 203/1000\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0718 - val_loss: 0.0753\n",
            "Epoch 204/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0569 - val_loss: 0.0734\n",
            "Epoch 205/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0640 - val_loss: 0.0707\n",
            "Epoch 206/1000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0668 - val_loss: 0.0788\n",
            "Epoch 207/1000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0663 - val_loss: 0.0720\n",
            "Epoch 208/1000\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0690 - val_loss: 0.0812\n",
            "Epoch 209/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0691 - val_loss: 0.0745\n",
            "Epoch 210/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0752 - val_loss: 0.0772\n",
            "Epoch 211/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0654 - val_loss: 0.0776\n",
            "Epoch 212/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0704 - val_loss: 0.0647\n",
            "Epoch 213/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0591 - val_loss: 0.0751\n",
            "Epoch 214/1000\n",
            "80/80 [==============================] - 2s 18ms/step - loss: 0.0712 - val_loss: 0.0729\n",
            "Epoch 215/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0517 - val_loss: 0.0706\n",
            "Epoch 216/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0584 - val_loss: 0.0775\n",
            "Epoch 217/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0689 - val_loss: 0.0741\n",
            "Epoch 218/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0604 - val_loss: 0.0802\n",
            "Epoch 219/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0618 - val_loss: 0.0802\n",
            "Epoch 220/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0785 - val_loss: 0.0792\n",
            "Epoch 221/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0674 - val_loss: 0.0729\n",
            "Epoch 222/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0582 - val_loss: 0.0659\n",
            "Epoch 223/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0653 - val_loss: 0.0732\n",
            "Epoch 224/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0636 - val_loss: 0.0736\n",
            "Epoch 225/1000\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0575 - val_loss: 0.0658\n",
            "Epoch 226/1000\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0549 - val_loss: 0.0837\n",
            "Epoch 227/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0651 - val_loss: 0.0712\n",
            "Epoch 228/1000\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0672 - val_loss: 0.0706\n",
            "Epoch 229/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0605 - val_loss: 0.0852\n",
            "Epoch 230/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0600 - val_loss: 0.0710\n",
            "Epoch 231/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0677 - val_loss: 0.0675\n",
            "Epoch 232/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0701\n",
            "Epoch 233/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0686 - val_loss: 0.0792\n",
            "Epoch 234/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0721 - val_loss: 0.0732\n",
            "Epoch 235/1000\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0671 - val_loss: 0.0716\n",
            "Epoch 236/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0647 - val_loss: 0.0721\n",
            "Epoch 237/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0717 - val_loss: 0.0815\n",
            "Epoch 238/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0617 - val_loss: 0.0679\n",
            "Epoch 239/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0716\n",
            "Epoch 240/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.0690\n",
            "Epoch 241/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0618 - val_loss: 0.0749\n",
            "Epoch 242/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0529 - val_loss: 0.0628\n",
            "Epoch 243/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0596 - val_loss: 0.0633\n",
            "Epoch 244/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0487 - val_loss: 0.0681\n",
            "Epoch 245/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0734\n",
            "Epoch 246/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0492 - val_loss: 0.0676\n",
            "Epoch 247/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.0650\n",
            "Epoch 248/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0627 - val_loss: 0.0893\n",
            "Epoch 249/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0685 - val_loss: 0.0702\n",
            "Epoch 250/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0549 - val_loss: 0.0723\n",
            "Epoch 251/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0666 - val_loss: 0.0672\n",
            "Epoch 252/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0710 - val_loss: 0.0694\n",
            "Epoch 253/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0588 - val_loss: 0.0690\n",
            "Epoch 254/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0564 - val_loss: 0.0763\n",
            "Epoch 255/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0725 - val_loss: 0.0783\n",
            "Epoch 256/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0546 - val_loss: 0.0640\n",
            "Epoch 257/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0488 - val_loss: 0.0852\n",
            "Epoch 258/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0507 - val_loss: 0.0685\n",
            "Epoch 259/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0667\n",
            "Epoch 260/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.0789\n",
            "Epoch 261/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0631 - val_loss: 0.0836\n",
            "Epoch 262/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0641 - val_loss: 0.0827\n",
            "Epoch 263/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0554 - val_loss: 0.0611\n",
            "Epoch 264/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0576 - val_loss: 0.0697\n",
            "Epoch 265/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0670 - val_loss: 0.0769\n",
            "Epoch 266/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.0764\n",
            "Epoch 267/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.0780\n",
            "Epoch 268/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0615 - val_loss: 0.0695\n",
            "Epoch 269/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0583 - val_loss: 0.0804\n",
            "Epoch 270/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0633 - val_loss: 0.0763\n",
            "Epoch 271/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.0802\n",
            "Epoch 272/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.0744\n",
            "Epoch 273/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0576 - val_loss: 0.0656\n",
            "Epoch 274/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0554 - val_loss: 0.0754\n",
            "Epoch 275/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0498 - val_loss: 0.0609\n",
            "Epoch 276/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.0737\n",
            "Epoch 277/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0634 - val_loss: 0.0706\n",
            "Epoch 278/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0692 - val_loss: 0.0848\n",
            "Epoch 279/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0664 - val_loss: 0.0894\n",
            "Epoch 280/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0543 - val_loss: 0.0787\n",
            "Epoch 281/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0670\n",
            "Epoch 282/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0581 - val_loss: 0.0763\n",
            "Epoch 283/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0587 - val_loss: 0.0804\n",
            "Epoch 284/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0556 - val_loss: 0.0810\n",
            "Epoch 285/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0557 - val_loss: 0.0605\n",
            "Epoch 286/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0543 - val_loss: 0.0710\n",
            "Epoch 287/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0566 - val_loss: 0.0806\n",
            "Epoch 288/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0552 - val_loss: 0.0702\n",
            "Epoch 289/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.0709\n",
            "Epoch 290/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0677\n",
            "Epoch 291/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0535 - val_loss: 0.0584\n",
            "Epoch 292/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0551 - val_loss: 0.0724\n",
            "Epoch 293/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0525 - val_loss: 0.0718\n",
            "Epoch 294/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0558 - val_loss: 0.0680\n",
            "Epoch 295/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0601 - val_loss: 0.0622\n",
            "Epoch 296/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0590 - val_loss: 0.0584\n",
            "Epoch 297/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.0636\n",
            "Epoch 298/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0648 - val_loss: 0.0666\n",
            "Epoch 299/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0566 - val_loss: 0.0694\n",
            "Epoch 300/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0639\n",
            "Epoch 301/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0613 - val_loss: 0.0610\n",
            "Epoch 302/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0639 - val_loss: 0.0666\n",
            "Epoch 303/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0575 - val_loss: 0.0572\n",
            "Epoch 304/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0492 - val_loss: 0.0593\n",
            "Epoch 305/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.0727\n",
            "Epoch 306/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0542 - val_loss: 0.0728\n",
            "Epoch 307/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0538 - val_loss: 0.0638\n",
            "Epoch 308/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0636\n",
            "Epoch 309/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0545 - val_loss: 0.0652\n",
            "Epoch 310/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0624 - val_loss: 0.0659\n",
            "Epoch 311/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0470 - val_loss: 0.0595\n",
            "Epoch 312/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0533 - val_loss: 0.0626\n",
            "Epoch 313/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0442 - val_loss: 0.0628\n",
            "Epoch 314/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0532 - val_loss: 0.0625\n",
            "Epoch 315/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0627\n",
            "Epoch 316/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.0740\n",
            "Epoch 317/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0654\n",
            "Epoch 318/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0480 - val_loss: 0.0585\n",
            "Epoch 319/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0727 - val_loss: 0.0691\n",
            "Epoch 320/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.0662\n",
            "Epoch 321/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0547 - val_loss: 0.0573\n",
            "Epoch 322/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0558 - val_loss: 0.0686\n",
            "Epoch 323/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0536 - val_loss: 0.0672\n",
            "Epoch 324/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0502 - val_loss: 0.0641\n",
            "Epoch 325/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0482 - val_loss: 0.0672\n",
            "Epoch 326/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0533 - val_loss: 0.0632\n",
            "Epoch 327/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.0626\n",
            "Epoch 328/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0501 - val_loss: 0.0644\n",
            "Epoch 329/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0641\n",
            "Epoch 330/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0544 - val_loss: 0.0635\n",
            "Epoch 331/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0560 - val_loss: 0.0624\n",
            "Epoch 332/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0507 - val_loss: 0.0739\n",
            "Epoch 333/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0508 - val_loss: 0.0610\n",
            "Epoch 334/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0584\n",
            "Epoch 335/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0613\n",
            "Epoch 336/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0476 - val_loss: 0.0673\n",
            "Epoch 337/1000\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0491 - val_loss: 0.0556\n",
            "Epoch 338/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0491 - val_loss: 0.0588\n",
            "Epoch 339/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0465 - val_loss: 0.0641\n",
            "Epoch 340/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0623\n",
            "Epoch 341/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0493 - val_loss: 0.0609\n",
            "Epoch 342/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0633\n",
            "Epoch 343/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0565 - val_loss: 0.0564\n",
            "Epoch 344/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.0593\n",
            "Epoch 345/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0496 - val_loss: 0.0604\n",
            "Epoch 346/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0486 - val_loss: 0.0569\n",
            "Epoch 347/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.0557\n",
            "Epoch 348/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0491 - val_loss: 0.0558\n",
            "Epoch 349/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0471 - val_loss: 0.0577\n",
            "Epoch 350/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0554 - val_loss: 0.0523\n",
            "Epoch 351/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0492 - val_loss: 0.0601\n",
            "Epoch 352/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0440 - val_loss: 0.0575\n",
            "Epoch 353/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0436 - val_loss: 0.0519\n",
            "Epoch 354/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0476 - val_loss: 0.0599\n",
            "Epoch 355/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0505 - val_loss: 0.0703\n",
            "Epoch 356/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0473 - val_loss: 0.0612\n",
            "Epoch 357/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0428 - val_loss: 0.0551\n",
            "Epoch 358/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0420 - val_loss: 0.0655\n",
            "Epoch 359/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.0568\n",
            "Epoch 360/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0577 - val_loss: 0.0746\n",
            "Epoch 361/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.0542\n",
            "Epoch 362/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.0467\n",
            "Epoch 363/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.0559\n",
            "Epoch 364/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0537\n",
            "Epoch 365/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0478 - val_loss: 0.0541\n",
            "Epoch 366/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0574 - val_loss: 0.0538\n",
            "Epoch 367/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0471 - val_loss: 0.0610\n",
            "Epoch 368/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0443 - val_loss: 0.0545\n",
            "Epoch 369/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0511 - val_loss: 0.0588\n",
            "Epoch 370/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0433 - val_loss: 0.0556\n",
            "Epoch 371/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0486 - val_loss: 0.0617\n",
            "Epoch 372/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0482 - val_loss: 0.0649\n",
            "Epoch 373/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0639\n",
            "Epoch 374/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0541 - val_loss: 0.0590\n",
            "Epoch 375/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0416 - val_loss: 0.0632\n",
            "Epoch 376/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0468 - val_loss: 0.0543\n",
            "Epoch 377/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0445 - val_loss: 0.0650\n",
            "Epoch 378/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.0631\n",
            "Epoch 379/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.0633\n",
            "Epoch 380/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0488 - val_loss: 0.0629\n",
            "Epoch 381/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.0500\n",
            "Epoch 382/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0506 - val_loss: 0.0583\n",
            "Epoch 383/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0458 - val_loss: 0.0585\n",
            "Epoch 384/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0710\n",
            "Epoch 385/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0588\n",
            "Epoch 386/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0606\n",
            "Epoch 387/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.0554\n",
            "Epoch 388/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0702\n",
            "Epoch 389/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0531 - val_loss: 0.0640\n",
            "Epoch 390/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0491 - val_loss: 0.0529\n",
            "Epoch 391/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0518 - val_loss: 0.0583\n",
            "Epoch 392/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0459 - val_loss: 0.0597\n",
            "Epoch 393/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.0555\n",
            "Epoch 394/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0503 - val_loss: 0.0585\n",
            "Epoch 395/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0488 - val_loss: 0.0575\n",
            "Epoch 396/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0395 - val_loss: 0.0597\n",
            "Epoch 397/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.0554\n",
            "Epoch 398/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.0574\n",
            "Epoch 399/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0447 - val_loss: 0.0631\n",
            "Epoch 400/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0551 - val_loss: 0.0568\n",
            "Epoch 401/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0618 - val_loss: 0.0673\n",
            "Epoch 402/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0505 - val_loss: 0.0620\n",
            "Epoch 403/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0484 - val_loss: 0.0499\n",
            "Epoch 404/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0484 - val_loss: 0.0577\n",
            "Epoch 405/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0473 - val_loss: 0.0530\n",
            "Epoch 406/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.0634\n",
            "Epoch 407/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0457 - val_loss: 0.0645\n",
            "Epoch 408/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0622\n",
            "Epoch 409/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0471 - val_loss: 0.0575\n",
            "Epoch 410/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0500 - val_loss: 0.0619\n",
            "Epoch 411/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0423 - val_loss: 0.0566\n",
            "Epoch 412/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0454 - val_loss: 0.0558\n",
            "Epoch 413/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0526\n",
            "Epoch 414/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0464 - val_loss: 0.0524\n",
            "Epoch 415/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.0526\n",
            "Epoch 416/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.0464\n",
            "Epoch 417/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0429 - val_loss: 0.0547\n",
            "Epoch 418/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.0561\n",
            "Epoch 419/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0431 - val_loss: 0.0648\n",
            "Epoch 420/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0396 - val_loss: 0.0541\n",
            "Epoch 421/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0457\n",
            "Epoch 422/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0482 - val_loss: 0.0549\n",
            "Epoch 423/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0467 - val_loss: 0.0533\n",
            "Epoch 424/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0488\n",
            "Epoch 425/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.0567\n",
            "Epoch 426/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0502 - val_loss: 0.0649\n",
            "Epoch 427/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0579 - val_loss: 0.0677\n",
            "Epoch 428/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0529 - val_loss: 0.0525\n",
            "Epoch 429/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0567 - val_loss: 0.0643\n",
            "Epoch 430/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0527 - val_loss: 0.0513\n",
            "Epoch 431/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0561\n",
            "Epoch 432/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0511\n",
            "Epoch 433/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0458 - val_loss: 0.0593\n",
            "Epoch 434/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0502 - val_loss: 0.0609\n",
            "Epoch 435/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.0544\n",
            "Epoch 436/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0499 - val_loss: 0.0502\n",
            "Epoch 437/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0673\n",
            "Epoch 438/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0477 - val_loss: 0.0681\n",
            "Epoch 439/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0537 - val_loss: 0.0642\n",
            "Epoch 440/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0548\n",
            "Epoch 441/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0496 - val_loss: 0.0564\n",
            "Epoch 442/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.0568\n",
            "Epoch 443/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0478 - val_loss: 0.0538\n",
            "Epoch 444/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0596 - val_loss: 0.0656\n",
            "Epoch 445/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0441 - val_loss: 0.0588\n",
            "Epoch 446/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0504\n",
            "Epoch 447/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0535\n",
            "Epoch 448/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0408 - val_loss: 0.0543\n",
            "Epoch 449/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0370 - val_loss: 0.0519\n",
            "Epoch 450/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0425 - val_loss: 0.0506\n",
            "Epoch 451/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.0493\n",
            "Epoch 452/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0332 - val_loss: 0.0517\n",
            "Epoch 453/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0403 - val_loss: 0.0462\n",
            "Epoch 454/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.0469\n",
            "Epoch 455/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0452 - val_loss: 0.0541\n",
            "Epoch 456/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0493 - val_loss: 0.0575\n",
            "Epoch 457/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.0557\n",
            "Epoch 458/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0396 - val_loss: 0.0506\n",
            "Epoch 459/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0379 - val_loss: 0.0502\n",
            "Epoch 460/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0383 - val_loss: 0.0549\n",
            "Epoch 461/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0413 - val_loss: 0.0532\n",
            "Epoch 462/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.0568\n",
            "Epoch 463/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0519\n",
            "Epoch 464/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0403 - val_loss: 0.0547\n",
            "Epoch 465/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0609\n",
            "Epoch 466/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.0571\n",
            "Epoch 467/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0457 - val_loss: 0.0555\n",
            "Epoch 468/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0376 - val_loss: 0.0628\n",
            "Epoch 469/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0455 - val_loss: 0.0560\n",
            "Epoch 470/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0441 - val_loss: 0.0512\n",
            "Epoch 471/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.0594\n",
            "Epoch 472/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.0491\n",
            "Epoch 473/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.0570\n",
            "Epoch 474/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.0518\n",
            "Epoch 475/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.0576\n",
            "Epoch 476/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0404 - val_loss: 0.0542\n",
            "Epoch 477/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0444 - val_loss: 0.0544\n",
            "Epoch 478/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0459 - val_loss: 0.0630\n",
            "Epoch 479/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0570 - val_loss: 0.0671\n",
            "Epoch 480/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0508 - val_loss: 0.0612\n",
            "Epoch 481/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0509\n",
            "Epoch 482/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0495 - val_loss: 0.0608\n",
            "Epoch 483/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0420 - val_loss: 0.0631\n",
            "Epoch 484/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0579 - val_loss: 0.0604\n",
            "Epoch 485/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.0509\n",
            "Epoch 486/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0379 - val_loss: 0.0535\n",
            "Epoch 487/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0447 - val_loss: 0.0658\n",
            "Epoch 488/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0510 - val_loss: 0.0522\n",
            "Epoch 489/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0545 - val_loss: 0.0536\n",
            "Epoch 490/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0451 - val_loss: 0.0542\n",
            "Epoch 491/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0414 - val_loss: 0.0536\n",
            "Epoch 492/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0386 - val_loss: 0.0558\n",
            "Epoch 493/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0538\n",
            "Epoch 494/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0452\n",
            "Epoch 495/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0471 - val_loss: 0.0735\n",
            "Epoch 496/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0386 - val_loss: 0.0535\n",
            "Epoch 497/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.0506\n",
            "Epoch 498/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.0548\n",
            "Epoch 499/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0385 - val_loss: 0.0454\n",
            "Epoch 500/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0499\n",
            "Epoch 501/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0431 - val_loss: 0.0620\n",
            "Epoch 502/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0371 - val_loss: 0.0563\n",
            "Epoch 503/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0451 - val_loss: 0.0580\n",
            "Epoch 504/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0423 - val_loss: 0.0567\n",
            "Epoch 505/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0392 - val_loss: 0.0457\n",
            "Epoch 506/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0354 - val_loss: 0.0551\n",
            "Epoch 507/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0493\n",
            "Epoch 508/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.0556\n",
            "Epoch 509/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0432 - val_loss: 0.0527\n",
            "Epoch 510/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0476\n",
            "Epoch 511/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0579\n",
            "Epoch 512/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0469 - val_loss: 0.0501\n",
            "Epoch 513/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0375 - val_loss: 0.0514\n",
            "Epoch 514/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0376 - val_loss: 0.0557\n",
            "Epoch 515/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0428 - val_loss: 0.0516\n",
            "Epoch 516/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0469 - val_loss: 0.0518\n",
            "Epoch 517/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0515\n",
            "Epoch 518/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0414 - val_loss: 0.0469\n",
            "Epoch 519/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0376 - val_loss: 0.0506\n",
            "Epoch 520/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.0523\n",
            "Epoch 521/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0444 - val_loss: 0.0564\n",
            "Epoch 522/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0413 - val_loss: 0.0484\n",
            "Epoch 523/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0387 - val_loss: 0.0449\n",
            "Epoch 524/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0495 - val_loss: 0.0488\n",
            "Epoch 525/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0484 - val_loss: 0.0430\n",
            "Epoch 526/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0427 - val_loss: 0.0531\n",
            "Epoch 527/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0455\n",
            "Epoch 528/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0365 - val_loss: 0.0452\n",
            "Epoch 529/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.0479\n",
            "Epoch 530/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0471\n",
            "Epoch 531/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0443\n",
            "Epoch 532/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.0584\n",
            "Epoch 533/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0423 - val_loss: 0.0507\n",
            "Epoch 534/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0363 - val_loss: 0.0536\n",
            "Epoch 535/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0425 - val_loss: 0.0607\n",
            "Epoch 536/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0387 - val_loss: 0.0526\n",
            "Epoch 537/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0482\n",
            "Epoch 538/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.0547\n",
            "Epoch 539/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0408 - val_loss: 0.0629\n",
            "Epoch 540/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0584\n",
            "Epoch 541/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0459\n",
            "Epoch 542/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0453 - val_loss: 0.0461\n",
            "Epoch 543/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0432 - val_loss: 0.0424\n",
            "Epoch 544/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0385 - val_loss: 0.0591\n",
            "Epoch 545/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0352 - val_loss: 0.0448\n",
            "Epoch 546/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0470\n",
            "Epoch 547/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0451\n",
            "Epoch 548/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0518\n",
            "Epoch 549/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0490\n",
            "Epoch 550/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0423 - val_loss: 0.0492\n",
            "Epoch 551/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.0481\n",
            "Epoch 552/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0496\n",
            "Epoch 553/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0543\n",
            "Epoch 554/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0498\n",
            "Epoch 555/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0524\n",
            "Epoch 556/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0459 - val_loss: 0.0527\n",
            "Epoch 557/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0496\n",
            "Epoch 558/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.0534\n",
            "Epoch 559/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.0412\n",
            "Epoch 560/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0470\n",
            "Epoch 561/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0382 - val_loss: 0.0465\n",
            "Epoch 562/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0386 - val_loss: 0.0525\n",
            "Epoch 563/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0385 - val_loss: 0.0565\n",
            "Epoch 564/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0454 - val_loss: 0.0581\n",
            "Epoch 565/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0382 - val_loss: 0.0480\n",
            "Epoch 566/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0327 - val_loss: 0.0438\n",
            "Epoch 567/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.0467\n",
            "Epoch 568/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0421 - val_loss: 0.0425\n",
            "Epoch 569/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0447\n",
            "Epoch 570/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.0518\n",
            "Epoch 571/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.0472\n",
            "Epoch 572/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0380 - val_loss: 0.0557\n",
            "Epoch 573/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0534\n",
            "Epoch 574/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0431 - val_loss: 0.0498\n",
            "Epoch 575/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0516\n",
            "Epoch 576/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0413\n",
            "Epoch 577/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0381 - val_loss: 0.0509\n",
            "Epoch 578/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0458\n",
            "Epoch 579/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0436\n",
            "Epoch 580/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0322 - val_loss: 0.0380\n",
            "Epoch 581/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0335 - val_loss: 0.0452\n",
            "Epoch 582/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0374 - val_loss: 0.0499\n",
            "Epoch 583/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0346 - val_loss: 0.0419\n",
            "Epoch 584/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0429\n",
            "Epoch 585/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0417\n",
            "Epoch 586/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0482\n",
            "Epoch 587/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0457 - val_loss: 0.0560\n",
            "Epoch 588/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.0453\n",
            "Epoch 589/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.0479\n",
            "Epoch 590/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0494 - val_loss: 0.0543\n",
            "Epoch 591/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0516\n",
            "Epoch 592/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0438\n",
            "Epoch 593/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0491\n",
            "Epoch 594/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0468\n",
            "Epoch 595/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.0474\n",
            "Epoch 596/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.0529\n",
            "Epoch 597/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0483\n",
            "Epoch 598/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0367 - val_loss: 0.0450\n",
            "Epoch 599/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0444 - val_loss: 0.0449\n",
            "Epoch 600/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0356 - val_loss: 0.0437\n",
            "Epoch 601/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0375 - val_loss: 0.0445\n",
            "Epoch 602/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0474 - val_loss: 0.0469\n",
            "Epoch 603/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.0433\n",
            "Epoch 604/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0456\n",
            "Epoch 605/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0502 - val_loss: 0.0477\n",
            "Epoch 606/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0396 - val_loss: 0.0424\n",
            "Epoch 607/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0503\n",
            "Epoch 608/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0410 - val_loss: 0.0457\n",
            "Epoch 609/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0380 - val_loss: 0.0447\n",
            "Epoch 610/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0419\n",
            "Epoch 611/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0457\n",
            "Epoch 612/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0421\n",
            "Epoch 613/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0398\n",
            "Epoch 614/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.0408\n",
            "Epoch 615/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0413\n",
            "Epoch 616/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0336 - val_loss: 0.0420\n",
            "Epoch 617/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.0462\n",
            "Epoch 618/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0450 - val_loss: 0.0470\n",
            "Epoch 619/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0352 - val_loss: 0.0436\n",
            "Epoch 620/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0450 - val_loss: 0.0423\n",
            "Epoch 621/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0273 - val_loss: 0.0454\n",
            "Epoch 622/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0473\n",
            "Epoch 623/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0393\n",
            "Epoch 624/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0343 - val_loss: 0.0392\n",
            "Epoch 625/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0408 - val_loss: 0.0501\n",
            "Epoch 626/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.0538\n",
            "Epoch 627/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.0445\n",
            "Epoch 628/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0524\n",
            "Epoch 629/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0401\n",
            "Epoch 630/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0414 - val_loss: 0.0454\n",
            "Epoch 631/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0326 - val_loss: 0.0439\n",
            "Epoch 632/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0343 - val_loss: 0.0425\n",
            "Epoch 633/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0409\n",
            "Epoch 634/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0377\n",
            "Epoch 635/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0379 - val_loss: 0.0417\n",
            "Epoch 636/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0382 - val_loss: 0.0433\n",
            "Epoch 637/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0406 - val_loss: 0.0472\n",
            "Epoch 638/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0492\n",
            "Epoch 639/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0344 - val_loss: 0.0466\n",
            "Epoch 640/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0371 - val_loss: 0.0436\n",
            "Epoch 641/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0417 - val_loss: 0.0441\n",
            "Epoch 642/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0317 - val_loss: 0.0413\n",
            "Epoch 643/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0394\n",
            "Epoch 644/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0471\n",
            "Epoch 645/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0465\n",
            "Epoch 646/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0364 - val_loss: 0.0529\n",
            "Epoch 647/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0402\n",
            "Epoch 648/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0383\n",
            "Epoch 649/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0332 - val_loss: 0.0429\n",
            "Epoch 650/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.0373\n",
            "Epoch 651/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0323 - val_loss: 0.0400\n",
            "Epoch 652/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.0393\n",
            "Epoch 653/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.0398\n",
            "Epoch 654/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0329 - val_loss: 0.0440\n",
            "Epoch 655/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0336 - val_loss: 0.0463\n",
            "Epoch 656/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0412\n",
            "Epoch 657/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0371 - val_loss: 0.0442\n",
            "Epoch 658/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.0583\n",
            "Epoch 659/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.0502\n",
            "Epoch 660/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0468\n",
            "Epoch 661/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0429\n",
            "Epoch 662/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0331 - val_loss: 0.0432\n",
            "Epoch 663/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0407\n",
            "Epoch 664/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0388\n",
            "Epoch 665/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0359 - val_loss: 0.0380\n",
            "Epoch 666/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0336 - val_loss: 0.0361\n",
            "Epoch 667/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0385 - val_loss: 0.0517\n",
            "Epoch 668/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.0547\n",
            "Epoch 669/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0416 - val_loss: 0.0463\n",
            "Epoch 670/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0476\n",
            "Epoch 671/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.0478\n",
            "Epoch 672/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0368 - val_loss: 0.0438\n",
            "Epoch 673/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0301 - val_loss: 0.0415\n",
            "Epoch 674/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0365 - val_loss: 0.0485\n",
            "Epoch 675/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0368 - val_loss: 0.0377\n",
            "Epoch 676/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0326 - val_loss: 0.0467\n",
            "Epoch 677/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0379 - val_loss: 0.0417\n",
            "Epoch 678/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0407\n",
            "Epoch 679/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.0439\n",
            "Epoch 680/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0510\n",
            "Epoch 681/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0424\n",
            "Epoch 682/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.0443\n",
            "Epoch 683/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0443\n",
            "Epoch 684/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0455 - val_loss: 0.0474\n",
            "Epoch 685/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0425\n",
            "Epoch 686/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0479\n",
            "Epoch 687/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.0479\n",
            "Epoch 688/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0365 - val_loss: 0.0389\n",
            "Epoch 689/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0341 - val_loss: 0.0422\n",
            "Epoch 690/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0329 - val_loss: 0.0431\n",
            "Epoch 691/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0281 - val_loss: 0.0415\n",
            "Epoch 692/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0334 - val_loss: 0.0413\n",
            "Epoch 693/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0317 - val_loss: 0.0417\n",
            "Epoch 694/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0380 - val_loss: 0.0546\n",
            "Epoch 695/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0371 - val_loss: 0.0431\n",
            "Epoch 696/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0344 - val_loss: 0.0378\n",
            "Epoch 697/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.0408\n",
            "Epoch 698/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0422\n",
            "Epoch 699/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0406 - val_loss: 0.0478\n",
            "Epoch 700/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0329 - val_loss: 0.0456\n",
            "Epoch 701/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.0464\n",
            "Epoch 702/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0485\n",
            "Epoch 703/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0379 - val_loss: 0.0489\n",
            "Epoch 704/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0414\n",
            "Epoch 705/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0341 - val_loss: 0.0445\n",
            "Epoch 706/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0517\n",
            "Epoch 707/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0483\n",
            "Epoch 708/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.0541\n",
            "Epoch 709/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0393 - val_loss: 0.0471\n",
            "Epoch 710/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.0428\n",
            "Epoch 711/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0380 - val_loss: 0.0499\n",
            "Epoch 712/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0341 - val_loss: 0.0373\n",
            "Epoch 713/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0328 - val_loss: 0.0435\n",
            "Epoch 714/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0443\n",
            "Epoch 715/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0313 - val_loss: 0.0420\n",
            "Epoch 716/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.0432\n",
            "Epoch 717/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.0461\n",
            "Epoch 718/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0329 - val_loss: 0.0457\n",
            "Epoch 719/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0327 - val_loss: 0.0505\n",
            "Epoch 720/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0456\n",
            "Epoch 721/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0477\n",
            "Epoch 722/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0520\n",
            "Epoch 723/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0382 - val_loss: 0.0462\n",
            "Epoch 724/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0381\n",
            "Epoch 725/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0435\n",
            "Epoch 726/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0295 - val_loss: 0.0432\n",
            "Epoch 727/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0453\n",
            "Epoch 728/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.0366\n",
            "Epoch 729/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0398\n",
            "Epoch 730/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0331 - val_loss: 0.0465\n",
            "Epoch 731/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0345 - val_loss: 0.0415\n",
            "Epoch 732/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0292 - val_loss: 0.0445\n",
            "Epoch 733/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0445\n",
            "Epoch 734/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0423\n",
            "Epoch 735/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0501\n",
            "Epoch 736/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0582\n",
            "Epoch 737/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0363 - val_loss: 0.0436\n",
            "Epoch 738/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.0470\n",
            "Epoch 739/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.0498\n",
            "Epoch 740/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.0436\n",
            "Epoch 741/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0365\n",
            "Epoch 742/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0352 - val_loss: 0.0379\n",
            "Epoch 743/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0448\n",
            "Epoch 744/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0379\n",
            "Epoch 745/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0399\n",
            "Epoch 746/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0322 - val_loss: 0.0395\n",
            "Epoch 747/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0503\n",
            "Epoch 748/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.0503\n",
            "Epoch 749/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0394 - val_loss: 0.0467\n",
            "Epoch 750/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0389 - val_loss: 0.0480\n",
            "Epoch 751/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0425 - val_loss: 0.0477\n",
            "Epoch 752/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0367 - val_loss: 0.0396\n",
            "Epoch 753/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0299 - val_loss: 0.0361\n",
            "Epoch 754/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0378 - val_loss: 0.0458\n",
            "Epoch 755/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0370 - val_loss: 0.0452\n",
            "Epoch 756/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0385 - val_loss: 0.0456\n",
            "Epoch 757/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0472\n",
            "Epoch 758/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0339 - val_loss: 0.0469\n",
            "Epoch 759/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.0427\n",
            "Epoch 760/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0363 - val_loss: 0.0400\n",
            "Epoch 761/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0422\n",
            "Epoch 762/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0418\n",
            "Epoch 763/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0353 - val_loss: 0.0496\n",
            "Epoch 764/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0475\n",
            "Epoch 765/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0248 - val_loss: 0.0391\n",
            "Epoch 766/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.0387\n",
            "Epoch 767/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0375 - val_loss: 0.0457\n",
            "Epoch 768/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0412 - val_loss: 0.0424\n",
            "Epoch 769/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0412 - val_loss: 0.0382\n",
            "Epoch 770/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0334 - val_loss: 0.0375\n",
            "Epoch 771/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0252 - val_loss: 0.0385\n",
            "Epoch 772/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0384\n",
            "Epoch 773/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0348\n",
            "Epoch 774/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0297 - val_loss: 0.0400\n",
            "Epoch 775/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.0364\n",
            "Epoch 776/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0332 - val_loss: 0.0446\n",
            "Epoch 777/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0249 - val_loss: 0.0312\n",
            "Epoch 778/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0244 - val_loss: 0.0357\n",
            "Epoch 779/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0370 - val_loss: 0.0350\n",
            "Epoch 780/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0482 - val_loss: 0.0389\n",
            "Epoch 781/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0423\n",
            "Epoch 782/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0386 - val_loss: 0.0404\n",
            "Epoch 783/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.0388\n",
            "Epoch 784/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0429\n",
            "Epoch 785/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0366 - val_loss: 0.0425\n",
            "Epoch 786/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0224 - val_loss: 0.0366\n",
            "Epoch 787/1000\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.0280 - val_loss: 0.0342\n",
            "Epoch 788/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0313 - val_loss: 0.0435\n",
            "Epoch 789/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0296 - val_loss: 0.0371\n",
            "Epoch 790/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.0434\n",
            "Epoch 791/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0319 - val_loss: 0.0378\n",
            "Epoch 792/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.0402\n",
            "Epoch 793/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0396\n",
            "Epoch 794/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0362\n",
            "Epoch 795/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.0383\n",
            "Epoch 796/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0325 - val_loss: 0.0453\n",
            "Epoch 797/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0421\n",
            "Epoch 798/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0423\n",
            "Epoch 799/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.0410\n",
            "Epoch 800/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0344\n",
            "Epoch 801/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0417\n",
            "Epoch 802/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0335 - val_loss: 0.0390\n",
            "Epoch 803/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0336\n",
            "Epoch 804/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0346 - val_loss: 0.0397\n",
            "Epoch 805/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0317 - val_loss: 0.0360\n",
            "Epoch 806/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0359 - val_loss: 0.0437\n",
            "Epoch 807/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0277 - val_loss: 0.0445\n",
            "Epoch 808/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0304 - val_loss: 0.0371\n",
            "Epoch 809/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0462\n",
            "Epoch 810/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0366 - val_loss: 0.0468\n",
            "Epoch 811/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0342\n",
            "Epoch 812/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0249 - val_loss: 0.0329\n",
            "Epoch 813/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0303 - val_loss: 0.0448\n",
            "Epoch 814/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0368\n",
            "Epoch 815/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0235 - val_loss: 0.0391\n",
            "Epoch 816/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0398\n",
            "Epoch 817/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0388\n",
            "Epoch 818/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.0427\n",
            "Epoch 819/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0406 - val_loss: 0.0462\n",
            "Epoch 820/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.0377\n",
            "Epoch 821/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.0359\n",
            "Epoch 822/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0389\n",
            "Epoch 823/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0305 - val_loss: 0.0310\n",
            "Epoch 824/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0287 - val_loss: 0.0353\n",
            "Epoch 825/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0353 - val_loss: 0.0368\n",
            "Epoch 826/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0267 - val_loss: 0.0284\n",
            "Epoch 827/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.0421\n",
            "Epoch 828/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0381 - val_loss: 0.0434\n",
            "Epoch 829/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0350 - val_loss: 0.0392\n",
            "Epoch 830/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0370 - val_loss: 0.0332\n",
            "Epoch 831/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0392\n",
            "Epoch 832/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0400\n",
            "Epoch 833/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0326 - val_loss: 0.0375\n",
            "Epoch 834/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.0385\n",
            "Epoch 835/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0423\n",
            "Epoch 836/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.0405\n",
            "Epoch 837/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.0391\n",
            "Epoch 838/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0382\n",
            "Epoch 839/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0377\n",
            "Epoch 840/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0290 - val_loss: 0.0357\n",
            "Epoch 841/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0446\n",
            "Epoch 842/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0369 - val_loss: 0.0374\n",
            "Epoch 843/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0313 - val_loss: 0.0401\n",
            "Epoch 844/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0337 - val_loss: 0.0408\n",
            "Epoch 845/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0339 - val_loss: 0.0348\n",
            "Epoch 846/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0356 - val_loss: 0.0307\n",
            "Epoch 847/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0292 - val_loss: 0.0434\n",
            "Epoch 848/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0381\n",
            "Epoch 849/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0246 - val_loss: 0.0443\n",
            "Epoch 850/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0262 - val_loss: 0.0366\n",
            "Epoch 851/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0287 - val_loss: 0.0433\n",
            "Epoch 852/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.0429\n",
            "Epoch 853/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0415\n",
            "Epoch 854/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.0490\n",
            "Epoch 855/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0325 - val_loss: 0.0555\n",
            "Epoch 856/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0309 - val_loss: 0.0405\n",
            "Epoch 857/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0423\n",
            "Epoch 858/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0436\n",
            "Epoch 859/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.0382\n",
            "Epoch 860/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.0359\n",
            "Epoch 861/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0370\n",
            "Epoch 862/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0372 - val_loss: 0.0390\n",
            "Epoch 863/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0335 - val_loss: 0.0461\n",
            "Epoch 864/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0386 - val_loss: 0.0388\n",
            "Epoch 865/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0287 - val_loss: 0.0378\n",
            "Epoch 866/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0385\n",
            "Epoch 867/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0389\n",
            "Epoch 868/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0278 - val_loss: 0.0391\n",
            "Epoch 869/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0422\n",
            "Epoch 870/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0309 - val_loss: 0.0405\n",
            "Epoch 871/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0356\n",
            "Epoch 872/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0499\n",
            "Epoch 873/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0265 - val_loss: 0.0393\n",
            "Epoch 874/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0344 - val_loss: 0.0383\n",
            "Epoch 875/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0317 - val_loss: 0.0388\n",
            "Epoch 876/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0350 - val_loss: 0.0394\n",
            "Epoch 877/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.0386\n",
            "Epoch 878/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0426\n",
            "Epoch 879/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0303 - val_loss: 0.0351\n",
            "Epoch 880/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0392\n",
            "Epoch 881/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0280 - val_loss: 0.0362\n",
            "Epoch 882/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0295 - val_loss: 0.0423\n",
            "Epoch 883/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0228 - val_loss: 0.0348\n",
            "Epoch 884/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.0379\n",
            "Epoch 885/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0384\n",
            "Epoch 886/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0383 - val_loss: 0.0310\n",
            "Epoch 887/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0352 - val_loss: 0.0349\n",
            "Epoch 888/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0352\n",
            "Epoch 889/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.0462\n",
            "Epoch 890/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.0371\n",
            "Epoch 891/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0250 - val_loss: 0.0342\n",
            "Epoch 892/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0299 - val_loss: 0.0363\n",
            "Epoch 893/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0349\n",
            "Epoch 894/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0322 - val_loss: 0.0345\n",
            "Epoch 895/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.0360\n",
            "Epoch 896/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0432\n",
            "Epoch 897/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0246 - val_loss: 0.0379\n",
            "Epoch 898/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0232 - val_loss: 0.0320\n",
            "Epoch 899/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0277 - val_loss: 0.0360\n",
            "Epoch 900/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0312 - val_loss: 0.0315\n",
            "Epoch 901/1000\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0351 - val_loss: 0.0331\n",
            "Epoch 902/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0358 - val_loss: 0.0375\n",
            "Epoch 903/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0298 - val_loss: 0.0364\n",
            "Epoch 904/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0386 - val_loss: 0.0412\n",
            "Epoch 905/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0309 - val_loss: 0.0389\n",
            "Epoch 906/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0680\n",
            "Epoch 907/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0325 - val_loss: 0.0448\n",
            "Epoch 908/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0373\n",
            "Epoch 909/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0336 - val_loss: 0.0357\n",
            "Epoch 910/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0310 - val_loss: 0.0345\n",
            "Epoch 911/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.0382\n",
            "Epoch 912/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.0333\n",
            "Epoch 913/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0358\n",
            "Epoch 914/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0386\n",
            "Epoch 915/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0331\n",
            "Epoch 916/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0350\n",
            "Epoch 917/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.0397\n",
            "Epoch 918/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0393\n",
            "Epoch 919/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0325 - val_loss: 0.0374\n",
            "Epoch 920/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0253 - val_loss: 0.0384\n",
            "Epoch 921/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0318 - val_loss: 0.0380\n",
            "Epoch 922/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0388\n",
            "Epoch 923/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0383\n",
            "Epoch 924/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0368\n",
            "Epoch 925/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0308 - val_loss: 0.0385\n",
            "Epoch 926/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0328 - val_loss: 0.0463\n",
            "Epoch 927/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.0426\n",
            "Epoch 928/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0381\n",
            "Epoch 929/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0391\n",
            "Epoch 930/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0397\n",
            "Epoch 931/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0260 - val_loss: 0.0404\n",
            "Epoch 932/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0461\n",
            "Epoch 933/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0317 - val_loss: 0.0375\n",
            "Epoch 934/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0415\n",
            "Epoch 935/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0398 - val_loss: 0.0348\n",
            "Epoch 936/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0277 - val_loss: 0.0338\n",
            "Epoch 937/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0342 - val_loss: 0.0433\n",
            "Epoch 938/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0333 - val_loss: 0.0409\n",
            "Epoch 939/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0336 - val_loss: 0.0369\n",
            "Epoch 940/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0299 - val_loss: 0.0399\n",
            "Epoch 941/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.0480\n",
            "Epoch 942/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0287 - val_loss: 0.0434\n",
            "Epoch 943/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0364 - val_loss: 0.0365\n",
            "Epoch 944/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0297 - val_loss: 0.0409\n",
            "Epoch 945/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0311 - val_loss: 0.0381\n",
            "Epoch 946/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0308 - val_loss: 0.0386\n",
            "Epoch 947/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0418\n",
            "Epoch 948/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.0511\n",
            "Epoch 949/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0258 - val_loss: 0.0341\n",
            "Epoch 950/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0346 - val_loss: 0.0365\n",
            "Epoch 951/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0249 - val_loss: 0.0362\n",
            "Epoch 952/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0288 - val_loss: 0.0438\n",
            "Epoch 953/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0464\n",
            "Epoch 954/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.0419\n",
            "Epoch 955/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0375 - val_loss: 0.0472\n",
            "Epoch 956/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0327 - val_loss: 0.0400\n",
            "Epoch 957/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0243 - val_loss: 0.0384\n",
            "Epoch 958/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0306 - val_loss: 0.0420\n",
            "Epoch 959/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.0420\n",
            "Epoch 960/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0228 - val_loss: 0.0446\n",
            "Epoch 961/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0304 - val_loss: 0.0374\n",
            "Epoch 962/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.0406\n",
            "Epoch 963/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0438\n",
            "Epoch 964/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.0411\n",
            "Epoch 965/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.0362\n",
            "Epoch 966/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0375 - val_loss: 0.0382\n",
            "Epoch 967/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.0349\n",
            "Epoch 968/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.0380\n",
            "Epoch 969/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0297 - val_loss: 0.0366\n",
            "Epoch 970/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0313 - val_loss: 0.0369\n",
            "Epoch 971/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.0354\n",
            "Epoch 972/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0256 - val_loss: 0.0428\n",
            "Epoch 973/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0296 - val_loss: 0.0370\n",
            "Epoch 974/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0317 - val_loss: 0.0406\n",
            "Epoch 975/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0353 - val_loss: 0.0383\n",
            "Epoch 976/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0328 - val_loss: 0.0430\n",
            "Epoch 977/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0283 - val_loss: 0.0350\n",
            "Epoch 978/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0333\n",
            "Epoch 979/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0341\n",
            "Epoch 980/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0293 - val_loss: 0.0386\n",
            "Epoch 981/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0252 - val_loss: 0.0336\n",
            "Epoch 982/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0291 - val_loss: 0.0402\n",
            "Epoch 983/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.0361\n",
            "Epoch 984/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0262 - val_loss: 0.0353\n",
            "Epoch 985/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0407\n",
            "Epoch 986/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0323 - val_loss: 0.0388\n",
            "Epoch 987/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0251 - val_loss: 0.0368\n",
            "Epoch 988/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0396\n",
            "Epoch 989/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.0390\n",
            "Epoch 990/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0215 - val_loss: 0.0393\n",
            "Epoch 991/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.0402\n",
            "Epoch 992/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0313 - val_loss: 0.0411\n",
            "Epoch 993/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0240 - val_loss: 0.0376\n",
            "Epoch 994/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0266 - val_loss: 0.0448\n",
            "Epoch 995/1000\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0405\n",
            "Epoch 996/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.0484\n",
            "Epoch 997/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.0406\n",
            "Epoch 998/1000\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.0317 - val_loss: 0.0419\n",
            "Epoch 999/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0255 - val_loss: 0.0408\n",
            "Epoch 1000/1000\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.0244 - val_loss: 0.0340\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0340\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f47630e9160>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8\n",
        "vnoise=1\n",
        "\n",
        "# Train/ Val sets \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=42)\n",
        "train_set = train_df.values\n",
        "val_set = val_df.values\n",
        "\n",
        "train_set = np.asarray(train_set).astype(np.float32)\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "datagen_val = ImageDataGenerator()\n",
        "\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "nn=256  # units 200 to 256\n",
        "\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-4))    # Learning rate changes 1e-5 to 1e-4\n",
        "\n",
        "# Training\n",
        "nb_epochs=1000;    # epochs change to 1000\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "# Plot the curves\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "SsLDEkmiQdeD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoJ_LtWsjMAB"
      },
      "source": [
        "The Training Loss is around 0.03,  the Validation Loss is around 0.04, which is far more below the naive prediction 0.11."
      ],
      "id": "xoJ_LtWsjMAB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrEgpgcmMEWp"
      },
      "source": [
        "#**-- ABANDONED --**: Predict probability distribution over the 3 classes#\n",
        "**This part is actually classification based on most selected result, which is not to predict the distribution**  \n",
        "  \n",
        "**3 classes:**   \n",
        "\"Cannot see any difference\" - 0  [1,0,0] (one-hot encoding).  \n",
        "\"Just noticeable difference(JND)\" - 1  [0,1,0].    \n",
        "\"Clearly see a difference\" - 2  [0,0,1].  \n",
        "\n",
        "\n",
        "1.Use the \"cross entropy\" loss function between the prediction and the ground truth.  \n",
        "$H(p, q) = -\\sum_{i=1}^{C} p_i \\log(q_i)$  \n",
        "(Input the Euclidean distance + the outputs of the twin networks to the last layers)  \n",
        "**--> Question: Ground truth <-> Average color difference assessment? Rounding?**  \n",
        "--> In this trail, the ground truth will be chosen as the most selected results by paticipants in the experiment.  **--> Question: Is it reasonable?**  \n",
        "\n",
        "**--> Reply: What we want to predict for each tested pair is the probability aver the three classes, so that we can say, for example : \"for this pair, 70% of\n",
        "the observers would say JND, 25% would say 'see nothing' and 5% would say 'clear difference'. we have to train with real ground truth values, not integers. We have to learn to predict the true values, which are float numbers. And there is no problem for a Deep network to predict float numbers, even\n",
        "for a classification problem.**\n",
        "\n",
        "2.Add a softmax activation function in the last layer. Try to add a \"Temperature\" in this softmax.\n",
        "\n"
      ],
      "id": "JrEgpgcmMEWp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c3b4e356",
        "outputId": "c62253b7-4e6f-451f-811b-9e99fa6cb04c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "80/80 [==============================] - 10s 83ms/step - loss: 0.8158 - val_loss: 0.6156\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.7167 - val_loss: 0.5977\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.6546 - val_loss: 0.5902\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.6266 - val_loss: 0.5187\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.6133 - val_loss: 0.4934\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.5741 - val_loss: 0.5526\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.5589 - val_loss: 0.4466\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.5387 - val_loss: 0.4624\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.5078 - val_loss: 0.4515\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.5079 - val_loss: 0.4120\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.4956 - val_loss: 0.4444\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.4836 - val_loss: 0.4688\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.4830 - val_loss: 0.4648\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.4952 - val_loss: 0.4078\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.4613 - val_loss: 0.3997\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.4481 - val_loss: 0.3759\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.4484 - val_loss: 0.4183\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.4339 - val_loss: 0.3864\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 5s 69ms/step - loss: 0.4228 - val_loss: 0.3500\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.4208 - val_loss: 0.3619\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 5s 60ms/step - loss: 0.4056 - val_loss: 0.3500\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.4102 - val_loss: 0.3433\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.4045 - val_loss: 0.3553\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3990 - val_loss: 0.4026\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.4031 - val_loss: 0.3282\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3873 - val_loss: 0.3468\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.3924 - val_loss: 0.3513\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.3822 - val_loss: 0.3302\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3827 - val_loss: 0.3440\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.3694 - val_loss: 0.3405\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3709 - val_loss: 0.3451\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.3686 - val_loss: 0.2983\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.3574 - val_loss: 0.3065\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3609 - val_loss: 0.3311\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.3452 - val_loss: 0.2969\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.3630 - val_loss: 0.3129\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3497 - val_loss: 0.3383\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.3606 - val_loss: 0.3832\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3436 - val_loss: 0.2801\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.3340 - val_loss: 0.2856\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.3288 - val_loss: 0.2993\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3443 - val_loss: 0.2875\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.3296 - val_loss: 0.2908\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3269 - val_loss: 0.2694\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3237 - val_loss: 0.3151\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.3176 - val_loss: 0.2921\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3388 - val_loss: 0.3020\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.3230 - val_loss: 0.2708\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 7s 89ms/step - loss: 0.3136 - val_loss: 0.2554\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.3143 - val_loss: 0.2576\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.3093 - val_loss: 0.2709\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3137 - val_loss: 0.2474\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.3104 - val_loss: 0.2480\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.3004 - val_loss: 0.2602\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.3051 - val_loss: 0.2659\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.3048 - val_loss: 0.2464\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.3094 - val_loss: 0.2833\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.2954 - val_loss: 0.2849\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.3081 - val_loss: 0.2423\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2984 - val_loss: 0.2375\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2947 - val_loss: 0.2605\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2946 - val_loss: 0.2662\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2944 - val_loss: 0.2454\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2917 - val_loss: 0.2371\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2891 - val_loss: 0.2406\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2867 - val_loss: 0.2868\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.2845 - val_loss: 0.2326\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2847 - val_loss: 0.2341\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.2798 - val_loss: 0.2254\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2820 - val_loss: 0.2279\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.2898 - val_loss: 0.2282\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.2721 - val_loss: 0.2438\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2706 - val_loss: 0.2250\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2797 - val_loss: 0.2472\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2774 - val_loss: 0.2239\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.2763 - val_loss: 0.2328\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.2682 - val_loss: 0.2188\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2629 - val_loss: 0.2367\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.2697 - val_loss: 0.2851\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2746 - val_loss: 0.2361\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2678 - val_loss: 0.2195\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2631 - val_loss: 0.2150\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2632 - val_loss: 0.2113\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.2662 - val_loss: 0.2213\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.2555 - val_loss: 0.2391\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2630 - val_loss: 0.2174\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2588 - val_loss: 0.2377\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2562 - val_loss: 0.2375\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2711 - val_loss: 0.2127\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2564 - val_loss: 0.2391\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2588 - val_loss: 0.2107\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2544 - val_loss: 0.2226\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2489 - val_loss: 0.2131\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2465 - val_loss: 0.1912\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2547 - val_loss: 0.2012\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2463 - val_loss: 0.2125\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.2492 - val_loss: 0.2028\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.2517 - val_loss: 0.2051\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2482 - val_loss: 0.1975\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2567 - val_loss: 0.2154\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2484 - val_loss: 0.2062\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.2436 - val_loss: 0.2055\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.2421 - val_loss: 0.1903\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2417 - val_loss: 0.2155\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.2442 - val_loss: 0.2030\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.2426 - val_loss: 0.2064\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2416 - val_loss: 0.1938\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2426 - val_loss: 0.2019\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2427 - val_loss: 0.1976\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.2377 - val_loss: 0.2140\n",
            "Epoch 111/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.2371 - val_loss: 0.2058\n",
            "Epoch 112/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2404 - val_loss: 0.1884\n",
            "Epoch 113/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2355 - val_loss: 0.1996\n",
            "Epoch 114/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2391 - val_loss: 0.1975\n",
            "Epoch 115/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2340 - val_loss: 0.1907\n",
            "Epoch 116/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.2325 - val_loss: 0.1885\n",
            "Epoch 117/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2348 - val_loss: 0.1959\n",
            "Epoch 118/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.2333 - val_loss: 0.1931\n",
            "Epoch 119/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.2316 - val_loss: 0.1924\n",
            "Epoch 120/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2336 - val_loss: 0.1923\n",
            "Epoch 121/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.2329 - val_loss: 0.1968\n",
            "Epoch 122/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2339 - val_loss: 0.2425\n",
            "Epoch 123/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.2312 - val_loss: 0.1870\n",
            "Epoch 124/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.2288 - val_loss: 0.1864\n",
            "Epoch 125/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2285 - val_loss: 0.2003\n",
            "Epoch 126/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.2362 - val_loss: 0.1910\n",
            "Epoch 127/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2276 - val_loss: 0.1910\n",
            "Epoch 128/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.2362 - val_loss: 0.1898\n",
            "Epoch 129/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.2327 - val_loss: 0.1905\n",
            "Epoch 130/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2271 - val_loss: 0.2037\n",
            "Epoch 131/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.2335 - val_loss: 0.2049\n",
            "Epoch 132/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2315 - val_loss: 0.1815\n",
            "Epoch 133/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2212 - val_loss: 0.1788\n",
            "Epoch 134/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2226 - val_loss: 0.1966\n",
            "Epoch 135/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2218 - val_loss: 0.1905\n",
            "Epoch 136/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.2156 - val_loss: 0.1858\n",
            "Epoch 137/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2219 - val_loss: 0.1937\n",
            "Epoch 138/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2163 - val_loss: 0.1783\n",
            "Epoch 139/500\n",
            "80/80 [==============================] - 6s 82ms/step - loss: 0.2112 - val_loss: 0.1883\n",
            "Epoch 140/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2181 - val_loss: 0.1788\n",
            "Epoch 141/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2141 - val_loss: 0.1860\n",
            "Epoch 142/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2180 - val_loss: 0.1856\n",
            "Epoch 143/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2315 - val_loss: 0.2087\n",
            "Epoch 144/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.2266 - val_loss: 0.2026\n",
            "Epoch 145/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.2216 - val_loss: 0.1861\n",
            "Epoch 146/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2165 - val_loss: 0.1763\n",
            "Epoch 147/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2186 - val_loss: 0.1808\n",
            "Epoch 148/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2186 - val_loss: 0.1847\n",
            "Epoch 149/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.2164 - val_loss: 0.1856\n",
            "Epoch 150/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2167 - val_loss: 0.1897\n",
            "Epoch 151/500\n",
            "80/80 [==============================] - 5s 62ms/step - loss: 0.2129 - val_loss: 0.1843\n",
            "Epoch 152/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2084 - val_loss: 0.1808\n",
            "Epoch 153/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2132 - val_loss: 0.2070\n",
            "Epoch 154/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2159 - val_loss: 0.2053\n",
            "Epoch 155/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2144 - val_loss: 0.1792\n",
            "Epoch 156/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2202 - val_loss: 0.1880\n",
            "Epoch 157/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.2118 - val_loss: 0.1948\n",
            "Epoch 158/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.2172 - val_loss: 0.1957\n",
            "Epoch 159/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2158 - val_loss: 0.1832\n",
            "Epoch 160/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2069 - val_loss: 0.1985\n",
            "Epoch 161/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2109 - val_loss: 0.1830\n",
            "Epoch 162/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.2119 - val_loss: 0.1807\n",
            "Epoch 163/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.2108 - val_loss: 0.1869\n",
            "Epoch 164/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2116 - val_loss: 0.1935\n",
            "Epoch 165/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.2088 - val_loss: 0.1814\n",
            "Epoch 166/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.2064 - val_loss: 0.1807\n",
            "Epoch 167/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2018 - val_loss: 0.1840\n",
            "Epoch 168/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2063 - val_loss: 0.1879\n",
            "Epoch 169/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2012 - val_loss: 0.2007\n",
            "Epoch 170/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.2112 - val_loss: 0.1683\n",
            "Epoch 171/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.2020 - val_loss: 0.1767\n",
            "Epoch 172/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1990 - val_loss: 0.1687\n",
            "Epoch 173/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2015 - val_loss: 0.1750\n",
            "Epoch 174/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2009 - val_loss: 0.1704\n",
            "Epoch 175/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.2023 - val_loss: 0.1749\n",
            "Epoch 176/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1985 - val_loss: 0.1936\n",
            "Epoch 177/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1979 - val_loss: 0.2057\n",
            "Epoch 178/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.2101 - val_loss: 0.1890\n",
            "Epoch 179/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1994 - val_loss: 0.1794\n",
            "Epoch 180/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2055 - val_loss: 0.1765\n",
            "Epoch 181/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1950 - val_loss: 0.1827\n",
            "Epoch 182/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1946 - val_loss: 0.1832\n",
            "Epoch 183/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.2018 - val_loss: 0.1829\n",
            "Epoch 184/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.2017 - val_loss: 0.1838\n",
            "Epoch 185/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2035 - val_loss: 0.1695\n",
            "Epoch 186/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1933 - val_loss: 0.1778\n",
            "Epoch 187/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1886 - val_loss: 0.1945\n",
            "Epoch 188/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1988 - val_loss: 0.1721\n",
            "Epoch 189/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1911 - val_loss: 0.1918\n",
            "Epoch 190/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.2063 - val_loss: 0.2011\n",
            "Epoch 191/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.2017 - val_loss: 0.1820\n",
            "Epoch 192/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1969 - val_loss: 0.1889\n",
            "Epoch 193/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1938 - val_loss: 0.1856\n",
            "Epoch 194/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1883 - val_loss: 0.1727\n",
            "Epoch 195/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1840 - val_loss: 0.1891\n",
            "Epoch 196/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.2031 - val_loss: 0.1995\n",
            "Epoch 197/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1949 - val_loss: 0.1881\n",
            "Epoch 198/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1989 - val_loss: 0.1895\n",
            "Epoch 199/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1910 - val_loss: 0.1721\n",
            "Epoch 200/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1915 - val_loss: 0.1910\n",
            "Epoch 201/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1915 - val_loss: 0.1748\n",
            "Epoch 202/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1904 - val_loss: 0.1771\n",
            "Epoch 203/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1908 - val_loss: 0.2097\n",
            "Epoch 204/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1837 - val_loss: 0.1907\n",
            "Epoch 205/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1831 - val_loss: 0.2066\n",
            "Epoch 206/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.2084 - val_loss: 0.1881\n",
            "Epoch 207/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1907 - val_loss: 0.1874\n",
            "Epoch 208/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1829 - val_loss: 0.2126\n",
            "Epoch 209/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1880 - val_loss: 0.1750\n",
            "Epoch 210/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1848 - val_loss: 0.1808\n",
            "Epoch 211/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1890 - val_loss: 0.1938\n",
            "Epoch 212/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1910 - val_loss: 0.1838\n",
            "Epoch 213/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1852 - val_loss: 0.2084\n",
            "Epoch 214/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1829 - val_loss: 0.1900\n",
            "Epoch 215/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1872 - val_loss: 0.1913\n",
            "Epoch 216/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1841 - val_loss: 0.1863\n",
            "Epoch 217/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1752 - val_loss: 0.1957\n",
            "Epoch 218/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1871 - val_loss: 0.1806\n",
            "Epoch 219/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1809 - val_loss: 0.2046\n",
            "Epoch 220/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1816 - val_loss: 0.1963\n",
            "Epoch 221/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1844 - val_loss: 0.1875\n",
            "Epoch 222/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1832 - val_loss: 0.1918\n",
            "Epoch 223/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1874 - val_loss: 0.1858\n",
            "Epoch 224/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1808 - val_loss: 0.1822\n",
            "Epoch 225/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1786 - val_loss: 0.2313\n",
            "Epoch 226/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.2066 - val_loss: 0.1862\n",
            "Epoch 227/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1823 - val_loss: 0.1993\n",
            "Epoch 228/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1772 - val_loss: 0.2021\n",
            "Epoch 229/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1748 - val_loss: 0.1866\n",
            "Epoch 230/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1809 - val_loss: 0.1988\n",
            "Epoch 231/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1792 - val_loss: 0.1816\n",
            "Epoch 232/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1806 - val_loss: 0.1872\n",
            "Epoch 233/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1788 - val_loss: 0.2065\n",
            "Epoch 234/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1896 - val_loss: 0.1864\n",
            "Epoch 235/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1839 - val_loss: 0.1996\n",
            "Epoch 236/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1795 - val_loss: 0.1860\n",
            "Epoch 237/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1791 - val_loss: 0.1904\n",
            "Epoch 238/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1771 - val_loss: 0.1991\n",
            "Epoch 239/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1730 - val_loss: 0.1804\n",
            "Epoch 240/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1813 - val_loss: 0.2007\n",
            "Epoch 241/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1810 - val_loss: 0.1796\n",
            "Epoch 242/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1722 - val_loss: 0.2080\n",
            "Epoch 243/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1787 - val_loss: 0.1718\n",
            "Epoch 244/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1703 - val_loss: 0.2015\n",
            "Epoch 245/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1758 - val_loss: 0.1743\n",
            "Epoch 246/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1702 - val_loss: 0.1947\n",
            "Epoch 247/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1733 - val_loss: 0.1861\n",
            "Epoch 248/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1732 - val_loss: 0.1961\n",
            "Epoch 249/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1730 - val_loss: 0.1746\n",
            "Epoch 250/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1683 - val_loss: 0.2031\n",
            "Epoch 251/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1671 - val_loss: 0.1861\n",
            "Epoch 252/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1709 - val_loss: 0.2267\n",
            "Epoch 253/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1672 - val_loss: 0.1966\n",
            "Epoch 254/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1694 - val_loss: 0.1971\n",
            "Epoch 255/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1799 - val_loss: 0.2009\n",
            "Epoch 256/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1733 - val_loss: 0.1817\n",
            "Epoch 257/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1778 - val_loss: 0.2025\n",
            "Epoch 258/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1645 - val_loss: 0.1837\n",
            "Epoch 259/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1704 - val_loss: 0.1964\n",
            "Epoch 260/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1639 - val_loss: 0.2000\n",
            "Epoch 261/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1608 - val_loss: 0.1927\n",
            "Epoch 262/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1624 - val_loss: 0.2002\n",
            "Epoch 263/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1691 - val_loss: 0.1722\n",
            "Epoch 264/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1714 - val_loss: 0.2054\n",
            "Epoch 265/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1761 - val_loss: 0.2385\n",
            "Epoch 266/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1713 - val_loss: 0.1835\n",
            "Epoch 267/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1671 - val_loss: 0.1946\n",
            "Epoch 268/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1581 - val_loss: 0.2508\n",
            "Epoch 269/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1716 - val_loss: 0.1739\n",
            "Epoch 270/500\n",
            "80/80 [==============================] - 5s 63ms/step - loss: 0.1743 - val_loss: 0.2388\n",
            "Epoch 271/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1705 - val_loss: 0.1920\n",
            "Epoch 272/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1781 - val_loss: 0.1812\n",
            "Epoch 273/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.1598 - val_loss: 0.2134\n",
            "Epoch 274/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1606 - val_loss: 0.2020\n",
            "Epoch 275/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1626 - val_loss: 0.2126\n",
            "Epoch 276/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1576 - val_loss: 0.2356\n",
            "Epoch 277/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1752 - val_loss: 0.1910\n",
            "Epoch 278/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1657 - val_loss: 0.2010\n",
            "Epoch 279/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1654 - val_loss: 0.2235\n",
            "Epoch 280/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1708 - val_loss: 0.2418\n",
            "Epoch 281/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1851 - val_loss: 0.2091\n",
            "Epoch 282/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1633 - val_loss: 0.2153\n",
            "Epoch 283/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1664 - val_loss: 0.2262\n",
            "Epoch 284/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1703 - val_loss: 0.2195\n",
            "Epoch 285/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1823 - val_loss: 0.1903\n",
            "Epoch 286/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1652 - val_loss: 0.2056\n",
            "Epoch 287/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1618 - val_loss: 0.1961\n",
            "Epoch 288/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1742 - val_loss: 0.2058\n",
            "Epoch 289/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1614 - val_loss: 0.1943\n",
            "Epoch 290/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1620 - val_loss: 0.1892\n",
            "Epoch 291/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1568 - val_loss: 0.2166\n",
            "Epoch 292/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1619 - val_loss: 0.1900\n",
            "Epoch 293/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1608 - val_loss: 0.1816\n",
            "Epoch 294/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1576 - val_loss: 0.1840\n",
            "Epoch 295/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1640 - val_loss: 0.2011\n",
            "Epoch 296/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1592 - val_loss: 0.2242\n",
            "Epoch 297/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1558 - val_loss: 0.2149\n",
            "Epoch 298/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1524 - val_loss: 0.2541\n",
            "Epoch 299/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1596 - val_loss: 0.2267\n",
            "Epoch 300/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1565 - val_loss: 0.2325\n",
            "Epoch 301/500\n",
            "80/80 [==============================] - 5s 69ms/step - loss: 0.1557 - val_loss: 0.1920\n",
            "Epoch 302/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1569 - val_loss: 0.2127\n",
            "Epoch 303/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1509 - val_loss: 0.2839\n",
            "Epoch 304/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1632 - val_loss: 0.2408\n",
            "Epoch 305/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1625 - val_loss: 0.2756\n",
            "Epoch 306/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1510 - val_loss: 0.2017\n",
            "Epoch 307/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1489 - val_loss: 0.1987\n",
            "Epoch 308/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1510 - val_loss: 0.2415\n",
            "Epoch 309/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1537 - val_loss: 0.2361\n",
            "Epoch 310/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1550 - val_loss: 0.2151\n",
            "Epoch 311/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1515 - val_loss: 0.2046\n",
            "Epoch 312/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1520 - val_loss: 0.2097\n",
            "Epoch 313/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1493 - val_loss: 0.2417\n",
            "Epoch 314/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.1589 - val_loss: 0.1961\n",
            "Epoch 315/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.1534 - val_loss: 0.2083\n",
            "Epoch 316/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1525 - val_loss: 0.2050\n",
            "Epoch 317/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1527 - val_loss: 0.1900\n",
            "Epoch 318/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1550 - val_loss: 0.2008\n",
            "Epoch 319/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1590 - val_loss: 0.1793\n",
            "Epoch 320/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1541 - val_loss: 0.2218\n",
            "Epoch 321/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1598 - val_loss: 0.2151\n",
            "Epoch 322/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1549 - val_loss: 0.1967\n",
            "Epoch 323/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1580 - val_loss: 0.2392\n",
            "Epoch 324/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1550 - val_loss: 0.2021\n",
            "Epoch 325/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1579 - val_loss: 0.2207\n",
            "Epoch 326/500\n",
            "80/80 [==============================] - 8s 97ms/step - loss: 0.1578 - val_loss: 0.2271\n",
            "Epoch 327/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1499 - val_loss: 0.2217\n",
            "Epoch 328/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1492 - val_loss: 0.2169\n",
            "Epoch 329/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1703 - val_loss: 0.2163\n",
            "Epoch 330/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1723 - val_loss: 0.2407\n",
            "Epoch 331/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1667 - val_loss: 0.2054\n",
            "Epoch 332/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1669 - val_loss: 0.2116\n",
            "Epoch 333/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1451 - val_loss: 0.2739\n",
            "Epoch 334/500\n",
            "80/80 [==============================] - 8s 101ms/step - loss: 0.1556 - val_loss: 0.2386\n",
            "Epoch 335/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1603 - val_loss: 0.1972\n",
            "Epoch 336/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1506 - val_loss: 0.2127\n",
            "Epoch 337/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1496 - val_loss: 0.2007\n",
            "Epoch 338/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1435 - val_loss: 0.2306\n",
            "Epoch 339/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1489 - val_loss: 0.2237\n",
            "Epoch 340/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1447 - val_loss: 0.2353\n",
            "Epoch 341/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1501 - val_loss: 0.1868\n",
            "Epoch 342/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1538 - val_loss: 0.2089\n",
            "Epoch 343/500\n",
            "80/80 [==============================] - 5s 64ms/step - loss: 0.1497 - val_loss: 0.2134\n",
            "Epoch 344/500\n",
            "80/80 [==============================] - 7s 92ms/step - loss: 0.1503 - val_loss: 0.2007\n",
            "Epoch 345/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1561 - val_loss: 0.2731\n",
            "Epoch 346/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1555 - val_loss: 0.2031\n",
            "Epoch 347/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1555 - val_loss: 0.2048\n",
            "Epoch 348/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1591 - val_loss: 0.1787\n",
            "Epoch 349/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1462 - val_loss: 0.1910\n",
            "Epoch 350/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1485 - val_loss: 0.2170\n",
            "Epoch 351/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1572 - val_loss: 0.2091\n",
            "Epoch 352/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1481 - val_loss: 0.1951\n",
            "Epoch 353/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1454 - val_loss: 0.1940\n",
            "Epoch 354/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1507 - val_loss: 0.2202\n",
            "Epoch 355/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1507 - val_loss: 0.2537\n",
            "Epoch 356/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1492 - val_loss: 0.2055\n",
            "Epoch 357/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1449 - val_loss: 0.2136\n",
            "Epoch 358/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1515 - val_loss: 0.2277\n",
            "Epoch 359/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1561 - val_loss: 0.2184\n",
            "Epoch 360/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1487 - val_loss: 0.2076\n",
            "Epoch 361/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1481 - val_loss: 0.2321\n",
            "Epoch 362/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1470 - val_loss: 0.2461\n",
            "Epoch 363/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1444 - val_loss: 0.1949\n",
            "Epoch 364/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1454 - val_loss: 0.2109\n",
            "Epoch 365/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1446 - val_loss: 0.2695\n",
            "Epoch 366/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1441 - val_loss: 0.2488\n",
            "Epoch 367/500\n",
            "80/80 [==============================] - 7s 93ms/step - loss: 0.1530 - val_loss: 0.1783\n",
            "Epoch 368/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1385 - val_loss: 0.2030\n",
            "Epoch 369/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1439 - val_loss: 0.2070\n",
            "Epoch 370/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1425 - val_loss: 0.2097\n",
            "Epoch 371/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1459 - val_loss: 0.2205\n",
            "Epoch 372/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1476 - val_loss: 0.2607\n",
            "Epoch 373/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1565 - val_loss: 0.2310\n",
            "Epoch 374/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1422 - val_loss: 0.2218\n",
            "Epoch 375/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1512 - val_loss: 0.2365\n",
            "Epoch 376/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1448 - val_loss: 0.1995\n",
            "Epoch 377/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1433 - val_loss: 0.2538\n",
            "Epoch 378/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1426 - val_loss: 0.2181\n",
            "Epoch 379/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1399 - val_loss: 0.2385\n",
            "Epoch 380/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1389 - val_loss: 0.2485\n",
            "Epoch 381/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1471 - val_loss: 0.2176\n",
            "Epoch 382/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1388 - val_loss: 0.2407\n",
            "Epoch 383/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1407 - val_loss: 0.1860\n",
            "Epoch 384/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1364 - val_loss: 0.2588\n",
            "Epoch 385/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1413 - val_loss: 0.2352\n",
            "Epoch 386/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1426 - val_loss: 0.2192\n",
            "Epoch 387/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1404 - val_loss: 0.2077\n",
            "Epoch 388/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1390 - val_loss: 0.2081\n",
            "Epoch 389/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1375 - val_loss: 0.2179\n",
            "Epoch 390/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1402 - val_loss: 0.2416\n",
            "Epoch 391/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1368 - val_loss: 0.2502\n",
            "Epoch 392/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1432 - val_loss: 0.2395\n",
            "Epoch 393/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1452 - val_loss: 0.2519\n",
            "Epoch 394/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1582 - val_loss: 0.2509\n",
            "Epoch 395/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1461 - val_loss: 0.2450\n",
            "Epoch 396/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1417 - val_loss: 0.2053\n",
            "Epoch 397/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1401 - val_loss: 0.2575\n",
            "Epoch 398/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1419 - val_loss: 0.2159\n",
            "Epoch 399/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1411 - val_loss: 0.2084\n",
            "Epoch 400/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1349 - val_loss: 0.2429\n",
            "Epoch 401/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1450 - val_loss: 0.2502\n",
            "Epoch 402/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1506 - val_loss: 0.2060\n",
            "Epoch 403/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1460 - val_loss: 0.2603\n",
            "Epoch 404/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1391 - val_loss: 0.2270\n",
            "Epoch 405/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1326 - val_loss: 0.2177\n",
            "Epoch 406/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1362 - val_loss: 0.2571\n",
            "Epoch 407/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1319 - val_loss: 0.3000\n",
            "Epoch 408/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1449 - val_loss: 0.2153\n",
            "Epoch 409/500\n",
            "80/80 [==============================] - 6s 74ms/step - loss: 0.1374 - val_loss: 0.2501\n",
            "Epoch 410/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1334 - val_loss: 0.2011\n",
            "Epoch 411/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1370 - val_loss: 0.2145\n",
            "Epoch 412/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1317 - val_loss: 0.2196\n",
            "Epoch 413/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1380 - val_loss: 0.3266\n",
            "Epoch 414/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1428 - val_loss: 0.2270\n",
            "Epoch 415/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1319 - val_loss: 0.2572\n",
            "Epoch 416/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1379 - val_loss: 0.2274\n",
            "Epoch 417/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1319 - val_loss: 0.1960\n",
            "Epoch 418/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1359 - val_loss: 0.2444\n",
            "Epoch 419/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1360 - val_loss: 0.1998\n",
            "Epoch 420/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1391 - val_loss: 0.2491\n",
            "Epoch 421/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1345 - val_loss: 0.3121\n",
            "Epoch 422/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1340 - val_loss: 0.2189\n",
            "Epoch 423/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1364 - val_loss: 0.2289\n",
            "Epoch 424/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1376 - val_loss: 0.1961\n",
            "Epoch 425/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1350 - val_loss: 0.2738\n",
            "Epoch 426/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1301 - val_loss: 0.2477\n",
            "Epoch 427/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1350 - val_loss: 0.2681\n",
            "Epoch 428/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1317 - val_loss: 0.2648\n",
            "Epoch 429/500\n",
            "80/80 [==============================] - 6s 71ms/step - loss: 0.1311 - val_loss: 0.2119\n",
            "Epoch 430/500\n",
            "80/80 [==============================] - 6s 79ms/step - loss: 0.1342 - val_loss: 0.2608\n",
            "Epoch 431/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1307 - val_loss: 0.2532\n",
            "Epoch 432/500\n",
            "80/80 [==============================] - 7s 84ms/step - loss: 0.1384 - val_loss: 0.2247\n",
            "Epoch 433/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1396 - val_loss: 0.2130\n",
            "Epoch 434/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1311 - val_loss: 0.2440\n",
            "Epoch 435/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1424 - val_loss: 0.2398\n",
            "Epoch 436/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1330 - val_loss: 0.2471\n",
            "Epoch 437/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1365 - val_loss: 0.2253\n",
            "Epoch 438/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1298 - val_loss: 0.2204\n",
            "Epoch 439/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1425 - val_loss: 0.2832\n",
            "Epoch 440/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1410 - val_loss: 0.2008\n",
            "Epoch 441/500\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 0.1403 - val_loss: 0.2305\n",
            "Epoch 442/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1366 - val_loss: 0.2161\n",
            "Epoch 443/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1419 - val_loss: 0.2443\n",
            "Epoch 444/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1341 - val_loss: 0.2519\n",
            "Epoch 445/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1345 - val_loss: 0.2941\n",
            "Epoch 446/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1351 - val_loss: 0.2637\n",
            "Epoch 447/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1297 - val_loss: 0.2075\n",
            "Epoch 448/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1331 - val_loss: 0.2215\n",
            "Epoch 449/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1421 - val_loss: 0.2230\n",
            "Epoch 450/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1313 - val_loss: 0.2227\n",
            "Epoch 451/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1289 - val_loss: 0.2532\n",
            "Epoch 452/500\n",
            "80/80 [==============================] - 6s 76ms/step - loss: 0.1284 - val_loss: 0.2545\n",
            "Epoch 453/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1368 - val_loss: 0.2403\n",
            "Epoch 454/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1327 - val_loss: 0.1928\n",
            "Epoch 455/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1363 - val_loss: 0.2384\n",
            "Epoch 456/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1366 - val_loss: 0.2798\n",
            "Epoch 457/500\n",
            "80/80 [==============================] - 6s 77ms/step - loss: 0.1288 - val_loss: 0.2702\n",
            "Epoch 458/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1422 - val_loss: 0.2271\n",
            "Epoch 459/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1346 - val_loss: 0.2517\n",
            "Epoch 460/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1387 - val_loss: 0.1860\n",
            "Epoch 461/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1385 - val_loss: 0.2272\n",
            "Epoch 462/500\n",
            "80/80 [==============================] - 6s 75ms/step - loss: 0.1277 - val_loss: 0.2101\n",
            "Epoch 463/500\n",
            "80/80 [==============================] - 6s 78ms/step - loss: 0.1277 - val_loss: 0.2438\n",
            "Epoch 464/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1241 - val_loss: 0.2867\n",
            "Epoch 465/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1275 - val_loss: 0.2269\n",
            "Epoch 466/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1310 - val_loss: 0.2274\n",
            "Epoch 467/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1272 - val_loss: 0.2972\n",
            "Epoch 468/500\n",
            "80/80 [==============================] - 6s 81ms/step - loss: 0.1326 - val_loss: 0.2337\n",
            "Epoch 469/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1330 - val_loss: 0.1982\n",
            "Epoch 470/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1325 - val_loss: 0.2719\n",
            "Epoch 471/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1360 - val_loss: 0.2217\n",
            "Epoch 472/500\n",
            "80/80 [==============================] - 6s 73ms/step - loss: 0.1291 - val_loss: 0.2480\n",
            "Epoch 473/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1345 - val_loss: 0.2382\n",
            "Epoch 474/500\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.1294 - val_loss: 0.2364\n",
            "Epoch 475/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1229 - val_loss: 0.3287\n",
            "Epoch 476/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1324 - val_loss: 0.2134\n",
            "Epoch 477/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1299 - val_loss: 0.2393\n",
            "Epoch 478/500\n",
            "80/80 [==============================] - 7s 81ms/step - loss: 0.1265 - val_loss: 0.2690\n",
            "Epoch 479/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1301 - val_loss: 0.3002\n",
            "Epoch 480/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1296 - val_loss: 0.2220\n",
            "Epoch 481/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1622 - val_loss: 0.2185\n",
            "Epoch 482/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1441 - val_loss: 0.2303\n",
            "Epoch 483/500\n",
            "80/80 [==============================] - 7s 82ms/step - loss: 0.1312 - val_loss: 0.2117\n",
            "Epoch 484/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1267 - val_loss: 0.2667\n",
            "Epoch 485/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1303 - val_loss: 0.2424\n",
            "Epoch 486/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1247 - val_loss: 0.2393\n",
            "Epoch 487/500\n",
            "80/80 [==============================] - 6s 70ms/step - loss: 0.1252 - val_loss: 0.2330\n",
            "Epoch 488/500\n",
            "80/80 [==============================] - 7s 86ms/step - loss: 0.1269 - val_loss: 0.2697\n",
            "Epoch 489/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1324 - val_loss: 0.2618\n",
            "Epoch 490/500\n",
            "80/80 [==============================] - 7s 85ms/step - loss: 0.1278 - val_loss: 0.2494\n",
            "Epoch 491/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1252 - val_loss: 0.2463\n",
            "Epoch 492/500\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 0.1246 - val_loss: 0.2801\n",
            "Epoch 493/500\n",
            "80/80 [==============================] - 7s 83ms/step - loss: 0.1279 - val_loss: 0.2497\n",
            "Epoch 494/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1270 - val_loss: 0.1989\n",
            "Epoch 495/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1240 - val_loss: 0.2153\n",
            "Epoch 496/500\n",
            "80/80 [==============================] - 6s 72ms/step - loss: 0.1269 - val_loss: 0.2278\n",
            "Epoch 497/500\n",
            "80/80 [==============================] - 5s 66ms/step - loss: 0.1293 - val_loss: 0.2567\n",
            "Epoch 498/500\n",
            "80/80 [==============================] - 7s 87ms/step - loss: 0.1251 - val_loss: 0.2238\n",
            "Epoch 499/500\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.1276 - val_loss: 0.2430\n",
            "Epoch 500/500\n",
            "80/80 [==============================] - 6s 80ms/step - loss: 0.1220 - val_loss: 0.2428\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.2428\n",
            "5/5 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff0fdb9d5b0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "bs=8    # batch_size  \n",
        "vnoise=0    # the amount of noise to be added to training data\n",
        "nn=512    # training units number set as 256\n",
        "nb_epochs=500;    # training epochs change to 1000\n",
        "\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "subset_mode = results_subset.mode(axis=1)    # get mode along result subset - rows axis\n",
        "data_df['most_common'] = subset_mode.iloc[:, 0]    # extract most common element in each row\n",
        "\n",
        "# Data splitting(8:2)\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=42)    # split the data   \n",
        "train_set = train_df.values\n",
        "val_set = val_df.values\n",
        "\n",
        "# Get the Lab data (X) and most selected data (Y)\n",
        "train_set = np.asarray(train_set).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6].astype(int)    # convert the float type to int type\n",
        "Y_train = to_categorical(Y_train)    # one-hot encoding\n",
        "\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6].astype(int)\n",
        "Y_val = to_categorical(Y_val)\n",
        "\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "pred = Dense(3, activation='softmax')(dist)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 5e-5 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "c3b4e356"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqOqgMkNmUzr"
      },
      "source": [
        "#2. Predict Probability Distribution over the 3 Classes#  \n",
        "1.Use the \"cross entropy\" loss function between the prediction and the ground truth.  \n",
        "$H(p, q) = -\\sum_{i=1}^{C} p_i \\log(q_i)$  \n",
        "(Input the Euclidean distance + the outputs of the twin networks to the last layers)  \n",
        "  \n",
        "2.Add a softmax activation function in the last layer. Try to add a \"Temperature\" in this softmax."
      ],
      "id": "gqOqgMkNmUzr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxm_PyRkFQCm"
      },
      "source": [
        "##Dataset Processment **--> Run this! <--**##"
      ],
      "id": "Yxm_PyRkFQCm"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0Xeog_E6pv",
        "outputId": "e7a4c6e3-c3ed-4700-9a1d-648ecf87701e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.1  0.55 0.35]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.25 0.75]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.45 0.55]\n",
            " [0.   0.3  0.7 ]\n",
            " [0.05 0.4  0.55]\n",
            " [0.   0.   1.  ]\n",
            " [0.1  0.35 0.55]\n",
            " [0.   0.3  0.7 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.1  0.35 0.55]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.05 0.   0.95]\n",
            " [0.35 0.5  0.15]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.   0.95]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.2  0.75]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.1  0.35 0.55]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.2  0.75]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.05 0.   0.95]\n",
            " [0.2  0.75 0.05]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.   0.95]\n",
            " [0.05 0.   0.95]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.35 0.65]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.15 0.45 0.4 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.5  0.45]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.1  0.   0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.   0.95]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.1  0.45 0.45]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.1  0.3  0.6 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.65 0.3  0.05]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.3  0.25 0.45]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.4  0.55]\n",
            " [0.   0.3  0.7 ]\n",
            " [0.05 0.25 0.7 ]\n",
            " [0.15 0.45 0.4 ]\n",
            " [0.   0.4  0.6 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.   0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.4  0.6 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.4  0.55]\n",
            " [0.05 0.3  0.65]\n",
            " [0.1  0.65 0.25]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.1  0.85]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.4  0.55]\n",
            " [0.   0.05 0.95]\n",
            " [0.1  0.55 0.35]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.05 0.25 0.7 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.4  0.6 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.2  0.5  0.3 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.15 0.5  0.35]\n",
            " [0.   0.   1.  ]\n",
            " [0.15 0.5  0.35]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.35 0.55 0.1 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.15 0.85]\n",
            " [0.   0.25 0.75]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.35 0.6 ]\n",
            " [0.15 0.8  0.05]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.3  0.7 ]\n",
            " [0.1  0.65 0.25]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.25 0.75]\n",
            " [0.   0.05 0.95]\n",
            " [0.2  0.5  0.3 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.45 0.5 ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.1  0.5  0.4 ]\n",
            " [0.   0.45 0.55]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.2  0.7  0.1 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.35 0.65]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.05 0.1  0.85]\n",
            " [0.   0.05 0.95]\n",
            " [0.45 0.35 0.2 ]\n",
            " [0.05 0.8  0.15]\n",
            " [0.05 0.35 0.6 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.05 0.9 ]\n",
            " [0.   0.05 0.95]\n",
            " [0.05 0.3  0.65]\n",
            " [0.05 0.   0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.15 0.55 0.3 ]\n",
            " [0.   0.2  0.8 ]\n",
            " [0.05 0.35 0.6 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.2  0.5  0.3 ]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.   1.  ]\n",
            " [0.25 0.65 0.1 ]\n",
            " [0.05 0.1  0.85]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.05 0.95]\n",
            " [0.   0.   1.  ]\n",
            " [0.   0.15 0.85]\n",
            " [0.   0.1  0.9 ]\n",
            " [0.05 0.1  0.85]]\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "\n",
        "# count occurrences in each row, and make new columns\n",
        "def count_occurrences(row,element):\n",
        "    count = 0\n",
        "    for value in row:\n",
        "        if value == element:\n",
        "            count += 1\n",
        "    return count/20\n",
        "\n",
        "data_df['0occurrences'] = results_subset.apply(count_occurrences, args=(0,), axis=1)\n",
        "data_df['1occurrences'] = results_subset.apply(count_occurrences, args=(1,), axis=1)\n",
        "data_df['2occurrences'] = results_subset.apply(count_occurrences, args=(2,), axis=1)\n",
        "\n",
        "# Data splitting(7:3)\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "train_df, val_df = train_test_split(data_df, test_size=0.3, random_state=42)    # split the data   \n",
        "train_set = train_df.values\n",
        "val_set = val_df.values\n",
        "\n",
        "# Get the Lab data (X) and probability distribution (Y)\n",
        "train_set = np.asarray(train_set).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6:9]    # probability distribution columns\n",
        "Y_train = Y_train.reshape(-1,3)\n",
        "\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6:9]\n",
        "Y_val = Y_val.reshape(-1,3)\n",
        "print(Y_val)  # example: output to see the possibility distribution for validation set"
      ],
      "id": "2d0Xeog_E6pv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZHdyLOfNdHT"
      },
      "source": [
        "##2.1 Without Temperature in Softmax: Softmax only based the euclidean distance##\n",
        "\n",
        "**Result**  \n",
        "Around Training_loss = 0.38, validation_loss = 0.38 (500 Epochs)"
      ],
      "id": "aZHdyLOfNdHT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uuVsht8xCISl",
        "outputId": "072e2118-6785-47e3-c622-6f584eb5f907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "70/70 [==============================] - 7s 14ms/step - loss: 1.0549 - val_loss: 1.0402\n",
            "Epoch 2/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 1.0334 - val_loss: 1.0203\n",
            "Epoch 3/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 1.0166 - val_loss: 1.0025\n",
            "Epoch 4/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.9990 - val_loss: 0.9854\n",
            "Epoch 5/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.9815 - val_loss: 0.9649\n",
            "Epoch 6/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.9590 - val_loss: 0.9478\n",
            "Epoch 7/500\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.9390 - val_loss: 0.9233\n",
            "Epoch 8/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9141 - val_loss: 0.8934\n",
            "Epoch 9/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.8864 - val_loss: 0.8638\n",
            "Epoch 10/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.8583 - val_loss: 0.8317\n",
            "Epoch 11/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.8222 - val_loss: 0.7926\n",
            "Epoch 12/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.7818 - val_loss: 0.7491\n",
            "Epoch 13/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.7167 - val_loss: 0.6457\n",
            "Epoch 14/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6077 - val_loss: 0.5659\n",
            "Epoch 15/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.5538 - val_loss: 0.5319\n",
            "Epoch 16/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.5216 - val_loss: 0.5015\n",
            "Epoch 17/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.5012 - val_loss: 0.5009\n",
            "Epoch 18/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4878 - val_loss: 0.4787\n",
            "Epoch 19/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4817 - val_loss: 0.4668\n",
            "Epoch 20/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4727 - val_loss: 0.4616\n",
            "Epoch 21/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4674 - val_loss: 0.4580\n",
            "Epoch 22/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4674 - val_loss: 0.4576\n",
            "Epoch 23/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4614 - val_loss: 0.4487\n",
            "Epoch 24/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4575 - val_loss: 0.4460\n",
            "Epoch 25/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4583 - val_loss: 0.4464\n",
            "Epoch 26/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4533 - val_loss: 0.4384\n",
            "Epoch 27/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4477 - val_loss: 0.4368\n",
            "Epoch 28/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4475 - val_loss: 0.4361\n",
            "Epoch 29/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4460 - val_loss: 0.4354\n",
            "Epoch 30/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4448 - val_loss: 0.4298\n",
            "Epoch 31/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4420 - val_loss: 0.4262\n",
            "Epoch 32/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4437 - val_loss: 0.4260\n",
            "Epoch 33/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4405 - val_loss: 0.4315\n",
            "Epoch 34/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4433 - val_loss: 0.4221\n",
            "Epoch 35/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4381 - val_loss: 0.4230\n",
            "Epoch 36/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4374 - val_loss: 0.4176\n",
            "Epoch 37/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4363 - val_loss: 0.4194\n",
            "Epoch 38/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4360 - val_loss: 0.4195\n",
            "Epoch 39/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4349 - val_loss: 0.4207\n",
            "Epoch 40/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4342 - val_loss: 0.4157\n",
            "Epoch 41/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4328 - val_loss: 0.4156\n",
            "Epoch 42/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4339 - val_loss: 0.4144\n",
            "Epoch 43/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4318 - val_loss: 0.4300\n",
            "Epoch 44/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4342 - val_loss: 0.4142\n",
            "Epoch 45/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4317 - val_loss: 0.4121\n",
            "Epoch 46/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4312 - val_loss: 0.4089\n",
            "Epoch 47/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4290 - val_loss: 0.4112\n",
            "Epoch 48/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4276 - val_loss: 0.4072\n",
            "Epoch 49/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4276 - val_loss: 0.4130\n",
            "Epoch 50/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4304 - val_loss: 0.4083\n",
            "Epoch 51/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4300 - val_loss: 0.4095\n",
            "Epoch 52/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4287 - val_loss: 0.4075\n",
            "Epoch 53/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4276 - val_loss: 0.4052\n",
            "Epoch 54/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4286 - val_loss: 0.4105\n",
            "Epoch 55/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4288 - val_loss: 0.4048\n",
            "Epoch 56/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4260 - val_loss: 0.4090\n",
            "Epoch 57/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4264 - val_loss: 0.4045\n",
            "Epoch 58/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4273 - val_loss: 0.4118\n",
            "Epoch 59/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4274 - val_loss: 0.4132\n",
            "Epoch 60/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4281 - val_loss: 0.4046\n",
            "Epoch 61/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4246 - val_loss: 0.4090\n",
            "Epoch 62/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4246 - val_loss: 0.4040\n",
            "Epoch 63/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4242 - val_loss: 0.4040\n",
            "Epoch 64/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4247 - val_loss: 0.4020\n",
            "Epoch 65/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4255 - val_loss: 0.4034\n",
            "Epoch 66/500\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4247 - val_loss: 0.4032\n",
            "Epoch 67/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4233 - val_loss: 0.4074\n",
            "Epoch 68/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4231 - val_loss: 0.4105\n",
            "Epoch 69/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4223 - val_loss: 0.4011\n",
            "Epoch 70/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4231 - val_loss: 0.4096\n",
            "Epoch 71/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4239 - val_loss: 0.4017\n",
            "Epoch 72/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4220 - val_loss: 0.4028\n",
            "Epoch 73/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4233 - val_loss: 0.4010\n",
            "Epoch 74/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4212 - val_loss: 0.4020\n",
            "Epoch 75/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4222 - val_loss: 0.4006\n",
            "Epoch 76/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4216 - val_loss: 0.4001\n",
            "Epoch 77/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4214 - val_loss: 0.4058\n",
            "Epoch 78/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4238 - val_loss: 0.4038\n",
            "Epoch 79/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4192 - val_loss: 0.3993\n",
            "Epoch 80/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4213 - val_loss: 0.4051\n",
            "Epoch 81/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4206 - val_loss: 0.4014\n",
            "Epoch 82/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4198 - val_loss: 0.3968\n",
            "Epoch 83/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4190 - val_loss: 0.3988\n",
            "Epoch 84/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4200 - val_loss: 0.3986\n",
            "Epoch 85/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4211 - val_loss: 0.4007\n",
            "Epoch 86/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4210 - val_loss: 0.3991\n",
            "Epoch 87/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4206 - val_loss: 0.3965\n",
            "Epoch 88/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4197 - val_loss: 0.3968\n",
            "Epoch 89/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4201 - val_loss: 0.3998\n",
            "Epoch 90/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4180 - val_loss: 0.4034\n",
            "Epoch 91/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4190 - val_loss: 0.4023\n",
            "Epoch 92/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4187 - val_loss: 0.3982\n",
            "Epoch 93/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4185 - val_loss: 0.4052\n",
            "Epoch 94/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4186 - val_loss: 0.4029\n",
            "Epoch 95/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4196 - val_loss: 0.3965\n",
            "Epoch 96/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4189 - val_loss: 0.4009\n",
            "Epoch 97/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4182 - val_loss: 0.3997\n",
            "Epoch 98/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4183 - val_loss: 0.3989\n",
            "Epoch 99/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4189 - val_loss: 0.4062\n",
            "Epoch 100/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4198 - val_loss: 0.4005\n",
            "Epoch 101/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4156 - val_loss: 0.3994\n",
            "Epoch 102/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4172 - val_loss: 0.4080\n",
            "Epoch 103/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4182 - val_loss: 0.3984\n",
            "Epoch 104/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4213 - val_loss: 0.3991\n",
            "Epoch 105/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4182 - val_loss: 0.4068\n",
            "Epoch 106/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4169 - val_loss: 0.3975\n",
            "Epoch 107/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4150 - val_loss: 0.3972\n",
            "Epoch 108/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4173 - val_loss: 0.4031\n",
            "Epoch 109/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4160 - val_loss: 0.4007\n",
            "Epoch 110/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4132 - val_loss: 0.3959\n",
            "Epoch 111/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4143 - val_loss: 0.3941\n",
            "Epoch 112/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4145 - val_loss: 0.3968\n",
            "Epoch 113/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4138 - val_loss: 0.3944\n",
            "Epoch 114/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4131 - val_loss: 0.3945\n",
            "Epoch 115/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4151 - val_loss: 0.3972\n",
            "Epoch 116/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4141 - val_loss: 0.3975\n",
            "Epoch 117/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4133 - val_loss: 0.3972\n",
            "Epoch 118/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4133 - val_loss: 0.4082\n",
            "Epoch 119/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4151 - val_loss: 0.3943\n",
            "Epoch 120/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4120 - val_loss: 0.3948\n",
            "Epoch 121/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4159 - val_loss: 0.3957\n",
            "Epoch 122/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4120 - val_loss: 0.3903\n",
            "Epoch 123/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4135 - val_loss: 0.3947\n",
            "Epoch 124/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4115 - val_loss: 0.3957\n",
            "Epoch 125/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4111 - val_loss: 0.4002\n",
            "Epoch 126/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4133 - val_loss: 0.3957\n",
            "Epoch 127/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4107 - val_loss: 0.3948\n",
            "Epoch 128/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4100 - val_loss: 0.3945\n",
            "Epoch 129/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4114 - val_loss: 0.3969\n",
            "Epoch 130/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4120 - val_loss: 0.3931\n",
            "Epoch 131/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4110 - val_loss: 0.4010\n",
            "Epoch 132/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4131 - val_loss: 0.3934\n",
            "Epoch 133/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4095 - val_loss: 0.3993\n",
            "Epoch 134/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4137 - val_loss: 0.3957\n",
            "Epoch 135/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4103 - val_loss: 0.3968\n",
            "Epoch 136/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4096 - val_loss: 0.3972\n",
            "Epoch 137/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4094 - val_loss: 0.3965\n",
            "Epoch 138/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4094 - val_loss: 0.3942\n",
            "Epoch 139/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4088 - val_loss: 0.3911\n",
            "Epoch 140/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4085 - val_loss: 0.3913\n",
            "Epoch 141/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4096 - val_loss: 0.3891\n",
            "Epoch 142/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4089 - val_loss: 0.3921\n",
            "Epoch 143/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4094 - val_loss: 0.3973\n",
            "Epoch 144/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4087 - val_loss: 0.3913\n",
            "Epoch 145/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4080 - val_loss: 0.3893\n",
            "Epoch 146/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4113 - val_loss: 0.3913\n",
            "Epoch 147/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4078 - val_loss: 0.3943\n",
            "Epoch 148/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4076 - val_loss: 0.3894\n",
            "Epoch 149/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4106 - val_loss: 0.3918\n",
            "Epoch 150/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4105 - val_loss: 0.3952\n",
            "Epoch 151/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4082 - val_loss: 0.3862\n",
            "Epoch 152/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4082 - val_loss: 0.3894\n",
            "Epoch 153/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4062 - val_loss: 0.3956\n",
            "Epoch 154/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4069 - val_loss: 0.3899\n",
            "Epoch 155/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4074 - val_loss: 0.3906\n",
            "Epoch 156/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4081 - val_loss: 0.3898\n",
            "Epoch 157/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4072 - val_loss: 0.4017\n",
            "Epoch 158/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4085 - val_loss: 0.4162\n",
            "Epoch 159/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4078 - val_loss: 0.3897\n",
            "Epoch 160/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4080 - val_loss: 0.3947\n",
            "Epoch 161/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4075 - val_loss: 0.3937\n",
            "Epoch 162/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4063 - val_loss: 0.3928\n",
            "Epoch 163/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4055 - val_loss: 0.3903\n",
            "Epoch 164/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4072 - val_loss: 0.3890\n",
            "Epoch 165/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4054 - val_loss: 0.3908\n",
            "Epoch 166/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4050 - val_loss: 0.3948\n",
            "Epoch 167/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4065 - val_loss: 0.3894\n",
            "Epoch 168/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4045 - val_loss: 0.3930\n",
            "Epoch 169/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4061 - val_loss: 0.3895\n",
            "Epoch 170/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4048 - val_loss: 0.3894\n",
            "Epoch 171/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4048 - val_loss: 0.3922\n",
            "Epoch 172/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4045 - val_loss: 0.3932\n",
            "Epoch 173/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4033 - val_loss: 0.3929\n",
            "Epoch 174/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4039 - val_loss: 0.3902\n",
            "Epoch 175/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4045 - val_loss: 0.3903\n",
            "Epoch 176/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.4030 - val_loss: 0.3907\n",
            "Epoch 177/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4041 - val_loss: 0.3913\n",
            "Epoch 178/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4041 - val_loss: 0.3921\n",
            "Epoch 179/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4051 - val_loss: 0.3912\n",
            "Epoch 180/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4046 - val_loss: 0.3977\n",
            "Epoch 181/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4036 - val_loss: 0.3911\n",
            "Epoch 182/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4044 - val_loss: 0.3857\n",
            "Epoch 183/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4018 - val_loss: 0.3949\n",
            "Epoch 184/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4025 - val_loss: 0.3973\n",
            "Epoch 185/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4046 - val_loss: 0.3874\n",
            "Epoch 186/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4010 - val_loss: 0.3903\n",
            "Epoch 187/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4015 - val_loss: 0.3940\n",
            "Epoch 188/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4035 - val_loss: 0.3947\n",
            "Epoch 189/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4042 - val_loss: 0.3867\n",
            "Epoch 190/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4028 - val_loss: 0.3879\n",
            "Epoch 191/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4015 - val_loss: 0.3887\n",
            "Epoch 192/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4020 - val_loss: 0.3873\n",
            "Epoch 193/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4017 - val_loss: 0.3866\n",
            "Epoch 194/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4012 - val_loss: 0.3869\n",
            "Epoch 195/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4035 - val_loss: 0.3916\n",
            "Epoch 196/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4001 - val_loss: 0.3873\n",
            "Epoch 197/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4016 - val_loss: 0.3848\n",
            "Epoch 198/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3994 - val_loss: 0.3884\n",
            "Epoch 199/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4006 - val_loss: 0.3845\n",
            "Epoch 200/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3865\n",
            "Epoch 201/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4013 - val_loss: 0.3867\n",
            "Epoch 202/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4032 - val_loss: 0.3905\n",
            "Epoch 203/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3894\n",
            "Epoch 204/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4017 - val_loss: 0.3873\n",
            "Epoch 205/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4008 - val_loss: 0.3871\n",
            "Epoch 206/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4000 - val_loss: 0.3882\n",
            "Epoch 207/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4003 - val_loss: 0.3924\n",
            "Epoch 208/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3987 - val_loss: 0.3847\n",
            "Epoch 209/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4000 - val_loss: 0.3932\n",
            "Epoch 210/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4023 - val_loss: 0.3912\n",
            "Epoch 211/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4001 - val_loss: 0.3876\n",
            "Epoch 212/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4001 - val_loss: 0.3849\n",
            "Epoch 213/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4010 - val_loss: 0.3834\n",
            "Epoch 214/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4009 - val_loss: 0.3895\n",
            "Epoch 215/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4005 - val_loss: 0.3888\n",
            "Epoch 216/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4005 - val_loss: 0.3896\n",
            "Epoch 217/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3985 - val_loss: 0.3850\n",
            "Epoch 218/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3994 - val_loss: 0.3876\n",
            "Epoch 219/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3993 - val_loss: 0.3865\n",
            "Epoch 220/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3984 - val_loss: 0.3873\n",
            "Epoch 221/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3989 - val_loss: 0.3866\n",
            "Epoch 222/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4011 - val_loss: 0.3831\n",
            "Epoch 223/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3977 - val_loss: 0.3845\n",
            "Epoch 224/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4006 - val_loss: 0.3941\n",
            "Epoch 225/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4012 - val_loss: 0.3880\n",
            "Epoch 226/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3977 - val_loss: 0.3924\n",
            "Epoch 227/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3991 - val_loss: 0.3884\n",
            "Epoch 228/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3991 - val_loss: 0.3885\n",
            "Epoch 229/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3985 - val_loss: 0.3870\n",
            "Epoch 230/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3984 - val_loss: 0.3863\n",
            "Epoch 231/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3977 - val_loss: 0.3854\n",
            "Epoch 232/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3983 - val_loss: 0.3853\n",
            "Epoch 233/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3979 - val_loss: 0.3842\n",
            "Epoch 234/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3975 - val_loss: 0.3889\n",
            "Epoch 235/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.3857\n",
            "Epoch 236/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3968 - val_loss: 0.3866\n",
            "Epoch 237/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3986 - val_loss: 0.3842\n",
            "Epoch 238/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3978 - val_loss: 0.3820\n",
            "Epoch 239/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3975 - val_loss: 0.3878\n",
            "Epoch 240/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3977 - val_loss: 0.3833\n",
            "Epoch 241/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3980 - val_loss: 0.3903\n",
            "Epoch 242/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.3866\n",
            "Epoch 243/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3957 - val_loss: 0.3862\n",
            "Epoch 244/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3968 - val_loss: 0.3841\n",
            "Epoch 245/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3981 - val_loss: 0.3873\n",
            "Epoch 246/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3983 - val_loss: 0.3872\n",
            "Epoch 247/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3973 - val_loss: 0.3880\n",
            "Epoch 248/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3964 - val_loss: 0.3846\n",
            "Epoch 249/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3977 - val_loss: 0.3944\n",
            "Epoch 250/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3979 - val_loss: 0.3830\n",
            "Epoch 251/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3976 - val_loss: 0.3843\n",
            "Epoch 252/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3953 - val_loss: 0.3845\n",
            "Epoch 253/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3961 - val_loss: 0.3859\n",
            "Epoch 254/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3960 - val_loss: 0.3849\n",
            "Epoch 255/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3966 - val_loss: 0.3847\n",
            "Epoch 256/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3976 - val_loss: 0.3831\n",
            "Epoch 257/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3869\n",
            "Epoch 258/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3966 - val_loss: 0.3842\n",
            "Epoch 259/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3960 - val_loss: 0.3848\n",
            "Epoch 260/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3945 - val_loss: 0.3862\n",
            "Epoch 261/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3985 - val_loss: 0.3843\n",
            "Epoch 262/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3961 - val_loss: 0.3848\n",
            "Epoch 263/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3966 - val_loss: 0.3837\n",
            "Epoch 264/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3960 - val_loss: 0.3849\n",
            "Epoch 265/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3934 - val_loss: 0.3819\n",
            "Epoch 266/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3950 - val_loss: 0.3884\n",
            "Epoch 267/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3950 - val_loss: 0.3850\n",
            "Epoch 268/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3932 - val_loss: 0.3866\n",
            "Epoch 269/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3958 - val_loss: 0.3841\n",
            "Epoch 270/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3955 - val_loss: 0.3829\n",
            "Epoch 271/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3957 - val_loss: 0.3854\n",
            "Epoch 272/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3946 - val_loss: 0.3831\n",
            "Epoch 273/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3936 - val_loss: 0.3901\n",
            "Epoch 274/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3837\n",
            "Epoch 275/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3940 - val_loss: 0.3843\n",
            "Epoch 276/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3939 - val_loss: 0.3883\n",
            "Epoch 277/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3954 - val_loss: 0.3849\n",
            "Epoch 278/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3933 - val_loss: 0.3827\n",
            "Epoch 279/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3936 - val_loss: 0.3852\n",
            "Epoch 280/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3943 - val_loss: 0.3840\n",
            "Epoch 281/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3930 - val_loss: 0.3846\n",
            "Epoch 282/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3933 - val_loss: 0.3858\n",
            "Epoch 283/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3938 - val_loss: 0.3873\n",
            "Epoch 284/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3935 - val_loss: 0.3891\n",
            "Epoch 285/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3954 - val_loss: 0.3846\n",
            "Epoch 286/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3932 - val_loss: 0.3842\n",
            "Epoch 287/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3946 - val_loss: 0.3862\n",
            "Epoch 288/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3831\n",
            "Epoch 289/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3942 - val_loss: 0.3865\n",
            "Epoch 290/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3963 - val_loss: 0.3863\n",
            "Epoch 291/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3948 - val_loss: 0.3841\n",
            "Epoch 292/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3936 - val_loss: 0.3842\n",
            "Epoch 293/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3924 - val_loss: 0.3819\n",
            "Epoch 294/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3931 - val_loss: 0.3881\n",
            "Epoch 295/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3929 - val_loss: 0.3818\n",
            "Epoch 296/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3947 - val_loss: 0.3808\n",
            "Epoch 297/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3933 - val_loss: 0.3884\n",
            "Epoch 298/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3957 - val_loss: 0.3927\n",
            "Epoch 299/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3943 - val_loss: 0.3850\n",
            "Epoch 300/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3934 - val_loss: 0.3817\n",
            "Epoch 301/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3930 - val_loss: 0.3904\n",
            "Epoch 302/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3931 - val_loss: 0.3861\n",
            "Epoch 303/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3933 - val_loss: 0.3839\n",
            "Epoch 304/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3915 - val_loss: 0.3879\n",
            "Epoch 305/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3922 - val_loss: 0.3820\n",
            "Epoch 306/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3913 - val_loss: 0.3880\n",
            "Epoch 307/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.3816\n",
            "Epoch 308/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3919 - val_loss: 0.3806\n",
            "Epoch 309/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3931 - val_loss: 0.3811\n",
            "Epoch 310/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3904 - val_loss: 0.3853\n",
            "Epoch 311/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3918 - val_loss: 0.3864\n",
            "Epoch 312/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3909 - val_loss: 0.3880\n",
            "Epoch 313/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3900\n",
            "Epoch 314/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3912 - val_loss: 0.3816\n",
            "Epoch 315/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3907 - val_loss: 0.3799\n",
            "Epoch 316/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3922 - val_loss: 0.3836\n",
            "Epoch 317/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3902 - val_loss: 0.3793\n",
            "Epoch 318/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3913 - val_loss: 0.3813\n",
            "Epoch 319/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3914 - val_loss: 0.3827\n",
            "Epoch 320/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3919 - val_loss: 0.3807\n",
            "Epoch 321/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3921 - val_loss: 0.3823\n",
            "Epoch 322/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3914 - val_loss: 0.3831\n",
            "Epoch 323/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3919 - val_loss: 0.3786\n",
            "Epoch 324/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3917 - val_loss: 0.3863\n",
            "Epoch 325/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3925 - val_loss: 0.3825\n",
            "Epoch 326/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3919 - val_loss: 0.3827\n",
            "Epoch 327/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3905 - val_loss: 0.3810\n",
            "Epoch 328/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3907 - val_loss: 0.3841\n",
            "Epoch 329/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3945 - val_loss: 0.3803\n",
            "Epoch 330/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3895 - val_loss: 0.3875\n",
            "Epoch 331/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3898 - val_loss: 0.3797\n",
            "Epoch 332/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3885 - val_loss: 0.3854\n",
            "Epoch 333/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3895 - val_loss: 0.3850\n",
            "Epoch 334/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.3813\n",
            "Epoch 335/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3897 - val_loss: 0.3860\n",
            "Epoch 336/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3917 - val_loss: 0.3849\n",
            "Epoch 337/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3903 - val_loss: 0.3833\n",
            "Epoch 338/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3909 - val_loss: 0.3898\n",
            "Epoch 339/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3907 - val_loss: 0.3842\n",
            "Epoch 340/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3894\n",
            "Epoch 341/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3911 - val_loss: 0.3811\n",
            "Epoch 342/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3894 - val_loss: 0.3826\n",
            "Epoch 343/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3892 - val_loss: 0.3799\n",
            "Epoch 344/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3893 - val_loss: 0.3786\n",
            "Epoch 345/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3905 - val_loss: 0.3844\n",
            "Epoch 346/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3895 - val_loss: 0.3830\n",
            "Epoch 347/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3896 - val_loss: 0.3811\n",
            "Epoch 348/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3886 - val_loss: 0.3796\n",
            "Epoch 349/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3897 - val_loss: 0.3874\n",
            "Epoch 350/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3887 - val_loss: 0.3809\n",
            "Epoch 351/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3887 - val_loss: 0.3814\n",
            "Epoch 352/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3893 - val_loss: 0.3787\n",
            "Epoch 353/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3884 - val_loss: 0.3809\n",
            "Epoch 354/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3794\n",
            "Epoch 355/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3897 - val_loss: 0.3882\n",
            "Epoch 356/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3850\n",
            "Epoch 357/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3911 - val_loss: 0.3862\n",
            "Epoch 358/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3835\n",
            "Epoch 359/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3887 - val_loss: 0.3821\n",
            "Epoch 360/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3879 - val_loss: 0.3787\n",
            "Epoch 361/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3912 - val_loss: 0.3838\n",
            "Epoch 362/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3884 - val_loss: 0.3874\n",
            "Epoch 363/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3888 - val_loss: 0.3845\n",
            "Epoch 364/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3860\n",
            "Epoch 365/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3883 - val_loss: 0.3823\n",
            "Epoch 366/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.3882 - val_loss: 0.3823\n",
            "Epoch 367/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3861\n",
            "Epoch 368/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3878 - val_loss: 0.3883\n",
            "Epoch 369/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3877 - val_loss: 0.3845\n",
            "Epoch 370/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3885 - val_loss: 0.3832\n",
            "Epoch 371/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3872 - val_loss: 0.3821\n",
            "Epoch 372/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3893 - val_loss: 0.3865\n",
            "Epoch 373/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3890 - val_loss: 0.3880\n",
            "Epoch 374/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3904\n",
            "Epoch 375/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3907 - val_loss: 0.3806\n",
            "Epoch 376/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3877 - val_loss: 0.3850\n",
            "Epoch 377/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3828\n",
            "Epoch 378/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3878 - val_loss: 0.3869\n",
            "Epoch 379/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3896 - val_loss: 0.3860\n",
            "Epoch 380/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3870 - val_loss: 0.3825\n",
            "Epoch 381/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3885\n",
            "Epoch 382/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3880 - val_loss: 0.3846\n",
            "Epoch 383/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3875 - val_loss: 0.3820\n",
            "Epoch 384/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3825\n",
            "Epoch 385/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3860 - val_loss: 0.3848\n",
            "Epoch 386/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3875 - val_loss: 0.3826\n",
            "Epoch 387/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3867 - val_loss: 0.3822\n",
            "Epoch 388/500\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3873 - val_loss: 0.3842\n",
            "Epoch 389/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3868 - val_loss: 0.3806\n",
            "Epoch 390/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3872 - val_loss: 0.3876\n",
            "Epoch 391/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3887 - val_loss: 0.3810\n",
            "Epoch 392/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3860 - val_loss: 0.3806\n",
            "Epoch 393/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3855 - val_loss: 0.3863\n",
            "Epoch 394/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3867 - val_loss: 0.3827\n",
            "Epoch 395/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3865 - val_loss: 0.3864\n",
            "Epoch 396/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3865 - val_loss: 0.3862\n",
            "Epoch 397/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3860\n",
            "Epoch 398/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3853\n",
            "Epoch 399/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.3830\n",
            "Epoch 400/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3876 - val_loss: 0.3850\n",
            "Epoch 401/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3835\n",
            "Epoch 402/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3873 - val_loss: 0.3841\n",
            "Epoch 403/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3844 - val_loss: 0.3829\n",
            "Epoch 404/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3862 - val_loss: 0.3845\n",
            "Epoch 405/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3859 - val_loss: 0.3827\n",
            "Epoch 406/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3857\n",
            "Epoch 407/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3859\n",
            "Epoch 408/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3857 - val_loss: 0.3827\n",
            "Epoch 409/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3861 - val_loss: 0.3861\n",
            "Epoch 410/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3885 - val_loss: 0.3828\n",
            "Epoch 411/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3865 - val_loss: 0.3859\n",
            "Epoch 412/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3865 - val_loss: 0.3817\n",
            "Epoch 413/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3861 - val_loss: 0.3853\n",
            "Epoch 414/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.3813\n",
            "Epoch 415/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.3832\n",
            "Epoch 416/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3864 - val_loss: 0.3816\n",
            "Epoch 417/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.3880\n",
            "Epoch 418/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3873 - val_loss: 0.3858\n",
            "Epoch 419/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3858 - val_loss: 0.3821\n",
            "Epoch 420/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3850 - val_loss: 0.3834\n",
            "Epoch 421/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3839\n",
            "Epoch 422/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3856 - val_loss: 0.3823\n",
            "Epoch 423/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3853\n",
            "Epoch 424/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3877 - val_loss: 0.3830\n",
            "Epoch 425/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3856 - val_loss: 0.3839\n",
            "Epoch 426/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3890\n",
            "Epoch 427/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3853 - val_loss: 0.3879\n",
            "Epoch 428/500\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3843 - val_loss: 0.3823\n",
            "Epoch 429/500\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3852 - val_loss: 0.3857\n",
            "Epoch 430/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3869 - val_loss: 0.3828\n",
            "Epoch 431/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3846 - val_loss: 0.3830\n",
            "Epoch 432/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3850 - val_loss: 0.3842\n",
            "Epoch 433/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3858\n",
            "Epoch 434/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3844\n",
            "Epoch 435/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.3839\n",
            "Epoch 436/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3853 - val_loss: 0.3842\n",
            "Epoch 437/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.3855\n",
            "Epoch 438/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3834 - val_loss: 0.3837\n",
            "Epoch 439/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3881\n",
            "Epoch 440/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3855 - val_loss: 0.3905\n",
            "Epoch 441/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3867 - val_loss: 0.3897\n",
            "Epoch 442/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3855\n",
            "Epoch 443/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3854 - val_loss: 0.3923\n",
            "Epoch 444/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3869 - val_loss: 0.3871\n",
            "Epoch 445/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3854\n",
            "Epoch 446/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3843 - val_loss: 0.3926\n",
            "Epoch 447/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3849 - val_loss: 0.3825\n",
            "Epoch 448/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3851 - val_loss: 0.3828\n",
            "Epoch 449/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3843 - val_loss: 0.3853\n",
            "Epoch 450/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3850 - val_loss: 0.3842\n",
            "Epoch 451/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3850 - val_loss: 0.3834\n",
            "Epoch 452/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3844 - val_loss: 0.3874\n",
            "Epoch 453/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3853 - val_loss: 0.3805\n",
            "Epoch 454/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3845 - val_loss: 0.3858\n",
            "Epoch 455/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.3829\n",
            "Epoch 456/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.3839\n",
            "Epoch 457/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3849 - val_loss: 0.3794\n",
            "Epoch 458/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3839 - val_loss: 0.3794\n",
            "Epoch 459/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3833\n",
            "Epoch 460/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3839 - val_loss: 0.3816\n",
            "Epoch 461/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3829 - val_loss: 0.3826\n",
            "Epoch 462/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3822 - val_loss: 0.3830\n",
            "Epoch 463/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3827 - val_loss: 0.3828\n",
            "Epoch 464/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3844 - val_loss: 0.3868\n",
            "Epoch 465/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.3887\n",
            "Epoch 466/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3833 - val_loss: 0.3856\n",
            "Epoch 467/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3844 - val_loss: 0.3913\n",
            "Epoch 468/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3831 - val_loss: 0.3844\n",
            "Epoch 469/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3846 - val_loss: 0.3846\n",
            "Epoch 470/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3857 - val_loss: 0.3870\n",
            "Epoch 471/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3841 - val_loss: 0.3834\n",
            "Epoch 472/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3827 - val_loss: 0.3881\n",
            "Epoch 473/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3829 - val_loss: 0.3837\n",
            "Epoch 474/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3822 - val_loss: 0.3830\n",
            "Epoch 475/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3851\n",
            "Epoch 476/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3842 - val_loss: 0.3826\n",
            "Epoch 477/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3823 - val_loss: 0.3859\n",
            "Epoch 478/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3833 - val_loss: 0.3865\n",
            "Epoch 479/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3837 - val_loss: 0.3840\n",
            "Epoch 480/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3836 - val_loss: 0.3886\n",
            "Epoch 481/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3840 - val_loss: 0.3876\n",
            "Epoch 482/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3831 - val_loss: 0.3892\n",
            "Epoch 483/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3802\n",
            "Epoch 484/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3845 - val_loss: 0.3861\n",
            "Epoch 485/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3842 - val_loss: 0.3889\n",
            "Epoch 486/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3820 - val_loss: 0.3899\n",
            "Epoch 487/500\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3818 - val_loss: 0.3860\n",
            "Epoch 488/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3855 - val_loss: 0.3848\n",
            "Epoch 489/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3828 - val_loss: 0.3829\n",
            "Epoch 490/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3837\n",
            "Epoch 491/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3828 - val_loss: 0.3843\n",
            "Epoch 492/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3830 - val_loss: 0.3811\n",
            "Epoch 493/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.3906\n",
            "Epoch 494/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3877\n",
            "Epoch 495/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3836 - val_loss: 0.3804\n",
            "Epoch 496/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3822 - val_loss: 0.3875\n",
            "Epoch 497/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3832 - val_loss: 0.3806\n",
            "Epoch 498/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3835 - val_loss: 0.3871\n",
            "Epoch 499/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3833 - val_loss: 0.3868\n",
            "Epoch 500/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3829 - val_loss: 0.3860\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3860\n",
            "8/8 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fce863eb100>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHMCAYAAAAzqWlnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB7qklEQVR4nO3dd3hUVf7H8fedzEx6JYSEhB66NCkqoICIImLvuquCa1nLrvqz7GLDuott1VXXXQvq2kBWlCZWEBVEmiBNek9IQjLpZcr9/TFkYEyomcwM4fN6Hh+ZO3funPkmJB/OOfccwzRNExEREZEmyhLqBoiIiIg0JoUdERERadIUdkRERKRJU9gRERGRJk1hR0RERJo0hR0RERFp0hR2REREpElT2BEREZEmTWFHREREmjSFHZEQMgyDoUOHNvg6Q4cOxTCMhjeoiQlUfUXk2KawI8c1wzCO6L+33nor1E2WRhAO3wdvvfXWUV+7tl0iUj9rqBsgEkoPP/xwnWPPP/88xcXF/PnPfyYpKcnvud69ewf0/desWUNMTEyDr/POO+9QUVERgBYdn0L9fSAijcvQRqAi/tq2bcvWrVvZvHkzbdu2DXVzpAEMw2DIkCHMnTv3iF8b7O+Dt956izFjxjBx4kSuu+66I3ptba+OfpyL1E/DWCKHqXZeTE1NDY8++iidO3cmMjLS94upuLiYp59+mtNPP52srCzsdjvNmzfnvPPOY8GCBfVes745JePHj8cwDObOncuUKVMYMGAAMTExpKSkcMUVV7Bz584Dtm1/c+fOxTAMxo8fz88//8w555xDUlISMTExDBkyhPnz59fbppycHMaMGUNaWhrR0dH07t2bt99+2+96h6Mh9SgoKODGG28kIyODyMhIunfvzsSJE+t9TU1NDY899hgdOnQgMjKSdu3a8cADD1BdXX1Y7TwaCxcu5JJLLiE9PR273U6rVq246aab2LVrV51zN23axI033kh2djbR0dGkpKTQo0cPbr75Zvbs2QN4v35jxowBYMyYMX5DZlu2bAlo26urq/n73/9Ojx49iImJISEhgVNPPZXJkyfXe/60adMYPny472vRsmVLhgwZwiuvvHLEn3N/H3zwAcOGDSMpKYmoqCi6du3K448/Xu/X7bvvvuPcc88lKyuLyMhI0tPTOfnkk3nkkUcCUxRp8jSMJXKELr74YhYtWsTZZ5/NBRdcQFpaGuAdkrr//vs57bTTOOecc0hOTmbbtm1MmzaNzz77jOnTpzNy5MjDfp9XXnmFadOmcd555zFkyBAWLlzIpEmTWL58OT///DORkZGHdZ3Fixfz1FNPccopp/CHP/yBbdu28b///Y/hw4fz888/07lzZ9+5eXl5nHLKKWzdupXTTjuNgQMHkpubyy233MKZZ555RHU62no4HA4GDRqE3W7nkksuobq6mo8++oixY8disVi49tprfeeapslll13Gp59+SocOHbjtttuoqanhzTff5Jdffjmi9h6uN998kxtvvJHIyEjOO+88WrVqxfr163n99deZPn06P/74I61btwa8wbF///6UlJQwatQoLr74Yqqqqti8eTP//e9/ue2222jWrBnXXXcdSUlJfPrpp5x//vl+w2S/HUJriJqaGs466yy+/fZbunTpwq233kpFRQVTpkzh8ssv5+eff+bJJ5/0nf+f//yHm266ifT0dM4991xSU1PJy8tjxYoVTJw4kVtuueWIPmetsWPHMnHiRLKysrj44otJSkrixx9/5MEHH+Trr7/myy+/xGr1/nqaPXs255xzDgkJCZx33nlkZmZSWFjImjVreOWVV+odghSpwxQRP23atDEBc/PmzX7HhwwZYgJmjx49zPz8/Dqvczgc9R7fvn27mZGRYXbp0qXOc4A5ZMgQv2MPP/ywCZjx8fHmihUr/J678sorTcCcNGlSvW3b35w5c0zABMyJEyf6Pffqq6+agPnHP/7R7/jYsWNNwLz33nv9jv/888+m3W43AfPhhx+u8znqc7T1AMzrr7/edLlcvuOrVq0yIyIizK5du/qd/95775mAefLJJ5uVlZW+43v27DHbt29fb30PV33fB7/++qtps9nMDh06mDt27PA7/6uvvjItFot5wQUX+I69+OKLJmA+//zzda5fVlZmVlRU+B5PnDix3q/V4ait26E8+eSTJmCeffbZptPp9B3fvXu37/P+8MMPvuMnnniiabfbzd27d9e51v5f26P5nBdeeKHfcdPc972//3UuuugiEzB//vnng7ZB5GA0jCVyhB577DFSU1PrHE9MTKz3eFZWFpdccglr165l27Zth/0+f/rTn+jRo4ffsRtuuAGAn3766bCvM2jQoDpzQMaOHYvVavW7Tk1NDR988AGJiYk88MADfuf36tWLa6655rDfE46+HjExMTz33HNERET4jnXr1o1BgwaxZs0aysrKfMdrh7aefPJJoqKifMdTUlJ48MEHj6i9h+Nf//oXTqeTF154gczMTL/nhg8fznnnncf06dMpLS31ey46OrrOtWJjY+s93pjefPNNDMPgueee8/WcAKSlpfnq9frrr/u9xmq1YrPZ6lyrvq/t4XzOF154AavVyptvvlnn/AcffJBmzZrx3nvvHda162uDSH00jCVyhAYMGHDA53744QdeeOEFFixYQF5eHjU1NX7P79y50zfEcSj9+vWrc6xVq1YAFBUVHXZ767uOzWajRYsWftf59ddfqayspF+/fsTHx9d5zeDBg+v8IjyUo6lHx44dSUhIqHOt/T97XFwcAEuXLsVisTB48OA65zfG+jq1c42+/fZbFi1aVOf5vLw83G4369ato2/fvpx33nmMGzeOW2+9lc8//5yzzjqLQYMG0a1bt6DfKl5aWsqGDRvIzMykS5cudZ4//fTTAVi2bJnv2NVXX83//d//0a1bN6644gqGDBnCoEGDaN68ud9rD/dzVlRUsHz5clJTU3n++efrbWdkZCRr1qzxa8PHH3/MSSedxOWXX86wYcMYNGgQWVlZDSmHHGcUdkSOUHp6er3Hp06dyiWXXEJUVBQjRoygQ4cOxMbGYrFYmDt3Lt9+++0RTZqtb65G7b/G3W53g65Te639r1NcXAxAixYt6j3/QMcP5GjrcbD2AnXanJKSUm/Pw4G+Tg1RO9H26aefPuh5tb1Pbdq04aeffmL8+PHMnj2bjz/+GPAGt7vvvps//elPAW/jgdR+fTMyMup9vva4w+HwHbvrrrtITU3llVde4cUXX+T555/33eH29NNP+4L04X7OoqIiTNMkPz//sCcXX3TRRcyYMYNnn32WN998k3//+98A9O3bl7/97W+MGDHiyIshxx2FHZEjdKB/kT/44IPY7XYWL15M165d/Z676aab+Pbbb4PRvKNW25uye/fuep8/0PEDCUY9EhMTKSwsxOl01gk8ubm5Db5+fe8H3uBQX+9Tfbp27cqkSZNwuVwsX76cr776in/+85/8+c9/JjY2luuvvz7g7axPbdsPVJecnBy/82pdc801XHPNNTgcDubPn8/UqVN58803Oeuss1i7dq2vl+dwPmfttfv06cPSpUsPu+3nnHMO55xzDuXl5SxcuJAZM2bwr3/9i9GjR7Ns2TK6det2xPWQ44vm7IgEyIYNG+jWrVudX+wej4fvv/8+RK06fF26dCE6OpoVK1bUmXMCHPFnCEY9TjzxxANe72jW1jmUk08+GfDeCn2krFYrffv25b777uODDz4A4JNPPvE9XztH6Uh67Y5EfHw8HTp0YOfOnaxfv77O83PmzAG8Na1PUlISo0aN4rXXXuO6666jsLCQefPm1TnvYJ8zLi6O7t27s2rVKgoLC4/4M8TGxnL66afz3HPPMW7cOGpqavjss8+O+Dpy/FHYEQmQtm3bsn79er+1VkzTZPz48axevTqELTs8drudyy+/nOLiYh5//HG/55YvX84777xzRNcLRj1q16a5//77qaqq8h0vLCys8xkC4bbbbsNms3HnnXeybt26Os/X1NT4BaElS5b4ho/2V9tLtv/q2bW3Zh/JJPYjNXbsWEzT5J577vELVQUFBTz22GO+c2rNmTOn3oUK8/LygH3tP5LPedddd1FTU8PYsWP9hsxqFRUV+fX6zJs3D5fLdVjXFjkQDWOJBMidd97JzTffTJ8+fbj44oux2Wz88MMPrF69mnPPPZfp06eHuomH9Pe//51vvvmGp556ioULFzJw4EBycnKYPHkyo0aN4pNPPsFiObx/IwWjHldeeSWTJk1i2rRpnHDCCZx//vk4nU6mTJlC//792bhxY4PfY39dunThzTffZOzYsXTv3p2RI0fSqVMnnE4n27Zt47vvvqN58+asXbsWgP/+97/8+9//ZvDgwXTo0IHk5GQ2btzI9OnTiYyM5I477vBd+5RTTiEmJobnn3+ePXv2+OYc3X777XWGlg7kYCsvv/LKK9x999189tlnfPrpp/Tq1YtRo0ZRUVHBRx99RF5eHvfee6/fZO8LL7yQuLg4Tj75ZNq2bYtpmnz33XcsWrSIvn37csYZZxzx5xw7dixLlizhlVdeoUOHDpx11lm0bt2awsJCNm/ezLx58xgzZgyvvvoq4L0rcefOnQwaNIi2bdtit9tZsmQJ33zzDW3atOGKK644rNrIcS6U972LhKNDrbNzMBMnTjR79eplxsTEmM2aNTMvuOACc8WKFb71Q+bMmeN3PgdZZ+e355qmaW7evNkEzGuvvfaQbatdZ+dA6+K0adPGbNOmTZ3jO3bsMK+55hozNTXVjIqKMnv16mW+9dZb5kcffWQC5j/+8Y+D1mB/gahHrWuvvbber0t1dbX5yCOPmO3atTPtdrvZpk0bc9y4cWZVVVXA19mptWLFCvPaa681W7dubdrtdjM5Odns3r27eeONN5pff/2177wff/zRvPnmm82ePXuaycnJZlRUlNmhQwfzuuuuM3/55Zc61/3ss8/Mk08+2YyNjfWtnVPf+/9W7bkH+6+oqMg0TdOsrKw0n3jiCbN79+5mVFSUGRcXZw4aNMh8//3361z3X//6l3nBBReY7dq1M6Ojo83k5GSzd+/e5oQJE8ySkpKj/pymaZrTp083zznnHLN58+amzWYzW7RoYfbv39+8//77zTVr1vjOmzRpknnFFVeY2dnZZmxsrBkfH292797dHDdunJmXl3fI2oiYpmlqbywROSz3338/Tz75JLNnz+ass84KdXNERA6bwo6I+Nm1axctW7b0O/bLL78wcOBA7HY7O3fu9FvAT0Qk3GnOjoj46devH9nZ2ZxwwgnExsayfv16Zs6cicfj4d///reCjogcc9SzIyJ+HnnkET755BO2bNlCaWkpSUlJnHzyydx9992NsiqxiEhjU9gRERGRJk3r7IiIiEiTprAjIiIiTZrCjoiIiDRpCjsiIiLSpOnW872Kiorq3X+loZo3b05+fn7Aryv+VOfgUa2DQ3UODtU5eAJda6vVSnJy8uGdG7B3Pca5XC6cTmdAr2kYhu/auumt8ajOwaNaB4fqHByqc/CEutYaxhIREZEmTWFHREREmjSFHREREWnSFHZERESkSdMEZRERaXJcLhcVFRWHPK+yspKampogtEiOtNamaWK1WomNjW3weyvsiIhIk+JyuSgvLyc+Ph6L5eADGDabLeB34kr9jqbW5eXlVFdXExkZ2aD31jCWiIg0KRUVFYcVdCT8xcTEUF1d3eDr6DtBRESaHAWdpqF2fZ6G0neDiIiINGkKOyIiItKkKeyIiIg0MSeddBKvvfZaQK41f/58MjMzKS4uDsj1QkF3Y4mIiISBSy65hG7duvHoo482+FqzZs0iJiYmAK1qGhR2GonpdkNZCS7cQESomyMiIsc40zRxu91YrYf+1d2sWbMgtOjYoWGsxrJuJe67ryX/0btC3RIREQlzd9xxBwsWLOCNN94gMzOTzMxMJk2aRGZmJt988w0jR46kXbt2/PTTT2zZsoUxY8bQq1cvOnbsyKhRo5g3b57f9X47jJWZmcn777/P9ddfT4cOHRg0aBBffPHFUbd35syZDBs2jHbt2nHSSSfx6quv+j3/1ltvMWjQINq3b0+vXr244YYbfM/NmDGD4cOH06FDB7p3787ll19+WAtANoR6dhpLfCIAHkehEqWISAiZpgk19a/VYnrcmI25qKA98rBun3700UfZtGkTXbp04e677wbg119/BeDJJ5/koYceonXr1iQmJrJr1y5OP/107rvvPux2O1OmTGHMmDHMmzePzMzMA77Hc889xwMPPMADDzzAxIkTue2221i4cCHJyclH9JFWrFjBzTffzF133cV5553H4sWLGTduHMnJyVx++eUsX76chx56iBdffJF+/frhcDhYuHAhALt37+bWW2/l/vvv5+yzz6asrIyFCxd6v0aNSGGnsSTsDTulxRgeNxiKPCIiIVFTjee2y+p9quHL1R2c5aXJEBl1yPMSEhKw2+1ERUWRlpYGwIYNGwC45557OO2003znJicn0717d9/je++9l9mzZ/PFF18wZsyYA77HZZddxgUXXADAX/7yF9544w1+/vlnhg0bdkSf6T//+Q+DBw/mzjvvBKBDhw6sX7+eV199lcsvv5ydO3cSExPDGWecQVxcHFlZWZxwwgkA5OXl4XK5GDVqFFlZWQB07dr1iN7/aOg3cGOJTQDDANOEstJQt0ZERI5RPXv29HtcXl7Oo48+ypAhQ+jatSsdO3Zk/fr17Ny586DX2T9UxMTEEB8fT0FBwRG3Z/369fTv39/vWP/+/dm8eTNut5vTTjuNrKwsTjnlFG6//XY+/vhjKisrAejWrRuDBw9m+PDh3Hjjjbz33ns4HI4jbsORUs9OIzEiIiA2zht0Sot9w1oiIhJk9khvD0s9Gn1vLHvD9nQC6txV9eijj/Ldd9/x4IMP0rZtW6KiorjxxhsPucmmzWbze2wYBh6Pp8Ht+624uDhmz57N/PnzmTdvHs888wzPPvssX375JTExMXz44YcsXryYb7/9lokTJzJhwgRmzJhB69atA96WWurZaUzxSQCYpcfu2gQiIsc6wzAwIqNC898RbHdgs9kOK3wsXryYSy+9lLPPPpuuXbuSlpbGjh07GlKiI9KxY0cWLVrkd2zRokW0b9+eiAjv3cdWq5XTTjuNBx54gK+++oodO3bw3XffAd6vR//+/bn77rv5/PPPsdlsfPbZZ43aZvXsNKb4RMjZDiUKOyIicnCtWrVi2bJlbN++ndjY2AMGn3bt2vHZZ58xYsQIDMPg6aefbpQemgO56aabGDVqFP/4xz8477zzWLJkCRMnTuTJJ58E4Msvv2Tbtm2cdNJJJCUl8fXXX+PxeMjOzmbp0qV8//33DBkyhNTUVJYuXUphYSEdO3Zs1DYr7DSSdQWVvNzifBKjTuaRUkeomyMiImHupptu4o477mDo0KFUVVXx3HPP1Xveww8/zF133cX5559PSkoKt956K2VlZUFrZ48ePXj11Vd55plneOGFF0hLS+Oee+7h8ssvByAxMZHPPvuM5557jqqqKtq1a8fLL79Mly5dWL16NQsXLuT111+nrKyMzMxMHnroIU4//fRGbbNhNvb9XseI/Pz8gI7bbiys4q7PtpBcXcKbzdZjOf/qgF1b/BmGQUZGBjk5OY1+++LxTrUODtW5YUpKSkhISDiscxt9zo74HG2tD/T1tNlsNG/e/LCuoTk7jSQpyjtuWWyPw605OyIiIiGjYaxGkhRlxcDEY1goKalCC3eLiEg4uu+++/j444/rfe6iiy5iwoQJQW5R4CnsNJIIi0FihInDbVBU4VTYERGRsHTPPfdw88031/tcfHx8kFvTOBR2GlFypAVHBRRVuULdFBERkXqlpqaSmpoa6mY0Ks3ZaUTJMd4FnIqcaJKhiIhIiCjsNKLkeO9+KEURMVBWEuLWiIiIHJ8UdhpRcowdgCJ7AhTtCXFrREREjk8KO40oJdo7JaooMh6KC0PcGhERkeOTwk4jSt4bdhz2eEz17IiIiISEwk4jqg07RfYEcCjsiIhI+Nq+fTuZmZmsXLky1E0JOIWdRrQv7MRjOjSMJSIiB3bJJZfw0EMPBex6d9xxB2PHjg3Y9Y5lCjuNqDbs1ETYqXBoywgREZFQUNhpRFFWCzHeLbIoKqsObWNERCRs3XHHHSxYsIA33niDzMxMMjMz2b59O2vXruV3v/sdHTt2pFevXtx+++0UFu4bKZgxYwbDhw+nQ4cOdO/encsvv5yKigqeffZZPvroIz7//HPf9ebPn3/E7VqwYAHnnHMO7dq1o0+fPjz55JO4XPsWyj3Q+wPMnz+fc845h+zsbLp27co555zDjh07Gl6so6AVlBtZs2grFWUuiqpctA51Y0REjkOmaVLtrn9hVzcenC5Po713ZISBYRiHPO/RRx9l06ZNdOnShbvvvhsAq9XKOeecw5VXXsn48eOpqqriiSee4KabbuKjjz5i9+7d3Hrrrdx///2cffbZlJWVsXDhQkzT5Oabb2b9+vWUlZXx3HPPAZCUlHREbc/JyeH3v/89l112GS+88AIbNmzgnnvuITIykv/7v/876Pu7XC6uv/56rrrqKl5++WWcTicrVqw4rFo0BoWdRpYaH8X2sjIc7ghMZw2GzR7qJomIHFeq3SaXT1oXkveedHknoqyH/gWfkJCA3W4nKiqKtLQ0AJ5//nlOOOEE/vrXv/rOe/bZZ+nfvz8bN26koqICl8vFqFGjyMrKAqBr166+c6OioqipqfFd70i9/fbbtGzZkieeeALDMMjOziY3N5cnn3ySO++8k7y8vAO+f1FRESUlJZxxxhm0bdsWgG7duuF0Oo+qLQ0VVmFn9erVTJs2jc2bN1NUVMTdd9/NgAEDDvqaVatW8c4777B9+3aaNWvGxRdfzNChQ4PT4MOQmhADOWV778gqhObpoW6SiIgcA1avXs38+fPp2LFjnee2bt3KkCFDGDx4MMOHD2fIkCEMGTKEc84554h7cA5kw4YN9O3b1683pn///pSXl5OTk0O3bt0O+P7JyclcdtllXH311Zx66qmceuqpXHTRRaSkpASkbUcqrMJOdXU1bdu25fTTT+eZZ5455Pl5eXn8/e9/Z8SIEdx+++2sXLmSV199laSkJHr37t34DT4MqXGRgPeOLIUdEZHgi4wwmHR5p3qfs1ltOF2N19sQGXH0wzYVFRWMGDGCcePG1XmuRYsWRERE8OGHH7J48WK+/fZbJk6cyIQJE5gxYwatWzf+xIlDvf8//vEPrr/+eubMmcO0adN46qmn+OCDD+jbt2+jt+23wirs9OnThz59+hz2+V988QVpaWlcc801AGRlZbF27VpmzpwZPmEntnbLiHhMxx5CM1opInL8MgzjgENJNpuFiDC5V8dms+Hx7Js/dMIJJzBr1ixatWqF1Vr/r2vDMOjfvz/9+/fnzjvvZMCAAXz22WfcdNNN2O123G73UbcnOzubWbNmYZqmr3dn0aJFxMXFkZGRccj3r/0MJ5xwArfffjvnnXcen3zyicLOkVq/fj09evTwO9arVy/eeuutA77G6XT6jRkahkF0dLTvz4FkGAapcd6wUxiZgOEoDNnkrKastqaqbeNTrYNDdT4+tWrVimXLlrF9+3ZiY2O57rrreP/997nlllu45ZZbSEpKYsuWLXz66ac888wzLF++nO+//54hQ4aQmprK0qVLKSws9A17ZWVlMXfuXDZs2EBKSgrx8fHYbLbDbs+1117L66+/zgMPPMCYMWPYuHEjzz77LDfeeCMWi4WlS5ce8P23bdvGe++9x4gRI0hPT2fjxo1s3ryZiy+++Khq09C/C8d02HE4HCQmJvodS0xMpLKykpqaGuz2upOBp06dypQpU3yP27Vrx4QJE2jevHmjtLF5tfcWwcLIRGKcOSTvTcMSeOnpGiIMFtU6OFTno1NZWXlEv9SP5NzGdNttt3HbbbcxbNgwKisrWbx4MTNnzuTRRx/lqquuoqamhqysLE4//XQiIyNJTk7mp59+4o033qC0tJSsrCweeeQRzjrrLMAbVn788UdGjRpFeXk5U6dOZdCgQQd8/9reI6vVis1mo3Xr1nzwwQc88sgjjBgxgqSkJK6++mruvvturFbrQd8/Ly+PjRs38tFHH1FUVESLFi0YM2YMY8eOxWI5sp40u93u60k6WoZpmvXfjxdil1122SEnKP/5z39m6NChXHjhhb5jS5cu5e9//zvvvvtuvWHnQD07+fn5fmsHBIJhGFTZ47n0jYVEuar5oPoLIm66N6DvId46p6enk5ubS5h+OzcZqnVwqM4NU1xcTEJCwmGda7PZQnaH0PHmaGtdUlJSp2MDvKHscDsqjumenaSkJIqL/VcmLi4uJjo6ut6gA95iHyjFN8YPlbS9E5SrrJGU7y4hXj+4Go1pmvrFECSqdXCoziJeDf17cEyHnY4dO7Js2TK/YytWrKBTp/pn3YdCjN1KbIRJudugsKKG+FA3SEREjksvvvgi//znP+t97qSTTuLdd98NcouCJ6zCTlVVFbm5ub7HeXl5bNmyhbi4OFJTU3n//fcpLCzktttuA+DMM8/k888/591332XYsGGsXLmSBQsW8Je//CVUH6FeKdFWysvc7Kkyab3frHYREZFg+f3vf8+5555b73NRUVFBbk1whVXY2bhxI4888ojv8TvvvAPAkCFDuPXWWykqKqKgoMD3fFpaGn/5y194++23mTVrFs2aNePmm28Om9vOazWLtbO9rJJCayyUlUL84Y0li4iIBEpycjLJycmhbkZIhFXY6d69O5MnTz7g87feemu9r3nqqacas1kN1izWDlSyJzIBivco7IiIiARReKyk1MQ1i/FmysLIRCgqPMTZIiLSEJrULb+lsBMESVHesOOwx2E69oS4NSIiTZvVaqW8vFyhpwmoqakJyDzXsBrGaqqSor1lLrHFeffHEhGRRhMbG0t1dTWlpaWHPNdut1NTUxOEVsnR1NowDOLi4hr83go7QZAUFQGAwx4PpdtD3BoRkaYvMjKSyMjIg55jGAYZGRnk5OSoF6iRhbrWGsYKgv2HsSgrCXFrREREji8KO0FQG3YqrNHUlJWFuDUiIiLHF4WdIIi1W7Aa3m674krtwSIiIhJMCjtBYBgGiTbvbHJHtSfErRERETm+KOwESWLtJGWnoYlwIiIiQaSwEyRJMd5d2Iut0VBZEeLWiIiIHD8UdoKkNuw47PFQVhzi1oiIiBw/FHaCJGXvwoKF9gQo1e3nIiIiwaKwEyT79sdK0Fo7IiIiQaSwEyTN9vbs7IlMxCzVMJaIiEiwKOwESbMYG7B353MNY4mIiASNwk6Q1A5jOezxuBV2REREgkZhJ0gSoyKIwMRjWCgqqwp1c0RERI4bCjtBYjEMkiPcAOypcoe4NSIiIscPhZ0gauZdaoc9NaFth4iIyPFEYSeIkn1bRoS4ISIiIscRhZ0git+7inKpS2UXEREJFv3WDaKE2CgASi02TKfGskRERIJBYSeI4veGnTJrjNbaERERCRKFnSCKj/TO2Sm1xWozUBERkSBR2AmifWFHPTsiIiLBorATRAn2fWHH1GagIiIiQaGwE0S+nh1rDGgzUBERkaBQ2Ami2rBTbovBXaKeHRERkWBQ2AmiuL3DWABl5RUhbImIiMjxQ2EniCIsBrGGd1+skorqELdGRETk+KCwE2TxVu//Syu1Z4SIiEgwKOwEWbzNAKCsWjufi4iIBIPCTpDFRHq7dipqFHZERESCQWEnyGKjbACUu8H0KPCIiIg0NoWdIIuJigSgIiIKystC3BoREZGmT2EnyGL3rrVTYY3SwoIiIiJBoLATZDF719opt6pnR0REJBgUdoIs1uYtebk1GirKQ9waERGRpk9hJ8hi7fuGscwK9eyIiIg0NmuoG/Bbs2fPZvr06TgcDtq0acPYsWPJzs6u91yXy8Unn3zCt99+S2FhIS1btuTqq6+md+/ewW30EYjZ27NTEREFCjsiIiKNLqx6dubPn88777zDJZdcwoQJE2jTpg1PPPEExcX1T+T98MMP+fLLLxkzZgzPPfccI0aM4Omnn2bz5s1Bbvnh84Uda5SGsURERIIgrMLOjBkzGD58OMOGDSMrK4sbbrgBu93OnDlz6j3/u+++48ILL+TEE0+kRYsWnHnmmfTp04fp06cHueWHL843QTlaPTsiIiJBEDbDWC6Xi02bNnHBBRf4jlksFnr06MG6devqfY3T6cRut/sds9vt/Prrrwd8H6fTidO5b18qwzCIjo72/TmQaq+3/3X3n7NDRVnA3/N4VF+dpXGo1sGhOgeH6hw8oa512ISdkpISPB4PSUlJfseTkpLYtWtXva/p1asXM2bMoGvXrrRo0YKVK1fy008/4fF4Dvg+U6dOZcqUKb7H7dq1Y8KECTRv3jwgn6M+6enpvj9HJzmBjVRH2LE6PaRnZDTa+x5v9q+zNC7VOjhU5+BQnYMnVLUOm7BzNMaMGcOrr77KHXfcgWEYtGjRgqFDhx5w2AvgwgsvZPTo0b7HtSkzPz8fl8sV0PYZhkF6ejq5ubmYpgmAy2P6ni8sKsHMyQnoex6P6quzNA7VOjhU5+BQnYOnMWpttVoPu6MibMJOQkICFosFh8Phd9zhcNTp7dn/Nffeey81NTWUlZWRnJzMe++9R4sWLQ74PjabDZvNVu9zjfXNbpqm79oRBkRaTKo9BuVVTpL0Fyxg9q+zNC7VOjhU5+BQnYMnVLUOmwnKVquV9u3bs3LlSt8xj8fDypUr6dSp00Ffa7fbSUlJwe12s3DhQvr169fYzW2QWKu3N6ncdeDhNhEREQmMsOnZARg9ejQvv/wy7du3Jzs7m1mzZlFdXc3QoUMBeOmll0hJSeGqq64CYP369RQWFtK2bVsKCwv56KOPME2T888/P4Sf4tBibBYKazxU1CjsiIiINLawCjsDBw6kpKSEyZMn43A4aNu2LePGjfMNYxUUFPjN5HY6nXz44Yfk5eURFRVFnz59uO2224iNjQ3RJzg8sfYIKPdQYVowXS4Ma1h9GURERJqUsPstO3LkSEaOHFnvc+PHj/d73K1bN/7xj38EoVWBFRNpA5zeVZQryyE+MdRNEhERabLCZs7O8SQ2cr+dz7WKsoiISKNS2AmBWFvtwoJaRVlERKSxKeyEQO3+WOXWKChX2BEREWlMCjshEGPftxmoWalhLBERkcaksBMCtcNY6tkRERFpfAo7IRBb27MTEaU5OyIiIo1MYScEaufsVOhuLBERkUansBMCuhtLREQkeBR2QqB2gnK5NQpTYUdERKRRKeyEwL6eHQ1jiYiINDaFnRCo7dlxWmw4KypD3BoREZGmTWEnBKKt+8peXu0MYUtERESaPoWdEIiwGER7R7KoqHaHtjEiIiJNnMJOiPi2jHCbmKYZ4taIiIg0XQo7IRJr3ztJOSISampC3BoREZGmS2EnRPaFnSiorghxa0RERJouhZ0QibHvd/t5VVWIWyMiItJ0KeyEyL7NQKOhWmFHRESksSjshIhvM1BrFFRrrR0REZHGorATIr67sTSMJSIi0qgUdkIkxm+CssKOiIhIY1HYCZFYX89ONGaVhrFEREQai8JOiNQOY2nOjoiISONS2AmR2P1vPdcwloiISKNR2AmR2P17djRBWUREpNEo7ISI/wRlDWOJiIg0FoWdEInZf4KyhrFEREQajcJOiETvDTtuSwSuquoQt0ZERKTpUtgJkSjrvtJX1rhC2BIREZGmTWEnRKwWA6thAlDlVNgRERFpLAo7IRS1t/rVNe7QNkRERKQJU9gJoci91a9yeULbEBERkSZMYSeEoq0GANUKOyIiIo1GYSeEIvdOUq50myFuiYiISNOlsBNCUTbvwoLVHgPTVOARERFpDAo7IRS5N+xUWWxQUxPi1oiIiDRNCjshFGW3AlAVYdeWESIiIo1EYSeEfMNYEXbtfC4iItJIrKFuwG/Nnj2b6dOn43A4aNOmDWPHjiU7O/uA58+cOZMvvviCgoICEhISOOmkk7jqqquw2+1BbPXRidp7N1aVxQ5V6tkRERFpDGHVszN//nzeeecdLrnkEiZMmECbNm144oknKC4urvf877//nvfff59LL72Uf/zjH9x8880sWLCADz74IMgtPzq1W0ZUaxhLRESk0YRV2JkxYwbDhw9n2LBhZGVlccMNN2C325kzZ0695//666907tyZwYMHk5aWRq9evRg0aBAbNmwIcsuPTm3YqYqIhCoNY4mIiDSGsAk7LpeLTZs20aNHD98xi8VCjx49WLduXb2v6dy5M5s2bfKFm927d7Ns2TL69OkTlDY31L6wozk7IiIijSVs5uyUlJTg8XhISkryO56UlMSuXbvqfc3gwYMpKSnhwQcfBMDtdjNixAguuuiiA76P0+nE6XT6HhuGQXR0tO/PgVR7vQNdN8pWO4xlg+qqgL//8eJQdZbAUa2DQ3UODtU5eEJd67AJO0dj1apVTJ06lT/84Q907NiR3NxcJk6cyJQpU7jkkkvqfc3UqVOZMmWK73G7du2YMGECzZs3b7R2pqen13+8yAByqYqIJCHSRnxGRqO14XhwoDpL4KnWwaE6B4fqHDyhqnXYhJ2EhAQsFgsOh8PvuMPhqNPbU2vSpEmcdtppDB8+HIDWrVtTVVXFf/7zHy666CIslrqjdBdeeCGjR4/2Pa5Nmfn5+bhcrsB8mP2unZ6eTm5ubr0rJFeVlXr/b7FTkrebspycgL7/8eJQdZbAUa2DQ3UODtU5eBqj1lar9bA7KsIm7FitVtq3b8/KlSsZMGAAAB6Ph5UrVzJy5Mh6X1NdXV2nS6y+gLM/m82GzWar97nG+mY3TbPea0dG7N0INMKOWVWlv2wNdKA6S+Cp1sGhOgeH6hw8oap12IQdgNGjR/Pyyy/Tvn17srOzmTVrFtXV1QwdOhSAl156iZSUFK666ioA+vbty8yZM2nXrp1vGGvSpEn07dv3kKEnHPhPUNat5yIiIo0hrMLOwIEDKSkpYfLkyTgcDtq2bcu4ceN8w1gFBQV+PTkXX3wxhmHw4YcfUlhYSEJCAn379uXKK68M0Sc4Mr5FBXU3loiISKMJq7ADMHLkyAMOW40fP97vcUREBJdeeimXXnppEFoWeJH7LyqodXZEREQaRfiP9TRh0bVhx2LDre0iREREGoXCTgjV9uyYhoWaGuchzhYREZGjobATQpHWffOPqqoDe9u7iIiIeCnshJDFMIi0eG/Bq/aEuDEiIiJNlMJOiEXt/QpUKeyIiIg0CoWdEKtdWLDK1JdCRESkMeg3bIhF1a6irLAjIiLSKPQbNsR8qygr7IiIiDQK/YYNsdo7sqr0pRAREWkU+g0bYlG2vQsLEqGN6ERERBqBwk6IRdkigL37Y7m01o6IiEigKeyEWG3YqbbYwaVVlEVERAJNYSfEIm3evVi9PTsKOyIiIoGmsBNi0fsPYzkVdkRERAJNYSfEam89r1bPjoiISKNQ2Amx2lvPKyMiFXZEREQagcJOiKlnR0REpHEp7ISYL+xYNGdHRESkMSjshJhvu4gIm3p2REREGoHCTojZ9m4E6rRY1bMjIiLSCKwNeXFBQQEFBQV06dLFd2zLli3MmDEDp9PJoEGDGDBgQIMb2ZRF7I2bHsOinh0REZFG0KCenTfffJOPPvrI99jhcPDII4+wcOFC1qxZw7PPPsvChQsb3MimLMLw9uy4jAiFHRERkUbQoLCzceNGevTo4Xs8b948ampqePrpp3n11Vfp0aMH06dPb3Ajm7IIizfseAwLpsKOiIhIwDUo7JSVlZGYmOh7vGTJErp160Z6ejoWi4UBAwawc+fOBjeyKfPr2dGcHRERkYBrUNhJSEggPz8fgPLyctavX0+vXr18z3s8HjweT8Na2MTtvRkLt2HRruciIiKNoEETlHv06MFnn31GTEwMq1atwjRNvwnJO3bsoFmzZg1uZFNm2TuM5bZEgLMmxK0RERFpehoUdq666ipycnL473//i9Vq5fe//z1paWkAOJ1OFixYwKBBgwLS0KbKuncYy627sURERBpFg8JOUlISjz32GBUVFdjtdqzWfZczTZMHH3yQ1NTUBjeyKYvwDWPpbiwREZHG0KCwUysmJqbOMbvdTtu2bQNx+Sat9m4s9eyIiIg0jgaFnV9++YXNmzdz3nnn+Y598803fPTRR7hcLgYNGsQ111yDxaKFmg8kYv9hLKcmKIuIiARag1LIRx99xJYtW3yPt23bxmuvvUZCQgLdunXjs88+Y9q0aQ1tY5NW27NjGhY86tkREREJuAaFnZ07d9KhQwff43nz5hEdHc2jjz7KnXfeyfDhw5k3b16DG9mU7d0aCwC3bj0XEREJuAaFnaqqKqKjo32Pf/75Z3r37k1kZCQA2dnZvnV4pH5Wy76049YwloiISMA1KOykpqayceNGAHJzc9m+fTs9e/b0PV9WVobNZmtYC5s4i7Ev7Lhc7hC2REREpGlq0ATlwYMHM2XKFAoLC9mxYwexsbH079/f9/ymTZvIyMhocCObsoj94qbbrbAjIiISaA0KOxdddBEul4tly5aRmprKLbfcQmxsLODt1Vm1ahWjRo0KSEObKothYMHEg4HLra01REREAq1BYSciIoIrr7ySK6+8ss5zcXFxvPbaaw25/HEjwgCPCR6PGeqmiIiINDkBWVQQvJOVCwoKAO9cnqioqEBdusmLAJyAS2FHREQk4BocdjZs2MB7773H2rVrfTucWywWunTpwu9+9zu/W9OlfhGGCaaBx1TYERERCbQGhZ3169czfvx4rFYrp59+OpmZmYB3/Z0ffviBhx9+mPHjx5OdnX1E1509ezbTp0/H4XDQpk0bxo4de8BrjB8/ntWrV9c53qdPH/76178e+YcKgdq1dtSzIyIiEngNCjsffvghKSkpPPbYYyQlJfk9d+mll/Lggw/ywQcf8OCDDx72NefPn88777zDDTfcQMeOHZk5cyZPPPEEzz//PImJiXXOv/vuu3HttxhfaWkp99xzD6eccspRf65gqw07bmUdERGRgGvQOjvr169nxIgRdYIOeHdEP+OMM1i/fv0RXXPGjBkMHz6cYcOGkZWVxQ033IDdbmfOnDn1nh8XF0dSUpLvvxUrVhAZGcnJJ598NB8pJHxhRz07IiIiAdegnh3DMA66NozH48HYb9G8Q3G5XGzatIkLLrjAd8xisdCjRw/WrVt3WNf45ptvGDhw4AEnSDudTpzOfXtQGYbhWwX6SNp6OGqvd6jr+nY+N82At+F4cLh1loZTrYNDdQ4O1Tl4Ql3rBoWdzp078/nnnzN48GCaN2/u91xBQQFffPEFXbp0OezrlZSU4PF46vQUJSUlsWvXrkO+fsOGDWzfvp0//vGPBzxn6tSpTJkyxfe4Xbt2TJgwoU77Ayk9Pf2gz9utv0K1B4wILcLYAIeqswSOah0cqnNwqM7BE6paNyjsXHnllTz88MPccccdDBgwwPeLeteuXSxevBiLxVLvGjyN5ZtvvqF169YHnRB94YUXMnr0aN/j2pSZn5/vN/cnEAzDID09ndzcXMyD3Gll7L2LrcblJicnJ6BtOB4cbp2l4VTr4FCdg0N1Dp7GqLXVaj3sjooGhZ127drx5JNP8sEHH7B48WJqamoAsNvt9O7dm0svvZT4+PjDvl5CQgIWiwWHw+F33OFw1DsvaH9VVVX88MMPXH755Qc9z2azHXC/rsb6ZjdN86DXtuydOeU2G68Nx4ND1VkCR7UODtU5OFTn4AlVrRu8zk5WVhb33HMPHo+HkpISYF9o+fjjj5k0aRKTJk06vMZYrbRv356VK1cyYMAAwDvvZ+XKlYwcOfKgr/3xxx9xuVyceuqpDftAIWA1DMDU3VgiIiKNIGArKFsslkP2vhyO0aNH8/LLL9O+fXuys7OZNWsW1dXVDB06FICXXnqJlJQUrrrqKr/XffPNN/Tv3/+IepLChXeCsonb1CQ5ERGRQAtY2AmUgQMHUlJSwuTJk3E4HLRt25Zx48b5glRBQUGd2dy7du1i7dq1PPDAAyFoccNF+Iax1LUjIiISaGEXdgBGjhx5wGGr8ePH1znWsmVLJk+e3MitajwRRu2t5+rZERERCbQGLSoogWHZO0P5wCsWiYiIyNE64p6dTZs2Hfa5hYWFR3r545J176KCLo1iiYiIBNwRh51jZXPNY0ntCsoeDEytoiwiIhJQRxx2DrY6sRydiL2bY7mMCPB4ICIixC0SERFpOo447NTeAi6BE7F3zo7HsIDHrbAjIiISQJqgHAZqh7FcRgS4A7tlhYiIyPFOYScM1PbsuA0LuD0hbo2IiEjTorATBiL2rirotuwdxhIREZGAUdgJA7W3nrs1jCUiIhJwCjthIMIXdjSMJSIiEmgKO2Fg753n3rCjYSwREZGAUtgJAxF+w1gKOyIiIoGksBMGfBuBGhaFHRERkQBT2AkDfhOUNYwlIiISUAo7YWDvMjuasyMiItIIFHbCgLV2GMsSAS7dei4iIhJICjthwO/Wc49uPRcREQkkhZ0wsHcB5b27nmsYS0REJJAUdsJA7d1YHt2NJSIiEnAKO2HAf9dzhR0REZFAUtgJA1a/OTsKOyIiIoGksBMGfGHHop4dERGRQFPYCQPW/SYomwo7IiIiAaWwEwZqe3acFt2NJSIiEmgKO2HA6pugbNUwloiISIAp7IQBX9hRz46IiEjAKeyEAb+NQNWzIyIiElAKO2HAuv86O+rZERERCSiFnTDgN4ylnh0REZGAUtgJA9YIraAsIiLSWBR2woDV0ARlERGRxqKwEwZ067mIiEjjUdgJA75hLPXsiIiIBJzCThio7dnxGBbcLoUdERGRQFLYCQPW/b4KbrcndA0RERFpghR2woBtb88OgMtjhrAlIiIiTY/CThiIUNgRERFpNAo7YcBiGFjwhhyXR8NYIiIigaSwEyastWHHrZ4dERGRQLKGugG/NXv2bKZPn47D4aBNmzaMHTuW7OzsA55fXl7OBx98wE8//URZWRnNmzfn2muv5cQTTwxiqxvOapjUmODUOjsiIiIBFVZhZ/78+bzzzjvccMMNdOzYkZkzZ/LEE0/w/PPPk5iYWOd8l8vF448/TkJCAnfddRcpKSkUFBQQExMTgtY3jNUwwQS3S8NYIiIigRRWYWfGjBkMHz6cYcOGAXDDDTewdOlS5syZwwUXXFDn/G+++YaysjIee+wxrFbvR0lLSwtmkwPGtneOslPr7IiIiARU2IQdl8vFpk2b/EKNxWKhR48erFu3rt7XLFmyhI4dO/LGG2+wePFiEhISGDRoEBdccAEWS/3TkZxOJ06n0/fYMAyio6N9fw6k2usdznWte09xu9wBb0dTdyR1loZRrYNDdQ4O1Tl4Ql3rsAk7JSUleDwekpKS/I4nJSWxa9euel+ze/du8vPzGTx4MH/961/Jzc3l9ddfx+12c+mll9b7mqlTpzJlyhTf43bt2jFhwgSaN28esM/yW+np6Yc8x25dDS4wLRYyMjIarS1N2eHUWQJDtQ4O1Tk4VOfgCVWtwybsHA3TNElISOCmm27CYrHQvn17CgsLmTZt2gHDzoUXXsjo0aN9j2tTZn5+Pi6XK6DtMwyD9PR0cnNzMc2D32VlmB4ggqqqanJycgLajqbuSOosDaNaB4fqHByqc/A0Rq2tVuthd1SETdhJSEjAYrHgcDj8jjscjjq9PbWSkpKwWq1+Q1aZmZk4HA5cLpdvHs/+bDYbNput3us11je7aZqHvLZv53OXR3/pjtLh1FkCQ7UODtU5OFTn4AlVrcNmnR2r1Ur79u1ZuXKl75jH42HlypV06tSp3td07tyZ3NxcPPstxJeTk0NycnK9QSec2Wp3PtcKyiIiIgEVNmEHYPTo0Xz99dfMnTuXHTt28Prrr1NdXc3QoUMBeOmll3j//fd955955pmUlZXx1ltvsWvXLpYuXcrUqVM566yzQvQJjp51b++USxuBioiIBFRYdX8MHDiQkpISJk+ejMPhoG3btowbN843jFVQUOA3kzs1NZX777+ft99+m3vuuYeUlBTOPvvsem9TD3fWCAMw1bMjIiISYGEVdgBGjhzJyJEj631u/PjxdY516tSJJ554opFb1fisERbAo7AjIiISYGE1jHU8i4iIAMDlabyJ0iIiIscjhZ0wYbXunbNjiQCX8xBni4iIyOFS2AkTNuvenh0jAmpqQtwaERGRpkNhJ0x45+zs7dlxKuyIiIgEisJOmPDdem4o7IiIiASSwk6YsNYuKqiwIyIiElAKO2HCVrtdhIaxREREAkphJ0zs7djBZVg1QVlERCSAFHbChG8YSz07IiIiAaWwEyZqdz13WqwKOyIiIgGksBMmovYuKlhtsWNqGEtERCRgFHbCRIzN+6WosEaqZ0dERCSAFHbCRG3YqYyIUtgREREJIIWdMBFj824XUWGNAmd1iFsjIiLSdCjshAn/YSxtBCoiIhIoCjthwhd2IqK0zo6IiEgAKeyEiejaOTvWKMyqyhC3RkREpOlQ2AkTsXbvnB2PYaGqSnN2REREAkVhJ0xERhhYMAGorNKcHRERkUBR2AkThmEQY/GGnfJqhR0REZFAUdgJI9HekSwqnJ7QNkRERKQJUdgJI7FW7/8VdkRERAJHYSeM1N6RVeEKcUNERESaEIWdMBJj93btVKpjR0REJGAUdsJITKQ37FSYEZged4hbIyIi0jQo7ISRmEg7ABURkVBVFeLWiIiINA0KO2EkNmpvz441CiorQtwaERGRpkFhJ4z4Jihbo6BKYUdERCQQFHbCSKzNu9BOZUQkVJaHuDUiIiJNg8JOGPHr2anUZqAiIiKBoLATRmJqw05EFKZ6dkRERAJCYSeMxPj17GjOjoiISCAo7ISRmL1zdiqskVBWEuLWiIiINA0KO2Ek1l47jBUNJY7QNkZERKSJUNgJI7UTlKuskbhLi0PcGhERkaZBYSeM1M7ZAags1QRlERGRQFDYCSP2CAtWwwSgoly3nouIiASCwk6YibEaAFRUVoe4JSIiIk2DNdQNqM/s2bOZPn06DoeDNm3aMHbsWLKzs+s9d+7cubzyyit+x2w2G++9914wmhpwMbYISpxuKmrcmC4XhjUsv0QiIiLHjLD7TTp//nzeeecdbrjhBjp27MjMmTN54okneP7550lMTKz3NdHR0bzwwgtBbmnjiIm0QoWbSmsUlBVDUrNQN0lEROSYFnbDWDNmzGD48OEMGzaMrKwsbrjhBux2O3PmzDngawzDICkpye+/Y1WMvXatnSjdfi4iIhIAYdWz43K52LRpExdccIHvmMVioUePHqxbt+6Ar6uqquKWW27BNE3atWvHlVdeSatWrYLQ4sCrvSPruW5X03OPg+TWIW6QiIjIMS6swk5JSQkej6dOz0xSUhK7du2q9zUtW7bkj3/8I23atKGiooJp06bxwAMP8Nxzz9GsWd0hIKfTidPp9D02DIPo6GjfnwOp9npHct1T2yTw044yAJbnVjAswG1qio6mznJ0VOvgUJ2DQ3UOnlDXOqzCztHo1KkTnTp18nt855138uWXX3LFFVfUOX/q1KlMmTLF97hdu3ZMmDCB5s2bN1ob09PTD/vcKzIymP/jKha4k6mqdpGRkdFo7WpqjqTO0jCqdXCozsGhOgdPqGodVmEnISEBi8WCw+HwO+5wOA57Ho7VaqVdu3bk5ubW+/yFF17I6NGjfY9rU2Z+fj4ul+uo2n0ghmGQnp5Obm4upmke9uuS7QZUQn5hCTk5OQFtU1N0tHWWI6daB4fqHByqc/A0Rq2tVuthd1SEVdixWq20b9+elStXMmDAAAA8Hg8rV65k5MiRh3UNj8fDtm3b6NOnT73P22w2bDZbvc811je7aZpHdO2E+CiohOJqt/4CHoEjrbMcPdU6OFTn4FCdgydUtQ6rsAMwevRoXn75Zdq3b092djazZs2iurqaoUOHAvDSSy+RkpLCVVddBcCUKVPo2LEj6enplJeXM23aNPLz8xk+fHgIP0XDJCTFQ56TEpeBaZoaTxYREWmAsAs7AwcOpKSkhMmTJ+NwOGjbti3jxo3zDWMVFBT4/fIvKyvj3//+Nw6Hg9jYWNq3b8/jjz9OVlZWiD5BwyWmJAN5lFiioKwU4hNC3SQREZFjVtiFHYCRI0cecNhq/Pjxfo+vu+46rrvuusZvVBAlxkUCUGKPhZztEN89xC0SERE5doXdooICiZHeDFpsi8VctSzErRERETm2KeyEoYQo7yrKZbZYXL8sDnFrREREjm0KO2Eo3h5B7ayk0tw8TEdhSNsjIiJyLFPYCUMRFoO4SG/vTrE9FrasD3GLREREjl0KO2EqJco7b2dPZBLm1g0hbo2IiMixS2EnTLVM8C58uDOmOeYWhR0REZGjpbATpjITvLef74puDls3aHVPERGRo6SwE6YyE+wA7IxrAaXFsG1jiFskIiJybFLYCVO1YWdXvHfXc8/kNzCrKkPZJBERkWOSwk6Yyoz3hp1CSzQVEZGwbhWefz4W4laJiIgcexR2wlRcZARpsd5Jysu7ne49uG4lZtGeELZKRETk2KOwE8YGtY4H4Pve50H7zgCYKxaFskkiIiLHHIWdMHZaW+9u54t3llHUYyAA5pIfQtkkERGRY47CThhrlxxJ59RonB6TZy092BOZBGuW4378Lsz83FA3T0RE5JigsBPGDMPgqp6pAKwucvHUgD96n9i6Ac//3gpdw0RERI4hCjthrndGLNf3TQNgY0Qy1Snp3id+WYxZVRHClomIiBwbFHaOAed1SSE52ooH2Pp/z0NaS6ipwVyuycoiIiKHorBzjMhOiQJgQ2EVxoBTATAXfRfKJomIiBwTFHaOEdnNvGFnbUElRn9v2GH5T5g/Lwxhq0RERMKfws4xoleLGAC+31rKrUtcbGrfFwDPy0/gefcVzOrqUDZPREQkbCnsHCO6psVwevtEAHaW1DD9xMuhRz8AzG9n43niLsyl8zHzczHzcjBdzlA2V0REJGxYQ90AOXw3929BeY2bhTvK+LXKRsSfHsL8ZQmet16AnO14/vV337nG0LMxrv5j6BorIiISJtSzcwyJtFr48ykZGEBumZOiShdGj75Yrr+rzrnm3M80tCUiIoLCzjEn1h5B66RIAJbllANgdOuN5a7HMM6/CgzDd675v4mYLldI2ikiIhIuNIx1DDoxI5atjmpeXpjLspxyRndOpnPXXhhde2EOOA1z3ueYn0/FnDMLc+VSjF4DMM66ECOpWaibLiIiEnQKO8egK3umkltWw4LtZczbUsKCbaX8c3Q7MuLtGGkt4YLfQ1QM5jczID8X86tpmPNmY5w4CNIzIbkZrP4ZY8BpGD37h/rjiIiINCqFnWNQpNXCfadmsqGwilcW5rKpqJoftpZyyQnNWLW7gswEO0mjL8c8/RzMFYsw58yCTb9i/jjH7zrmT/Ow/PGv0PskjP2Gv0RERJoSzdk5RhmGQcdm0YzsmAzAgu2lLNhWyrivtvH09zu958TEYTl5GJa/PIXlrsegeTpuDCa3OYNVie3ANPG88iTmB//BNE08k9/APeE+PLM+wnR6b103PZ6QfUYREZFAUM/OMe6krDheXeTdRmLCd96QszKvErfHJMLi7a0xDAO69sLy+KvM3ejgw5/yAJha9Rnmj3Mw58zEXLoAigsBMDeswZwxCSKjoLwM4/yrME4fDRtWY25YC6YH4/yrMSIiQvOhRUREjoDCzjEuKdrK4DYJzNtSgrnf8ZzSGrISI/3ONSwWtpbsW2zQcv2deNpmY374mi/o0DwdqqugxAHOGgDMT97F/ORd/zdObYFx2lmY1VWwbhVkZGGktmiETygiItIwCjtNwBU9Uvlhawnu/dLO8lzv3J3fzsXZPxCV17iJHX4uZkYrzMXfQ2IyxjmXQXUV5hefQmoa5uIfYPUy7wtSmkNhvvc6/30Z98zJUFEGVZVgWDCGjcIYNByapWEu+AYAo2tviEvASPQOt5kFuzEXf48x7ByMyKjGKomIiIiPwk4TkJlg55mRbdlQWMV3W0tYkVvBfxbvxm2anNclxe/ckmq378/55U5i7REY3XpjdOu97ySrDePC3wF4b2X/aR5G83To1B3cbjyP3Qk5233BB4sFPB7Mb2Z47wDbjy9cte2I0aYD5rezvY8LdsPgEdAm2xfIzJwd3t6kjFYYNttR1cLctQ2iYjBSUo/q9SIi0vQo7DQR7VOiaJ8SRazdworcCgCmrNpDrM3C15uKKa1288QZrSko3zeMVVDhom3ywa9rREZhnHrmvgOWCCwPveAd9srdCTFx0KYDLPsRzxdTYcdmqKmB9CzYvRPMvXFny3rMLet9lzG/ne0NPulZGEPOwly3Cpb96H3SbodOPTBi4jAuuBoirJCUDE4X5rIFGHEJ0K0XGBZYtQyzIBfz8jGYWzfgefJuiI3H8tgrGDFxAamtiIgc2xR2mpiBreJ5cGgWj83dQXGVmxd/zPU9N2dzCfkV+1ZUzi8/us1CDasVmqV5/6vVdyARfQdiVpRDaTGkZcCmX6HUgeeHrzFsdrBaMUtLYOWSfa/L3YE56Y19j6NjobIcVi7BBMyfvvUet9oAE1wub29RfKK3R6m4CABHRSnun74HtxtKHJgf/Ad+fyts3QhtOmDY/ecviYjI8UNhp4kxDIN+mXGc0iqOBdvLALAY4DHhzaV5fucWVAR+KwkjJhZiYr0POnQBIKL3yX7neBZ9h7lgDpYLfoe5YTXmd19AQhKWy66Hlq1h/WrMhd9izpu970W1u7jb7BAR4Q1U+ymb+p7fY/PHuZg/zvU+SM/C8qeHvENxtc/vyfMOd8XGYZom7MmD4iLMtSu8q01bj24YraFKqt3888cczmifyEmt4kPSBhGRpkZhp4m6rk8aGfF2RnVKJspqYczHG3B6TL9z8sr29exUuzxUuTwkRh34W6Kk2k1+uZMOKQ2bWGzpfyr0PxUAo3V7OH20/wmdumN06o456hLYugFO6Ou9O8zl8t4t5vHAprXe4a30TDz33wQV5dCuE5Zr/wS7d+L5z9Pg3hvmcnfgGXcjJKaA3e7dVuP7L71zk049E3PrBtiwZt/7b98Mw0ZhdO6BuWMzNGuBER3je9qsrIBSB+Ts9N6G3/ukBtVjf28vy+OnHWX8tKOMT6/uErDriogczxR2mqj0eDvX9tk3zDRuSCbztpQwb8u+u7a+31bCsF0JJEVZ+e/P+azMq+DZs9vSOrHukM/6PZU8OmcHJdVunh3ZluxmjX8nlbH/UNn+t7VHREDnHr6Hlj/+lfiCXMpOHuYd7spsjeWuR/F8/A5Gj36YsyZ75xHVriOUv3dor6Ya8+vpdd7XXPID5pIfvO9ZsBvSMrDc8zdvb9P6VbD8J7/zLQ/8A6NNh4N+FnPLekhIwkhpftDztjq0U72ISKAp7BwnTmwZx4kt4xhzYhpRVguv/JTL3M0lPDJnh995j36znbsGtaR5rI3H5+7g9PaJnNwqjjeX5Pnu5Fq4o5T0eBtxdv9FBb/Y4GDq6kKu7JlKhAGD2iQE5bNZuvYiIWMk5Tk53iEpwOh0AhF/eQoAMz0Tzw9fYzn1TMzqKsyF32KktsA4oS+e77/w9hitWgbmb1aLLtjt/X9eDp57rjvg+3v+PQFjyEiIicPo0AVME3PTr94hPasNrDY8L4yH5GZYLh2LZ+ILEBmF5fe3YvTxH+KrdGrFahGRQDPM2t8Ox7n8/HyczqObsHsghmGQkZFBzn6/hMNFpdPDdR+vp8pVf7usFnAd5PduclQEL53b3hd48sud/OGTjX7nPDa8FT3TYwPW5gMxDINfHBY27Mrngq4puD0mFoPD2u9re3E1E5fmcVmnODo7tng3St2Th7ltI+aSBd6hsJ1boaYakpp57zzbvnnfbfcN1aMflkvHYm7bCD8vZGz0GRRFeIfMPnb8D6NDF4zTRnpvya8ow8hoFZj3PUrh/D3dlKjOwaE6B09j1Npms9G8+cF7y2uFZc/O7NmzmT59Og6HgzZt2jB27Fiys7MP+boffviBF154gX79+nHvvfcGoaXHrmibhXsGZ/LKT7nsqWei8sGCDkBRlZv520o5MzsJgA9/Kahzzi+7K/zCzp4KJ7PXOxjRIYlYu4WSajcZ8fbDam9uaQ3WCIPUmH0Th3/OKWfa2kJu6JfO/TO8QSsjzsaLP+bQr2Ucdw5qecjrPvXdTrYV17Amv5IPLuvjPdgsDaPTCXDG+QDeO8h+XQFde2HEeicNm0vnYxYVYvTsh/n9V1CYh+kohPWrvQEpPhGiY7wLLpY4/N80Ick7PLd5HfyyGM8vi31PVZ56lu/PNSuWYP95Ieb/3vYeMAyMk4dhbt/knaidkIRxwoneOUOJKXUXkDRN2LXNe158ot9xbfwqIseTsAs78+fP55133uGGG26gY8eOzJw5kyeeeILnn3+exMTEA74uLy+P//73v3Tt2jWIrT229cuM480Ls1mwvZTvt5ZwXR/vENcz3+/k571r9ewvPc5G7n6Tml9emOvbg+urjcV1zl+/pwqn24Mtwrvf7PsrCvhqYzHT1haSGmMjp7SGJ0a0pmvzmDqvBSiocLJhTxVdmkdzx6wtGAY8fVYb3zYYD3+zHYDVszb7XvOvRbspq/Ewd0sJt52cwc6SatokRR7wl/u2Yu+WGBUHGT4y4hOg32D/YycOpPaKtQswApguJ1RVetcCYu9GqpvWQlwi5obVkL8b46TTvD1Iyxbi+eA/3rlEzdNxde1DlWXffKnyC8Zg/34m5O3ae3HTtzK17/2W/4T53qvecJXdFXbvgrISSE71rm6dnwuG4V392mbDctXNeKa9DyXFWMY9jbnoe8yl8zFO6ItxxnkYFu/Xyty2EYodGD36+r+fy0XN+jWYMQne64qIHAPCbhhr3LhxdOjQgeuvvx4Aj8fDH//4R84++2wuuOCCel/j8Xh4+OGHGTZsGGvWrKG8vPyIe3aOt2Gsg6lwunljSR6xNguLdpYRa49gY2EVDw9rxez1Rfy4vYz6Pk1Wgp0dJTV1jndOjWJQ6wT++3N+nTvCOqRE8dzZbQH8Ni91e0xum7GZXaX+1+vYLIrxw1oRYTG4YvK6g36OMzok8tXGYkZ0SOS2kzPqPef899b6/nw4dz853R7KajwkRwfm3wlmeSkUFkBWW3JKnfxx+ibfc/8c3Y5WMRbMH+dA0R7AxFyxGPJyvGsRgXeytsesO9/oaDRPh8w23iGzVd4tQoxBZ2D0P9UbpNatwjPjQ+/6SQlJGMPPxRhwmm9PNDNnO573/43R/1SMU06HHVu8yxCkZdTf67RzC7Rs473tv6L8kJO8jyfH6s+OY43qHDwaxtqPy+Vi06ZNfqHGYrHQo0cP1q078C+2KVOmkJCQwOmnn86aNWsOeB6A0+n0CzWGYRAdHe37cyDVXu9YGzKItVv50yneIaDr+3mPeUwTi2HQp2UcHtNkc2EV/1u9h++3lmK1GHRLi+Z3vZpz7+db61zv14Iqfi2oqve9NhZWsXJ3BQUVLv6zKJeuaTHcPaglC7aX1gk64O0tunrK+nquVFdtb9OXG4vpmxnHwNb+E6Yratx+j50eE/veXqgat4cXF+TQKjGSy3vs23riP4vz+Gqjg7sGtSQz3k6HZtGH1ZYDMeISYG8v0O7fLPJYXuPBkhQFp+0b2uL8qwEwqyohMgrDMDCdTsz5X2Fu3YiRnoW5dgVERWP0HYTRsRsUF+FZOh9z+of1t6FzD8xNv3p7gfJz/Z4zf/gK84ev6r6oxIE59b+Yn74HzVpAcgrs2g5lJZhrV3g3l927kSwRVshqi+W0s7xDcR26YH41zbu+0n4sv78VkpthtMjEaFH/EKTp8XivcYz9nTpSx+rPjmON6hw8oa51WIWdkpISPB4PSUlJfseTkpLYtWtXva9Zu3Yt33zzDU899dRhvcfUqVOZMmWK73G7du2YMGHCYafDo5Genn7ok44xmS1hUPd2LN3hoHVyDM3jvMMvT0Ul8PW6PEZ2bcEORyULtxayuaCc3NJq3B6Ty0/MYtJS/zvA7v9qm+/Pi3eW8ci3OeSWesNRq6RotjsqG9zer7dUcG7fjjg9HmLtVtbuLmXiz1v8zjFjkshI8c4xmvbLLuZtKQHgspOyyUiIIqekii/2rsfzzPe7sBjwwsW9cHlMMpOiadfMfzL2Tkcljkon3TMO76600l3b/R7bYhPJyDjMPb5aj633cEWNi0lLdzD6d7eTfP4VbFu8hD3te9H+szdw5+USO+piYoeOxLl1I+VzZ2OJS8C1cxuu3TupXr5o33YfQETzdOwdu2JtkUnNulV4qqtwblgD+Tne//bnrPFu9GqamDXVsHUDnv9uOOhH8Pz3Zd+fjcgorC1bY7pdWNMziezaE2urdpS89x/ce/KIPm0EpseDOy+HyJ79iDvnMso/m0L5nNkYNjtxoy/FXbCbmvWrsSQkETf6MuxtszFNk+qfF+LcuonYkRdhREbiLthN/gO3EdX3FJJuuCusfvE1xZ8d4Uh1Dp5Q1TqshrEKCwu5+eabefzxx+nUqZPv+Lvvvsvq1at58skn/c6vrKzk7rvv5g9/+AN9+ngnl7788ssHHcY6UM9Ofn4+LldgVxQ2DIP09HRyc3OP+y7SSqeH7cXVtEuOZHV+JT9sLeHElnE8+e2+4JMYGUFpjZvaka70OBvPnd2OOz/bTJXLw8DW8Xy2zgHAKa3iyS93cvOAdLY6qikx7Xyxehd7KlykRFt9c4uax1rJL/d+XWNsFgzgsh6pvLU0r85Q3LD2ibROjGRkxyTumb3FNyR3WtsE4uwRzFpXdNDPeF6XFK7vm4ZhGLg9JmM+Xo+jys1Lo9vTOmnfXBy3x2RtQSVdUqPZVFjFjpIahrVP5NWfcv3e45aT0hnZ0X/zsmqXhwiLgdVyeL+QX1ywi682FtOxWRTjhmRx06cbqXGbvHhOO9omR+F0e7Ba6u8pMR17wOnE88NXGCnNMQaPwBIR4fc9be7cirlmOZ6vZ0ByMyKu+xM4CjGrKzE69/Bu6bF5He7Xn/Mu/JjZ2ruAo93uXfsIvGshrV/lncwdn+ida+R212lPg2W08g735e7cdywmzvt+++vQhYgrb4LqSkhpjrliEebuXRBhxejQ2dtjVrt57e5dmLt3YvToV3eorqIcc81y77wnmx2qK/G8/hxEWL3LDsTtWyHbrK6CbRu9SxYMHYUlKlo/O4JAP6ODpzFqbbVaj81hrISEBCwWCw6Hw++4w+Go09sDsHv3bvLz85kwYYLvWG0Rr7jiCp5//vk6KdJms2E7wI7ajfXNbprmcf8XKcpq0HHvQoQ9W8TQs0UMHtOkf2YcVS4P952aSXxkBF9ucPDSwlySo6386ZQMYu0Wnju7LaYJ5TVuvt1cwsDW8dy+3xycTqnRZGRkcF77KNx7k9KHvxSQU1rDBV2b8cGKfH7OrfBNQp64d9uMVol2MuLtrNpdQbnTw5xN3mGvt5f5b6tR28NzICnRVgorXUxbW0irRDtnZiexIrccR5X3F/b3W4sZYUviuR92cUqreDYXVfP1pmL+0DeN15d43yvObmFHif+Cgq8szOWHrSVc0zuN7GZRFFe5uGPWFlKirdw9uCUp0VYirRYcVS7WFVTSNimKtDj/7+3aobz1e6p4bfFuavauKLlydwWfrNnD3M0lDG6dwP8NrmfYKDEFAMveoTPY93fE9z3dsjVGy9ZYTh/N6vxKMuPsJDVPx+/XfnY3LI/9yzv8ZLViVpSBLRIqy1n8wzK+iurAzWfXkOjIweg7CKqroLAAc8NqjLh4zKIC70axWzdCfCLGiad4J2HvXcPInDPT+zg6BmP0FVBRhvObmfyt89U0T0vi5uXveTelzfHvOQPqBh2AjWtxP35nvV9r80u8c5viE73vv2WdN7RltoGoaO9aSx27Qe5OzGULoHLvRP+IvT9q967q7d68DssVN0BUNJ4vP/Xelbe3apZvP8cceRGVHTri3rQBOvfw3+rEUQiV5Q1ehsDz/quYK5diufdvGEnNGnStY51+RgdPqGodVj074J2gnJ2dzdix3m55j8fDLbfcwsiRI+tMUK6pqSE313+OwYcffkhVVRXXXXcdLVu2xGo9vDynCcrho8btwXaA3gaPaWLgP+57qDrvLqvh/i+3+W2C2jrRztMj2xJltTB9baEvdNSyGHD/kCxySmuYtraQvPK6vX7D2ycyqHU8J7aM5X+rC/nvz961d8aemMb6PZV8t7UUgE7NorBbLazcXfcOt1pndEhkWU45eypctE+OZFPRvuBjAPedlsmmwiomr9zjO94nI5ZzOiXzzA+7qHJ5e2geHJpFp9QofthayhcbHKzbU/9cqcgIg+q9wcdiwOTLO/numgNYnlvO7jInIzokHrTWLo/JE3N3sKOkhrxyJwmRETx3dluaxx7e3mL7TxDv1CyKh09vVWexykMxHXswl8zH6HMKRop32G/FdgcPzvP+bPjgzFSiZn0AGVkYaRnQ8QSwWiE/BzNnJ+ak16BtRyznXgk11d6tRhx7/N8kqRlG5xMwl/3oXXOpETzb7SqWJ3fiH4ueo1nNbwJ2q3YYPfqB6fGu+l1TA9ndMLr2wjjtTMjZgblupbcnaeDp3knrOdsxd26F6FiMqCjM8jLY/CtktMI4cSDmpNcBMEZdhmX/OwoLCzDnzcboc4q3By4xxRsKnTW+gHW4yxeYHrd3Qnv7zhiWCO+edGWlRzQZ3ayqAFskRsSRfV8cjnD9GT119R5+2lHGXYNa8tjcHaREWxl/esPX2FpXUEmMzeK7ozXQ9r/J5Lc0Qfk3Ro8ezcsvv0z79u3Jzs5m1qxZVFdXM3ToUABeeuklUlJSuOqqq7Db7bRu3drv9bGx3nkTvz0uxw77fr90f8tyFPMpWsTZeenc9uypcNE81kpxlZuUaKvvL+U5nZM5pXU8FTUebp/pvY39rOwk+mXGAXBulxSqXB42FlYxfW0Rp7SKI7/CxUXdUnztubBrCt9vLWFzUXWdDVcPFDj2t/+t+5HWfZ//pKw4Fu4o4+/zdtZ5zbKccpbllPseuzwmj87Z7tsO5GCq9zvJY8LGwmraJkcSZbVQ4XTz0NfeXpBm0Vb67q0DwNzNxbwxZT1D2ybw+96pzN9WytL92lBS7eaFBTk8fkZrdpbUsGRXGR7TJCHSyunt/ZeOcP/mzrx1e6qY+WuR34RwAKfbpKDCWWdNpvxyJ09+u4MR2UmMGn6u33O7qvZde6cllo7X3l63CFntMLLaYfYbBKbpu+3ectejmD99hzHkLMylC6DEgXHulRgREeTtzOXVH7ZzfmoNPeJNjMhIzD353hCRkOQNFx4P7tYdWe2Oo3NaDJGb1mD0GwT2SMhsC3Y75qfvYX4zA2LjIaU5WzO784OtNwDfnT6Gnx0GzV1l3LL4dW9/z/bNmNs3+7d/w2ocW7fy7jon/fas4aSCVQCYG9fyW36VXr8ac/3qfc/NmozHYoA9CnPVUvj1F+9p8+YzL60Pg/KXU2qNoV/ROowzLwCLBfPbz6B1B4x2nbyfq6oSY8CpUFQIkZHQshXmF596w2HeLowzzoNzLsPzt3u89fzD/2F06g4Jyb66+9rj8cD2zRTu2MXfcpNpv2UJN9m2Yrn9Qdi9E3PFIoxTz/Lbr+5ATKcT4zc9+WbtiuldemBEHfoGg6Ndl2pPhZMoq4XYwwzvczYV89ri3XRLi2bRTu/fqfHfbGdHSQ1bHdU4Kl0kHeVdoKZpsr2khns+34o9wuDDyzodMJT8VrXLw68FlXRtHkO128OnawoxDLiiR6rfz+OZvxbx1rI8/m9QS04Ow02Mwy7sDBw4kJKSEiZPnozD4aBt27aMGzfON4xVUFAQVhMI5dgQZbWQmeD9Zdk81v+Hq8XYu1hhDJyQFs3GwmrO75pS5/Xd02Lonlb/D9gIi8GfTs7glZ9yWb833JzcKo5mMTZm/lrku0bVIVZrTI+zMaRtAmvyK8lKsHPvqZnc/+U21hYceJJ2u+RIHh/emns+3+q7g61lvI3eGbFUOL09PrVhymLgmxMVb7fQLjmKFbsruO8L7110p7dPJN6+rz6TVu7h3eX5dE6N5to+afx7US7lNR4+XVvIrwWVFFbW7Q39ZXcFK3LLeXd5Ab/u1+5fdleQnRJFfGQEmQl27BF1/x6/v6KARTvLuGtgS1ru/Xq9sWQ3n6138OdTMji1TQJWC0xbW8SMX4vIK3fy70W7Gdw6nmq3yeKdZWx1VPPZeofvmjuKa+j4m7vmymvcVDg9NI+1eX+e7N97ldGKKR3O5KtvHTwwdAQZ8XbfHKn/rHeyxBnPkhz4Y6sWfLGhmLsH9aLlsFG+11c6Pfx93g5+zq1gdFwyN4w9i98yLrue3WdfzetL8rmoezPmbCqBDd42z7S0YU+UtyfxpPte56clv9LBKOeXChtjnKtJ7dIZ88tPcRQUclf/u3DY41mW0pmTSjd6hwABOnTBaJMNrdt7h/AMC8QlYDRLw1z2I+bCb8FmZaO9OUWRCfSbMalOG8f1uQWXxcqMVt5Ne59e/AIdPv943wlrV2CuXUFeZBLrE1oz8POPfcOXJbYYnut6FUOMDIaxy3v33VfTAFid2BbnlE/oVfSMdy5TUgrzsofxka0jt+z6kq4bFlAZYefRPrewJc7K+vSTGDPvU2x/vpLdZhTNq4p4e7PJspjWjC//jtSevTD6DvRu7hsVAwbsiojnp8VrOWfd59jveBiy2nl7p2w2PG//E5YugKQUzFPPpLLfKXgcDoxufXwfzaypBpsd84tPMD//GOOysRgDhviCWVmNm3/+sIP+rlyGD+iEkbhvbp257EdyHBXcmZeJgcGfT8ng5FZxsCfPt0zDb211VPPywlycHtMXdAC/pTy2OKrpvTfsFFa6WLqrjGHtEg8ZWooqXTw2dzsbC709kjVukw2FVVgMKK12M3VNITf2a0Gr3/T2rN9TycNfb6d87/D/Nb2bM29LCVv27t/XITmKk/YLNe8uz6fGbfK3eTt55PRWVLs9ZCVE4qh00aFZFNG2wPfMHYmwG8YKFQ1jHbsCWedql4cat0l85NH/xfxpRynfbinhD31bkBAZwadrCom1R3Ba2wQ+/KWAwW3iuXv2vlv0XxjVltIaN063SUa8neaxNhZsK6VvZiwxtgiqXB7eX56Po8rNjf1a8PWmYtJibazbU0mVy8N5XVLIiLdTVuMmr8xJWqyNuP3aX1bj5uqPvLfr7z9EdutJ6ZRWu3nn5wBtfYE3eG0uqiYhMsK3l9qBtEmKPODGpwbQPyuOzqnRvuFB8AbG3/VKrTPseDCXdG9G35axTFtbRL/MWPpkxPLInB3sLKnhjwNa8N3WUrIS7JzfJYXk6AgKKlzcPG3fekepMVaeHNGaHcU1PDp3R53rd06N5raT06l2eUiKsjLhu52+wBtltXBxtxTWFlSSX+4kt8xJ/8w47hiYwaNzdvDL7goiDEiOtlJQz0rmv9U7PYZHhrdmxa4SXlyY5zc0+87F2STYLVC0B6OZf9d+frmTJ77dwZnZSYzqlIzpdOK2RHDxh94lPa7eNZcks5puJ3RgckVzWrVqwbsb/Hskx8bnM3rn997J4y3bYERYMLdu5NbY4eRENePWdVMY7lgNVRVMzTyV/3Y4B4Ck6hL+vPZDehVtYHdUMrcPuAeXxUpmRR7nbv+ONuU5/PXE2wBoVZ7L84ue44Hef2RNUjvfe/95zQfsjkrhw3ZnkVJdTGGkt5dwSO4S/rx2EuURUUS5q4nY24c1ZuCDFNvjuXzzF+RGN2NXbBplEdGUW6NIqyqkd+E6Ru/8nkJ7Am3KczHw9n65W3Xgk9hubLAm06amiNUxLenq2Ex+VDJn5i3B06odJ7RrwfS1hbyZPhSAh9a9T++YGigqYGd2P/LXrWdtYlsmtx0BgAU4t+pXRiz/lJZdO/HdgEvZ5I7mkrJfiCzZw7dR7fi0OpUdZQf/+3JNRg0tWyTTzlnI+A12cqoNesQ46dSqGVf2ycBaUuhdcmK/Hi/TNBn35TZW5/v/Y2n/f/SA9+/tY8NbU1ztYs6mEr7c4KD4EH9/20e6eLSbQXy37nhMk8s+XFdnHbVasTYLwzsk8ddzepK/e3dIhrEUdvZS2Dl2HYt1nrg0j0/WFHLnwAyGtjvwyuCBctdnm9lYWM2N/VqQGmuloNzFqE5J5JY5efCrbXRLi6FfZhzT1xayqaia9smRJEdbWbjDfwKvATw2uhvO8lJmrSskxhZBy3g7H+zdLuT1Czrwf7O3ULx3cnaHlChu6t/Cb/0lq8XAtd8PxSFtE7isRzNaxNrZ6qjm7WV5rDjI/KZjQbzdgj3Cwp7KwN7hCd5Qc8MnG6l2m6TH2cgrd+Ix4eb+LdhT4eK8LsnERUZQ4zaJjDDYUFjFtLVFvon2r5zbnl8LKnlt8e6Drhz+W6e1TeCOUzLYXlzNQ19vp39WHGP6pPnWverWPJq/ndkGs6qC5+fnMDdn38/ThAgPL/S18d5mN1/tl60tQLLVzR7XvnB+qrWI71zJREUY3qUCDtJEO27+uH4q/25/LpkV+ZyXu4AP24wgx5502J8roaaM1GoHu6KbU2U99FyWP6z/hOlZp7I72jupO8pVzdm75rMlNoNlzQ68MKnV42LI7qV8nTEAgDhnBVaPC0fkvqUpJiz5J/f3+SMui5VIdw3VEXW304nwuHFb6v5j7PcbZ3Je3iK+630u3Wp2Y+3ai68SuvLh2oPfYHEoGa4Scqz72niNYxGTkvtSbVpIryygV4LJ585DB45uzey8PXZwyObsKOzspbBz7DoW6+zymBRVug57Im9DlVS7+TmnnEGt4w/Z7V07R6HK5eGZ73dRXuNmq6OaXhmx/N+gTFpntfSrdbXLw6NzttM2OYob+rVg8c4yHtvbA3Jlj1Qu79GMt5d5V88ee2IaFgP++uU21uz91+ajw1vR6zcbxm4rrmb2uiJW7K5ge3ENV/ZIJTEqgg9/KfDd5Vbrg8s6MuG7XfycU85N/VswokMSD3+zjVV5/v+ajbVbiLFa/HpDDqV7WnSd6+yvZbyNXaX+Pze6No/mTydnsDq/gpcX5vr9C7pDShRbiqrqnVfVsVkUzWNtzN/mndh+ctsUftxSWOe8NomRbC2uJivBztMj2/Da4jy+2bRvzlfbpEjcpsn24rqLcjZUh5Qo2iZF8vXe9/vtFjJ/OS2TTs2iuHv2Vgp/E/SirMZBNx4e2i7Rb+7ayI5JREYYfLr24Es+HIkbeyURGR3Fqz/tPmAvRKDEuatoX7aLFYntD+v87NLtPB29ll0njWLR5gLOqdnEN816snD5JpYmdTysa4zY9SNftjy53uMn569kT2QiE7PPpdIaddDrxDorKLfF0M2xift/eZOrT33c99ykb//K5riWPNftavKiUw5yFa9Ltn5Nl+ItRMVEc9a/X1fYCTWFnWOX6tz4alfQPtxar99TyfxtpVzcrZnfkFqtnSU1TF5ZwLB2ifTOiK3nCl5uj4mjykWzvRvAbiqs4r4vtmKLMDgrO4neGbH0So/F7THZUVJDm/3WM3K6TV78MYf0OBvndEr2Te78ZlMxGwuruLR7MywGJERZKa124zZNlu4q55M1hWx1VPPnUzI4vX0iO0tqeHtZHlFW7zpNA1rF4XSbtE+OIjPBzis/5bKrpIbrTkwj2mqhVaLdN6+wwummtNrNL7sr2FRUzZg+zSmocDF5ZQHfbPL/F/cFXVM4KzuJP07fRGSEwexbT+WnX7eybFcZH/7ym7vDgLsGZjCkXaJvuYaGqh16tEcYPDmiNfO2lDC6czLVbpPbZ2w+9AXq8fbF2ZTVuHli7k7ffLI2SZG8MKotGwqr+MsXW3F54NwuyVx/YhpzN5fw2pLdON0mz45sy6q8Cl5dtBvwLvFw84AWrMmr5IsNDv48MIPnfthFlcvEYrB3cv2Bu4EGto7nnsEtsRgGv+wu59M1hfTKiMNji2bNjgJ2lzlxeky6p8VwevtEXliQQ+vESHaX1fjdHVmrWYyV87okM3FpPlkJdipdHkqr3bRJiiQt1sZlJzSj0ulh3JfbSImxYo8wSI62ckmGhxOqcvi1ZQ/choUOMSZLvplP106taNG97t6OpSVl3Pl1DiXVHi7pFEeX9AR+3l3J/1bXDcL7s5ge0qoKiTFd/KV/Es13b4bdO6kqK8Ps0BXXaWczc+YC+v/yGZ/HdyMvMonC2FRiqkoYv/59ck8+m9RYO3G4uLC4FwAxFpN3Fz4O5aXkRSXzz+5Xk2jUMD/Ou1F3nwQPy0r850Q+6VpIl40/YbTpQNaDzyjshJrCzrFLdQ6ecKh1XpkTw6DResWqXR7WFlTSs0VMo90MYZreSaJpsTY+WeOd7P2nkzNIj7ezsdA7V+bUE9qTk5ODx+Nhc1E10TYL/1m0my2Oaga1jmfMiWlEWAxq3B6emLvDb/PeZtFWWsTZ8JhgjzB8w4JTrujMspwy5m0pYWdJDZuLqn13av3n/PZ8s6mY3hmxdTbn/d+qPczZXOzXW/T3Ea15ct5OSqrddXp4ANJibbx2gfcW862Oav60907Hq3ulctkJ3jvuSqrdVLs8pMZY/QJieY138nhJlYsHvt5O/8w4ft+7ua924P1ezC2tobTGTftkb09FTlkNv+ZX8s7P+fTPjGN5bgWxdgtPnNG63ruiDvf7Obe0hp0lNeypdFHhdBNnj6BtUhTZzaLYWVJDepwNiwGVLg8xv5mIW17jJtpmOao7SWvt/5lrvbc8n7mbi3n8jNbklDp9GyPbIwyu7dOc3mnRZLpLIKV5nTve6ly/ugpqajDiE7x/tkT43cX235/z+d+qPTw0LIs+cU7MNSsw0rN8Swhsc1QzeWUBV/RIZeqaQhZuLyUt2kJiTCQPDM3y9ia73bRs1UphJ9QUdo5dqnPwqNbBcaR1drpNftldTrTNwo/by7iwa4qvJ8tR6eLRuTvo2zKWq3vt+8Xg9ph4TO+Qodtj0in10LdhL95ZxhtLdjO4TQJX92rO7rIaFu8sZ0S2d97ZE3N3UOM2OaFFDL3SYzmhxb7QNPmXApbmlDPutEwSosLjRuCm8v1smiYzfi1iR0kNg9vE06PFgXtLj4bbY1Ja7T7qW98h9OvsKOzspbBz7FKdg0e1Dg7VOThU5+AJddg5eN+WiIiIyDFOYUdERESaNIUdERERadIUdkRERKRJU9gRERGRJk1hR0RERJo0hR0RERFp0hR2REREpElT2BEREZEmTWFHREREmjSFHREREWnSFHZERESkSVPYERERkSZNYUdERESaNGuoGxAurNbGK0VjXlv2UZ2DR7UODtU5OFTn4AlkrY/kWoZpmmbA3llEREQkzGgYqxFVVlZy3333UVlZGeqmNGmqc/Co1sGhOgeH6hw8oa61wk4jMk2TzZs3o86zxqU6B49qHRyqc3CozsET6lor7IiIiEiTprAjIiIiTZrCTiOy2Wxccskl2Gy2UDelSVOdg0e1Dg7VOThU5+AJda11N5aIiIg0aerZERERkSZNYUdERESaNIUdERERadIUdkRERKRJ04YgjWT27NlMnz4dh8NBmzZtGDt2LNnZ2aFu1jFl9erVTJs2jc2bN1NUVMTdd9/NgAEDfM+bpsnkyZP5+uuvKS8vp0uXLvzhD38gIyPDd05ZWRlvvvkmS5YswTAMTjrpJMaMGUNUVFQoPlLYmTp1Kj/99BM7d+7EbrfTqVMnfve739GyZUvfOTU1NbzzzjvMnz8fp9NJr169+MMf/kBSUpLvnIKCAl577TVWrVpFVFQUQ4YM4aqrriIiIiIEnyo8ffHFF3zxxRfk5+cDkJWVxSWXXEKfPn0A1bmxfPLJJ7z//vuMGjWK6667DlCtA2Xy5MlMmTLF71jLli15/vnngfCqs+7GagTz58/npZde4oYbbqBjx47MnDmTH3/8keeff57ExMRQN++YsWzZMn799Vfat2/PM888UyfsfPLJJ3zyySfceuutpKWlMWnSJLZt28Zzzz2H3W4H4Mknn6SoqIgbb7wRt9vNK6+8QocOHfjzn/8cqo8VVp544gkGDRpEhw4dcLvdfPDBB2zfvp3nnnvOFwhfe+01li5dyq233kpMTAxvvPEGFouFxx57DACPx8M999xDUlISv//97ykqKuKll15i+PDhXHXVVaH8eGFl8eLFWCwWMjIyME2Tb7/9lmnTpvHUU0/RqlUr1bkRbNiwgX/84x/ExMTQvXt3X9hRrQNj8uTJLFy4kAcffNB3zGKxkJCQAIRZnU0JuL/+9a/m66+/7nvsdrvNG2+80Zw6dWroGnWMu/TSS82FCxf6Hns8HvOGG24wP/30U9+x8vJy86qrrjK///570zRNc/v27eall15qbtiwwXfOsmXLzMsuu8zcs2dP8Bp/DCkuLjYvvfRSc9WqVaZpemt6xRVXmAsWLPCds2PHDvPSSy81f/31V9M0TXPp0qXmZZddZhYVFfnO+fzzz81rrrnGdDqdQW3/sea6664zv/76a9W5EVRWVpp/+tOfzOXLl5sPP/ywOXHiRNM09T0dSJMmTTLvvvvuep8Ltzprzk6AuVwuNm3aRI8ePXzHLBYLPXr0YN26dSFsWdOSl5eHw+GgZ8+evmMxMTFkZ2f76rxu3TpiY2Pp0KGD75wePXpgGAYbNmwIepuPBRUVFQDExcUBsGnTJtxut9/3c2ZmJqmpqX51bt26tV/XdO/evamsrGT79u3Ba/wxxOPx8MMPP1BdXU2nTp1U50bw+uuv06dPH7+fEaDv6UDLzc3lpptu4rbbbuPFF1+koKAACL86a85OgJWUlODxePy+eABJSUns2rUrNI1qghwOB0CdYcHExETfcw6Hw9edWisiIoK4uDjfObKPx+PhrbfeonPnzrRu3Rrw1tBqtRIbG+t37m/r/Nvv99qvi+rsb9u2bdx///04nU6ioqK4++67ycrKYsuWLapzAP3www9s3ryZv/3tb3We0/d04HTs2JFbbrmFli1bUlRUxJQpU3jooYd49tlnw67OCjsiAsAbb7zB9u3befTRR0PdlCarZcuWPP3001RUVPDjjz/y8ssv88gjj4S6WU1KQUEBb731Fg888IBv7p40jtrJ9QBt2rTxhZ8FCxaEXe0VdgIsISEBi8VSJ5XWl2Dl6NXWsri4mOTkZN/x4uJi2rZt6zunpKTE73Vut5uysjJ9LX7jjTfeYOnSpTzyyCM0a9bMdzwpKQmXy0V5ebnfv9CKi4t9NUxKSqozLFhcXOx7TvaxWq2kp6cD0L59ezZu3MisWbMYOHCg6hwgmzZtori4mPvuu893zOPxsGbNGmbPns3999+vWjeS2NhYWrZsSW5uLj179gyrOmvOToBZrVbat2/PypUrfcc8Hg8rV66kU6dOIWxZ05KWlkZSUhK//PKL71hFRQUbNmzw1blTp06Ul5ezadMm3zkrV67ENE0tA7CXaZq88cYb/PTTTzz00EOkpaX5Pd++fXsiIiL86rxr1y4KCgr86rxt2zbfDymAFStWEB0dTVZWVnA+yDHK4/HgdDpV5wDq0aMHzzzzDE899ZTvvw4dOjB48GDfn1XrxlFVVUVubi5JSUlh9z2tnp1GMHr0aF5++WXat29PdnY2s2bNorq6mqFDh4a6aceU2r84tfLy8tiyZQtxcXGkpqYyatQoPv74YzIyMkhLS+PDDz8kOTmZ/v37A951THr37s2///1vbrjhBlwuF2+++SYDBw4kJSUlVB8rrLzxxht8//333HvvvURHR/t6JGNiYrDb7cTExHD66afzzjvvEBcXR0xMDG+++SadOnXy/cDq1asXWVlZvPTSS1x99dU4HA4+/PBDzjrrLO0mvZ/333+f3r17k5qaSlVVFd9//z2rV6/m/vvvV50DKDo62jfnrFZkZCTx8fG+46p1YLzzzjv069eP1NRUioqKmDx5MhaLhcGDB4fd97TW2Wkks2fPZtq0aTgcDtq2bcuYMWPo2LFjqJt1TFm1alW98xmGDBnCrbfe6ltU8KuvvqKiooIuXbpw/fXX+y2IV1ZWxhtvvOG3qODYsWO1qOBel112Wb3Hb7nlFl84r10Y7IcffsDlctW7MFh+fj6vv/46q1atIjIykiFDhnD11VdrAbb9/Otf/2LlypUUFRURExNDmzZtOP/88313C6nOjWf8+PG0bdu2zqKCqnXDPP/886xZs4bS0lISEhLo0qULV1xxhW+oNpzqrLAjIiIiTZrm7IiIiEiTprAjIiIiTZrCjoiIiDRpCjsiIiLSpCnsiIiISJOmsCMiIiJNmsKOiIiINGkKOyJyXJo7dy6XXXYZGzduDHVTRKSRabsIEWkUc+fO5ZVXXjng848//niT2i9u0aJFPPvss7z11ltERUUxceJEtm7dyvjx40PdNJHjnsKOiDSqyy67rM4Go4BvSfmmYv369bRu3dq3Fcm6des44YQTQtwqEQGFHRFpZH369KFDhw6hbkaj27hxo2//u5qaGrZs2cKFF14Y4laJCCjsiEiI5eXlcdttt/G73/0Oi8XCrFmzKC4uJjs7m+uvv77ODtYrV65k8uTJbN68mYiICLp168ZVV11FVlaW33mFhYVMmjSJn3/+mdLSUpKTk+nduzdjxozBat33o8/pdPL2228zb948ampq6NmzJzfddBMJCQmHbHtJSYnvzxs3bqRfv36UlJSwceNG3G43LVq0oKSkhMjISCIjIxtYKRE5WtoIVEQaRe2cnQcffJA2bdr4PWcYBvHx8cC+sNO6dWsqKys588wzcTqdzJo1C4vFwjPPPOPbJXnFihX87W9/Iy0tjeHDh1NTU8Nnn32Gx+NhwoQJvuGywsJC/vrXv1JRUcHw4cPJzMyksLCQH3/8kccff5zY2Fhf+9q1a0dsbCwDBgwgLy+PWbNmcdJJJ3HnnXce8jMeaNf437rkkksO+1wRCTz17IhIo3rsscfqHLPZbLz33nt+x3Jzc3nxxRdJSUkBoHfv3owbN45PP/2Ua6+9FoB3332XuLg4nnjiCeLi4gDo378/9957L5MnT+a2224D4P3338fhcPDkk0/6DaFdfvnl/Pbfd3FxcTzwwAMYhgGAaZp89tlnVFRUEBMTc9DP9sADDwDw448/smjRIm6//XYA3nvvPZKTkxk1ahQALVq0OIxKiUhjUdgRkUZ1/fXXk5GR4XfMYqm76kX//v19QQcgOzubjh07smzZMq699lqKiorYsmUL5513ni/oALRp04aePXuybNkyADweD4sWLaJv3771zhWqDTW1zjjjDL9jXbt2ZebMmeTn59fpkfqtnj17AvDFF19wwgkn0LNnTzweD7m5uZx99tm+50UktBR2RKRRZWdnH9YE5d8GotpjCxYsACA/Px+Ali1b1jkvMzOT5cuXU1VVRVVVFZWVlXXm+hxIamqq3+PY2FgAysvLD/q6srIyPB4PAKtXr+aiiy6ipKSEbdu2+d6/pKQEu93uu0NLREJDYUdEjmv19TIBdYa7fuu+++7zBTCAd955h3feecf3+C9/+QsAQ4YM4dZbbw1AS0XkaCnsiEhYyMnJqfdY8+bNAXz/37VrV53zdu3aRXx8PFFRUdjtdqKjo9m2bVujtvf222+npqaGRYsWsWDBAv70pz8B8OGHHxIfH88555wD4Dc0JyKhoe0iRCQsLFq0iMLCQt/jDRs2sH79enr37g1AcnIybdu25dtvv/UbYtq2bRvLly+nT58+gLenpn///ixZsqTerSACdQNqly5d6NmzJ5WVlXTq1ImePXvSs2dPCgoK6Nu3r+/xb2+JF5HgU8+OiDSqZcuWsXPnzjrHO3fu7HeXUnp6Og8++KDfrefx8fGcf/75vnN+97vf8be//Y0HHniAYcOGUVNTw+zZs4mJifG7tfuqq65ixYoVjB8/nuHDh5OVlUVRURE//vgjjz76qG9eTiD8+uuvnHHGGQDs3r0bh8NB586dA3Z9EWk4hR0RaVSTJ0+u9/gtt9ziF3ZOO+00LBYLM2fOpKSkhOzsbMaOHUtycrLvnJ49ezJu3DgmT57M5MmTfYsKXn311X5bUqSkpPDkk0/y4Ycf8v3331NZWUlKSgq9e/cO6OJ+DoeD3bt3+8LNunXriI6OplWrVgF7DxFpOC0qKCIhtf8Kyuedd16omyMiTZDm7IiIiEiTprAjIiIiTZrCjoiIiDRpmrMjIiIiTZp6dkRERKRJU9gRERGRJk1hR0RERJo0hR0RERFp0hR2REREpElT2BEREZEmTWFHREREmjSFHREREWnSFHZERESkSft/q4K5e95IvWkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=500;    # training epochs\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!! softmax only based the euclidean distance\n",
        "pred = Dense(3, activation='softmax')(dist)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "uuVsht8xCISl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw3rR3HbKElf"
      },
      "source": [
        "##2.2 Without Temperature in Softmax: Softmax based the euclidean distance and the output of Siamese Neural Network##\n",
        "\n",
        "**Result**  \n",
        "Around Training_loss = 0.38, validation_loss = 0.40"
      ],
      "id": "Xw3rR3HbKElf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mJDUSNqnmVMa",
        "outputId": "7a096751-07c4-4f9f-baf3-49d2b849e8be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "70/70 [==============================] - 9s 14ms/step - loss: 1.0322 - val_loss: 0.9992\n",
            "Epoch 2/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.9072 - val_loss: 0.7348\n",
            "Epoch 3/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6831 - val_loss: 0.6107\n",
            "Epoch 4/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6404 - val_loss: 0.6026\n",
            "Epoch 5/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6309 - val_loss: 0.5979\n",
            "Epoch 6/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6244 - val_loss: 0.5931\n",
            "Epoch 7/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6212 - val_loss: 0.5877\n",
            "Epoch 8/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.6134 - val_loss: 0.5828\n",
            "Epoch 9/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6035 - val_loss: 0.5774\n",
            "Epoch 10/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5953 - val_loss: 0.5724\n",
            "Epoch 11/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5865 - val_loss: 0.5651\n",
            "Epoch 12/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5817 - val_loss: 0.5605\n",
            "Epoch 13/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5770 - val_loss: 0.5562\n",
            "Epoch 14/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5740 - val_loss: 0.5557\n",
            "Epoch 15/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5697 - val_loss: 0.5530\n",
            "Epoch 16/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5680 - val_loss: 0.5500\n",
            "Epoch 17/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5667 - val_loss: 0.5468\n",
            "Epoch 18/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5634 - val_loss: 0.5462\n",
            "Epoch 19/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5612 - val_loss: 0.5444\n",
            "Epoch 20/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5594 - val_loss: 0.5412\n",
            "Epoch 21/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5580 - val_loss: 0.5424\n",
            "Epoch 22/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5562 - val_loss: 0.5377\n",
            "Epoch 23/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5541 - val_loss: 0.5376\n",
            "Epoch 24/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5519 - val_loss: 0.5355\n",
            "Epoch 25/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5502 - val_loss: 0.5340\n",
            "Epoch 26/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5489 - val_loss: 0.5346\n",
            "Epoch 27/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5478 - val_loss: 0.5321\n",
            "Epoch 28/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5448 - val_loss: 0.5287\n",
            "Epoch 29/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5454 - val_loss: 0.5267\n",
            "Epoch 30/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5418 - val_loss: 0.5266\n",
            "Epoch 31/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5405 - val_loss: 0.5254\n",
            "Epoch 32/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5397 - val_loss: 0.5233\n",
            "Epoch 33/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5381 - val_loss: 0.5232\n",
            "Epoch 34/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5354 - val_loss: 0.5211\n",
            "Epoch 35/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5344 - val_loss: 0.5206\n",
            "Epoch 36/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5335 - val_loss: 0.5185\n",
            "Epoch 37/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5317 - val_loss: 0.5204\n",
            "Epoch 38/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5302 - val_loss: 0.5168\n",
            "Epoch 39/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5298 - val_loss: 0.5124\n",
            "Epoch 40/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5278 - val_loss: 0.5146\n",
            "Epoch 41/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5264 - val_loss: 0.5125\n",
            "Epoch 42/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5256 - val_loss: 0.5114\n",
            "Epoch 43/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5235 - val_loss: 0.5088\n",
            "Epoch 44/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5240 - val_loss: 0.5085\n",
            "Epoch 45/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5213 - val_loss: 0.5070\n",
            "Epoch 46/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5199 - val_loss: 0.5053\n",
            "Epoch 47/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5183 - val_loss: 0.5044\n",
            "Epoch 48/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5176 - val_loss: 0.5041\n",
            "Epoch 49/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5162 - val_loss: 0.5010\n",
            "Epoch 50/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5154 - val_loss: 0.4998\n",
            "Epoch 51/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5144 - val_loss: 0.5000\n",
            "Epoch 52/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5127 - val_loss: 0.4979\n",
            "Epoch 53/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5121 - val_loss: 0.4964\n",
            "Epoch 54/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5099 - val_loss: 0.4987\n",
            "Epoch 55/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5094 - val_loss: 0.4967\n",
            "Epoch 56/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5084 - val_loss: 0.4962\n",
            "Epoch 57/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5070 - val_loss: 0.4956\n",
            "Epoch 58/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5064 - val_loss: 0.4944\n",
            "Epoch 59/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5056 - val_loss: 0.4918\n",
            "Epoch 60/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5041 - val_loss: 0.4918\n",
            "Epoch 61/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5034 - val_loss: 0.4907\n",
            "Epoch 62/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5017 - val_loss: 0.4897\n",
            "Epoch 63/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5011 - val_loss: 0.4867\n",
            "Epoch 64/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5005 - val_loss: 0.4874\n",
            "Epoch 65/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4997 - val_loss: 0.4873\n",
            "Epoch 66/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4981 - val_loss: 0.4872\n",
            "Epoch 67/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4980 - val_loss: 0.4851\n",
            "Epoch 68/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4963 - val_loss: 0.4831\n",
            "Epoch 69/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4953 - val_loss: 0.4811\n",
            "Epoch 70/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4954 - val_loss: 0.4821\n",
            "Epoch 71/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4956 - val_loss: 0.4818\n",
            "Epoch 72/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4929 - val_loss: 0.4799\n",
            "Epoch 73/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4925 - val_loss: 0.4799\n",
            "Epoch 74/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4911 - val_loss: 0.4787\n",
            "Epoch 75/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4907 - val_loss: 0.4769\n",
            "Epoch 76/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4895 - val_loss: 0.4768\n",
            "Epoch 77/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4881 - val_loss: 0.4768\n",
            "Epoch 78/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4879 - val_loss: 0.4763\n",
            "Epoch 79/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4869 - val_loss: 0.4741\n",
            "Epoch 80/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4853 - val_loss: 0.4736\n",
            "Epoch 81/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4850 - val_loss: 0.4737\n",
            "Epoch 82/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4843 - val_loss: 0.4730\n",
            "Epoch 83/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4837 - val_loss: 0.4708\n",
            "Epoch 84/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4828 - val_loss: 0.4693\n",
            "Epoch 85/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4817 - val_loss: 0.4703\n",
            "Epoch 86/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4810 - val_loss: 0.4692\n",
            "Epoch 87/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4796 - val_loss: 0.4686\n",
            "Epoch 88/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4795 - val_loss: 0.4669\n",
            "Epoch 89/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4784 - val_loss: 0.4675\n",
            "Epoch 90/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4776 - val_loss: 0.4650\n",
            "Epoch 91/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4773 - val_loss: 0.4644\n",
            "Epoch 92/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4764 - val_loss: 0.4652\n",
            "Epoch 93/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4755 - val_loss: 0.4643\n",
            "Epoch 94/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4749 - val_loss: 0.4655\n",
            "Epoch 95/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4747 - val_loss: 0.4633\n",
            "Epoch 96/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4740 - val_loss: 0.4619\n",
            "Epoch 97/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4731 - val_loss: 0.4608\n",
            "Epoch 98/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4721 - val_loss: 0.4622\n",
            "Epoch 99/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4716 - val_loss: 0.4594\n",
            "Epoch 100/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4706 - val_loss: 0.4605\n",
            "Epoch 101/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4702 - val_loss: 0.4577\n",
            "Epoch 102/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4694 - val_loss: 0.4582\n",
            "Epoch 103/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4695 - val_loss: 0.4581\n",
            "Epoch 104/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4683 - val_loss: 0.4569\n",
            "Epoch 105/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4677 - val_loss: 0.4578\n",
            "Epoch 106/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4666 - val_loss: 0.4580\n",
            "Epoch 107/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4670 - val_loss: 0.4542\n",
            "Epoch 108/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4653 - val_loss: 0.4548\n",
            "Epoch 109/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4654 - val_loss: 0.4538\n",
            "Epoch 110/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4646 - val_loss: 0.4547\n",
            "Epoch 111/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4643 - val_loss: 0.4552\n",
            "Epoch 112/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4638 - val_loss: 0.4531\n",
            "Epoch 113/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4625 - val_loss: 0.4507\n",
            "Epoch 114/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4619 - val_loss: 0.4522\n",
            "Epoch 115/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4615 - val_loss: 0.4510\n",
            "Epoch 116/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4613 - val_loss: 0.4493\n",
            "Epoch 117/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4603 - val_loss: 0.4495\n",
            "Epoch 118/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4599 - val_loss: 0.4491\n",
            "Epoch 119/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4598 - val_loss: 0.4476\n",
            "Epoch 120/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4588 - val_loss: 0.4470\n",
            "Epoch 121/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4587 - val_loss: 0.4488\n",
            "Epoch 122/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4572 - val_loss: 0.4468\n",
            "Epoch 123/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4569 - val_loss: 0.4453\n",
            "Epoch 124/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4566 - val_loss: 0.4459\n",
            "Epoch 125/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4558 - val_loss: 0.4447\n",
            "Epoch 126/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4557 - val_loss: 0.4440\n",
            "Epoch 127/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4540 - val_loss: 0.4443\n",
            "Epoch 128/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4548 - val_loss: 0.4448\n",
            "Epoch 129/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4538 - val_loss: 0.4454\n",
            "Epoch 130/1000\n",
            "70/70 [==============================] - 2s 27ms/step - loss: 0.4539 - val_loss: 0.4439\n",
            "Epoch 131/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4530 - val_loss: 0.4422\n",
            "Epoch 132/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4526 - val_loss: 0.4416\n",
            "Epoch 133/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4520 - val_loss: 0.4400\n",
            "Epoch 134/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4508 - val_loss: 0.4439\n",
            "Epoch 135/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4514 - val_loss: 0.4395\n",
            "Epoch 136/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4501 - val_loss: 0.4426\n",
            "Epoch 137/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4499 - val_loss: 0.4416\n",
            "Epoch 138/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4495 - val_loss: 0.4397\n",
            "Epoch 139/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4492 - val_loss: 0.4383\n",
            "Epoch 140/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4482 - val_loss: 0.4390\n",
            "Epoch 141/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4478 - val_loss: 0.4393\n",
            "Epoch 142/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4480 - val_loss: 0.4375\n",
            "Epoch 143/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4472 - val_loss: 0.4381\n",
            "Epoch 144/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4469 - val_loss: 0.4351\n",
            "Epoch 145/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4468 - val_loss: 0.4356\n",
            "Epoch 146/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4452 - val_loss: 0.4367\n",
            "Epoch 147/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4455 - val_loss: 0.4361\n",
            "Epoch 148/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4446 - val_loss: 0.4344\n",
            "Epoch 149/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.4443 - val_loss: 0.4336\n",
            "Epoch 150/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4448 - val_loss: 0.4344\n",
            "Epoch 151/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4445 - val_loss: 0.4314\n",
            "Epoch 152/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4435 - val_loss: 0.4335\n",
            "Epoch 153/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4430 - val_loss: 0.4310\n",
            "Epoch 154/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4423 - val_loss: 0.4321\n",
            "Epoch 155/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4413 - val_loss: 0.4336\n",
            "Epoch 156/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4415 - val_loss: 0.4314\n",
            "Epoch 157/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4413 - val_loss: 0.4307\n",
            "Epoch 158/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4408 - val_loss: 0.4310\n",
            "Epoch 159/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4398 - val_loss: 0.4301\n",
            "Epoch 160/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4403 - val_loss: 0.4287\n",
            "Epoch 161/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4403 - val_loss: 0.4302\n",
            "Epoch 162/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4399 - val_loss: 0.4322\n",
            "Epoch 163/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4391 - val_loss: 0.4279\n",
            "Epoch 164/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4381 - val_loss: 0.4290\n",
            "Epoch 165/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4380 - val_loss: 0.4267\n",
            "Epoch 166/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4374 - val_loss: 0.4283\n",
            "Epoch 167/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4368 - val_loss: 0.4272\n",
            "Epoch 168/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4363 - val_loss: 0.4274\n",
            "Epoch 169/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4361 - val_loss: 0.4256\n",
            "Epoch 170/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4367 - val_loss: 0.4249\n",
            "Epoch 171/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4363 - val_loss: 0.4250\n",
            "Epoch 172/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4355 - val_loss: 0.4254\n",
            "Epoch 173/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4348 - val_loss: 0.4253\n",
            "Epoch 174/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4346 - val_loss: 0.4250\n",
            "Epoch 175/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4341 - val_loss: 0.4242\n",
            "Epoch 176/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4336 - val_loss: 0.4247\n",
            "Epoch 177/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4336 - val_loss: 0.4230\n",
            "Epoch 178/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4338 - val_loss: 0.4232\n",
            "Epoch 179/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4329 - val_loss: 0.4230\n",
            "Epoch 180/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4323 - val_loss: 0.4222\n",
            "Epoch 181/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4321 - val_loss: 0.4228\n",
            "Epoch 182/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4322 - val_loss: 0.4217\n",
            "Epoch 183/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4312 - val_loss: 0.4221\n",
            "Epoch 184/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4314 - val_loss: 0.4203\n",
            "Epoch 185/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4317 - val_loss: 0.4201\n",
            "Epoch 186/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4305 - val_loss: 0.4200\n",
            "Epoch 187/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4301 - val_loss: 0.4198\n",
            "Epoch 188/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4293 - val_loss: 0.4210\n",
            "Epoch 189/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4291 - val_loss: 0.4204\n",
            "Epoch 190/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4298 - val_loss: 0.4200\n",
            "Epoch 191/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4284 - val_loss: 0.4198\n",
            "Epoch 192/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4283 - val_loss: 0.4200\n",
            "Epoch 193/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4282 - val_loss: 0.4190\n",
            "Epoch 194/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4276 - val_loss: 0.4171\n",
            "Epoch 195/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4277 - val_loss: 0.4194\n",
            "Epoch 196/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4279 - val_loss: 0.4172\n",
            "Epoch 197/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4269 - val_loss: 0.4157\n",
            "Epoch 198/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4262 - val_loss: 0.4181\n",
            "Epoch 199/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4262 - val_loss: 0.4157\n",
            "Epoch 200/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4261 - val_loss: 0.4159\n",
            "Epoch 201/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4259 - val_loss: 0.4166\n",
            "Epoch 202/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4255 - val_loss: 0.4167\n",
            "Epoch 203/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4249 - val_loss: 0.4163\n",
            "Epoch 204/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4255 - val_loss: 0.4162\n",
            "Epoch 205/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4247 - val_loss: 0.4161\n",
            "Epoch 206/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4245 - val_loss: 0.4156\n",
            "Epoch 207/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4245 - val_loss: 0.4156\n",
            "Epoch 208/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4237 - val_loss: 0.4145\n",
            "Epoch 209/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4236 - val_loss: 0.4146\n",
            "Epoch 210/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4232 - val_loss: 0.4142\n",
            "Epoch 211/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4233 - val_loss: 0.4137\n",
            "Epoch 212/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4229 - val_loss: 0.4144\n",
            "Epoch 213/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4229 - val_loss: 0.4125\n",
            "Epoch 214/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4219 - val_loss: 0.4139\n",
            "Epoch 215/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4214 - val_loss: 0.4138\n",
            "Epoch 216/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4217 - val_loss: 0.4137\n",
            "Epoch 217/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4216 - val_loss: 0.4143\n",
            "Epoch 218/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4211 - val_loss: 0.4124\n",
            "Epoch 219/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4211 - val_loss: 0.4118\n",
            "Epoch 220/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4203 - val_loss: 0.4120\n",
            "Epoch 221/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4203 - val_loss: 0.4109\n",
            "Epoch 222/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4202 - val_loss: 0.4116\n",
            "Epoch 223/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4195 - val_loss: 0.4128\n",
            "Epoch 224/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4196 - val_loss: 0.4106\n",
            "Epoch 225/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4193 - val_loss: 0.4129\n",
            "Epoch 226/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4197 - val_loss: 0.4132\n",
            "Epoch 227/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4200 - val_loss: 0.4105\n",
            "Epoch 228/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4183 - val_loss: 0.4114\n",
            "Epoch 229/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4188 - val_loss: 0.4092\n",
            "Epoch 230/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4183 - val_loss: 0.4099\n",
            "Epoch 231/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4186 - val_loss: 0.4108\n",
            "Epoch 232/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4183 - val_loss: 0.4092\n",
            "Epoch 233/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4183 - val_loss: 0.4075\n",
            "Epoch 234/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4186 - val_loss: 0.4073\n",
            "Epoch 235/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4180 - val_loss: 0.4077\n",
            "Epoch 236/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4167 - val_loss: 0.4102\n",
            "Epoch 237/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4179 - val_loss: 0.4093\n",
            "Epoch 238/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4165 - val_loss: 0.4104\n",
            "Epoch 239/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4164 - val_loss: 0.4065\n",
            "Epoch 240/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4160 - val_loss: 0.4078\n",
            "Epoch 241/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4166 - val_loss: 0.4067\n",
            "Epoch 242/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4164 - val_loss: 0.4064\n",
            "Epoch 243/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4161 - val_loss: 0.4079\n",
            "Epoch 244/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4161 - val_loss: 0.4086\n",
            "Epoch 245/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4159 - val_loss: 0.4065\n",
            "Epoch 246/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4156 - val_loss: 0.4063\n",
            "Epoch 247/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4149 - val_loss: 0.4061\n",
            "Epoch 248/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4147 - val_loss: 0.4058\n",
            "Epoch 249/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4144 - val_loss: 0.4050\n",
            "Epoch 250/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4149 - val_loss: 0.4053\n",
            "Epoch 251/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4140 - val_loss: 0.4049\n",
            "Epoch 252/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4144 - val_loss: 0.4068\n",
            "Epoch 253/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4141 - val_loss: 0.4054\n",
            "Epoch 254/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4132 - val_loss: 0.4057\n",
            "Epoch 255/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4133 - val_loss: 0.4051\n",
            "Epoch 256/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4139 - val_loss: 0.4051\n",
            "Epoch 257/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4131 - val_loss: 0.4057\n",
            "Epoch 258/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4129 - val_loss: 0.4029\n",
            "Epoch 259/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4127 - val_loss: 0.4042\n",
            "Epoch 260/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4128 - val_loss: 0.4045\n",
            "Epoch 261/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4122 - val_loss: 0.4049\n",
            "Epoch 262/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4128 - val_loss: 0.4048\n",
            "Epoch 263/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4124 - val_loss: 0.4044\n",
            "Epoch 264/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4122 - val_loss: 0.4033\n",
            "Epoch 265/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4112 - val_loss: 0.4026\n",
            "Epoch 266/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4119 - val_loss: 0.4048\n",
            "Epoch 267/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4114 - val_loss: 0.4026\n",
            "Epoch 268/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4111 - val_loss: 0.4036\n",
            "Epoch 269/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4109 - val_loss: 0.4023\n",
            "Epoch 270/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4113 - val_loss: 0.4007\n",
            "Epoch 271/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4107 - val_loss: 0.4023\n",
            "Epoch 272/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4108 - val_loss: 0.4014\n",
            "Epoch 273/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4099 - val_loss: 0.4024\n",
            "Epoch 274/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4104 - val_loss: 0.4010\n",
            "Epoch 275/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4104 - val_loss: 0.4018\n",
            "Epoch 276/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4103 - val_loss: 0.3998\n",
            "Epoch 277/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4105 - val_loss: 0.4010\n",
            "Epoch 278/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4097 - val_loss: 0.4003\n",
            "Epoch 279/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4098 - val_loss: 0.4015\n",
            "Epoch 280/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4096 - val_loss: 0.4020\n",
            "Epoch 281/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4091 - val_loss: 0.4007\n",
            "Epoch 282/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4093 - val_loss: 0.4014\n",
            "Epoch 283/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4086 - val_loss: 0.4005\n",
            "Epoch 284/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4088 - val_loss: 0.4009\n",
            "Epoch 285/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4084 - val_loss: 0.4000\n",
            "Epoch 286/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4084 - val_loss: 0.3999\n",
            "Epoch 287/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4087 - val_loss: 0.4005\n",
            "Epoch 288/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4083 - val_loss: 0.4018\n",
            "Epoch 289/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4084 - val_loss: 0.3993\n",
            "Epoch 290/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4081 - val_loss: 0.3999\n",
            "Epoch 291/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4078 - val_loss: 0.4002\n",
            "Epoch 292/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4081 - val_loss: 0.3993\n",
            "Epoch 293/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4075 - val_loss: 0.4010\n",
            "Epoch 294/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4077 - val_loss: 0.3996\n",
            "Epoch 295/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4074 - val_loss: 0.4015\n",
            "Epoch 296/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4074 - val_loss: 0.3988\n",
            "Epoch 297/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4073 - val_loss: 0.3988\n",
            "Epoch 298/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4071 - val_loss: 0.3999\n",
            "Epoch 299/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4068 - val_loss: 0.4006\n",
            "Epoch 300/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4073 - val_loss: 0.4004\n",
            "Epoch 301/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4065 - val_loss: 0.4022\n",
            "Epoch 302/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4069 - val_loss: 0.4001\n",
            "Epoch 303/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4067 - val_loss: 0.3993\n",
            "Epoch 304/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4061 - val_loss: 0.3974\n",
            "Epoch 305/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4070 - val_loss: 0.3968\n",
            "Epoch 306/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4059 - val_loss: 0.3972\n",
            "Epoch 307/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4061 - val_loss: 0.3966\n",
            "Epoch 308/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4056 - val_loss: 0.3980\n",
            "Epoch 309/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4056 - val_loss: 0.3982\n",
            "Epoch 310/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4056 - val_loss: 0.3975\n",
            "Epoch 311/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4055 - val_loss: 0.3989\n",
            "Epoch 312/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4056 - val_loss: 0.3982\n",
            "Epoch 313/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4050 - val_loss: 0.3981\n",
            "Epoch 314/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4048 - val_loss: 0.3967\n",
            "Epoch 315/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4051 - val_loss: 0.4000\n",
            "Epoch 316/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4048 - val_loss: 0.3958\n",
            "Epoch 317/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4058 - val_loss: 0.3993\n",
            "Epoch 318/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.4049 - val_loss: 0.3951\n",
            "Epoch 319/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4040 - val_loss: 0.3967\n",
            "Epoch 320/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4047 - val_loss: 0.3976\n",
            "Epoch 321/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4041 - val_loss: 0.3958\n",
            "Epoch 322/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4044 - val_loss: 0.3958\n",
            "Epoch 323/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4041 - val_loss: 0.3960\n",
            "Epoch 324/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4039 - val_loss: 0.3970\n",
            "Epoch 325/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4037 - val_loss: 0.3974\n",
            "Epoch 326/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4040 - val_loss: 0.3952\n",
            "Epoch 327/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4040 - val_loss: 0.3960\n",
            "Epoch 328/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4036 - val_loss: 0.3959\n",
            "Epoch 329/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4035 - val_loss: 0.3968\n",
            "Epoch 330/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4032 - val_loss: 0.3985\n",
            "Epoch 331/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4039 - val_loss: 0.3958\n",
            "Epoch 332/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4032 - val_loss: 0.3963\n",
            "Epoch 333/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4033 - val_loss: 0.3965\n",
            "Epoch 334/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4029 - val_loss: 0.3973\n",
            "Epoch 335/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4026 - val_loss: 0.3964\n",
            "Epoch 336/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4023 - val_loss: 0.3975\n",
            "Epoch 337/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4025 - val_loss: 0.3996\n",
            "Epoch 338/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4030 - val_loss: 0.3961\n",
            "Epoch 339/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4030 - val_loss: 0.3950\n",
            "Epoch 340/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4025 - val_loss: 0.3952\n",
            "Epoch 341/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4024 - val_loss: 0.3974\n",
            "Epoch 342/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4024 - val_loss: 0.3959\n",
            "Epoch 343/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4023 - val_loss: 0.3947\n",
            "Epoch 344/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4020 - val_loss: 0.3951\n",
            "Epoch 345/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4021 - val_loss: 0.3942\n",
            "Epoch 346/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4014 - val_loss: 0.3952\n",
            "Epoch 347/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4020 - val_loss: 0.3965\n",
            "Epoch 348/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4015 - val_loss: 0.3960\n",
            "Epoch 349/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4012 - val_loss: 0.3964\n",
            "Epoch 350/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4013 - val_loss: 0.3958\n",
            "Epoch 351/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4020 - val_loss: 0.3937\n",
            "Epoch 352/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3953\n",
            "Epoch 353/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4012 - val_loss: 0.3958\n",
            "Epoch 354/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4011 - val_loss: 0.3938\n",
            "Epoch 355/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3945\n",
            "Epoch 356/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4012 - val_loss: 0.3930\n",
            "Epoch 357/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4011 - val_loss: 0.3943\n",
            "Epoch 358/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4009 - val_loss: 0.3940\n",
            "Epoch 359/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4006 - val_loss: 0.3948\n",
            "Epoch 360/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4004 - val_loss: 0.3942\n",
            "Epoch 361/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3998 - val_loss: 0.3959\n",
            "Epoch 362/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4007 - val_loss: 0.3952\n",
            "Epoch 363/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.3964\n",
            "Epoch 364/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4009 - val_loss: 0.3935\n",
            "Epoch 365/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4012 - val_loss: 0.3959\n",
            "Epoch 366/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4002 - val_loss: 0.3934\n",
            "Epoch 367/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3997 - val_loss: 0.3939\n",
            "Epoch 368/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3996 - val_loss: 0.3925\n",
            "Epoch 369/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4007 - val_loss: 0.3935\n",
            "Epoch 370/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4000 - val_loss: 0.3930\n",
            "Epoch 371/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3998 - val_loss: 0.3920\n",
            "Epoch 372/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4004 - val_loss: 0.3943\n",
            "Epoch 373/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4005 - val_loss: 0.3915\n",
            "Epoch 374/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3997 - val_loss: 0.3948\n",
            "Epoch 375/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3998 - val_loss: 0.3939\n",
            "Epoch 376/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3993 - val_loss: 0.3931\n",
            "Epoch 377/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3990 - val_loss: 0.3952\n",
            "Epoch 378/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3994 - val_loss: 0.3939\n",
            "Epoch 379/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3992 - val_loss: 0.3930\n",
            "Epoch 380/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3999 - val_loss: 0.3945\n",
            "Epoch 381/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3987 - val_loss: 0.3931\n",
            "Epoch 382/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3983 - val_loss: 0.3925\n",
            "Epoch 383/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3996 - val_loss: 0.3918\n",
            "Epoch 384/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3987 - val_loss: 0.3938\n",
            "Epoch 385/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3995 - val_loss: 0.3940\n",
            "Epoch 386/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3986 - val_loss: 0.3907\n",
            "Epoch 387/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3994 - val_loss: 0.3912\n",
            "Epoch 388/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3992 - val_loss: 0.3909\n",
            "Epoch 389/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3992 - val_loss: 0.3939\n",
            "Epoch 390/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3985 - val_loss: 0.3918\n",
            "Epoch 391/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3981 - val_loss: 0.3924\n",
            "Epoch 392/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3981 - val_loss: 0.3925\n",
            "Epoch 393/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3981 - val_loss: 0.3923\n",
            "Epoch 394/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3974 - val_loss: 0.3916\n",
            "Epoch 395/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3978 - val_loss: 0.3925\n",
            "Epoch 396/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3980 - val_loss: 0.3942\n",
            "Epoch 397/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3984 - val_loss: 0.3941\n",
            "Epoch 398/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3979 - val_loss: 0.3922\n",
            "Epoch 399/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3977 - val_loss: 0.3926\n",
            "Epoch 400/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3972 - val_loss: 0.3909\n",
            "Epoch 401/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3980 - val_loss: 0.3928\n",
            "Epoch 402/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3976 - val_loss: 0.3913\n",
            "Epoch 403/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3978 - val_loss: 0.3920\n",
            "Epoch 404/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3975 - val_loss: 0.3904\n",
            "Epoch 405/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3974 - val_loss: 0.3928\n",
            "Epoch 406/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3975 - val_loss: 0.3920\n",
            "Epoch 407/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.3926\n",
            "Epoch 408/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3975 - val_loss: 0.3918\n",
            "Epoch 409/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3970 - val_loss: 0.3913\n",
            "Epoch 410/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3975 - val_loss: 0.3921\n",
            "Epoch 411/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3975 - val_loss: 0.3923\n",
            "Epoch 412/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3966 - val_loss: 0.3944\n",
            "Epoch 413/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3970 - val_loss: 0.3948\n",
            "Epoch 414/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3969 - val_loss: 0.3918\n",
            "Epoch 415/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3964 - val_loss: 0.3923\n",
            "Epoch 416/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3967 - val_loss: 0.3913\n",
            "Epoch 417/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3968 - val_loss: 0.3930\n",
            "Epoch 418/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3933\n",
            "Epoch 419/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3970 - val_loss: 0.3917\n",
            "Epoch 420/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3964 - val_loss: 0.3935\n",
            "Epoch 421/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3965 - val_loss: 0.3938\n",
            "Epoch 422/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3966 - val_loss: 0.3928\n",
            "Epoch 423/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3963 - val_loss: 0.3939\n",
            "Epoch 424/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3961 - val_loss: 0.3900\n",
            "Epoch 425/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3912\n",
            "Epoch 426/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3915\n",
            "Epoch 427/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3964 - val_loss: 0.3900\n",
            "Epoch 428/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3962 - val_loss: 0.3915\n",
            "Epoch 429/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3959 - val_loss: 0.3925\n",
            "Epoch 430/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3958 - val_loss: 0.3927\n",
            "Epoch 431/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3956 - val_loss: 0.3910\n",
            "Epoch 432/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3957 - val_loss: 0.3937\n",
            "Epoch 433/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3957 - val_loss: 0.3922\n",
            "Epoch 434/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3959 - val_loss: 0.3901\n",
            "Epoch 435/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3953 - val_loss: 0.3937\n",
            "Epoch 436/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3954 - val_loss: 0.3904\n",
            "Epoch 437/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3953 - val_loss: 0.3898\n",
            "Epoch 438/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3958 - val_loss: 0.3931\n",
            "Epoch 439/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3924\n",
            "Epoch 440/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3953 - val_loss: 0.3932\n",
            "Epoch 441/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3957 - val_loss: 0.3915\n",
            "Epoch 442/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3952 - val_loss: 0.3904\n",
            "Epoch 443/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3955 - val_loss: 0.3932\n",
            "Epoch 444/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3958 - val_loss: 0.3941\n",
            "Epoch 445/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3949 - val_loss: 0.3916\n",
            "Epoch 446/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3955 - val_loss: 0.3937\n",
            "Epoch 447/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3950 - val_loss: 0.3908\n",
            "Epoch 448/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3946 - val_loss: 0.3911\n",
            "Epoch 449/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3944 - val_loss: 0.3910\n",
            "Epoch 450/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3949 - val_loss: 0.3907\n",
            "Epoch 451/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3950 - val_loss: 0.3925\n",
            "Epoch 452/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3946 - val_loss: 0.3927\n",
            "Epoch 453/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3949 - val_loss: 0.3918\n",
            "Epoch 454/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3951 - val_loss: 0.3923\n",
            "Epoch 455/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3945 - val_loss: 0.3922\n",
            "Epoch 456/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3950 - val_loss: 0.3917\n",
            "Epoch 457/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3947 - val_loss: 0.3910\n",
            "Epoch 458/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3942 - val_loss: 0.3929\n",
            "Epoch 459/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3942 - val_loss: 0.3910\n",
            "Epoch 460/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3952 - val_loss: 0.3931\n",
            "Epoch 461/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3946 - val_loss: 0.3928\n",
            "Epoch 462/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3905\n",
            "Epoch 463/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3936 - val_loss: 0.3962\n",
            "Epoch 464/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3940 - val_loss: 0.3906\n",
            "Epoch 465/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3926\n",
            "Epoch 466/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3937 - val_loss: 0.3903\n",
            "Epoch 467/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3940 - val_loss: 0.3906\n",
            "Epoch 468/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3947 - val_loss: 0.3931\n",
            "Epoch 469/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3936 - val_loss: 0.3900\n",
            "Epoch 470/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3938 - val_loss: 0.3917\n",
            "Epoch 471/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3941 - val_loss: 0.3936\n",
            "Epoch 472/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3937 - val_loss: 0.3933\n",
            "Epoch 473/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3935 - val_loss: 0.3910\n",
            "Epoch 474/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3942 - val_loss: 0.3917\n",
            "Epoch 475/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3946 - val_loss: 0.3912\n",
            "Epoch 476/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3935 - val_loss: 0.3922\n",
            "Epoch 477/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3938 - val_loss: 0.3933\n",
            "Epoch 478/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3923\n",
            "Epoch 479/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3934 - val_loss: 0.3948\n",
            "Epoch 480/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3933 - val_loss: 0.3912\n",
            "Epoch 481/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3936 - val_loss: 0.3907\n",
            "Epoch 482/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3936 - val_loss: 0.3905\n",
            "Epoch 483/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3929 - val_loss: 0.3922\n",
            "Epoch 484/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3931 - val_loss: 0.3924\n",
            "Epoch 485/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3935 - val_loss: 0.3914\n",
            "Epoch 486/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3931 - val_loss: 0.3918\n",
            "Epoch 487/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3933 - val_loss: 0.3906\n",
            "Epoch 488/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3937 - val_loss: 0.3940\n",
            "Epoch 489/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3938 - val_loss: 0.3943\n",
            "Epoch 490/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3927 - val_loss: 0.3910\n",
            "Epoch 491/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3929 - val_loss: 0.3898\n",
            "Epoch 492/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3928 - val_loss: 0.3903\n",
            "Epoch 493/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3936 - val_loss: 0.3930\n",
            "Epoch 494/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3927 - val_loss: 0.3916\n",
            "Epoch 495/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3929 - val_loss: 0.3917\n",
            "Epoch 496/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3930 - val_loss: 0.3904\n",
            "Epoch 497/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3926 - val_loss: 0.3913\n",
            "Epoch 498/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3945\n",
            "Epoch 499/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3925 - val_loss: 0.3919\n",
            "Epoch 500/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3928 - val_loss: 0.3898\n",
            "Epoch 501/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3929 - val_loss: 0.3925\n",
            "Epoch 502/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3924 - val_loss: 0.3940\n",
            "Epoch 503/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3925 - val_loss: 0.3938\n",
            "Epoch 504/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3926 - val_loss: 0.3912\n",
            "Epoch 505/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3926\n",
            "Epoch 506/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3920 - val_loss: 0.3942\n",
            "Epoch 507/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3925 - val_loss: 0.3925\n",
            "Epoch 508/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3923 - val_loss: 0.3958\n",
            "Epoch 509/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3925 - val_loss: 0.3944\n",
            "Epoch 510/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3921 - val_loss: 0.3944\n",
            "Epoch 511/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3922 - val_loss: 0.3907\n",
            "Epoch 512/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3927\n",
            "Epoch 513/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3922 - val_loss: 0.3944\n",
            "Epoch 514/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3918 - val_loss: 0.3921\n",
            "Epoch 515/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3918 - val_loss: 0.3927\n",
            "Epoch 516/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3921 - val_loss: 0.3927\n",
            "Epoch 517/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3916 - val_loss: 0.3909\n",
            "Epoch 518/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3918 - val_loss: 0.3908\n",
            "Epoch 519/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3915 - val_loss: 0.3934\n",
            "Epoch 520/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3913 - val_loss: 0.3916\n",
            "Epoch 521/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3920 - val_loss: 0.3907\n",
            "Epoch 522/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3919 - val_loss: 0.3935\n",
            "Epoch 523/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3915 - val_loss: 0.3921\n",
            "Epoch 524/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3921 - val_loss: 0.3920\n",
            "Epoch 525/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3924 - val_loss: 0.3929\n",
            "Epoch 526/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3911 - val_loss: 0.3914\n",
            "Epoch 527/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3917 - val_loss: 0.3918\n",
            "Epoch 528/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3915 - val_loss: 0.3942\n",
            "Epoch 529/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3918 - val_loss: 0.3934\n",
            "Epoch 530/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3920 - val_loss: 0.3918\n",
            "Epoch 531/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3914 - val_loss: 0.3935\n",
            "Epoch 532/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3915 - val_loss: 0.3931\n",
            "Epoch 533/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3914 - val_loss: 0.3910\n",
            "Epoch 534/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3909 - val_loss: 0.3911\n",
            "Epoch 535/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3911 - val_loss: 0.3937\n",
            "Epoch 536/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3908 - val_loss: 0.3940\n",
            "Epoch 537/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3908 - val_loss: 0.3901\n",
            "Epoch 538/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3909 - val_loss: 0.3944\n",
            "Epoch 539/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3909 - val_loss: 0.3945\n",
            "Epoch 540/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3906 - val_loss: 0.3929\n",
            "Epoch 541/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3910 - val_loss: 0.3942\n",
            "Epoch 542/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3907 - val_loss: 0.3930\n",
            "Epoch 543/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3910 - val_loss: 0.3909\n",
            "Epoch 544/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3912 - val_loss: 0.3952\n",
            "Epoch 545/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3904 - val_loss: 0.3932\n",
            "Epoch 546/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3911 - val_loss: 0.3928\n",
            "Epoch 547/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3909 - val_loss: 0.3950\n",
            "Epoch 548/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3907 - val_loss: 0.3900\n",
            "Epoch 549/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3913 - val_loss: 0.3925\n",
            "Epoch 550/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3904 - val_loss: 0.3948\n",
            "Epoch 551/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3909 - val_loss: 0.3905\n",
            "Epoch 552/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3907 - val_loss: 0.3936\n",
            "Epoch 553/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3906 - val_loss: 0.3924\n",
            "Epoch 554/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3899 - val_loss: 0.3936\n",
            "Epoch 555/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.3932\n",
            "Epoch 556/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.3940\n",
            "Epoch 557/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3904 - val_loss: 0.3942\n",
            "Epoch 558/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3895 - val_loss: 0.3931\n",
            "Epoch 559/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3904 - val_loss: 0.3951\n",
            "Epoch 560/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3900 - val_loss: 0.3897\n",
            "Epoch 561/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3899 - val_loss: 0.3937\n",
            "Epoch 562/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3966\n",
            "Epoch 563/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3904 - val_loss: 0.3938\n",
            "Epoch 564/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3899 - val_loss: 0.3903\n",
            "Epoch 565/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3901 - val_loss: 0.3945\n",
            "Epoch 566/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3902 - val_loss: 0.3948\n",
            "Epoch 567/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3904 - val_loss: 0.3939\n",
            "Epoch 568/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3912 - val_loss: 0.3912\n",
            "Epoch 569/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3900 - val_loss: 0.3930\n",
            "Epoch 570/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3894 - val_loss: 0.3932\n",
            "Epoch 571/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3901 - val_loss: 0.3943\n",
            "Epoch 572/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3908 - val_loss: 0.3954\n",
            "Epoch 573/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3900 - val_loss: 0.3937\n",
            "Epoch 574/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3896 - val_loss: 0.3921\n",
            "Epoch 575/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3902 - val_loss: 0.3940\n",
            "Epoch 576/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3895 - val_loss: 0.3936\n",
            "Epoch 577/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.3955\n",
            "Epoch 578/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3925\n",
            "Epoch 579/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3946\n",
            "Epoch 580/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3901 - val_loss: 0.3940\n",
            "Epoch 581/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3894 - val_loss: 0.3935\n",
            "Epoch 582/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3946\n",
            "Epoch 583/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3896 - val_loss: 0.3940\n",
            "Epoch 584/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3892 - val_loss: 0.3937\n",
            "Epoch 585/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3895 - val_loss: 0.3933\n",
            "Epoch 586/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3897 - val_loss: 0.3938\n",
            "Epoch 587/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3889 - val_loss: 0.3970\n",
            "Epoch 588/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3898 - val_loss: 0.3939\n",
            "Epoch 589/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3897 - val_loss: 0.3960\n",
            "Epoch 590/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3951\n",
            "Epoch 591/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3895 - val_loss: 0.3909\n",
            "Epoch 592/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3894 - val_loss: 0.3958\n",
            "Epoch 593/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3938\n",
            "Epoch 594/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3888 - val_loss: 0.3966\n",
            "Epoch 595/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3896 - val_loss: 0.3964\n",
            "Epoch 596/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3901 - val_loss: 0.3948\n",
            "Epoch 597/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3966\n",
            "Epoch 598/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3891 - val_loss: 0.3954\n",
            "Epoch 599/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3885 - val_loss: 0.3947\n",
            "Epoch 600/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3886 - val_loss: 0.3923\n",
            "Epoch 601/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.3970\n",
            "Epoch 602/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3890 - val_loss: 0.3973\n",
            "Epoch 603/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3887 - val_loss: 0.3964\n",
            "Epoch 604/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3889 - val_loss: 0.3954\n",
            "Epoch 605/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3891 - val_loss: 0.3988\n",
            "Epoch 606/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3888 - val_loss: 0.3965\n",
            "Epoch 607/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3890 - val_loss: 0.3961\n",
            "Epoch 608/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3887 - val_loss: 0.3963\n",
            "Epoch 609/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3891 - val_loss: 0.3948\n",
            "Epoch 610/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.3961\n",
            "Epoch 611/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3893 - val_loss: 0.3926\n",
            "Epoch 612/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3953\n",
            "Epoch 613/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3983\n",
            "Epoch 614/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3956\n",
            "Epoch 615/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3944\n",
            "Epoch 616/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3879 - val_loss: 0.3949\n",
            "Epoch 617/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3984\n",
            "Epoch 618/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3940\n",
            "Epoch 619/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3972\n",
            "Epoch 620/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3890 - val_loss: 0.3906\n",
            "Epoch 621/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3964\n",
            "Epoch 622/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3885 - val_loss: 0.3954\n",
            "Epoch 623/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3886 - val_loss: 0.3949\n",
            "Epoch 624/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3875 - val_loss: 0.3942\n",
            "Epoch 625/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3884 - val_loss: 0.3954\n",
            "Epoch 626/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3884 - val_loss: 0.3986\n",
            "Epoch 627/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3887 - val_loss: 0.3961\n",
            "Epoch 628/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3877 - val_loss: 0.3963\n",
            "Epoch 629/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3955\n",
            "Epoch 630/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3881 - val_loss: 0.3975\n",
            "Epoch 631/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3957\n",
            "Epoch 632/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3958\n",
            "Epoch 633/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3877 - val_loss: 0.3973\n",
            "Epoch 634/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3972\n",
            "Epoch 635/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3949\n",
            "Epoch 636/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3962\n",
            "Epoch 637/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3970\n",
            "Epoch 638/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3883 - val_loss: 0.3974\n",
            "Epoch 639/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3946\n",
            "Epoch 640/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3877 - val_loss: 0.3966\n",
            "Epoch 641/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3880 - val_loss: 0.3980\n",
            "Epoch 642/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3878 - val_loss: 0.3944\n",
            "Epoch 643/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3886 - val_loss: 0.3968\n",
            "Epoch 644/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3876 - val_loss: 0.3949\n",
            "Epoch 645/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3880 - val_loss: 0.3947\n",
            "Epoch 646/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3876 - val_loss: 0.3969\n",
            "Epoch 647/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3947\n",
            "Epoch 648/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3873 - val_loss: 0.3975\n",
            "Epoch 649/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3958\n",
            "Epoch 650/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.3953\n",
            "Epoch 651/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3871 - val_loss: 0.3954\n",
            "Epoch 652/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3873 - val_loss: 0.3979\n",
            "Epoch 653/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3875 - val_loss: 0.3947\n",
            "Epoch 654/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3875 - val_loss: 0.3990\n",
            "Epoch 655/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3878 - val_loss: 0.3972\n",
            "Epoch 656/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3979\n",
            "Epoch 657/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3936\n",
            "Epoch 658/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3876 - val_loss: 0.3970\n",
            "Epoch 659/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3958\n",
            "Epoch 660/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3872 - val_loss: 0.3960\n",
            "Epoch 661/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3870 - val_loss: 0.4016\n",
            "Epoch 662/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3879 - val_loss: 0.3944\n",
            "Epoch 663/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3871 - val_loss: 0.3962\n",
            "Epoch 664/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3880 - val_loss: 0.3962\n",
            "Epoch 665/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3873 - val_loss: 0.3982\n",
            "Epoch 666/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3974\n",
            "Epoch 667/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3877 - val_loss: 0.3970\n",
            "Epoch 668/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3869 - val_loss: 0.3999\n",
            "Epoch 669/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3988\n",
            "Epoch 670/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3943\n",
            "Epoch 671/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3985\n",
            "Epoch 672/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3873 - val_loss: 0.4009\n",
            "Epoch 673/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3874 - val_loss: 0.3964\n",
            "Epoch 674/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3989\n",
            "Epoch 675/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3980\n",
            "Epoch 676/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3971\n",
            "Epoch 677/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3867 - val_loss: 0.3992\n",
            "Epoch 678/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3869 - val_loss: 0.3978\n",
            "Epoch 679/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3864 - val_loss: 0.3967\n",
            "Epoch 680/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3874 - val_loss: 0.3969\n",
            "Epoch 681/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3862 - val_loss: 0.3979\n",
            "Epoch 682/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3869 - val_loss: 0.3997\n",
            "Epoch 683/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3867 - val_loss: 0.3985\n",
            "Epoch 684/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3867 - val_loss: 0.3987\n",
            "Epoch 685/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.4019\n",
            "Epoch 686/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3998\n",
            "Epoch 687/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3981\n",
            "Epoch 688/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3865 - val_loss: 0.3988\n",
            "Epoch 689/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3988\n",
            "Epoch 690/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3866 - val_loss: 0.3983\n",
            "Epoch 691/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3978\n",
            "Epoch 692/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3986\n",
            "Epoch 693/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3859 - val_loss: 0.4002\n",
            "Epoch 694/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.3975\n",
            "Epoch 695/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3988\n",
            "Epoch 696/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3862 - val_loss: 0.4000\n",
            "Epoch 697/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3863 - val_loss: 0.3977\n",
            "Epoch 698/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3866 - val_loss: 0.3974\n",
            "Epoch 699/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3863 - val_loss: 0.3960\n",
            "Epoch 700/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3857 - val_loss: 0.3999\n",
            "Epoch 701/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3863 - val_loss: 0.3996\n",
            "Epoch 702/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3861 - val_loss: 0.3976\n",
            "Epoch 703/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3859 - val_loss: 0.3958\n",
            "Epoch 704/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3862 - val_loss: 0.3982\n",
            "Epoch 705/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3862 - val_loss: 0.3966\n",
            "Epoch 706/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3858 - val_loss: 0.3991\n",
            "Epoch 707/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3996\n",
            "Epoch 708/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.4002\n",
            "Epoch 709/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3998\n",
            "Epoch 710/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3947\n",
            "Epoch 711/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3865 - val_loss: 0.3959\n",
            "Epoch 712/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3969\n",
            "Epoch 713/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3981\n",
            "Epoch 714/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.3990\n",
            "Epoch 715/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.3999\n",
            "Epoch 716/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.4010\n",
            "Epoch 717/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3857 - val_loss: 0.3957\n",
            "Epoch 718/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3866 - val_loss: 0.3966\n",
            "Epoch 719/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3861 - val_loss: 0.3987\n",
            "Epoch 720/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3863 - val_loss: 0.3964\n",
            "Epoch 721/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3855 - val_loss: 0.3984\n",
            "Epoch 722/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3854 - val_loss: 0.3989\n",
            "Epoch 723/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.3976\n",
            "Epoch 724/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.3993\n",
            "Epoch 725/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3851 - val_loss: 0.4013\n",
            "Epoch 726/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3860 - val_loss: 0.3988\n",
            "Epoch 727/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3974\n",
            "Epoch 728/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.4010\n",
            "Epoch 729/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.4009\n",
            "Epoch 730/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.4002\n",
            "Epoch 731/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3850 - val_loss: 0.3985\n",
            "Epoch 732/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.4005\n",
            "Epoch 733/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.3974\n",
            "Epoch 734/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.3989\n",
            "Epoch 735/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.3977\n",
            "Epoch 736/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3855 - val_loss: 0.4007\n",
            "Epoch 737/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3857 - val_loss: 0.3976\n",
            "Epoch 738/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3860 - val_loss: 0.4032\n",
            "Epoch 739/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3850 - val_loss: 0.3970\n",
            "Epoch 740/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3855 - val_loss: 0.3994\n",
            "Epoch 741/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3852 - val_loss: 0.4007\n",
            "Epoch 742/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.3995\n",
            "Epoch 743/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3979\n",
            "Epoch 744/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.3987\n",
            "Epoch 745/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.4009\n",
            "Epoch 746/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3846 - val_loss: 0.4021\n",
            "Epoch 747/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.4027\n",
            "Epoch 748/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.4010\n",
            "Epoch 749/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3844 - val_loss: 0.3998\n",
            "Epoch 750/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3991\n",
            "Epoch 751/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3844 - val_loss: 0.3990\n",
            "Epoch 752/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3842 - val_loss: 0.4001\n",
            "Epoch 753/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3847 - val_loss: 0.4027\n",
            "Epoch 754/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3853 - val_loss: 0.3991\n",
            "Epoch 755/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3852 - val_loss: 0.3987\n",
            "Epoch 756/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3848 - val_loss: 0.4018\n",
            "Epoch 757/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3853 - val_loss: 0.4006\n",
            "Epoch 758/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3848 - val_loss: 0.4013\n",
            "Epoch 759/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3842 - val_loss: 0.3985\n",
            "Epoch 760/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3843 - val_loss: 0.4027\n",
            "Epoch 761/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3844 - val_loss: 0.4015\n",
            "Epoch 762/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.3982\n",
            "Epoch 763/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3852 - val_loss: 0.3992\n",
            "Epoch 764/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.4015\n",
            "Epoch 765/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.4027\n",
            "Epoch 766/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.4012\n",
            "Epoch 767/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3849 - val_loss: 0.3991\n",
            "Epoch 768/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4030\n",
            "Epoch 769/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.4020\n",
            "Epoch 770/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.4014\n",
            "Epoch 771/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3833 - val_loss: 0.4042\n",
            "Epoch 772/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.4055\n",
            "Epoch 773/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4034\n",
            "Epoch 774/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3841 - val_loss: 0.4038\n",
            "Epoch 775/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3842 - val_loss: 0.3996\n",
            "Epoch 776/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3841 - val_loss: 0.4054\n",
            "Epoch 777/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3847 - val_loss: 0.4019\n",
            "Epoch 778/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3838 - val_loss: 0.4019\n",
            "Epoch 779/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4028\n",
            "Epoch 780/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3845 - val_loss: 0.4014\n",
            "Epoch 781/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4026\n",
            "Epoch 782/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3841 - val_loss: 0.4012\n",
            "Epoch 783/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.4035\n",
            "Epoch 784/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.4027\n",
            "Epoch 785/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.4032\n",
            "Epoch 786/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3836 - val_loss: 0.4021\n",
            "Epoch 787/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3839 - val_loss: 0.3994\n",
            "Epoch 788/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.4040\n",
            "Epoch 789/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3839 - val_loss: 0.4016\n",
            "Epoch 790/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.4051\n",
            "Epoch 791/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3841 - val_loss: 0.4040\n",
            "Epoch 792/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3839 - val_loss: 0.4035\n",
            "Epoch 793/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3840 - val_loss: 0.4013\n",
            "Epoch 794/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3834 - val_loss: 0.4027\n",
            "Epoch 795/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3834 - val_loss: 0.4067\n",
            "Epoch 796/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3847 - val_loss: 0.4012\n",
            "Epoch 797/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3838 - val_loss: 0.4018\n",
            "Epoch 798/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.4013\n",
            "Epoch 799/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.4004\n",
            "Epoch 800/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.4024\n",
            "Epoch 801/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.4038\n",
            "Epoch 802/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3834 - val_loss: 0.3995\n",
            "Epoch 803/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3835 - val_loss: 0.4023\n",
            "Epoch 804/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3835 - val_loss: 0.4019\n",
            "Epoch 805/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.4021\n",
            "Epoch 806/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4004\n",
            "Epoch 807/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.4056\n",
            "Epoch 808/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.4026\n",
            "Epoch 809/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.4034\n",
            "Epoch 810/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.4040\n",
            "Epoch 811/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3840 - val_loss: 0.4030\n",
            "Epoch 812/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3834 - val_loss: 0.4021\n",
            "Epoch 813/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3833 - val_loss: 0.4025\n",
            "Epoch 814/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3832 - val_loss: 0.4002\n",
            "Epoch 815/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3836 - val_loss: 0.4022\n",
            "Epoch 816/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3830 - val_loss: 0.4018\n",
            "Epoch 817/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3834 - val_loss: 0.4023\n",
            "Epoch 818/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.4012\n",
            "Epoch 819/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.4072\n",
            "Epoch 820/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.4049\n",
            "Epoch 821/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3827 - val_loss: 0.4065\n",
            "Epoch 822/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.3998\n",
            "Epoch 823/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3830 - val_loss: 0.4039\n",
            "Epoch 824/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.4028\n",
            "Epoch 825/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.4065\n",
            "Epoch 826/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3829 - val_loss: 0.4064\n",
            "Epoch 827/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.4017\n",
            "Epoch 828/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.4034\n",
            "Epoch 829/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.4023\n",
            "Epoch 830/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3828 - val_loss: 0.4032\n",
            "Epoch 831/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3835 - val_loss: 0.4054\n",
            "Epoch 832/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3834 - val_loss: 0.4037\n",
            "Epoch 833/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3829 - val_loss: 0.4013\n",
            "Epoch 834/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3832 - val_loss: 0.4027\n",
            "Epoch 835/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3828 - val_loss: 0.4019\n",
            "Epoch 836/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4034\n",
            "Epoch 837/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3828 - val_loss: 0.4026\n",
            "Epoch 838/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.4043\n",
            "Epoch 839/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.4032\n",
            "Epoch 840/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3831 - val_loss: 0.4037\n",
            "Epoch 841/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.4039\n",
            "Epoch 842/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.4067\n",
            "Epoch 843/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4038\n",
            "Epoch 844/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.4030\n",
            "Epoch 845/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3819 - val_loss: 0.4053\n",
            "Epoch 846/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3822 - val_loss: 0.4019\n",
            "Epoch 847/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.4029\n",
            "Epoch 848/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.4032\n",
            "Epoch 849/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3823 - val_loss: 0.4048\n",
            "Epoch 850/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3826 - val_loss: 0.3999\n",
            "Epoch 851/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3822 - val_loss: 0.4001\n",
            "Epoch 852/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3820 - val_loss: 0.4027\n",
            "Epoch 853/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3818 - val_loss: 0.4046\n",
            "Epoch 854/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3821 - val_loss: 0.4056\n",
            "Epoch 855/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.4029\n",
            "Epoch 856/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4038\n",
            "Epoch 857/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.4055\n",
            "Epoch 858/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.4041\n",
            "Epoch 859/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4067\n",
            "Epoch 860/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.4050\n",
            "Epoch 861/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.4063\n",
            "Epoch 862/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.4048\n",
            "Epoch 863/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3817 - val_loss: 0.4062\n",
            "Epoch 864/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4071\n",
            "Epoch 865/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3821 - val_loss: 0.4036\n",
            "Epoch 866/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4037\n",
            "Epoch 867/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3824 - val_loss: 0.4074\n",
            "Epoch 868/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.4035\n",
            "Epoch 869/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3822 - val_loss: 0.4047\n",
            "Epoch 870/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3817 - val_loss: 0.4026\n",
            "Epoch 871/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3818 - val_loss: 0.4041\n",
            "Epoch 872/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3825 - val_loss: 0.4060\n",
            "Epoch 873/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3816 - val_loss: 0.4026\n",
            "Epoch 874/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.4040\n",
            "Epoch 875/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3821 - val_loss: 0.4050\n",
            "Epoch 876/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3816 - val_loss: 0.4011\n",
            "Epoch 877/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.4040\n",
            "Epoch 878/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.4070\n",
            "Epoch 879/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.4033\n",
            "Epoch 880/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4048\n",
            "Epoch 881/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4064\n",
            "Epoch 882/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4073\n",
            "Epoch 883/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.4081\n",
            "Epoch 884/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3812 - val_loss: 0.4034\n",
            "Epoch 885/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4043\n",
            "Epoch 886/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3819 - val_loss: 0.4019\n",
            "Epoch 887/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4022\n",
            "Epoch 888/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3816 - val_loss: 0.4038\n",
            "Epoch 889/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3810 - val_loss: 0.4042\n",
            "Epoch 890/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3813 - val_loss: 0.4054\n",
            "Epoch 891/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3812 - val_loss: 0.4065\n",
            "Epoch 892/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3805 - val_loss: 0.4019\n",
            "Epoch 893/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3815 - val_loss: 0.4025\n",
            "Epoch 894/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.4061\n",
            "Epoch 895/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.4058\n",
            "Epoch 896/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.4074\n",
            "Epoch 897/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.4059\n",
            "Epoch 898/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4062\n",
            "Epoch 899/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.4080\n",
            "Epoch 900/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4055\n",
            "Epoch 901/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.4029\n",
            "Epoch 902/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3818 - val_loss: 0.4057\n",
            "Epoch 903/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3812 - val_loss: 0.4062\n",
            "Epoch 904/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4053\n",
            "Epoch 905/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.4053\n",
            "Epoch 906/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3806 - val_loss: 0.4065\n",
            "Epoch 907/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3815 - val_loss: 0.4043\n",
            "Epoch 908/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3813 - val_loss: 0.4069\n",
            "Epoch 909/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3819 - val_loss: 0.4024\n",
            "Epoch 910/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3812 - val_loss: 0.4051\n",
            "Epoch 911/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3809 - val_loss: 0.4055\n",
            "Epoch 912/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3807 - val_loss: 0.4039\n",
            "Epoch 913/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3815 - val_loss: 0.4109\n",
            "Epoch 914/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4070\n",
            "Epoch 915/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3815 - val_loss: 0.4087\n",
            "Epoch 916/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3813 - val_loss: 0.4066\n",
            "Epoch 917/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4030\n",
            "Epoch 918/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3806 - val_loss: 0.4066\n",
            "Epoch 919/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3810 - val_loss: 0.4089\n",
            "Epoch 920/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.4073\n",
            "Epoch 921/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.4045\n",
            "Epoch 922/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3805 - val_loss: 0.4071\n",
            "Epoch 923/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3803 - val_loss: 0.4064\n",
            "Epoch 924/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3806 - val_loss: 0.4081\n",
            "Epoch 925/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3808 - val_loss: 0.4066\n",
            "Epoch 926/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3808 - val_loss: 0.4069\n",
            "Epoch 927/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3805 - val_loss: 0.4063\n",
            "Epoch 928/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3809 - val_loss: 0.4065\n",
            "Epoch 929/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3807 - val_loss: 0.4074\n",
            "Epoch 930/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3806 - val_loss: 0.4103\n",
            "Epoch 931/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.4046\n",
            "Epoch 932/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3816 - val_loss: 0.4103\n",
            "Epoch 933/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.4075\n",
            "Epoch 934/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4078\n",
            "Epoch 935/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4093\n",
            "Epoch 936/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4108\n",
            "Epoch 937/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3809 - val_loss: 0.4104\n",
            "Epoch 938/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4136\n",
            "Epoch 939/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4075\n",
            "Epoch 940/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.4084\n",
            "Epoch 941/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.4086\n",
            "Epoch 942/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3800 - val_loss: 0.4110\n",
            "Epoch 943/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.4131\n",
            "Epoch 944/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3801 - val_loss: 0.4112\n",
            "Epoch 945/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3802 - val_loss: 0.4088\n",
            "Epoch 946/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3804 - val_loss: 0.4111\n",
            "Epoch 947/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3806 - val_loss: 0.4091\n",
            "Epoch 948/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3806 - val_loss: 0.4070\n",
            "Epoch 949/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3805 - val_loss: 0.4107\n",
            "Epoch 950/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4070\n",
            "Epoch 951/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4057\n",
            "Epoch 952/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4071\n",
            "Epoch 953/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4080\n",
            "Epoch 954/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.4089\n",
            "Epoch 955/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.4080\n",
            "Epoch 956/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.4084\n",
            "Epoch 957/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.4066\n",
            "Epoch 958/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3800 - val_loss: 0.4094\n",
            "Epoch 959/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4094\n",
            "Epoch 960/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.4067\n",
            "Epoch 961/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.4114\n",
            "Epoch 962/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4091\n",
            "Epoch 963/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.4091\n",
            "Epoch 964/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3801 - val_loss: 0.4090\n",
            "Epoch 965/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3795 - val_loss: 0.4092\n",
            "Epoch 966/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3794 - val_loss: 0.4129\n",
            "Epoch 967/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3801 - val_loss: 0.4098\n",
            "Epoch 968/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3788 - val_loss: 0.4099\n",
            "Epoch 969/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3798 - val_loss: 0.4074\n",
            "Epoch 970/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.4104\n",
            "Epoch 971/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.4117\n",
            "Epoch 972/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.4091\n",
            "Epoch 973/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4070\n",
            "Epoch 974/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.4112\n",
            "Epoch 975/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4072\n",
            "Epoch 976/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.4067\n",
            "Epoch 977/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.4056\n",
            "Epoch 978/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3798 - val_loss: 0.4108\n",
            "Epoch 979/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3797 - val_loss: 0.4074\n",
            "Epoch 980/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.4093\n",
            "Epoch 981/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.4061\n",
            "Epoch 982/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3795 - val_loss: 0.4089\n",
            "Epoch 983/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3795 - val_loss: 0.4094\n",
            "Epoch 984/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3803 - val_loss: 0.4087\n",
            "Epoch 985/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3801 - val_loss: 0.4093\n",
            "Epoch 986/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3796 - val_loss: 0.4095\n",
            "Epoch 987/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3797 - val_loss: 0.4089\n",
            "Epoch 988/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3792 - val_loss: 0.4056\n",
            "Epoch 989/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.4084\n",
            "Epoch 990/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.4107\n",
            "Epoch 991/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3798 - val_loss: 0.4090\n",
            "Epoch 992/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.4088\n",
            "Epoch 993/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.4066\n",
            "Epoch 994/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.4078\n",
            "Epoch 995/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.4054\n",
            "Epoch 996/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.4081\n",
            "Epoch 997/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.4051\n",
            "Epoch 998/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.4090\n",
            "Epoch 999/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3789 - val_loss: 0.4154\n",
            "Epoch 1000/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3797 - val_loss: 0.4065\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4065\n",
            "8/8 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb0a0124730>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000;    # training epochs\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "pred = Dense(3, activation='softmax')(con_value)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-5))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n"
      ],
      "id": "mJDUSNqnmVMa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl0GYtnNXFJC"
      },
      "source": [
        "##2.3 Add the Temperature in Softmax （3000 Epochs）##\n",
        "The formula of softmax is:  \n",
        "$\\large P_i=\\frac{e^{y_i}}{\\sum_{k=1}^n e^{y_k}}$  \n",
        "\n",
        "After add temperature is:  \n",
        "$\\large P_i=\\frac{e^{\\frac{y_i}T}}{\\sum_{k=1}^n e^{\\frac{y_k}T}}$  \n",
        "A higher temperature (above 1) makes the model less confident, meaning that the probability distribution becomes more flat.\n",
        " "
      ],
      "id": "Pl0GYtnNXFJC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yn1Ny_xiXDfL",
        "outputId": "3bd083ae-1d66-4fb1-9ce1-0d6fbfdc388d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3869 - val_loss: 0.3993\n",
            "Epoch 503/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3870 - val_loss: 0.3927\n",
            "Epoch 504/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3881 - val_loss: 0.3937\n",
            "Epoch 505/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3887 - val_loss: 0.3987\n",
            "Epoch 506/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3892 - val_loss: 0.3913\n",
            "Epoch 507/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3866 - val_loss: 0.3886\n",
            "Epoch 508/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3857 - val_loss: 0.3895\n",
            "Epoch 509/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3857 - val_loss: 0.3930\n",
            "Epoch 510/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3948\n",
            "Epoch 511/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3861 - val_loss: 0.3941\n",
            "Epoch 512/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3876 - val_loss: 0.3909\n",
            "Epoch 513/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3866 - val_loss: 0.3903\n",
            "Epoch 514/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3884 - val_loss: 0.3894\n",
            "Epoch 515/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3842 - val_loss: 0.3937\n",
            "Epoch 516/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3954\n",
            "Epoch 517/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.3858\n",
            "Epoch 518/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3911 - val_loss: 0.3923\n",
            "Epoch 519/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3887 - val_loss: 0.3895\n",
            "Epoch 520/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3866 - val_loss: 0.3889\n",
            "Epoch 521/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3859 - val_loss: 0.3894\n",
            "Epoch 522/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3860 - val_loss: 0.3920\n",
            "Epoch 523/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3884 - val_loss: 0.3936\n",
            "Epoch 524/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3860 - val_loss: 0.3907\n",
            "Epoch 525/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3861 - val_loss: 0.3905\n",
            "Epoch 526/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3941\n",
            "Epoch 527/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3866 - val_loss: 0.3907\n",
            "Epoch 528/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.3912\n",
            "Epoch 529/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3852 - val_loss: 0.3942\n",
            "Epoch 530/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3870 - val_loss: 0.3889\n",
            "Epoch 531/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3866 - val_loss: 0.3918\n",
            "Epoch 532/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3854 - val_loss: 0.3928\n",
            "Epoch 533/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3845 - val_loss: 0.3939\n",
            "Epoch 534/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3856 - val_loss: 0.3938\n",
            "Epoch 535/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3875 - val_loss: 0.3921\n",
            "Epoch 536/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3902 - val_loss: 0.3976\n",
            "Epoch 537/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3924\n",
            "Epoch 538/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3954\n",
            "Epoch 539/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3849 - val_loss: 0.3939\n",
            "Epoch 540/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3845 - val_loss: 0.3921\n",
            "Epoch 541/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3848 - val_loss: 0.4001\n",
            "Epoch 542/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.3890\n",
            "Epoch 543/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3871 - val_loss: 0.3902\n",
            "Epoch 544/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3855 - val_loss: 0.3934\n",
            "Epoch 545/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3834 - val_loss: 0.3898\n",
            "Epoch 546/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3866 - val_loss: 0.3907\n",
            "Epoch 547/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3845 - val_loss: 0.3935\n",
            "Epoch 548/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3850 - val_loss: 0.3970\n",
            "Epoch 549/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3848 - val_loss: 0.3957\n",
            "Epoch 550/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3854 - val_loss: 0.3875\n",
            "Epoch 551/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.4038\n",
            "Epoch 552/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3889 - val_loss: 0.3884\n",
            "Epoch 553/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3928\n",
            "Epoch 554/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.3886\n",
            "Epoch 555/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3883\n",
            "Epoch 556/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3904\n",
            "Epoch 557/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3868 - val_loss: 0.3909\n",
            "Epoch 558/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3845 - val_loss: 0.3964\n",
            "Epoch 559/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3851\n",
            "Epoch 560/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3834 - val_loss: 0.3970\n",
            "Epoch 561/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3860 - val_loss: 0.3919\n",
            "Epoch 562/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3827 - val_loss: 0.3893\n",
            "Epoch 563/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3860 - val_loss: 0.3887\n",
            "Epoch 564/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3864\n",
            "Epoch 565/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3851 - val_loss: 0.3903\n",
            "Epoch 566/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3843 - val_loss: 0.3897\n",
            "Epoch 567/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3845 - val_loss: 0.3870\n",
            "Epoch 568/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3854 - val_loss: 0.3928\n",
            "Epoch 569/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3877\n",
            "Epoch 570/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3961\n",
            "Epoch 571/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3841 - val_loss: 0.3879\n",
            "Epoch 572/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3899\n",
            "Epoch 573/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3851 - val_loss: 0.3886\n",
            "Epoch 574/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.3912\n",
            "Epoch 575/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3831 - val_loss: 0.3921\n",
            "Epoch 576/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3844 - val_loss: 0.3931\n",
            "Epoch 577/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.3904\n",
            "Epoch 578/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3833 - val_loss: 0.3885\n",
            "Epoch 579/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3851 - val_loss: 0.3872\n",
            "Epoch 580/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3893\n",
            "Epoch 581/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3848 - val_loss: 0.3935\n",
            "Epoch 582/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3825 - val_loss: 0.3923\n",
            "Epoch 583/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3828 - val_loss: 0.3939\n",
            "Epoch 584/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3842 - val_loss: 0.3924\n",
            "Epoch 585/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3837 - val_loss: 0.3899\n",
            "Epoch 586/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3835 - val_loss: 0.3873\n",
            "Epoch 587/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3837 - val_loss: 0.3845\n",
            "Epoch 588/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3910\n",
            "Epoch 589/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3892\n",
            "Epoch 590/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3817 - val_loss: 0.3892\n",
            "Epoch 591/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.3901\n",
            "Epoch 592/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3826 - val_loss: 0.3940\n",
            "Epoch 593/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3915\n",
            "Epoch 594/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.3923\n",
            "Epoch 595/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3982\n",
            "Epoch 596/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3852 - val_loss: 0.4021\n",
            "Epoch 597/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3846 - val_loss: 0.3882\n",
            "Epoch 598/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3967\n",
            "Epoch 599/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3854 - val_loss: 0.3907\n",
            "Epoch 600/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3906\n",
            "Epoch 601/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3849 - val_loss: 0.3911\n",
            "Epoch 602/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3843 - val_loss: 0.3883\n",
            "Epoch 603/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3835 - val_loss: 0.3944\n",
            "Epoch 604/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3858 - val_loss: 0.3939\n",
            "Epoch 605/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3930\n",
            "Epoch 606/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.3930\n",
            "Epoch 607/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3816 - val_loss: 0.3935\n",
            "Epoch 608/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3834 - val_loss: 0.3869\n",
            "Epoch 609/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3881\n",
            "Epoch 610/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3903\n",
            "Epoch 611/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3857 - val_loss: 0.3896\n",
            "Epoch 612/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3820 - val_loss: 0.3883\n",
            "Epoch 613/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3817 - val_loss: 0.3983\n",
            "Epoch 614/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3890\n",
            "Epoch 615/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3821 - val_loss: 0.3898\n",
            "Epoch 616/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3900\n",
            "Epoch 617/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3932\n",
            "Epoch 618/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3930\n",
            "Epoch 619/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3829 - val_loss: 0.3924\n",
            "Epoch 620/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3824 - val_loss: 0.3956\n",
            "Epoch 621/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3845 - val_loss: 0.3993\n",
            "Epoch 622/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3824 - val_loss: 0.3904\n",
            "Epoch 623/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3827 - val_loss: 0.3930\n",
            "Epoch 624/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3823 - val_loss: 0.3893\n",
            "Epoch 625/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3826 - val_loss: 0.3908\n",
            "Epoch 626/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.3888\n",
            "Epoch 627/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3905\n",
            "Epoch 628/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.3868\n",
            "Epoch 629/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3911\n",
            "Epoch 630/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3811 - val_loss: 0.3903\n",
            "Epoch 631/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3950\n",
            "Epoch 632/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.4025\n",
            "Epoch 633/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3912\n",
            "Epoch 634/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3896\n",
            "Epoch 635/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3913\n",
            "Epoch 636/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3910\n",
            "Epoch 637/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3838 - val_loss: 0.3951\n",
            "Epoch 638/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3823 - val_loss: 0.4005\n",
            "Epoch 639/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3838 - val_loss: 0.4005\n",
            "Epoch 640/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3842 - val_loss: 0.3912\n",
            "Epoch 641/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3820 - val_loss: 0.3876\n",
            "Epoch 642/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3828 - val_loss: 0.3993\n",
            "Epoch 643/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.3944\n",
            "Epoch 644/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3929\n",
            "Epoch 645/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.3902\n",
            "Epoch 646/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3919\n",
            "Epoch 647/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3823 - val_loss: 0.3986\n",
            "Epoch 648/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3862\n",
            "Epoch 649/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3871\n",
            "Epoch 650/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3886\n",
            "Epoch 651/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.3905\n",
            "Epoch 652/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3911\n",
            "Epoch 653/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.3919\n",
            "Epoch 654/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3836 - val_loss: 0.3912\n",
            "Epoch 655/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3810 - val_loss: 0.3874\n",
            "Epoch 656/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3823 - val_loss: 0.3942\n",
            "Epoch 657/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3821 - val_loss: 0.3952\n",
            "Epoch 658/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3834 - val_loss: 0.3891\n",
            "Epoch 659/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3823 - val_loss: 0.3889\n",
            "Epoch 660/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.3916\n",
            "Epoch 661/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3893\n",
            "Epoch 662/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3811 - val_loss: 0.3937\n",
            "Epoch 663/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3820 - val_loss: 0.3916\n",
            "Epoch 664/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3905\n",
            "Epoch 665/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3817 - val_loss: 0.3912\n",
            "Epoch 666/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3850 - val_loss: 0.3913\n",
            "Epoch 667/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.3904\n",
            "Epoch 668/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3889\n",
            "Epoch 669/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3835 - val_loss: 0.3912\n",
            "Epoch 670/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3898\n",
            "Epoch 671/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3832 - val_loss: 0.3915\n",
            "Epoch 672/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3834 - val_loss: 0.3978\n",
            "Epoch 673/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3826 - val_loss: 0.3898\n",
            "Epoch 674/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3827 - val_loss: 0.3891\n",
            "Epoch 675/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3824 - val_loss: 0.3895\n",
            "Epoch 676/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3819 - val_loss: 0.3893\n",
            "Epoch 677/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3812 - val_loss: 0.3919\n",
            "Epoch 678/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3952\n",
            "Epoch 679/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3810 - val_loss: 0.3898\n",
            "Epoch 680/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3912\n",
            "Epoch 681/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3938\n",
            "Epoch 682/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3824 - val_loss: 0.3955\n",
            "Epoch 683/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3805 - val_loss: 0.3861\n",
            "Epoch 684/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.3905\n",
            "Epoch 685/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3812 - val_loss: 0.3911\n",
            "Epoch 686/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3887\n",
            "Epoch 687/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3918\n",
            "Epoch 688/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3935\n",
            "Epoch 689/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3808 - val_loss: 0.3963\n",
            "Epoch 690/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.4024\n",
            "Epoch 691/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3824 - val_loss: 0.3901\n",
            "Epoch 692/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3835 - val_loss: 0.3897\n",
            "Epoch 693/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3825 - val_loss: 0.3910\n",
            "Epoch 694/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3819 - val_loss: 0.3945\n",
            "Epoch 695/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3912\n",
            "Epoch 696/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3920\n",
            "Epoch 697/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3804 - val_loss: 0.3915\n",
            "Epoch 698/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3909\n",
            "Epoch 699/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3913\n",
            "Epoch 700/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3796 - val_loss: 0.3951\n",
            "Epoch 701/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3818 - val_loss: 0.3899\n",
            "Epoch 702/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3945\n",
            "Epoch 703/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3808 - val_loss: 0.3915\n",
            "Epoch 704/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3912\n",
            "Epoch 705/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3949\n",
            "Epoch 706/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.3949\n",
            "Epoch 707/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3796 - val_loss: 0.3949\n",
            "Epoch 708/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3916\n",
            "Epoch 709/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3803 - val_loss: 0.3919\n",
            "Epoch 710/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3799 - val_loss: 0.3943\n",
            "Epoch 711/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3803 - val_loss: 0.3921\n",
            "Epoch 712/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3797 - val_loss: 0.3888\n",
            "Epoch 713/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3809 - val_loss: 0.3906\n",
            "Epoch 714/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3802 - val_loss: 0.3872\n",
            "Epoch 715/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3919\n",
            "Epoch 716/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3800 - val_loss: 0.3924\n",
            "Epoch 717/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3956\n",
            "Epoch 718/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3846 - val_loss: 0.3911\n",
            "Epoch 719/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3940\n",
            "Epoch 720/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3842 - val_loss: 0.3901\n",
            "Epoch 721/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3895\n",
            "Epoch 722/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3800 - val_loss: 0.3909\n",
            "Epoch 723/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3812 - val_loss: 0.3894\n",
            "Epoch 724/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3807 - val_loss: 0.3917\n",
            "Epoch 725/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3885\n",
            "Epoch 726/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3944\n",
            "Epoch 727/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3897\n",
            "Epoch 728/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3791 - val_loss: 0.3917\n",
            "Epoch 729/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3806 - val_loss: 0.3900\n",
            "Epoch 730/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3790 - val_loss: 0.3897\n",
            "Epoch 731/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3835 - val_loss: 0.3942\n",
            "Epoch 732/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3826 - val_loss: 0.3961\n",
            "Epoch 733/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3909\n",
            "Epoch 734/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3904\n",
            "Epoch 735/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3799 - val_loss: 0.3919\n",
            "Epoch 736/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3793 - val_loss: 0.3884\n",
            "Epoch 737/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.3912\n",
            "Epoch 738/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3920\n",
            "Epoch 739/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.3895\n",
            "Epoch 740/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.3902\n",
            "Epoch 741/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3906\n",
            "Epoch 742/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3894\n",
            "Epoch 743/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3911\n",
            "Epoch 744/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3903\n",
            "Epoch 745/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.3888\n",
            "Epoch 746/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3792 - val_loss: 0.3949\n",
            "Epoch 747/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3811 - val_loss: 0.3888\n",
            "Epoch 748/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3798 - val_loss: 0.3867\n",
            "Epoch 749/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3786 - val_loss: 0.3935\n",
            "Epoch 750/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3899\n",
            "Epoch 751/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.3984\n",
            "Epoch 752/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3802 - val_loss: 0.3902\n",
            "Epoch 753/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.3905\n",
            "Epoch 754/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3947\n",
            "Epoch 755/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3815 - val_loss: 0.3921\n",
            "Epoch 756/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3919\n",
            "Epoch 757/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3942\n",
            "Epoch 758/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3808 - val_loss: 0.3910\n",
            "Epoch 759/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3895\n",
            "Epoch 760/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3818 - val_loss: 0.3955\n",
            "Epoch 761/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3928\n",
            "Epoch 762/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3943\n",
            "Epoch 763/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3896\n",
            "Epoch 764/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3795 - val_loss: 0.3945\n",
            "Epoch 765/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3787 - val_loss: 0.3923\n",
            "Epoch 766/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3797 - val_loss: 0.3925\n",
            "Epoch 767/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3809 - val_loss: 0.3913\n",
            "Epoch 768/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3951\n",
            "Epoch 769/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3815 - val_loss: 0.3933\n",
            "Epoch 770/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3841 - val_loss: 0.3918\n",
            "Epoch 771/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3797 - val_loss: 0.3873\n",
            "Epoch 772/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3947\n",
            "Epoch 773/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3970\n",
            "Epoch 774/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3904\n",
            "Epoch 775/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3900\n",
            "Epoch 776/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3894\n",
            "Epoch 777/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3790 - val_loss: 0.3903\n",
            "Epoch 778/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3802 - val_loss: 0.3929\n",
            "Epoch 779/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.3898\n",
            "Epoch 780/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3878\n",
            "Epoch 781/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3883\n",
            "Epoch 782/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3792 - val_loss: 0.3923\n",
            "Epoch 783/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3790 - val_loss: 0.3895\n",
            "Epoch 784/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3801 - val_loss: 0.3908\n",
            "Epoch 785/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3782 - val_loss: 0.3897\n",
            "Epoch 786/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3800 - val_loss: 0.3916\n",
            "Epoch 787/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3950\n",
            "Epoch 788/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3911\n",
            "Epoch 789/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3910\n",
            "Epoch 790/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3937\n",
            "Epoch 791/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3807 - val_loss: 0.3903\n",
            "Epoch 792/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3823 - val_loss: 0.3907\n",
            "Epoch 793/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3971\n",
            "Epoch 794/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3837 - val_loss: 0.3931\n",
            "Epoch 795/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3922\n",
            "Epoch 796/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3859\n",
            "Epoch 797/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3889\n",
            "Epoch 798/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3909\n",
            "Epoch 799/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3907\n",
            "Epoch 800/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3812 - val_loss: 0.3917\n",
            "Epoch 801/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3801 - val_loss: 0.3916\n",
            "Epoch 802/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3796 - val_loss: 0.3899\n",
            "Epoch 803/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3811 - val_loss: 0.3938\n",
            "Epoch 804/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3788 - val_loss: 0.3891\n",
            "Epoch 805/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.3926\n",
            "Epoch 806/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3795 - val_loss: 0.3909\n",
            "Epoch 807/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3894\n",
            "Epoch 808/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3912\n",
            "Epoch 809/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3790 - val_loss: 0.3935\n",
            "Epoch 810/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3901\n",
            "Epoch 811/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3887\n",
            "Epoch 812/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3927\n",
            "Epoch 813/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3895\n",
            "Epoch 814/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3801 - val_loss: 0.3896\n",
            "Epoch 815/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3935\n",
            "Epoch 816/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3787 - val_loss: 0.3919\n",
            "Epoch 817/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3817 - val_loss: 0.3901\n",
            "Epoch 818/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3795 - val_loss: 0.3915\n",
            "Epoch 819/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3793 - val_loss: 0.3890\n",
            "Epoch 820/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3783 - val_loss: 0.3906\n",
            "Epoch 821/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3787 - val_loss: 0.3945\n",
            "Epoch 822/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3913\n",
            "Epoch 823/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3898\n",
            "Epoch 824/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3950\n",
            "Epoch 825/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3804 - val_loss: 0.3972\n",
            "Epoch 826/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3898\n",
            "Epoch 827/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3889\n",
            "Epoch 828/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3917\n",
            "Epoch 829/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3779 - val_loss: 0.3889\n",
            "Epoch 830/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3919\n",
            "Epoch 831/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3789 - val_loss: 0.3928\n",
            "Epoch 832/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.3976\n",
            "Epoch 833/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3896\n",
            "Epoch 834/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.3922\n",
            "Epoch 835/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3926\n",
            "Epoch 836/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3779 - val_loss: 0.3909\n",
            "Epoch 837/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3776 - val_loss: 0.3905\n",
            "Epoch 838/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3774 - val_loss: 0.3956\n",
            "Epoch 839/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3786 - val_loss: 0.3924\n",
            "Epoch 840/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3786 - val_loss: 0.3918\n",
            "Epoch 841/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.3916\n",
            "Epoch 842/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3801 - val_loss: 0.3908\n",
            "Epoch 843/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3943\n",
            "Epoch 844/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.3915\n",
            "Epoch 845/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3947\n",
            "Epoch 846/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3794 - val_loss: 0.3925\n",
            "Epoch 847/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3797 - val_loss: 0.3929\n",
            "Epoch 848/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3920\n",
            "Epoch 849/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3943\n",
            "Epoch 850/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3900\n",
            "Epoch 851/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3892\n",
            "Epoch 852/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3899\n",
            "Epoch 853/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3790 - val_loss: 0.3892\n",
            "Epoch 854/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3787 - val_loss: 0.3939\n",
            "Epoch 855/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3791 - val_loss: 0.3947\n",
            "Epoch 856/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3786 - val_loss: 0.3911\n",
            "Epoch 857/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3781 - val_loss: 0.3914\n",
            "Epoch 858/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3920\n",
            "Epoch 859/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.3910\n",
            "Epoch 860/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.3926\n",
            "Epoch 861/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3924\n",
            "Epoch 862/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3777 - val_loss: 0.3902\n",
            "Epoch 863/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3905\n",
            "Epoch 864/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3976\n",
            "Epoch 865/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3787 - val_loss: 0.3942\n",
            "Epoch 866/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3941\n",
            "Epoch 867/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3777 - val_loss: 0.3953\n",
            "Epoch 868/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3937\n",
            "Epoch 869/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3800 - val_loss: 0.3888\n",
            "Epoch 870/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3906\n",
            "Epoch 871/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3942\n",
            "Epoch 872/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3771 - val_loss: 0.3952\n",
            "Epoch 873/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3773 - val_loss: 0.3919\n",
            "Epoch 874/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3788 - val_loss: 0.3944\n",
            "Epoch 875/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3773 - val_loss: 0.3941\n",
            "Epoch 876/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3942\n",
            "Epoch 877/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3887\n",
            "Epoch 878/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3943\n",
            "Epoch 879/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3773 - val_loss: 0.3955\n",
            "Epoch 880/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3775 - val_loss: 0.3916\n",
            "Epoch 881/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3992\n",
            "Epoch 882/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3810 - val_loss: 0.3910\n",
            "Epoch 883/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3776 - val_loss: 0.3947\n",
            "Epoch 884/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3786 - val_loss: 0.3973\n",
            "Epoch 885/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3895\n",
            "Epoch 886/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3907\n",
            "Epoch 887/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3771 - val_loss: 0.3911\n",
            "Epoch 888/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3899\n",
            "Epoch 889/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3777 - val_loss: 0.3945\n",
            "Epoch 890/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3774 - val_loss: 0.3926\n",
            "Epoch 891/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3763 - val_loss: 0.3940\n",
            "Epoch 892/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3768 - val_loss: 0.3927\n",
            "Epoch 893/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3790 - val_loss: 0.3944\n",
            "Epoch 894/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3961\n",
            "Epoch 895/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3793 - val_loss: 0.4098\n",
            "Epoch 896/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3806 - val_loss: 0.3908\n",
            "Epoch 897/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3983\n",
            "Epoch 898/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3765 - val_loss: 0.3931\n",
            "Epoch 899/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3916\n",
            "Epoch 900/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3964\n",
            "Epoch 901/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3975\n",
            "Epoch 902/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.4002\n",
            "Epoch 903/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3939\n",
            "Epoch 904/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3947\n",
            "Epoch 905/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3917\n",
            "Epoch 906/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3764 - val_loss: 0.3904\n",
            "Epoch 907/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3758 - val_loss: 0.3909\n",
            "Epoch 908/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3779 - val_loss: 0.3933\n",
            "Epoch 909/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3771 - val_loss: 0.3926\n",
            "Epoch 910/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3770 - val_loss: 0.3913\n",
            "Epoch 911/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3772 - val_loss: 0.3934\n",
            "Epoch 912/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3957\n",
            "Epoch 913/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3771 - val_loss: 0.3919\n",
            "Epoch 914/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3958\n",
            "Epoch 915/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3775 - val_loss: 0.3919\n",
            "Epoch 916/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3910\n",
            "Epoch 917/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3782 - val_loss: 0.3912\n",
            "Epoch 918/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3786 - val_loss: 0.3986\n",
            "Epoch 919/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3938\n",
            "Epoch 920/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3772 - val_loss: 0.3965\n",
            "Epoch 921/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3778 - val_loss: 0.3985\n",
            "Epoch 922/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3941\n",
            "Epoch 923/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3758 - val_loss: 0.3948\n",
            "Epoch 924/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3945\n",
            "Epoch 925/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3907\n",
            "Epoch 926/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3781 - val_loss: 0.3971\n",
            "Epoch 927/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3798 - val_loss: 0.3964\n",
            "Epoch 928/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3772 - val_loss: 0.3927\n",
            "Epoch 929/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3766 - val_loss: 0.3951\n",
            "Epoch 930/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3766 - val_loss: 0.3934\n",
            "Epoch 931/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3790 - val_loss: 0.3969\n",
            "Epoch 932/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3802 - val_loss: 0.3961\n",
            "Epoch 933/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3961\n",
            "Epoch 934/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3940\n",
            "Epoch 935/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3787 - val_loss: 0.3969\n",
            "Epoch 936/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3789 - val_loss: 0.3962\n",
            "Epoch 937/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3783 - val_loss: 0.3973\n",
            "Epoch 938/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.3921\n",
            "Epoch 939/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3915\n",
            "Epoch 940/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3948\n",
            "Epoch 941/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3951\n",
            "Epoch 942/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3778 - val_loss: 0.3923\n",
            "Epoch 943/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3967\n",
            "Epoch 944/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3773 - val_loss: 0.3957\n",
            "Epoch 945/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3773 - val_loss: 0.3907\n",
            "Epoch 946/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3802 - val_loss: 0.3947\n",
            "Epoch 947/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3803 - val_loss: 0.3939\n",
            "Epoch 948/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3929\n",
            "Epoch 949/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3923\n",
            "Epoch 950/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3955\n",
            "Epoch 951/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3986\n",
            "Epoch 952/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3982\n",
            "Epoch 953/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3907\n",
            "Epoch 954/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3960\n",
            "Epoch 955/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3909\n",
            "Epoch 956/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3957\n",
            "Epoch 957/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3763 - val_loss: 0.3902\n",
            "Epoch 958/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3948\n",
            "Epoch 959/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3929\n",
            "Epoch 960/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3913\n",
            "Epoch 961/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3987\n",
            "Epoch 962/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3766 - val_loss: 0.3937\n",
            "Epoch 963/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3769 - val_loss: 0.3917\n",
            "Epoch 964/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3765 - val_loss: 0.3973\n",
            "Epoch 965/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3767 - val_loss: 0.3914\n",
            "Epoch 966/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3909\n",
            "Epoch 967/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3770 - val_loss: 0.3928\n",
            "Epoch 968/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3934\n",
            "Epoch 969/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3969\n",
            "Epoch 970/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3764 - val_loss: 0.3933\n",
            "Epoch 971/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3941\n",
            "Epoch 972/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3991\n",
            "Epoch 973/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3966\n",
            "Epoch 974/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3943\n",
            "Epoch 975/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3775 - val_loss: 0.3930\n",
            "Epoch 976/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3756 - val_loss: 0.3939\n",
            "Epoch 977/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3907\n",
            "Epoch 978/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3944\n",
            "Epoch 979/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3945\n",
            "Epoch 980/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3784 - val_loss: 0.3966\n",
            "Epoch 981/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3779 - val_loss: 0.3928\n",
            "Epoch 982/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3782 - val_loss: 0.3905\n",
            "Epoch 983/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3953\n",
            "Epoch 984/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3776 - val_loss: 0.3929\n",
            "Epoch 985/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3964\n",
            "Epoch 986/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.3972\n",
            "Epoch 987/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3932\n",
            "Epoch 988/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3925\n",
            "Epoch 989/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3917\n",
            "Epoch 990/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3931\n",
            "Epoch 991/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3955\n",
            "Epoch 992/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3988\n",
            "Epoch 993/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3777 - val_loss: 0.3933\n",
            "Epoch 994/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3939\n",
            "Epoch 995/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3936\n",
            "Epoch 996/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3949\n",
            "Epoch 997/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3789 - val_loss: 0.3938\n",
            "Epoch 998/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3767 - val_loss: 0.3938\n",
            "Epoch 999/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3780 - val_loss: 0.3947\n",
            "Epoch 1000/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3766 - val_loss: 0.3991\n",
            "Epoch 1001/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3767 - val_loss: 0.3945\n",
            "Epoch 1002/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3940\n",
            "Epoch 1003/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3953\n",
            "Epoch 1004/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3939\n",
            "Epoch 1005/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3760 - val_loss: 0.3989\n",
            "Epoch 1006/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3953\n",
            "Epoch 1007/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3937\n",
            "Epoch 1008/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3934\n",
            "Epoch 1009/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3764 - val_loss: 0.3956\n",
            "Epoch 1010/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3939\n",
            "Epoch 1011/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3931\n",
            "Epoch 1012/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3771 - val_loss: 0.3910\n",
            "Epoch 1013/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.3949\n",
            "Epoch 1014/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3766 - val_loss: 0.3926\n",
            "Epoch 1015/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3939\n",
            "Epoch 1016/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3744 - val_loss: 0.3966\n",
            "Epoch 1017/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3755 - val_loss: 0.3940\n",
            "Epoch 1018/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3763 - val_loss: 0.3971\n",
            "Epoch 1019/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3781 - val_loss: 0.3931\n",
            "Epoch 1020/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3770 - val_loss: 0.3916\n",
            "Epoch 1021/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3941\n",
            "Epoch 1022/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3987\n",
            "Epoch 1023/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3956\n",
            "Epoch 1024/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3934\n",
            "Epoch 1025/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3960\n",
            "Epoch 1026/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.3968\n",
            "Epoch 1027/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3777 - val_loss: 0.3928\n",
            "Epoch 1028/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3939\n",
            "Epoch 1029/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3966\n",
            "Epoch 1030/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.4011\n",
            "Epoch 1031/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3790 - val_loss: 0.3910\n",
            "Epoch 1032/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3970\n",
            "Epoch 1033/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3930\n",
            "Epoch 1034/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3941\n",
            "Epoch 1035/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3754 - val_loss: 0.3953\n",
            "Epoch 1036/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3755 - val_loss: 0.3954\n",
            "Epoch 1037/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3750 - val_loss: 0.4021\n",
            "Epoch 1038/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3980\n",
            "Epoch 1039/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3758 - val_loss: 0.3905\n",
            "Epoch 1040/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3922\n",
            "Epoch 1041/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3936\n",
            "Epoch 1042/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.3954\n",
            "Epoch 1043/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3978\n",
            "Epoch 1044/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3960\n",
            "Epoch 1045/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3937\n",
            "Epoch 1046/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.4004\n",
            "Epoch 1047/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3921\n",
            "Epoch 1048/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3966\n",
            "Epoch 1049/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3764 - val_loss: 0.3988\n",
            "Epoch 1050/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3908\n",
            "Epoch 1051/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3944\n",
            "Epoch 1052/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3937\n",
            "Epoch 1053/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3756 - val_loss: 0.3935\n",
            "Epoch 1054/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3756 - val_loss: 0.3966\n",
            "Epoch 1055/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3754 - val_loss: 0.3982\n",
            "Epoch 1056/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3757 - val_loss: 0.3945\n",
            "Epoch 1057/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3940\n",
            "Epoch 1058/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3758 - val_loss: 0.4000\n",
            "Epoch 1059/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3757 - val_loss: 0.3934\n",
            "Epoch 1060/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3970\n",
            "Epoch 1061/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3769 - val_loss: 0.3941\n",
            "Epoch 1062/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3930\n",
            "Epoch 1063/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3968\n",
            "Epoch 1064/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3956\n",
            "Epoch 1065/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3973\n",
            "Epoch 1066/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3936\n",
            "Epoch 1067/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3925\n",
            "Epoch 1068/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3968\n",
            "Epoch 1069/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3948\n",
            "Epoch 1070/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3768 - val_loss: 0.4032\n",
            "Epoch 1071/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3791 - val_loss: 0.3938\n",
            "Epoch 1072/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3768 - val_loss: 0.3935\n",
            "Epoch 1073/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3764 - val_loss: 0.3947\n",
            "Epoch 1074/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3749 - val_loss: 0.3973\n",
            "Epoch 1075/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3961\n",
            "Epoch 1076/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3929\n",
            "Epoch 1077/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3967\n",
            "Epoch 1078/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3995\n",
            "Epoch 1079/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3990\n",
            "Epoch 1080/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3763 - val_loss: 0.3933\n",
            "Epoch 1081/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3938\n",
            "Epoch 1082/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3736 - val_loss: 0.3940\n",
            "Epoch 1083/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3745 - val_loss: 0.3950\n",
            "Epoch 1084/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3966\n",
            "Epoch 1085/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3945\n",
            "Epoch 1086/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3944\n",
            "Epoch 1087/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3954\n",
            "Epoch 1088/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3956\n",
            "Epoch 1089/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3986\n",
            "Epoch 1090/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3779 - val_loss: 0.3925\n",
            "Epoch 1091/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3970\n",
            "Epoch 1092/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3753 - val_loss: 0.3980\n",
            "Epoch 1093/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3977\n",
            "Epoch 1094/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3995\n",
            "Epoch 1095/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3955\n",
            "Epoch 1096/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3991\n",
            "Epoch 1097/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3947\n",
            "Epoch 1098/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3956\n",
            "Epoch 1099/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.3921\n",
            "Epoch 1100/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3951\n",
            "Epoch 1101/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3768 - val_loss: 0.3961\n",
            "Epoch 1102/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3966\n",
            "Epoch 1103/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.4025\n",
            "Epoch 1104/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3974\n",
            "Epoch 1105/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3988\n",
            "Epoch 1106/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3933\n",
            "Epoch 1107/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3764 - val_loss: 0.3995\n",
            "Epoch 1108/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3758 - val_loss: 0.3951\n",
            "Epoch 1109/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3762 - val_loss: 0.3981\n",
            "Epoch 1110/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3803 - val_loss: 0.3954\n",
            "Epoch 1111/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3775 - val_loss: 0.3930\n",
            "Epoch 1112/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3936\n",
            "Epoch 1113/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3930\n",
            "Epoch 1114/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3963\n",
            "Epoch 1115/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.3925\n",
            "Epoch 1116/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3760 - val_loss: 0.3953\n",
            "Epoch 1117/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3936\n",
            "Epoch 1118/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3949\n",
            "Epoch 1119/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3772 - val_loss: 0.3940\n",
            "Epoch 1120/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3921\n",
            "Epoch 1121/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.4011\n",
            "Epoch 1122/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.3952\n",
            "Epoch 1123/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3935\n",
            "Epoch 1124/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3955\n",
            "Epoch 1125/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3956\n",
            "Epoch 1126/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3743 - val_loss: 0.3978\n",
            "Epoch 1127/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3753 - val_loss: 0.3947\n",
            "Epoch 1128/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3753 - val_loss: 0.3945\n",
            "Epoch 1129/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3919\n",
            "Epoch 1130/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3949\n",
            "Epoch 1131/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3938\n",
            "Epoch 1132/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.3963\n",
            "Epoch 1133/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3973\n",
            "Epoch 1134/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3952\n",
            "Epoch 1135/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3796 - val_loss: 0.3971\n",
            "Epoch 1136/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3919\n",
            "Epoch 1137/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3968\n",
            "Epoch 1138/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3923\n",
            "Epoch 1139/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3974\n",
            "Epoch 1140/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3995\n",
            "Epoch 1141/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3966\n",
            "Epoch 1142/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3945\n",
            "Epoch 1143/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3937\n",
            "Epoch 1144/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3750 - val_loss: 0.3965\n",
            "Epoch 1145/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3752 - val_loss: 0.3983\n",
            "Epoch 1146/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3753 - val_loss: 0.4025\n",
            "Epoch 1147/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3767 - val_loss: 0.3949\n",
            "Epoch 1148/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3947\n",
            "Epoch 1149/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3956\n",
            "Epoch 1150/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3761 - val_loss: 0.3956\n",
            "Epoch 1151/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3903\n",
            "Epoch 1152/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3937\n",
            "Epoch 1153/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3939\n",
            "Epoch 1154/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3948\n",
            "Epoch 1155/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3757 - val_loss: 0.3929\n",
            "Epoch 1156/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3926\n",
            "Epoch 1157/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3932\n",
            "Epoch 1158/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3947\n",
            "Epoch 1159/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3935\n",
            "Epoch 1160/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3972\n",
            "Epoch 1161/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3974\n",
            "Epoch 1162/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3750 - val_loss: 0.3972\n",
            "Epoch 1163/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3745 - val_loss: 0.3951\n",
            "Epoch 1164/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3750 - val_loss: 0.3965\n",
            "Epoch 1165/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3736 - val_loss: 0.3970\n",
            "Epoch 1166/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3979\n",
            "Epoch 1167/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3955\n",
            "Epoch 1168/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3954\n",
            "Epoch 1169/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3946\n",
            "Epoch 1170/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3729 - val_loss: 0.3966\n",
            "Epoch 1171/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3941\n",
            "Epoch 1172/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.4057\n",
            "Epoch 1173/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3805 - val_loss: 0.3962\n",
            "Epoch 1174/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3768 - val_loss: 0.4011\n",
            "Epoch 1175/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3784 - val_loss: 0.3952\n",
            "Epoch 1176/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3923\n",
            "Epoch 1177/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3758 - val_loss: 0.3948\n",
            "Epoch 1178/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3946\n",
            "Epoch 1179/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3937\n",
            "Epoch 1180/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3744 - val_loss: 0.3955\n",
            "Epoch 1181/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3743 - val_loss: 0.3940\n",
            "Epoch 1182/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3740 - val_loss: 0.3939\n",
            "Epoch 1183/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3740 - val_loss: 0.3938\n",
            "Epoch 1184/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3745 - val_loss: 0.3924\n",
            "Epoch 1185/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3945\n",
            "Epoch 1186/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3960\n",
            "Epoch 1187/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3756 - val_loss: 0.3945\n",
            "Epoch 1188/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3963\n",
            "Epoch 1189/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.4017\n",
            "Epoch 1190/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3751 - val_loss: 0.3971\n",
            "Epoch 1191/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3964\n",
            "Epoch 1192/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3973\n",
            "Epoch 1193/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3963\n",
            "Epoch 1194/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3984\n",
            "Epoch 1195/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3790 - val_loss: 0.3992\n",
            "Epoch 1196/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3958\n",
            "Epoch 1197/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3771 - val_loss: 0.3959\n",
            "Epoch 1198/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3976\n",
            "Epoch 1199/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3754 - val_loss: 0.3988\n",
            "Epoch 1200/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3740 - val_loss: 0.3949\n",
            "Epoch 1201/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3749 - val_loss: 0.3958\n",
            "Epoch 1202/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3748 - val_loss: 0.3952\n",
            "Epoch 1203/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3751 - val_loss: 0.3993\n",
            "Epoch 1204/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3948\n",
            "Epoch 1205/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.3954\n",
            "Epoch 1206/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3956\n",
            "Epoch 1207/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3748 - val_loss: 0.3954\n",
            "Epoch 1208/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3769 - val_loss: 0.3943\n",
            "Epoch 1209/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3998\n",
            "Epoch 1210/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3974\n",
            "Epoch 1211/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3981\n",
            "Epoch 1212/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3755 - val_loss: 0.3948\n",
            "Epoch 1213/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3977\n",
            "Epoch 1214/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3972\n",
            "Epoch 1215/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3949\n",
            "Epoch 1216/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3954\n",
            "Epoch 1217/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3970\n",
            "Epoch 1218/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3744 - val_loss: 0.4000\n",
            "Epoch 1219/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3748 - val_loss: 0.3992\n",
            "Epoch 1220/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3755 - val_loss: 0.3989\n",
            "Epoch 1221/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.4008\n",
            "Epoch 1222/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3964\n",
            "Epoch 1223/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3986\n",
            "Epoch 1224/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3958\n",
            "Epoch 1225/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3958\n",
            "Epoch 1226/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3970\n",
            "Epoch 1227/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3928\n",
            "Epoch 1228/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3769 - val_loss: 0.3940\n",
            "Epoch 1229/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3970\n",
            "Epoch 1230/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3965\n",
            "Epoch 1231/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3934\n",
            "Epoch 1232/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3958\n",
            "Epoch 1233/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3944\n",
            "Epoch 1234/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3937\n",
            "Epoch 1235/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3960\n",
            "Epoch 1236/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3764 - val_loss: 0.3953\n",
            "Epoch 1237/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3751 - val_loss: 0.3948\n",
            "Epoch 1238/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3740 - val_loss: 0.3936\n",
            "Epoch 1239/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3986\n",
            "Epoch 1240/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3972\n",
            "Epoch 1241/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3991\n",
            "Epoch 1242/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3946\n",
            "Epoch 1243/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.4013\n",
            "Epoch 1244/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3949\n",
            "Epoch 1245/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3960\n",
            "Epoch 1246/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3751 - val_loss: 0.3963\n",
            "Epoch 1247/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3963\n",
            "Epoch 1248/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3754 - val_loss: 0.3958\n",
            "Epoch 1249/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3969\n",
            "Epoch 1250/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.3967\n",
            "Epoch 1251/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3982\n",
            "Epoch 1252/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3981\n",
            "Epoch 1253/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3962\n",
            "Epoch 1254/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3738 - val_loss: 0.3983\n",
            "Epoch 1255/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3746 - val_loss: 0.3973\n",
            "Epoch 1256/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3770 - val_loss: 0.3967\n",
            "Epoch 1257/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3740 - val_loss: 0.3937\n",
            "Epoch 1258/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3960\n",
            "Epoch 1259/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3972\n",
            "Epoch 1260/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3980\n",
            "Epoch 1261/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3971\n",
            "Epoch 1262/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3987\n",
            "Epoch 1263/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3756 - val_loss: 0.3972\n",
            "Epoch 1264/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3940\n",
            "Epoch 1265/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3993\n",
            "Epoch 1266/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3961\n",
            "Epoch 1267/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.4054\n",
            "Epoch 1268/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3965\n",
            "Epoch 1269/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3777 - val_loss: 0.3959\n",
            "Epoch 1270/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3754 - val_loss: 0.3961\n",
            "Epoch 1271/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3969\n",
            "Epoch 1272/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3755 - val_loss: 0.3984\n",
            "Epoch 1273/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3753 - val_loss: 0.3981\n",
            "Epoch 1274/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3745 - val_loss: 0.3986\n",
            "Epoch 1275/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3737 - val_loss: 0.3959\n",
            "Epoch 1276/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3744 - val_loss: 0.3969\n",
            "Epoch 1277/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3753 - val_loss: 0.3955\n",
            "Epoch 1278/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3948\n",
            "Epoch 1279/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3933\n",
            "Epoch 1280/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3965\n",
            "Epoch 1281/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3970\n",
            "Epoch 1282/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3973\n",
            "Epoch 1283/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3933\n",
            "Epoch 1284/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3937\n",
            "Epoch 1285/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3930\n",
            "Epoch 1286/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3973\n",
            "Epoch 1287/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - val_loss: 0.3971\n",
            "Epoch 1288/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3988\n",
            "Epoch 1289/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.4006\n",
            "Epoch 1290/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3739 - val_loss: 0.3989\n",
            "Epoch 1291/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3727 - val_loss: 0.3988\n",
            "Epoch 1292/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3740 - val_loss: 0.4013\n",
            "Epoch 1293/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3744 - val_loss: 0.3957\n",
            "Epoch 1294/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3728 - val_loss: 0.3945\n",
            "Epoch 1295/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3972\n",
            "Epoch 1296/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.4004\n",
            "Epoch 1297/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.4003\n",
            "Epoch 1298/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3766 - val_loss: 0.3961\n",
            "Epoch 1299/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3938\n",
            "Epoch 1300/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3976\n",
            "Epoch 1301/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3973\n",
            "Epoch 1302/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3962\n",
            "Epoch 1303/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3952\n",
            "Epoch 1304/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3945\n",
            "Epoch 1305/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3975\n",
            "Epoch 1306/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3993\n",
            "Epoch 1307/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3781 - val_loss: 0.3968\n",
            "Epoch 1308/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3970\n",
            "Epoch 1309/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3739 - val_loss: 0.3942\n",
            "Epoch 1310/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3734 - val_loss: 0.3963\n",
            "Epoch 1311/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3733 - val_loss: 0.3960\n",
            "Epoch 1312/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3731 - val_loss: 0.3975\n",
            "Epoch 1313/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3981\n",
            "Epoch 1314/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3972\n",
            "Epoch 1315/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3947\n",
            "Epoch 1316/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3942\n",
            "Epoch 1317/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3963\n",
            "Epoch 1318/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3763 - val_loss: 0.3988\n",
            "Epoch 1319/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3959\n",
            "Epoch 1320/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3959\n",
            "Epoch 1321/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3944\n",
            "Epoch 1322/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3963\n",
            "Epoch 1323/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.3930\n",
            "Epoch 1324/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3946\n",
            "Epoch 1325/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3962\n",
            "Epoch 1326/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.3989\n",
            "Epoch 1327/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3737 - val_loss: 0.3962\n",
            "Epoch 1328/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3734 - val_loss: 0.3990\n",
            "Epoch 1329/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3750 - val_loss: 0.3971\n",
            "Epoch 1330/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3735 - val_loss: 0.3979\n",
            "Epoch 1331/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3726 - val_loss: 0.3982\n",
            "Epoch 1332/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.4005\n",
            "Epoch 1333/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3757 - val_loss: 0.3992\n",
            "Epoch 1334/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3967\n",
            "Epoch 1335/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3968\n",
            "Epoch 1336/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3983\n",
            "Epoch 1337/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.4027\n",
            "Epoch 1338/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3807 - val_loss: 0.4012\n",
            "Epoch 1339/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.3990\n",
            "Epoch 1340/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3770 - val_loss: 0.3961\n",
            "Epoch 1341/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3960\n",
            "Epoch 1342/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3966\n",
            "Epoch 1343/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3957\n",
            "Epoch 1344/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3982\n",
            "Epoch 1345/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3722 - val_loss: 0.3978\n",
            "Epoch 1346/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3723 - val_loss: 0.3992\n",
            "Epoch 1347/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3736 - val_loss: 0.3994\n",
            "Epoch 1348/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3745 - val_loss: 0.3984\n",
            "Epoch 1349/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3728 - val_loss: 0.4006\n",
            "Epoch 1350/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3948\n",
            "Epoch 1351/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3983\n",
            "Epoch 1352/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3979\n",
            "Epoch 1353/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3971\n",
            "Epoch 1354/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3963\n",
            "Epoch 1355/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3968\n",
            "Epoch 1356/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3956\n",
            "Epoch 1357/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3987\n",
            "Epoch 1358/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.4004\n",
            "Epoch 1359/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.3983\n",
            "Epoch 1360/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3988\n",
            "Epoch 1361/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3980\n",
            "Epoch 1362/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.4011\n",
            "Epoch 1363/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3735 - val_loss: 0.4009\n",
            "Epoch 1364/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3737 - val_loss: 0.3972\n",
            "Epoch 1365/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.3978\n",
            "Epoch 1366/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3749 - val_loss: 0.3982\n",
            "Epoch 1367/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3727 - val_loss: 0.3970\n",
            "Epoch 1368/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4008\n",
            "Epoch 1369/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.4035\n",
            "Epoch 1370/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.4027\n",
            "Epoch 1371/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3986\n",
            "Epoch 1372/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3995\n",
            "Epoch 1373/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3739 - val_loss: 0.3976\n",
            "Epoch 1374/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3996\n",
            "Epoch 1375/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.4011\n",
            "Epoch 1376/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3982\n",
            "Epoch 1377/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3785 - val_loss: 0.4001\n",
            "Epoch 1378/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3981\n",
            "Epoch 1379/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.4016\n",
            "Epoch 1380/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.4010\n",
            "Epoch 1381/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3954\n",
            "Epoch 1382/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3743 - val_loss: 0.4034\n",
            "Epoch 1383/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3746 - val_loss: 0.3989\n",
            "Epoch 1384/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3723 - val_loss: 0.3993\n",
            "Epoch 1385/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3726 - val_loss: 0.3990\n",
            "Epoch 1386/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3753 - val_loss: 0.3983\n",
            "Epoch 1387/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3765 - val_loss: 0.3951\n",
            "Epoch 1388/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3972\n",
            "Epoch 1389/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3959\n",
            "Epoch 1390/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3972\n",
            "Epoch 1391/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.3956\n",
            "Epoch 1392/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3978\n",
            "Epoch 1393/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3984\n",
            "Epoch 1394/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3984\n",
            "Epoch 1395/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3978\n",
            "Epoch 1396/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3969\n",
            "Epoch 1397/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.3972\n",
            "Epoch 1398/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3997\n",
            "Epoch 1399/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3725 - val_loss: 0.3986\n",
            "Epoch 1400/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3721 - val_loss: 0.3987\n",
            "Epoch 1401/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3736 - val_loss: 0.3986\n",
            "Epoch 1402/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3728 - val_loss: 0.3966\n",
            "Epoch 1403/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3724 - val_loss: 0.4043\n",
            "Epoch 1404/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.4008\n",
            "Epoch 1405/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3971\n",
            "Epoch 1406/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3961\n",
            "Epoch 1407/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3975\n",
            "Epoch 1408/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.3985\n",
            "Epoch 1409/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.4006\n",
            "Epoch 1410/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3984\n",
            "Epoch 1411/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3731 - val_loss: 0.4001\n",
            "Epoch 1412/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3993\n",
            "Epoch 1413/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3953\n",
            "Epoch 1414/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3982\n",
            "Epoch 1415/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3948\n",
            "Epoch 1416/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3971\n",
            "Epoch 1417/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3988\n",
            "Epoch 1418/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3734 - val_loss: 0.3993\n",
            "Epoch 1419/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3728 - val_loss: 0.3977\n",
            "Epoch 1420/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3986\n",
            "Epoch 1421/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3727 - val_loss: 0.3978\n",
            "Epoch 1422/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3733 - val_loss: 0.3966\n",
            "Epoch 1423/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.3981\n",
            "Epoch 1424/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.4000\n",
            "Epoch 1425/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3992\n",
            "Epoch 1426/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3976\n",
            "Epoch 1427/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3975\n",
            "Epoch 1428/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3986\n",
            "Epoch 1429/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3981\n",
            "Epoch 1430/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3976\n",
            "Epoch 1431/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3989\n",
            "Epoch 1432/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3964\n",
            "Epoch 1433/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3954\n",
            "Epoch 1434/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3949\n",
            "Epoch 1435/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3986\n",
            "Epoch 1436/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.4015\n",
            "Epoch 1437/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3727 - val_loss: 0.4005\n",
            "Epoch 1438/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3726 - val_loss: 0.3960\n",
            "Epoch 1439/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3729 - val_loss: 0.3988\n",
            "Epoch 1440/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3745 - val_loss: 0.3989\n",
            "Epoch 1441/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3977\n",
            "Epoch 1442/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3999\n",
            "Epoch 1443/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3749 - val_loss: 0.3985\n",
            "Epoch 1444/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.3948\n",
            "Epoch 1445/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3956\n",
            "Epoch 1446/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3948\n",
            "Epoch 1447/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.3959\n",
            "Epoch 1448/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3990\n",
            "Epoch 1449/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3727 - val_loss: 0.3988\n",
            "Epoch 1450/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.3963\n",
            "Epoch 1451/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.4009\n",
            "Epoch 1452/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.3985\n",
            "Epoch 1453/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3731 - val_loss: 0.3998\n",
            "Epoch 1454/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.4004\n",
            "Epoch 1455/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3727 - val_loss: 0.3996\n",
            "Epoch 1456/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3729 - val_loss: 0.3998\n",
            "Epoch 1457/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3732 - val_loss: 0.3994\n",
            "Epoch 1458/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3728 - val_loss: 0.3959\n",
            "Epoch 1459/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3727 - val_loss: 0.3991\n",
            "Epoch 1460/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3749 - val_loss: 0.3981\n",
            "Epoch 1461/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3968\n",
            "Epoch 1462/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3952\n",
            "Epoch 1463/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3972\n",
            "Epoch 1464/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.3975\n",
            "Epoch 1465/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3993\n",
            "Epoch 1466/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3980\n",
            "Epoch 1467/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3726 - val_loss: 0.3982\n",
            "Epoch 1468/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.4010\n",
            "Epoch 1469/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.4028\n",
            "Epoch 1470/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3996\n",
            "Epoch 1471/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3739 - val_loss: 0.4013\n",
            "Epoch 1472/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.4011\n",
            "Epoch 1473/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3719 - val_loss: 0.3987\n",
            "Epoch 1474/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3721 - val_loss: 0.4002\n",
            "Epoch 1475/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3724 - val_loss: 0.4019\n",
            "Epoch 1476/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3719 - val_loss: 0.4002\n",
            "Epoch 1477/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3718 - val_loss: 0.4023\n",
            "Epoch 1478/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3998\n",
            "Epoch 1479/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3994\n",
            "Epoch 1480/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.4024\n",
            "Epoch 1481/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.4035\n",
            "Epoch 1482/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.3979\n",
            "Epoch 1483/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3972\n",
            "Epoch 1484/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.3996\n",
            "Epoch 1485/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.4012\n",
            "Epoch 1486/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.4044\n",
            "Epoch 1487/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.4028\n",
            "Epoch 1488/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.4059\n",
            "Epoch 1489/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.4016\n",
            "Epoch 1490/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.4001\n",
            "Epoch 1491/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3716 - val_loss: 0.3997\n",
            "Epoch 1492/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3723 - val_loss: 0.4022\n",
            "Epoch 1493/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3765 - val_loss: 0.3988\n",
            "Epoch 1494/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3782 - val_loss: 0.3980\n",
            "Epoch 1495/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3805 - val_loss: 0.3964\n",
            "Epoch 1496/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3774 - val_loss: 0.3955\n",
            "Epoch 1497/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.4020\n",
            "Epoch 1498/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3952\n",
            "Epoch 1499/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3942\n",
            "Epoch 1500/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3957\n",
            "Epoch 1501/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3971\n",
            "Epoch 1502/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3985\n",
            "Epoch 1503/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.4032\n",
            "Epoch 1504/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3735 - val_loss: 0.3992\n",
            "Epoch 1505/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.4028\n",
            "Epoch 1506/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3995\n",
            "Epoch 1507/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3983\n",
            "Epoch 1508/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4002\n",
            "Epoch 1509/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3988\n",
            "Epoch 1510/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3736 - val_loss: 0.4037\n",
            "Epoch 1511/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3731 - val_loss: 0.3965\n",
            "Epoch 1512/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3725 - val_loss: 0.3990\n",
            "Epoch 1513/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3723 - val_loss: 0.4007\n",
            "Epoch 1514/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.4002\n",
            "Epoch 1515/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3974\n",
            "Epoch 1516/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.4001\n",
            "Epoch 1517/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4012\n",
            "Epoch 1518/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3985\n",
            "Epoch 1519/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.4032\n",
            "Epoch 1520/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3734 - val_loss: 0.4034\n",
            "Epoch 1521/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.4017\n",
            "Epoch 1522/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4017\n",
            "Epoch 1523/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4039\n",
            "Epoch 1524/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3997\n",
            "Epoch 1525/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4008\n",
            "Epoch 1526/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3726 - val_loss: 0.3961\n",
            "Epoch 1527/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.4021\n",
            "Epoch 1528/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3721 - val_loss: 0.4026\n",
            "Epoch 1529/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3730 - val_loss: 0.4045\n",
            "Epoch 1530/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3733 - val_loss: 0.4004\n",
            "Epoch 1531/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3738 - val_loss: 0.4005\n",
            "Epoch 1532/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.4022\n",
            "Epoch 1533/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3739 - val_loss: 0.4003\n",
            "Epoch 1534/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.4004\n",
            "Epoch 1535/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.4036\n",
            "Epoch 1536/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3739 - val_loss: 0.3971\n",
            "Epoch 1537/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3988\n",
            "Epoch 1538/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3946\n",
            "Epoch 1539/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4005\n",
            "Epoch 1540/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4019\n",
            "Epoch 1541/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.3983\n",
            "Epoch 1542/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3760 - val_loss: 0.3985\n",
            "Epoch 1543/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3980\n",
            "Epoch 1544/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.3958\n",
            "Epoch 1545/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3720 - val_loss: 0.3941\n",
            "Epoch 1546/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3719 - val_loss: 0.3976\n",
            "Epoch 1547/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.4002\n",
            "Epoch 1548/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3716 - val_loss: 0.3984\n",
            "Epoch 1549/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3728 - val_loss: 0.4035\n",
            "Epoch 1550/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3741 - val_loss: 0.4052\n",
            "Epoch 1551/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.4012\n",
            "Epoch 1552/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3983\n",
            "Epoch 1553/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3955\n",
            "Epoch 1554/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3969\n",
            "Epoch 1555/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4005\n",
            "Epoch 1556/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3984\n",
            "Epoch 1557/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3730 - val_loss: 0.4004\n",
            "Epoch 1558/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.3960\n",
            "Epoch 1559/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3987\n",
            "Epoch 1560/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.4001\n",
            "Epoch 1561/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4003\n",
            "Epoch 1562/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.4032\n",
            "Epoch 1563/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3743 - val_loss: 0.4013\n",
            "Epoch 1564/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3720 - val_loss: 0.3997\n",
            "Epoch 1565/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3717 - val_loss: 0.4009\n",
            "Epoch 1566/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3727 - val_loss: 0.3972\n",
            "Epoch 1567/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3717 - val_loss: 0.3978\n",
            "Epoch 1568/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.4067\n",
            "Epoch 1569/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - val_loss: 0.4021\n",
            "Epoch 1570/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4004\n",
            "Epoch 1571/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.3987\n",
            "Epoch 1572/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3985\n",
            "Epoch 1573/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.4019\n",
            "Epoch 1574/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.3964\n",
            "Epoch 1575/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.4011\n",
            "Epoch 1576/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3998\n",
            "Epoch 1577/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4007\n",
            "Epoch 1578/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.4016\n",
            "Epoch 1579/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3964\n",
            "Epoch 1580/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.3994\n",
            "Epoch 1581/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3721 - val_loss: 0.4021\n",
            "Epoch 1582/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3720 - val_loss: 0.4018\n",
            "Epoch 1583/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3730 - val_loss: 0.3987\n",
            "Epoch 1584/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3741 - val_loss: 0.4003\n",
            "Epoch 1585/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3725 - val_loss: 0.4004\n",
            "Epoch 1586/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.4024\n",
            "Epoch 1587/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4029\n",
            "Epoch 1588/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4024\n",
            "Epoch 1589/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4010\n",
            "Epoch 1590/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3969\n",
            "Epoch 1591/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.3983\n",
            "Epoch 1592/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4014\n",
            "Epoch 1593/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3964\n",
            "Epoch 1594/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3958\n",
            "Epoch 1595/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4032\n",
            "Epoch 1596/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3994\n",
            "Epoch 1597/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.4019\n",
            "Epoch 1598/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3744 - val_loss: 0.4005\n",
            "Epoch 1599/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3988\n",
            "Epoch 1600/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3715 - val_loss: 0.3974\n",
            "Epoch 1601/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3724 - val_loss: 0.3976\n",
            "Epoch 1602/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3723 - val_loss: 0.3972\n",
            "Epoch 1603/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3722 - val_loss: 0.3983\n",
            "Epoch 1604/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3994\n",
            "Epoch 1605/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4058\n",
            "Epoch 1606/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.4016\n",
            "Epoch 1607/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3991\n",
            "Epoch 1608/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4021\n",
            "Epoch 1609/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4039\n",
            "Epoch 1610/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4058\n",
            "Epoch 1611/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.4019\n",
            "Epoch 1612/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4027\n",
            "Epoch 1613/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.4005\n",
            "Epoch 1614/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4001\n",
            "Epoch 1615/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4008\n",
            "Epoch 1616/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4001\n",
            "Epoch 1617/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.4016\n",
            "Epoch 1618/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3711 - val_loss: 0.4008\n",
            "Epoch 1619/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3712 - val_loss: 0.3992\n",
            "Epoch 1620/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3715 - val_loss: 0.3986\n",
            "Epoch 1621/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3715 - val_loss: 0.4027\n",
            "Epoch 1622/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4021\n",
            "Epoch 1623/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4015\n",
            "Epoch 1624/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.3993\n",
            "Epoch 1625/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4042\n",
            "Epoch 1626/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.4004\n",
            "Epoch 1627/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.4018\n",
            "Epoch 1628/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.4010\n",
            "Epoch 1629/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3993\n",
            "Epoch 1630/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4041\n",
            "Epoch 1631/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3724 - val_loss: 0.3998\n",
            "Epoch 1632/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.4023\n",
            "Epoch 1633/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3997\n",
            "Epoch 1634/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.3992\n",
            "Epoch 1635/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3717 - val_loss: 0.3985\n",
            "Epoch 1636/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3722 - val_loss: 0.4001\n",
            "Epoch 1637/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.4001\n",
            "Epoch 1638/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.3981\n",
            "Epoch 1639/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4019\n",
            "Epoch 1640/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4036\n",
            "Epoch 1641/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3995\n",
            "Epoch 1642/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.3991\n",
            "Epoch 1643/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4025\n",
            "Epoch 1644/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.4014\n",
            "Epoch 1645/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3735 - val_loss: 0.4034\n",
            "Epoch 1646/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3739 - val_loss: 0.3986\n",
            "Epoch 1647/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3725 - val_loss: 0.3972\n",
            "Epoch 1648/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4014\n",
            "Epoch 1649/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3978\n",
            "Epoch 1650/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.4008\n",
            "Epoch 1651/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4034\n",
            "Epoch 1652/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3720 - val_loss: 0.3982\n",
            "Epoch 1653/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3728 - val_loss: 0.4031\n",
            "Epoch 1654/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3721 - val_loss: 0.4042\n",
            "Epoch 1655/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3732 - val_loss: 0.3986\n",
            "Epoch 1656/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3729 - val_loss: 0.4004\n",
            "Epoch 1657/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3740 - val_loss: 0.4072\n",
            "Epoch 1658/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3729 - val_loss: 0.4027\n",
            "Epoch 1659/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4028\n",
            "Epoch 1660/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4028\n",
            "Epoch 1661/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.4050\n",
            "Epoch 1662/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.4010\n",
            "Epoch 1663/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4026\n",
            "Epoch 1664/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4066\n",
            "Epoch 1665/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3984\n",
            "Epoch 1666/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.4016\n",
            "Epoch 1667/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3998\n",
            "Epoch 1668/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3989\n",
            "Epoch 1669/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3720 - val_loss: 0.4037\n",
            "Epoch 1670/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3715 - val_loss: 0.4013\n",
            "Epoch 1671/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3730 - val_loss: 0.4021\n",
            "Epoch 1672/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3718 - val_loss: 0.4000\n",
            "Epoch 1673/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.4009\n",
            "Epoch 1674/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3715 - val_loss: 0.4030\n",
            "Epoch 1675/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3728 - val_loss: 0.4046\n",
            "Epoch 1676/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.4030\n",
            "Epoch 1677/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.3998\n",
            "Epoch 1678/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4004\n",
            "Epoch 1679/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.4012\n",
            "Epoch 1680/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4020\n",
            "Epoch 1681/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.4000\n",
            "Epoch 1682/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.3969\n",
            "Epoch 1683/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.4006\n",
            "Epoch 1684/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4026\n",
            "Epoch 1685/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.3990\n",
            "Epoch 1686/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3718 - val_loss: 0.4053\n",
            "Epoch 1687/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3724 - val_loss: 0.4033\n",
            "Epoch 1688/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3709 - val_loss: 0.4001\n",
            "Epoch 1689/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3727 - val_loss: 0.4006\n",
            "Epoch 1690/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3976\n",
            "Epoch 1691/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.4019\n",
            "Epoch 1692/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3971\n",
            "Epoch 1693/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3976\n",
            "Epoch 1694/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4022\n",
            "Epoch 1695/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4021\n",
            "Epoch 1696/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3969\n",
            "Epoch 1697/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4035\n",
            "Epoch 1698/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.3995\n",
            "Epoch 1699/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3959\n",
            "Epoch 1700/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4003\n",
            "Epoch 1701/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3698 - val_loss: 0.4044\n",
            "Epoch 1702/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3995\n",
            "Epoch 1703/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3970\n",
            "Epoch 1704/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.4021\n",
            "Epoch 1705/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3714 - val_loss: 0.4006\n",
            "Epoch 1706/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3707 - val_loss: 0.4014\n",
            "Epoch 1707/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3710 - val_loss: 0.3972\n",
            "Epoch 1708/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.4020\n",
            "Epoch 1709/3000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3700 - val_loss: 0.3977\n",
            "Epoch 1710/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.3995\n",
            "Epoch 1711/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4039\n",
            "Epoch 1712/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4056\n",
            "Epoch 1713/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3995\n",
            "Epoch 1714/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3965\n",
            "Epoch 1715/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.3979\n",
            "Epoch 1716/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4013\n",
            "Epoch 1717/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4026\n",
            "Epoch 1718/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3999\n",
            "Epoch 1719/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3993\n",
            "Epoch 1720/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4010\n",
            "Epoch 1721/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.4008\n",
            "Epoch 1722/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3706 - val_loss: 0.4024\n",
            "Epoch 1723/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3699 - val_loss: 0.4017\n",
            "Epoch 1724/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3726 - val_loss: 0.4007\n",
            "Epoch 1725/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3709 - val_loss: 0.4007\n",
            "Epoch 1726/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.4016\n",
            "Epoch 1727/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.3987\n",
            "Epoch 1728/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.3966\n",
            "Epoch 1729/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3726 - val_loss: 0.3996\n",
            "Epoch 1730/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3717 - val_loss: 0.4012\n",
            "Epoch 1731/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4031\n",
            "Epoch 1732/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4049\n",
            "Epoch 1733/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4053\n",
            "Epoch 1734/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4029\n",
            "Epoch 1735/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.4057\n",
            "Epoch 1736/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.4043\n",
            "Epoch 1737/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4032\n",
            "Epoch 1738/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4009\n",
            "Epoch 1739/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3716 - val_loss: 0.4072\n",
            "Epoch 1740/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3701 - val_loss: 0.4052\n",
            "Epoch 1741/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3697 - val_loss: 0.4018\n",
            "Epoch 1742/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3700 - val_loss: 0.4028\n",
            "Epoch 1743/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4008\n",
            "Epoch 1744/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4032\n",
            "Epoch 1745/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.4027\n",
            "Epoch 1746/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3997\n",
            "Epoch 1747/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.4066\n",
            "Epoch 1748/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4050\n",
            "Epoch 1749/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3988\n",
            "Epoch 1750/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.4005\n",
            "Epoch 1751/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.4029\n",
            "Epoch 1752/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4027\n",
            "Epoch 1753/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4007\n",
            "Epoch 1754/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4054\n",
            "Epoch 1755/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4051\n",
            "Epoch 1756/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3709 - val_loss: 0.4014\n",
            "Epoch 1757/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.3980\n",
            "Epoch 1758/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3714 - val_loss: 0.3985\n",
            "Epoch 1759/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3703 - val_loss: 0.4049\n",
            "Epoch 1760/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3702 - val_loss: 0.4017\n",
            "Epoch 1761/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3995\n",
            "Epoch 1762/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4011\n",
            "Epoch 1763/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4030\n",
            "Epoch 1764/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3743 - val_loss: 0.4008\n",
            "Epoch 1765/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.4004\n",
            "Epoch 1766/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4051\n",
            "Epoch 1767/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4041\n",
            "Epoch 1768/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4000\n",
            "Epoch 1769/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4026\n",
            "Epoch 1770/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4015\n",
            "Epoch 1771/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4037\n",
            "Epoch 1772/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3987\n",
            "Epoch 1773/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4040\n",
            "Epoch 1774/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4032\n",
            "Epoch 1775/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3713 - val_loss: 0.4046\n",
            "Epoch 1776/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3736 - val_loss: 0.3947\n",
            "Epoch 1777/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3714 - val_loss: 0.3975\n",
            "Epoch 1778/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3699 - val_loss: 0.3989\n",
            "Epoch 1779/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3695 - val_loss: 0.4027\n",
            "Epoch 1780/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.4014\n",
            "Epoch 1781/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.4009\n",
            "Epoch 1782/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4025\n",
            "Epoch 1783/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4021\n",
            "Epoch 1784/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3723 - val_loss: 0.4002\n",
            "Epoch 1785/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.4002\n",
            "Epoch 1786/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4040\n",
            "Epoch 1787/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4000\n",
            "Epoch 1788/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.3972\n",
            "Epoch 1789/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4038\n",
            "Epoch 1790/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.3989\n",
            "Epoch 1791/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4090\n",
            "Epoch 1792/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3996\n",
            "Epoch 1793/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.4015\n",
            "Epoch 1794/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3697 - val_loss: 0.4010\n",
            "Epoch 1795/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3708 - val_loss: 0.4063\n",
            "Epoch 1796/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.4027\n",
            "Epoch 1797/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3717 - val_loss: 0.4056\n",
            "Epoch 1798/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4004\n",
            "Epoch 1799/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4028\n",
            "Epoch 1800/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.4023\n",
            "Epoch 1801/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4014\n",
            "Epoch 1802/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.4041\n",
            "Epoch 1803/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4028\n",
            "Epoch 1804/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4019\n",
            "Epoch 1805/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4051\n",
            "Epoch 1806/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.3988\n",
            "Epoch 1807/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4016\n",
            "Epoch 1808/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4069\n",
            "Epoch 1809/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4087\n",
            "Epoch 1810/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.4054\n",
            "Epoch 1811/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3727 - val_loss: 0.4016\n",
            "Epoch 1812/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.4051\n",
            "Epoch 1813/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3714 - val_loss: 0.4004\n",
            "Epoch 1814/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3714 - val_loss: 0.4018\n",
            "Epoch 1815/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3701 - val_loss: 0.4015\n",
            "Epoch 1816/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4044\n",
            "Epoch 1817/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.4002\n",
            "Epoch 1818/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4043\n",
            "Epoch 1819/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4032\n",
            "Epoch 1820/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4014\n",
            "Epoch 1821/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4022\n",
            "Epoch 1822/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4070\n",
            "Epoch 1823/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4030\n",
            "Epoch 1824/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.4063\n",
            "Epoch 1825/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4014\n",
            "Epoch 1826/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4017\n",
            "Epoch 1827/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4006\n",
            "Epoch 1828/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4030\n",
            "Epoch 1829/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3703 - val_loss: 0.4008\n",
            "Epoch 1830/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3698 - val_loss: 0.4030\n",
            "Epoch 1831/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3695 - val_loss: 0.4012\n",
            "Epoch 1832/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3693 - val_loss: 0.4038\n",
            "Epoch 1833/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.3999\n",
            "Epoch 1834/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4042\n",
            "Epoch 1835/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4009\n",
            "Epoch 1836/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4001\n",
            "Epoch 1837/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4048\n",
            "Epoch 1838/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4049\n",
            "Epoch 1839/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4032\n",
            "Epoch 1840/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4053\n",
            "Epoch 1841/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4005\n",
            "Epoch 1842/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4002\n",
            "Epoch 1843/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4073\n",
            "Epoch 1844/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3743 - val_loss: 0.4009\n",
            "Epoch 1845/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3737 - val_loss: 0.3985\n",
            "Epoch 1846/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3980\n",
            "Epoch 1847/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3708 - val_loss: 0.4005\n",
            "Epoch 1848/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3713 - val_loss: 0.3973\n",
            "Epoch 1849/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3704 - val_loss: 0.4011\n",
            "Epoch 1850/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3707 - val_loss: 0.4012\n",
            "Epoch 1851/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4000\n",
            "Epoch 1852/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3698 - val_loss: 0.4001\n",
            "Epoch 1853/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4001\n",
            "Epoch 1854/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4043\n",
            "Epoch 1855/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.3980\n",
            "Epoch 1856/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.3982\n",
            "Epoch 1857/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4015\n",
            "Epoch 1858/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4024\n",
            "Epoch 1859/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4011\n",
            "Epoch 1860/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4003\n",
            "Epoch 1861/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4031\n",
            "Epoch 1862/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4000\n",
            "Epoch 1863/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4029\n",
            "Epoch 1864/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3721 - val_loss: 0.4022\n",
            "Epoch 1865/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.4003\n",
            "Epoch 1866/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3712 - val_loss: 0.3967\n",
            "Epoch 1867/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3698 - val_loss: 0.3977\n",
            "Epoch 1868/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4025\n",
            "Epoch 1869/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3926\n",
            "Epoch 1870/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3997\n",
            "Epoch 1871/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.4021\n",
            "Epoch 1872/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4009\n",
            "Epoch 1873/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4004\n",
            "Epoch 1874/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4026\n",
            "Epoch 1875/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4014\n",
            "Epoch 1876/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.4036\n",
            "Epoch 1877/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.3979\n",
            "Epoch 1878/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4030\n",
            "Epoch 1879/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4053\n",
            "Epoch 1880/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4071\n",
            "Epoch 1881/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3695 - val_loss: 0.4026\n",
            "Epoch 1882/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3695 - val_loss: 0.4031\n",
            "Epoch 1883/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3698 - val_loss: 0.4035\n",
            "Epoch 1884/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.4026\n",
            "Epoch 1885/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3692 - val_loss: 0.4032\n",
            "Epoch 1886/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3706 - val_loss: 0.4045\n",
            "Epoch 1887/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4028\n",
            "Epoch 1888/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3975\n",
            "Epoch 1889/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4041\n",
            "Epoch 1890/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.4040\n",
            "Epoch 1891/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.4011\n",
            "Epoch 1892/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3719 - val_loss: 0.4018\n",
            "Epoch 1893/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4039\n",
            "Epoch 1894/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4030\n",
            "Epoch 1895/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4028\n",
            "Epoch 1896/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4012\n",
            "Epoch 1897/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.3988\n",
            "Epoch 1898/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4046\n",
            "Epoch 1899/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.4063\n",
            "Epoch 1900/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3707 - val_loss: 0.4079\n",
            "Epoch 1901/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3697 - val_loss: 0.4020\n",
            "Epoch 1902/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4057\n",
            "Epoch 1903/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3703 - val_loss: 0.4009\n",
            "Epoch 1904/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4043\n",
            "Epoch 1905/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.4009\n",
            "Epoch 1906/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3773 - val_loss: 0.4013\n",
            "Epoch 1907/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3716 - val_loss: 0.4036\n",
            "Epoch 1908/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3997\n",
            "Epoch 1909/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3960\n",
            "Epoch 1910/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4054\n",
            "Epoch 1911/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.3996\n",
            "Epoch 1912/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4018\n",
            "Epoch 1913/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3959\n",
            "Epoch 1914/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.4020\n",
            "Epoch 1915/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3720 - val_loss: 0.4021\n",
            "Epoch 1916/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4003\n",
            "Epoch 1917/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.3953\n",
            "Epoch 1918/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3693 - val_loss: 0.3988\n",
            "Epoch 1919/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3710 - val_loss: 0.3995\n",
            "Epoch 1920/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.4013\n",
            "Epoch 1921/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.4037\n",
            "Epoch 1922/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3713 - val_loss: 0.4034\n",
            "Epoch 1923/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4016\n",
            "Epoch 1924/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4011\n",
            "Epoch 1925/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.4026\n",
            "Epoch 1926/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3990\n",
            "Epoch 1927/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.3993\n",
            "Epoch 1928/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4036\n",
            "Epoch 1929/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4015\n",
            "Epoch 1930/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4006\n",
            "Epoch 1931/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4017\n",
            "Epoch 1932/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4056\n",
            "Epoch 1933/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.4021\n",
            "Epoch 1934/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3689 - val_loss: 0.4069\n",
            "Epoch 1935/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3687 - val_loss: 0.4053\n",
            "Epoch 1936/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3685 - val_loss: 0.4048\n",
            "Epoch 1937/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3691 - val_loss: 0.4024\n",
            "Epoch 1938/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3693 - val_loss: 0.4035\n",
            "Epoch 1939/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4043\n",
            "Epoch 1940/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4025\n",
            "Epoch 1941/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.4052\n",
            "Epoch 1942/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4043\n",
            "Epoch 1943/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.4008\n",
            "Epoch 1944/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4039\n",
            "Epoch 1945/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4009\n",
            "Epoch 1946/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4023\n",
            "Epoch 1947/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4003\n",
            "Epoch 1948/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3706 - val_loss: 0.3977\n",
            "Epoch 1949/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4005\n",
            "Epoch 1950/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.3980\n",
            "Epoch 1951/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4034\n",
            "Epoch 1952/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3688 - val_loss: 0.4016\n",
            "Epoch 1953/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.4049\n",
            "Epoch 1954/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3685 - val_loss: 0.4021\n",
            "Epoch 1955/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3694 - val_loss: 0.4030\n",
            "Epoch 1956/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3705 - val_loss: 0.4052\n",
            "Epoch 1957/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4090\n",
            "Epoch 1958/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4036\n",
            "Epoch 1959/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4078\n",
            "Epoch 1960/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4012\n",
            "Epoch 1961/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4052\n",
            "Epoch 1962/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4018\n",
            "Epoch 1963/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3691 - val_loss: 0.4047\n",
            "Epoch 1964/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3996\n",
            "Epoch 1965/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4059\n",
            "Epoch 1966/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4049\n",
            "Epoch 1967/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.4095\n",
            "Epoch 1968/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4069\n",
            "Epoch 1969/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3699 - val_loss: 0.4022\n",
            "Epoch 1970/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.4027\n",
            "Epoch 1971/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3709 - val_loss: 0.4033\n",
            "Epoch 1972/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.4027\n",
            "Epoch 1973/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3695 - val_loss: 0.4063\n",
            "Epoch 1974/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4003\n",
            "Epoch 1975/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4034\n",
            "Epoch 1976/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4002\n",
            "Epoch 1977/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4004\n",
            "Epoch 1978/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4042\n",
            "Epoch 1979/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4042\n",
            "Epoch 1980/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4054\n",
            "Epoch 1981/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4011\n",
            "Epoch 1982/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.4057\n",
            "Epoch 1983/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.4008\n",
            "Epoch 1984/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4026\n",
            "Epoch 1985/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3998\n",
            "Epoch 1986/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4029\n",
            "Epoch 1987/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3693 - val_loss: 0.4021\n",
            "Epoch 1988/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.4055\n",
            "Epoch 1989/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3685 - val_loss: 0.4014\n",
            "Epoch 1990/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3687 - val_loss: 0.4043\n",
            "Epoch 1991/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4052\n",
            "Epoch 1992/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.4040\n",
            "Epoch 1993/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4046\n",
            "Epoch 1994/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4037\n",
            "Epoch 1995/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4077\n",
            "Epoch 1996/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4068\n",
            "Epoch 1997/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4034\n",
            "Epoch 1998/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4028\n",
            "Epoch 1999/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4002\n",
            "Epoch 2000/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4026\n",
            "Epoch 2001/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4008\n",
            "Epoch 2002/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4067\n",
            "Epoch 2003/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.4028\n",
            "Epoch 2004/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3690 - val_loss: 0.3995\n",
            "Epoch 2005/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3692 - val_loss: 0.4026\n",
            "Epoch 2006/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3682 - val_loss: 0.4026\n",
            "Epoch 2007/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3695 - val_loss: 0.4009\n",
            "Epoch 2008/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4020\n",
            "Epoch 2009/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4029\n",
            "Epoch 2010/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4008\n",
            "Epoch 2011/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.3983\n",
            "Epoch 2012/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4003\n",
            "Epoch 2013/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.3979\n",
            "Epoch 2014/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4039\n",
            "Epoch 2015/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4027\n",
            "Epoch 2016/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4078\n",
            "Epoch 2017/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.4061\n",
            "Epoch 2018/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.4003\n",
            "Epoch 2019/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3712 - val_loss: 0.4080\n",
            "Epoch 2020/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4074\n",
            "Epoch 2021/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3710 - val_loss: 0.4051\n",
            "Epoch 2022/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3709 - val_loss: 0.4083\n",
            "Epoch 2023/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3692 - val_loss: 0.4071\n",
            "Epoch 2024/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3685 - val_loss: 0.4037\n",
            "Epoch 2025/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3684 - val_loss: 0.4030\n",
            "Epoch 2026/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.4026\n",
            "Epoch 2027/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4052\n",
            "Epoch 2028/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3681 - val_loss: 0.4043\n",
            "Epoch 2029/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4056\n",
            "Epoch 2030/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4031\n",
            "Epoch 2031/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.3983\n",
            "Epoch 2032/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4048\n",
            "Epoch 2033/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4030\n",
            "Epoch 2034/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4052\n",
            "Epoch 2035/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4061\n",
            "Epoch 2036/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4062\n",
            "Epoch 2037/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4057\n",
            "Epoch 2038/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4040\n",
            "Epoch 2039/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3686 - val_loss: 0.4049\n",
            "Epoch 2040/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3688 - val_loss: 0.3997\n",
            "Epoch 2041/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.4021\n",
            "Epoch 2042/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3683 - val_loss: 0.4045\n",
            "Epoch 2043/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4086\n",
            "Epoch 2044/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3700 - val_loss: 0.4024\n",
            "Epoch 2045/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4058\n",
            "Epoch 2046/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4067\n",
            "Epoch 2047/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4070\n",
            "Epoch 2048/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4068\n",
            "Epoch 2049/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4069\n",
            "Epoch 2050/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4053\n",
            "Epoch 2051/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4024\n",
            "Epoch 2052/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4018\n",
            "Epoch 2053/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4014\n",
            "Epoch 2054/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4032\n",
            "Epoch 2055/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4044\n",
            "Epoch 2056/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4049\n",
            "Epoch 2057/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.4022\n",
            "Epoch 2058/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3688 - val_loss: 0.4085\n",
            "Epoch 2059/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.4044\n",
            "Epoch 2060/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3700 - val_loss: 0.4035\n",
            "Epoch 2061/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3690 - val_loss: 0.4091\n",
            "Epoch 2062/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.4051\n",
            "Epoch 2063/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3998\n",
            "Epoch 2064/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.4027\n",
            "Epoch 2065/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3997\n",
            "Epoch 2066/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4032\n",
            "Epoch 2067/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4005\n",
            "Epoch 2068/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4060\n",
            "Epoch 2069/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4034\n",
            "Epoch 2070/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4032\n",
            "Epoch 2071/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4031\n",
            "Epoch 2072/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.4047\n",
            "Epoch 2073/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.4044\n",
            "Epoch 2074/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4064\n",
            "Epoch 2075/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4030\n",
            "Epoch 2076/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3703 - val_loss: 0.4029\n",
            "Epoch 2077/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4000\n",
            "Epoch 2078/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3689 - val_loss: 0.4025\n",
            "Epoch 2079/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4020\n",
            "Epoch 2080/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3698 - val_loss: 0.4081\n",
            "Epoch 2081/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4034\n",
            "Epoch 2082/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4059\n",
            "Epoch 2083/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.4037\n",
            "Epoch 2084/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4042\n",
            "Epoch 2085/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4039\n",
            "Epoch 2086/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4058\n",
            "Epoch 2087/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4029\n",
            "Epoch 2088/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3678 - val_loss: 0.4053\n",
            "Epoch 2089/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4044\n",
            "Epoch 2090/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4057\n",
            "Epoch 2091/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.4006\n",
            "Epoch 2092/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4053\n",
            "Epoch 2093/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4044\n",
            "Epoch 2094/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3704 - val_loss: 0.4051\n",
            "Epoch 2095/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3705 - val_loss: 0.4076\n",
            "Epoch 2096/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.4029\n",
            "Epoch 2097/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3701 - val_loss: 0.4026\n",
            "Epoch 2098/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4031\n",
            "Epoch 2099/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3707 - val_loss: 0.4021\n",
            "Epoch 2100/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4036\n",
            "Epoch 2101/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3718 - val_loss: 0.4003\n",
            "Epoch 2102/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.3995\n",
            "Epoch 2103/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4005\n",
            "Epoch 2104/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4016\n",
            "Epoch 2105/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.3994\n",
            "Epoch 2106/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4063\n",
            "Epoch 2107/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4074\n",
            "Epoch 2108/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4049\n",
            "Epoch 2109/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4011\n",
            "Epoch 2110/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4054\n",
            "Epoch 2111/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4005\n",
            "Epoch 2112/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3684 - val_loss: 0.4065\n",
            "Epoch 2113/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3687 - val_loss: 0.4030\n",
            "Epoch 2114/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3692 - val_loss: 0.4024\n",
            "Epoch 2115/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3698 - val_loss: 0.4051\n",
            "Epoch 2116/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4076\n",
            "Epoch 2117/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4097\n",
            "Epoch 2118/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3680 - val_loss: 0.4060\n",
            "Epoch 2119/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4034\n",
            "Epoch 2120/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4071\n",
            "Epoch 2121/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4072\n",
            "Epoch 2122/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4077\n",
            "Epoch 2123/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.4040\n",
            "Epoch 2124/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3708 - val_loss: 0.4048\n",
            "Epoch 2125/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4044\n",
            "Epoch 2126/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4038\n",
            "Epoch 2127/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4058\n",
            "Epoch 2128/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4059\n",
            "Epoch 2129/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3683 - val_loss: 0.4054\n",
            "Epoch 2130/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3683 - val_loss: 0.4118\n",
            "Epoch 2131/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3691 - val_loss: 0.4014\n",
            "Epoch 2132/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3685 - val_loss: 0.4077\n",
            "Epoch 2133/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3683 - val_loss: 0.4085\n",
            "Epoch 2134/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.4081\n",
            "Epoch 2135/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4075\n",
            "Epoch 2136/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4051\n",
            "Epoch 2137/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.4036\n",
            "Epoch 2138/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4034\n",
            "Epoch 2139/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4069\n",
            "Epoch 2140/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4072\n",
            "Epoch 2141/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4049\n",
            "Epoch 2142/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4066\n",
            "Epoch 2143/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4052\n",
            "Epoch 2144/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.4015\n",
            "Epoch 2145/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.4012\n",
            "Epoch 2146/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.4078\n",
            "Epoch 2147/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3706 - val_loss: 0.4061\n",
            "Epoch 2148/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3713 - val_loss: 0.4014\n",
            "Epoch 2149/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.4032\n",
            "Epoch 2150/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3692 - val_loss: 0.4027\n",
            "Epoch 2151/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4010\n",
            "Epoch 2152/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4033\n",
            "Epoch 2153/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4037\n",
            "Epoch 2154/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4052\n",
            "Epoch 2155/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4059\n",
            "Epoch 2156/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.4055\n",
            "Epoch 2157/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4044\n",
            "Epoch 2158/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.4075\n",
            "Epoch 2159/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4029\n",
            "Epoch 2160/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4009\n",
            "Epoch 2161/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4037\n",
            "Epoch 2162/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3701 - val_loss: 0.4010\n",
            "Epoch 2163/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3680 - val_loss: 0.4074\n",
            "Epoch 2164/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3688 - val_loss: 0.4044\n",
            "Epoch 2165/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3687 - val_loss: 0.4056\n",
            "Epoch 2166/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3685 - val_loss: 0.4063\n",
            "Epoch 2167/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3680 - val_loss: 0.4058\n",
            "Epoch 2168/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4062\n",
            "Epoch 2169/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4030\n",
            "Epoch 2170/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4043\n",
            "Epoch 2171/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4021\n",
            "Epoch 2172/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4045\n",
            "Epoch 2173/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3704 - val_loss: 0.4084\n",
            "Epoch 2174/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4040\n",
            "Epoch 2175/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4069\n",
            "Epoch 2176/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4016\n",
            "Epoch 2177/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4028\n",
            "Epoch 2178/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4086\n",
            "Epoch 2179/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4051\n",
            "Epoch 2180/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4070\n",
            "Epoch 2181/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3682 - val_loss: 0.4061\n",
            "Epoch 2182/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3696 - val_loss: 0.4055\n",
            "Epoch 2183/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3681 - val_loss: 0.4013\n",
            "Epoch 2184/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3682 - val_loss: 0.4033\n",
            "Epoch 2185/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4029\n",
            "Epoch 2186/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4056\n",
            "Epoch 2187/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4052\n",
            "Epoch 2188/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4095\n",
            "Epoch 2189/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4101\n",
            "Epoch 2190/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.4102\n",
            "Epoch 2191/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4046\n",
            "Epoch 2192/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4053\n",
            "Epoch 2193/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4077\n",
            "Epoch 2194/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4068\n",
            "Epoch 2195/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4049\n",
            "Epoch 2196/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4057\n",
            "Epoch 2197/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.4019\n",
            "Epoch 2198/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3681 - val_loss: 0.4050\n",
            "Epoch 2199/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4066\n",
            "Epoch 2200/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3680 - val_loss: 0.4096\n",
            "Epoch 2201/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3677 - val_loss: 0.4092\n",
            "Epoch 2202/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3689 - val_loss: 0.4056\n",
            "Epoch 2203/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3683 - val_loss: 0.4039\n",
            "Epoch 2204/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4044\n",
            "Epoch 2205/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4140\n",
            "Epoch 2206/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3695 - val_loss: 0.4074\n",
            "Epoch 2207/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4061\n",
            "Epoch 2208/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4048\n",
            "Epoch 2209/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4047\n",
            "Epoch 2210/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.4068\n",
            "Epoch 2211/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4070\n",
            "Epoch 2212/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3675 - val_loss: 0.4066\n",
            "Epoch 2213/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3676 - val_loss: 0.4101\n",
            "Epoch 2214/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4026\n",
            "Epoch 2215/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4079\n",
            "Epoch 2216/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3680 - val_loss: 0.4074\n",
            "Epoch 2217/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3722 - val_loss: 0.4109\n",
            "Epoch 2218/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3716 - val_loss: 0.4078\n",
            "Epoch 2219/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3704 - val_loss: 0.4088\n",
            "Epoch 2220/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.4021\n",
            "Epoch 2221/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.4043\n",
            "Epoch 2222/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3682 - val_loss: 0.4025\n",
            "Epoch 2223/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4033\n",
            "Epoch 2224/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4057\n",
            "Epoch 2225/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3675 - val_loss: 0.4078\n",
            "Epoch 2226/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4086\n",
            "Epoch 2227/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4069\n",
            "Epoch 2228/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.4112\n",
            "Epoch 2229/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4097\n",
            "Epoch 2230/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4102\n",
            "Epoch 2231/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4069\n",
            "Epoch 2232/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4059\n",
            "Epoch 2233/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3681 - val_loss: 0.4061\n",
            "Epoch 2234/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3693 - val_loss: 0.4051\n",
            "Epoch 2235/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3689 - val_loss: 0.4085\n",
            "Epoch 2236/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3700 - val_loss: 0.4058\n",
            "Epoch 2237/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3729 - val_loss: 0.4059\n",
            "Epoch 2238/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.4070\n",
            "Epoch 2239/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.4048\n",
            "Epoch 2240/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3997\n",
            "Epoch 2241/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4040\n",
            "Epoch 2242/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4000\n",
            "Epoch 2243/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4020\n",
            "Epoch 2244/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4070\n",
            "Epoch 2245/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3671 - val_loss: 0.4064\n",
            "Epoch 2246/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3671 - val_loss: 0.4051\n",
            "Epoch 2247/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4079\n",
            "Epoch 2248/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3680 - val_loss: 0.4052\n",
            "Epoch 2249/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3690 - val_loss: 0.4057\n",
            "Epoch 2250/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4065\n",
            "Epoch 2251/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3686 - val_loss: 0.4059\n",
            "Epoch 2252/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3676 - val_loss: 0.4062\n",
            "Epoch 2253/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3674 - val_loss: 0.4045\n",
            "Epoch 2254/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3681 - val_loss: 0.4058\n",
            "Epoch 2255/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4041\n",
            "Epoch 2256/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4044\n",
            "Epoch 2257/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4030\n",
            "Epoch 2258/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4068\n",
            "Epoch 2259/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3676 - val_loss: 0.4080\n",
            "Epoch 2260/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4038\n",
            "Epoch 2261/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3703 - val_loss: 0.4047\n",
            "Epoch 2262/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4076\n",
            "Epoch 2263/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4053\n",
            "Epoch 2264/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4033\n",
            "Epoch 2265/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3669 - val_loss: 0.4051\n",
            "Epoch 2266/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4109\n",
            "Epoch 2267/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.4028\n",
            "Epoch 2268/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3676 - val_loss: 0.4020\n",
            "Epoch 2269/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3681 - val_loss: 0.4060\n",
            "Epoch 2270/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3752 - val_loss: 0.4017\n",
            "Epoch 2271/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3729 - val_loss: 0.4048\n",
            "Epoch 2272/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.4060\n",
            "Epoch 2273/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3711 - val_loss: 0.4021\n",
            "Epoch 2274/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3984\n",
            "Epoch 2275/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4037\n",
            "Epoch 2276/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4013\n",
            "Epoch 2277/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.3976\n",
            "Epoch 2278/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3691 - val_loss: 0.4005\n",
            "Epoch 2279/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4022\n",
            "Epoch 2280/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3675 - val_loss: 0.4005\n",
            "Epoch 2281/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4046\n",
            "Epoch 2282/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.4082\n",
            "Epoch 2283/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4037\n",
            "Epoch 2284/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3673 - val_loss: 0.4006\n",
            "Epoch 2285/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3668 - val_loss: 0.4023\n",
            "Epoch 2286/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3674 - val_loss: 0.4056\n",
            "Epoch 2287/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3670 - val_loss: 0.4057\n",
            "Epoch 2288/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3681 - val_loss: 0.4037\n",
            "Epoch 2289/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4039\n",
            "Epoch 2290/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4014\n",
            "Epoch 2291/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4050\n",
            "Epoch 2292/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4076\n",
            "Epoch 2293/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3709 - val_loss: 0.4065\n",
            "Epoch 2294/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4009\n",
            "Epoch 2295/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4069\n",
            "Epoch 2296/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3675 - val_loss: 0.4027\n",
            "Epoch 2297/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4081\n",
            "Epoch 2298/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4017\n",
            "Epoch 2299/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4036\n",
            "Epoch 2300/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3678 - val_loss: 0.4017\n",
            "Epoch 2301/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4017\n",
            "Epoch 2302/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3675 - val_loss: 0.4059\n",
            "Epoch 2303/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3681 - val_loss: 0.4047\n",
            "Epoch 2304/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3675 - val_loss: 0.4034\n",
            "Epoch 2305/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3681 - val_loss: 0.4051\n",
            "Epoch 2306/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3677 - val_loss: 0.4045\n",
            "Epoch 2307/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4053\n",
            "Epoch 2308/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4063\n",
            "Epoch 2309/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4062\n",
            "Epoch 2310/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.4096\n",
            "Epoch 2311/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4051\n",
            "Epoch 2312/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4078\n",
            "Epoch 2313/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4042\n",
            "Epoch 2314/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4033\n",
            "Epoch 2315/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4052\n",
            "Epoch 2316/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3696 - val_loss: 0.4032\n",
            "Epoch 2317/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4041\n",
            "Epoch 2318/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4066\n",
            "Epoch 2319/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4037\n",
            "Epoch 2320/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3671 - val_loss: 0.4063\n",
            "Epoch 2321/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3665 - val_loss: 0.4083\n",
            "Epoch 2322/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3687 - val_loss: 0.4041\n",
            "Epoch 2323/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3677 - val_loss: 0.4040\n",
            "Epoch 2324/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3673 - val_loss: 0.4052\n",
            "Epoch 2325/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4077\n",
            "Epoch 2326/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4063\n",
            "Epoch 2327/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4083\n",
            "Epoch 2328/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4062\n",
            "Epoch 2329/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4058\n",
            "Epoch 2330/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3676 - val_loss: 0.4039\n",
            "Epoch 2331/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4044\n",
            "Epoch 2332/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4027\n",
            "Epoch 2333/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4081\n",
            "Epoch 2334/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4086\n",
            "Epoch 2335/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3695 - val_loss: 0.4018\n",
            "Epoch 2336/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4047\n",
            "Epoch 2337/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3685 - val_loss: 0.4056\n",
            "Epoch 2338/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3687 - val_loss: 0.4052\n",
            "Epoch 2339/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3692 - val_loss: 0.4026\n",
            "Epoch 2340/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3716 - val_loss: 0.4001\n",
            "Epoch 2341/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3684 - val_loss: 0.4051\n",
            "Epoch 2342/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4045\n",
            "Epoch 2343/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4039\n",
            "Epoch 2344/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4060\n",
            "Epoch 2345/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4057\n",
            "Epoch 2346/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3669 - val_loss: 0.4064\n",
            "Epoch 2347/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4025\n",
            "Epoch 2348/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4035\n",
            "Epoch 2349/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4024\n",
            "Epoch 2350/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3668 - val_loss: 0.4043\n",
            "Epoch 2351/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4063\n",
            "Epoch 2352/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4072\n",
            "Epoch 2353/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4085\n",
            "Epoch 2354/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3974\n",
            "Epoch 2355/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3678 - val_loss: 0.4038\n",
            "Epoch 2356/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3668 - val_loss: 0.3988\n",
            "Epoch 2357/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3667 - val_loss: 0.4024\n",
            "Epoch 2358/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3678 - val_loss: 0.4111\n",
            "Epoch 2359/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3679 - val_loss: 0.4039\n",
            "Epoch 2360/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4059\n",
            "Epoch 2361/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4017\n",
            "Epoch 2362/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4083\n",
            "Epoch 2363/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4063\n",
            "Epoch 2364/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3685 - val_loss: 0.4053\n",
            "Epoch 2365/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4053\n",
            "Epoch 2366/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4070\n",
            "Epoch 2367/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4031\n",
            "Epoch 2368/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4062\n",
            "Epoch 2369/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4060\n",
            "Epoch 2370/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4044\n",
            "Epoch 2371/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4046\n",
            "Epoch 2372/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3669 - val_loss: 0.4045\n",
            "Epoch 2373/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3667 - val_loss: 0.4073\n",
            "Epoch 2374/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3689 - val_loss: 0.4062\n",
            "Epoch 2375/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3674 - val_loss: 0.4014\n",
            "Epoch 2376/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4038\n",
            "Epoch 2377/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4036\n",
            "Epoch 2378/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4017\n",
            "Epoch 2379/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3670 - val_loss: 0.4041\n",
            "Epoch 2380/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4047\n",
            "Epoch 2381/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4049\n",
            "Epoch 2382/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.4072\n",
            "Epoch 2383/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.4022\n",
            "Epoch 2384/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.4052\n",
            "Epoch 2385/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.4016\n",
            "Epoch 2386/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4029\n",
            "Epoch 2387/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4051\n",
            "Epoch 2388/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3679 - val_loss: 0.4021\n",
            "Epoch 2389/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3682 - val_loss: 0.4043\n",
            "Epoch 2390/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3676 - val_loss: 0.4018\n",
            "Epoch 2391/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3680 - val_loss: 0.4056\n",
            "Epoch 2392/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3684 - val_loss: 0.4059\n",
            "Epoch 2393/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3674 - val_loss: 0.4081\n",
            "Epoch 2394/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4065\n",
            "Epoch 2395/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3694 - val_loss: 0.4022\n",
            "Epoch 2396/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4055\n",
            "Epoch 2397/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4057\n",
            "Epoch 2398/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3681 - val_loss: 0.4076\n",
            "Epoch 2399/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.4049\n",
            "Epoch 2400/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4080\n",
            "Epoch 2401/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4056\n",
            "Epoch 2402/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4086\n",
            "Epoch 2403/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4033\n",
            "Epoch 2404/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4057\n",
            "Epoch 2405/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3675 - val_loss: 0.4066\n",
            "Epoch 2406/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3668 - val_loss: 0.4069\n",
            "Epoch 2407/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3677 - val_loss: 0.4086\n",
            "Epoch 2408/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3682 - val_loss: 0.4076\n",
            "Epoch 2409/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3691 - val_loss: 0.4026\n",
            "Epoch 2410/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.4033\n",
            "Epoch 2411/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4042\n",
            "Epoch 2412/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.4031\n",
            "Epoch 2413/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4044\n",
            "Epoch 2414/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4047\n",
            "Epoch 2415/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4068\n",
            "Epoch 2416/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4070\n",
            "Epoch 2417/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4088\n",
            "Epoch 2418/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3668 - val_loss: 0.4045\n",
            "Epoch 2419/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4074\n",
            "Epoch 2420/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4041\n",
            "Epoch 2421/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4052\n",
            "Epoch 2422/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3681 - val_loss: 0.4051\n",
            "Epoch 2423/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3686 - val_loss: 0.4073\n",
            "Epoch 2424/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3688 - val_loss: 0.4024\n",
            "Epoch 2425/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3679 - val_loss: 0.4027\n",
            "Epoch 2426/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3680 - val_loss: 0.4008\n",
            "Epoch 2427/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4101\n",
            "Epoch 2428/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3668 - val_loss: 0.4066\n",
            "Epoch 2429/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4040\n",
            "Epoch 2430/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4055\n",
            "Epoch 2431/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4038\n",
            "Epoch 2432/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4087\n",
            "Epoch 2433/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4105\n",
            "Epoch 2434/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3685 - val_loss: 0.4097\n",
            "Epoch 2435/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4073\n",
            "Epoch 2436/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4085\n",
            "Epoch 2437/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4088\n",
            "Epoch 2438/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4050\n",
            "Epoch 2439/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4032\n",
            "Epoch 2440/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3672 - val_loss: 0.4065\n",
            "Epoch 2441/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3661 - val_loss: 0.4059\n",
            "Epoch 2442/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4043\n",
            "Epoch 2443/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.4036\n",
            "Epoch 2444/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3683 - val_loss: 0.4086\n",
            "Epoch 2445/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3685 - val_loss: 0.4070\n",
            "Epoch 2446/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4105\n",
            "Epoch 2447/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3683 - val_loss: 0.4107\n",
            "Epoch 2448/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4067\n",
            "Epoch 2449/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4078\n",
            "Epoch 2450/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4117\n",
            "Epoch 2451/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3669 - val_loss: 0.4056\n",
            "Epoch 2452/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4055\n",
            "Epoch 2453/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4080\n",
            "Epoch 2454/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4036\n",
            "Epoch 2455/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4041\n",
            "Epoch 2456/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3662 - val_loss: 0.4099\n",
            "Epoch 2457/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3674 - val_loss: 0.4075\n",
            "Epoch 2458/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3684 - val_loss: 0.4050\n",
            "Epoch 2459/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3684 - val_loss: 0.4059\n",
            "Epoch 2460/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3668 - val_loss: 0.4093\n",
            "Epoch 2461/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3673 - val_loss: 0.4063\n",
            "Epoch 2462/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3670 - val_loss: 0.4031\n",
            "Epoch 2463/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4063\n",
            "Epoch 2464/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4076\n",
            "Epoch 2465/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4128\n",
            "Epoch 2466/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4126\n",
            "Epoch 2467/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4081\n",
            "Epoch 2468/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4134\n",
            "Epoch 2469/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4075\n",
            "Epoch 2470/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4074\n",
            "Epoch 2471/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4106\n",
            "Epoch 2472/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4095\n",
            "Epoch 2473/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4095\n",
            "Epoch 2474/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3665 - val_loss: 0.4057\n",
            "Epoch 2475/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3680 - val_loss: 0.4074\n",
            "Epoch 2476/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3673 - val_loss: 0.4097\n",
            "Epoch 2477/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.4047\n",
            "Epoch 2478/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3670 - val_loss: 0.4086\n",
            "Epoch 2479/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4044\n",
            "Epoch 2480/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4087\n",
            "Epoch 2481/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.4147\n",
            "Epoch 2482/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4054\n",
            "Epoch 2483/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.4107\n",
            "Epoch 2484/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3685 - val_loss: 0.4129\n",
            "Epoch 2485/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4064\n",
            "Epoch 2486/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4082\n",
            "Epoch 2487/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4104\n",
            "Epoch 2488/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4105\n",
            "Epoch 2489/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4094\n",
            "Epoch 2490/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4081\n",
            "Epoch 2491/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3673 - val_loss: 0.4047\n",
            "Epoch 2492/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3672 - val_loss: 0.4099\n",
            "Epoch 2493/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3670 - val_loss: 0.4097\n",
            "Epoch 2494/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3676 - val_loss: 0.4029\n",
            "Epoch 2495/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4021\n",
            "Epoch 2496/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4093\n",
            "Epoch 2497/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4079\n",
            "Epoch 2498/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4084\n",
            "Epoch 2499/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4097\n",
            "Epoch 2500/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4077\n",
            "Epoch 2501/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4085\n",
            "Epoch 2502/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3675 - val_loss: 0.4004\n",
            "Epoch 2503/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4084\n",
            "Epoch 2504/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4080\n",
            "Epoch 2505/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3692 - val_loss: 0.4068\n",
            "Epoch 2506/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4063\n",
            "Epoch 2507/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3673 - val_loss: 0.4049\n",
            "Epoch 2508/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3667 - val_loss: 0.4094\n",
            "Epoch 2509/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3661 - val_loss: 0.4073\n",
            "Epoch 2510/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3661 - val_loss: 0.4069\n",
            "Epoch 2511/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3661 - val_loss: 0.4092\n",
            "Epoch 2512/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4067\n",
            "Epoch 2513/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3684 - val_loss: 0.4049\n",
            "Epoch 2514/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4097\n",
            "Epoch 2515/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4125\n",
            "Epoch 2516/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4123\n",
            "Epoch 2517/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4074\n",
            "Epoch 2518/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4061\n",
            "Epoch 2519/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4081\n",
            "Epoch 2520/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4084\n",
            "Epoch 2521/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4056\n",
            "Epoch 2522/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4088\n",
            "Epoch 2523/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4054\n",
            "Epoch 2524/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4086\n",
            "Epoch 2525/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3663 - val_loss: 0.4061\n",
            "Epoch 2526/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3679 - val_loss: 0.4098\n",
            "Epoch 2527/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3692 - val_loss: 0.4030\n",
            "Epoch 2528/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3693 - val_loss: 0.4066\n",
            "Epoch 2529/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4062\n",
            "Epoch 2530/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4040\n",
            "Epoch 2531/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3690 - val_loss: 0.4054\n",
            "Epoch 2532/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4119\n",
            "Epoch 2533/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4041\n",
            "Epoch 2534/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4100\n",
            "Epoch 2535/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3661 - val_loss: 0.4071\n",
            "Epoch 2536/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4111\n",
            "Epoch 2537/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3670 - val_loss: 0.4110\n",
            "Epoch 2538/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3669 - val_loss: 0.4109\n",
            "Epoch 2539/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4081\n",
            "Epoch 2540/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3667 - val_loss: 0.4101\n",
            "Epoch 2541/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3678 - val_loss: 0.4080\n",
            "Epoch 2542/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3663 - val_loss: 0.4105\n",
            "Epoch 2543/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3667 - val_loss: 0.4107\n",
            "Epoch 2544/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4111\n",
            "Epoch 2545/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3679 - val_loss: 0.4066\n",
            "Epoch 2546/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3661 - val_loss: 0.4037\n",
            "Epoch 2547/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4164\n",
            "Epoch 2548/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4052\n",
            "Epoch 2549/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4142\n",
            "Epoch 2550/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4103\n",
            "Epoch 2551/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3675 - val_loss: 0.4106\n",
            "Epoch 2552/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4127\n",
            "Epoch 2553/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4065\n",
            "Epoch 2554/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4110\n",
            "Epoch 2555/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4092\n",
            "Epoch 2556/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4071\n",
            "Epoch 2557/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4082\n",
            "Epoch 2558/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4060\n",
            "Epoch 2559/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3667 - val_loss: 0.4068\n",
            "Epoch 2560/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3668 - val_loss: 0.4061\n",
            "Epoch 2561/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3662 - val_loss: 0.4124\n",
            "Epoch 2562/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3670 - val_loss: 0.4091\n",
            "Epoch 2563/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4084\n",
            "Epoch 2564/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4086\n",
            "Epoch 2565/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3671 - val_loss: 0.4103\n",
            "Epoch 2566/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4064\n",
            "Epoch 2567/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4050\n",
            "Epoch 2568/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4114\n",
            "Epoch 2569/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4079\n",
            "Epoch 2570/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3668 - val_loss: 0.4065\n",
            "Epoch 2571/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4085\n",
            "Epoch 2572/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4016\n",
            "Epoch 2573/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3674 - val_loss: 0.4117\n",
            "Epoch 2574/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4079\n",
            "Epoch 2575/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4118\n",
            "Epoch 2576/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3665 - val_loss: 0.4082\n",
            "Epoch 2577/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3663 - val_loss: 0.4065\n",
            "Epoch 2578/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3655 - val_loss: 0.4027\n",
            "Epoch 2579/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3659 - val_loss: 0.4065\n",
            "Epoch 2580/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3659 - val_loss: 0.4038\n",
            "Epoch 2581/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4082\n",
            "Epoch 2582/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3712 - val_loss: 0.4076\n",
            "Epoch 2583/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4076\n",
            "Epoch 2584/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3678 - val_loss: 0.4077\n",
            "Epoch 2585/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4088\n",
            "Epoch 2586/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.3996\n",
            "Epoch 2587/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4017\n",
            "Epoch 2588/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4100\n",
            "Epoch 2589/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3681 - val_loss: 0.4057\n",
            "Epoch 2590/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4099\n",
            "Epoch 2591/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4045\n",
            "Epoch 2592/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4077\n",
            "Epoch 2593/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.4048\n",
            "Epoch 2594/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.4070\n",
            "Epoch 2595/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4083\n",
            "Epoch 2596/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3667 - val_loss: 0.4086\n",
            "Epoch 2597/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.4052\n",
            "Epoch 2598/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3695 - val_loss: 0.4106\n",
            "Epoch 2599/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3676 - val_loss: 0.4078\n",
            "Epoch 2600/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4059\n",
            "Epoch 2601/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4105\n",
            "Epoch 2602/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3669 - val_loss: 0.4115\n",
            "Epoch 2603/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4051\n",
            "Epoch 2604/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4066\n",
            "Epoch 2605/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4046\n",
            "Epoch 2606/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4076\n",
            "Epoch 2607/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4101\n",
            "Epoch 2608/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4047\n",
            "Epoch 2609/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4086\n",
            "Epoch 2610/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3663 - val_loss: 0.4104\n",
            "Epoch 2611/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4086\n",
            "Epoch 2612/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3668 - val_loss: 0.4064\n",
            "Epoch 2613/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3669 - val_loss: 0.4099\n",
            "Epoch 2614/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3675 - val_loss: 0.4077\n",
            "Epoch 2615/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3665 - val_loss: 0.4082\n",
            "Epoch 2616/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4084\n",
            "Epoch 2617/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3680 - val_loss: 0.4041\n",
            "Epoch 2618/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4066\n",
            "Epoch 2619/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3664 - val_loss: 0.4085\n",
            "Epoch 2620/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4071\n",
            "Epoch 2621/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4097\n",
            "Epoch 2622/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4036\n",
            "Epoch 2623/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4077\n",
            "Epoch 2624/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3662 - val_loss: 0.4126\n",
            "Epoch 2625/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4137\n",
            "Epoch 2626/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4098\n",
            "Epoch 2627/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4095\n",
            "Epoch 2628/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4090\n",
            "Epoch 2629/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3666 - val_loss: 0.4126\n",
            "Epoch 2630/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3676 - val_loss: 0.4094\n",
            "Epoch 2631/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3670 - val_loss: 0.4114\n",
            "Epoch 2632/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3669 - val_loss: 0.4099\n",
            "Epoch 2633/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4100\n",
            "Epoch 2634/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4109\n",
            "Epoch 2635/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4113\n",
            "Epoch 2636/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4093\n",
            "Epoch 2637/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4112\n",
            "Epoch 2638/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4139\n",
            "Epoch 2639/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4116\n",
            "Epoch 2640/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4105\n",
            "Epoch 2641/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4059\n",
            "Epoch 2642/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4085\n",
            "Epoch 2643/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.4074\n",
            "Epoch 2644/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4069\n",
            "Epoch 2645/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3672 - val_loss: 0.4117\n",
            "Epoch 2646/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3691 - val_loss: 0.4118\n",
            "Epoch 2647/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3688 - val_loss: 0.4104\n",
            "Epoch 2648/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3694 - val_loss: 0.4083\n",
            "Epoch 2649/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3684 - val_loss: 0.4062\n",
            "Epoch 2650/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3679 - val_loss: 0.4075\n",
            "Epoch 2651/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4079\n",
            "Epoch 2652/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4099\n",
            "Epoch 2653/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3664 - val_loss: 0.4099\n",
            "Epoch 2654/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4088\n",
            "Epoch 2655/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3663 - val_loss: 0.4114\n",
            "Epoch 2656/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4058\n",
            "Epoch 2657/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4125\n",
            "Epoch 2658/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4114\n",
            "Epoch 2659/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4107\n",
            "Epoch 2660/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4095\n",
            "Epoch 2661/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4075\n",
            "Epoch 2662/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4042\n",
            "Epoch 2663/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3667 - val_loss: 0.4068\n",
            "Epoch 2664/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3661 - val_loss: 0.4121\n",
            "Epoch 2665/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3669 - val_loss: 0.4096\n",
            "Epoch 2666/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3666 - val_loss: 0.4078\n",
            "Epoch 2667/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4132\n",
            "Epoch 2668/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3677 - val_loss: 0.4075\n",
            "Epoch 2669/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4079\n",
            "Epoch 2670/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4079\n",
            "Epoch 2671/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.4096\n",
            "Epoch 2672/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4063\n",
            "Epoch 2673/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4053\n",
            "Epoch 2674/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4091\n",
            "Epoch 2675/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4065\n",
            "Epoch 2676/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3661 - val_loss: 0.4102\n",
            "Epoch 2677/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4087\n",
            "Epoch 2678/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4078\n",
            "Epoch 2679/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3657 - val_loss: 0.4129\n",
            "Epoch 2680/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3659 - val_loss: 0.4138\n",
            "Epoch 2681/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3653 - val_loss: 0.4132\n",
            "Epoch 2682/3000\n",
            "70/70 [==============================] - 1s 19ms/step - loss: 0.3663 - val_loss: 0.4114\n",
            "Epoch 2683/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3675 - val_loss: 0.4106\n",
            "Epoch 2684/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4108\n",
            "Epoch 2685/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4143\n",
            "Epoch 2686/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4098\n",
            "Epoch 2687/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3672 - val_loss: 0.4110\n",
            "Epoch 2688/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4100\n",
            "Epoch 2689/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4124\n",
            "Epoch 2690/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4118\n",
            "Epoch 2691/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4107\n",
            "Epoch 2692/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4140\n",
            "Epoch 2693/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3671 - val_loss: 0.4053\n",
            "Epoch 2694/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4130\n",
            "Epoch 2695/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4101\n",
            "Epoch 2696/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3657 - val_loss: 0.4094\n",
            "Epoch 2697/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3649 - val_loss: 0.4112\n",
            "Epoch 2698/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3657 - val_loss: 0.4125\n",
            "Epoch 2699/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3664 - val_loss: 0.4107\n",
            "Epoch 2700/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3662 - val_loss: 0.4133\n",
            "Epoch 2701/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4123\n",
            "Epoch 2702/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3686 - val_loss: 0.4081\n",
            "Epoch 2703/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4128\n",
            "Epoch 2704/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4106\n",
            "Epoch 2705/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3667 - val_loss: 0.4093\n",
            "Epoch 2706/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4073\n",
            "Epoch 2707/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4109\n",
            "Epoch 2708/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4147\n",
            "Epoch 2709/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3698 - val_loss: 0.4092\n",
            "Epoch 2710/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4107\n",
            "Epoch 2711/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.4170\n",
            "Epoch 2712/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4081\n",
            "Epoch 2713/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3722 - val_loss: 0.4127\n",
            "Epoch 2714/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3708 - val_loss: 0.4106\n",
            "Epoch 2715/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3683 - val_loss: 0.4104\n",
            "Epoch 2716/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3669 - val_loss: 0.4086\n",
            "Epoch 2717/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4077\n",
            "Epoch 2718/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3663 - val_loss: 0.4111\n",
            "Epoch 2719/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4099\n",
            "Epoch 2720/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4099\n",
            "Epoch 2721/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4092\n",
            "Epoch 2722/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4103\n",
            "Epoch 2723/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - val_loss: 0.4069\n",
            "Epoch 2724/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4084\n",
            "Epoch 2725/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3654 - val_loss: 0.4082\n",
            "Epoch 2726/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4074\n",
            "Epoch 2727/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4045\n",
            "Epoch 2728/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4084\n",
            "Epoch 2729/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4074\n",
            "Epoch 2730/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3660 - val_loss: 0.4087\n",
            "Epoch 2731/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3665 - val_loss: 0.4071\n",
            "Epoch 2732/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3689 - val_loss: 0.4073\n",
            "Epoch 2733/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3698 - val_loss: 0.4112\n",
            "Epoch 2734/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4074\n",
            "Epoch 2735/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4101\n",
            "Epoch 2736/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4063\n",
            "Epoch 2737/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4065\n",
            "Epoch 2738/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4096\n",
            "Epoch 2739/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4066\n",
            "Epoch 2740/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4043\n",
            "Epoch 2741/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3664 - val_loss: 0.4058\n",
            "Epoch 2742/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4123\n",
            "Epoch 2743/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4091\n",
            "Epoch 2744/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3664 - val_loss: 0.4120\n",
            "Epoch 2745/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4072\n",
            "Epoch 2746/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3664 - val_loss: 0.4107\n",
            "Epoch 2747/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3658 - val_loss: 0.4105\n",
            "Epoch 2748/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3664 - val_loss: 0.4079\n",
            "Epoch 2749/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3657 - val_loss: 0.4094\n",
            "Epoch 2750/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4121\n",
            "Epoch 2751/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3650 - val_loss: 0.4061\n",
            "Epoch 2752/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4140\n",
            "Epoch 2753/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3678 - val_loss: 0.4138\n",
            "Epoch 2754/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.4162\n",
            "Epoch 2755/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.4023\n",
            "Epoch 2756/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.4065\n",
            "Epoch 2757/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4035\n",
            "Epoch 2758/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3677 - val_loss: 0.4067\n",
            "Epoch 2759/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4077\n",
            "Epoch 2760/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4040\n",
            "Epoch 2761/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4054\n",
            "Epoch 2762/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4066\n",
            "Epoch 2763/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3659 - val_loss: 0.4079\n",
            "Epoch 2764/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3650 - val_loss: 0.4116\n",
            "Epoch 2765/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3649 - val_loss: 0.4070\n",
            "Epoch 2766/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3654 - val_loss: 0.4089\n",
            "Epoch 2767/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4108\n",
            "Epoch 2768/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3651 - val_loss: 0.4084\n",
            "Epoch 2769/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4135\n",
            "Epoch 2770/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4092\n",
            "Epoch 2771/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4111\n",
            "Epoch 2772/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4078\n",
            "Epoch 2773/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4079\n",
            "Epoch 2774/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3662 - val_loss: 0.4079\n",
            "Epoch 2775/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4117\n",
            "Epoch 2776/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3673 - val_loss: 0.4064\n",
            "Epoch 2777/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3672 - val_loss: 0.4086\n",
            "Epoch 2778/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3653 - val_loss: 0.4093\n",
            "Epoch 2779/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4054\n",
            "Epoch 2780/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4113\n",
            "Epoch 2781/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3661 - val_loss: 0.4116\n",
            "Epoch 2782/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4102\n",
            "Epoch 2783/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3676 - val_loss: 0.4046\n",
            "Epoch 2784/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3660 - val_loss: 0.4098\n",
            "Epoch 2785/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4089\n",
            "Epoch 2786/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4068\n",
            "Epoch 2787/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4119\n",
            "Epoch 2788/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4098\n",
            "Epoch 2789/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3652 - val_loss: 0.4101\n",
            "Epoch 2790/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3649 - val_loss: 0.4101\n",
            "Epoch 2791/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3664 - val_loss: 0.4090\n",
            "Epoch 2792/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4115\n",
            "Epoch 2793/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4129\n",
            "Epoch 2794/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4101\n",
            "Epoch 2795/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4059\n",
            "Epoch 2796/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4110\n",
            "Epoch 2797/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3656 - val_loss: 0.4126\n",
            "Epoch 2798/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3673 - val_loss: 0.4149\n",
            "Epoch 2799/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3662 - val_loss: 0.4102\n",
            "Epoch 2800/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3663 - val_loss: 0.4174\n",
            "Epoch 2801/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3677 - val_loss: 0.4112\n",
            "Epoch 2802/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4110\n",
            "Epoch 2803/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4127\n",
            "Epoch 2804/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4102\n",
            "Epoch 2805/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4109\n",
            "Epoch 2806/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4093\n",
            "Epoch 2807/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4098\n",
            "Epoch 2808/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4128\n",
            "Epoch 2809/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4138\n",
            "Epoch 2810/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4163\n",
            "Epoch 2811/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.4101\n",
            "Epoch 2812/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4135\n",
            "Epoch 2813/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4054\n",
            "Epoch 2814/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3653 - val_loss: 0.4137\n",
            "Epoch 2815/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3655 - val_loss: 0.4069\n",
            "Epoch 2816/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3659 - val_loss: 0.4086\n",
            "Epoch 2817/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3660 - val_loss: 0.4130\n",
            "Epoch 2818/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4120\n",
            "Epoch 2819/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3661 - val_loss: 0.4069\n",
            "Epoch 2820/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3660 - val_loss: 0.4063\n",
            "Epoch 2821/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4123\n",
            "Epoch 2822/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3682 - val_loss: 0.4123\n",
            "Epoch 2823/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3662 - val_loss: 0.4123\n",
            "Epoch 2824/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4089\n",
            "Epoch 2825/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3667 - val_loss: 0.4114\n",
            "Epoch 2826/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3653 - val_loss: 0.4075\n",
            "Epoch 2827/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4102\n",
            "Epoch 2828/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4075\n",
            "Epoch 2829/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4112\n",
            "Epoch 2830/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3659 - val_loss: 0.4102\n",
            "Epoch 2831/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3656 - val_loss: 0.4132\n",
            "Epoch 2832/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3670 - val_loss: 0.4112\n",
            "Epoch 2833/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3668 - val_loss: 0.4116\n",
            "Epoch 2834/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3669 - val_loss: 0.4138\n",
            "Epoch 2835/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3674 - val_loss: 0.4115\n",
            "Epoch 2836/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3679 - val_loss: 0.4081\n",
            "Epoch 2837/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4104\n",
            "Epoch 2838/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4047\n",
            "Epoch 2839/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4097\n",
            "Epoch 2840/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4116\n",
            "Epoch 2841/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4107\n",
            "Epoch 2842/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4087\n",
            "Epoch 2843/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3660 - val_loss: 0.4098\n",
            "Epoch 2844/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4131\n",
            "Epoch 2845/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4131\n",
            "Epoch 2846/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3675 - val_loss: 0.4157\n",
            "Epoch 2847/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3664 - val_loss: 0.4140\n",
            "Epoch 2848/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3655 - val_loss: 0.4118\n",
            "Epoch 2849/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3650 - val_loss: 0.4104\n",
            "Epoch 2850/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3660 - val_loss: 0.4122\n",
            "Epoch 2851/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4114\n",
            "Epoch 2852/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4121\n",
            "Epoch 2853/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4118\n",
            "Epoch 2854/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4097\n",
            "Epoch 2855/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4133\n",
            "Epoch 2856/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4093\n",
            "Epoch 2857/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4102\n",
            "Epoch 2858/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4114\n",
            "Epoch 2859/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3662 - val_loss: 0.4142\n",
            "Epoch 2860/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4104\n",
            "Epoch 2861/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3657 - val_loss: 0.4109\n",
            "Epoch 2862/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4066\n",
            "Epoch 2863/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3674 - val_loss: 0.4086\n",
            "Epoch 2864/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3673 - val_loss: 0.4105\n",
            "Epoch 2865/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3681 - val_loss: 0.4083\n",
            "Epoch 2866/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3667 - val_loss: 0.4073\n",
            "Epoch 2867/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3656 - val_loss: 0.4146\n",
            "Epoch 2868/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3669 - val_loss: 0.4118\n",
            "Epoch 2869/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4142\n",
            "Epoch 2870/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4117\n",
            "Epoch 2871/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4134\n",
            "Epoch 2872/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3652 - val_loss: 0.4087\n",
            "Epoch 2873/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4144\n",
            "Epoch 2874/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3668 - val_loss: 0.4126\n",
            "Epoch 2875/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4100\n",
            "Epoch 2876/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4062\n",
            "Epoch 2877/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4100\n",
            "Epoch 2878/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4121\n",
            "Epoch 2879/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4120\n",
            "Epoch 2880/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3667 - val_loss: 0.4121\n",
            "Epoch 2881/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4134\n",
            "Epoch 2882/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3666 - val_loss: 0.4093\n",
            "Epoch 2883/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3663 - val_loss: 0.4122\n",
            "Epoch 2884/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3662 - val_loss: 0.4129\n",
            "Epoch 2885/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3657 - val_loss: 0.4119\n",
            "Epoch 2886/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4079\n",
            "Epoch 2887/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4095\n",
            "Epoch 2888/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4070\n",
            "Epoch 2889/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4064\n",
            "Epoch 2890/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3673 - val_loss: 0.4067\n",
            "Epoch 2891/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4087\n",
            "Epoch 2892/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3662 - val_loss: 0.4114\n",
            "Epoch 2893/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4088\n",
            "Epoch 2894/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4079\n",
            "Epoch 2895/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3656 - val_loss: 0.4082\n",
            "Epoch 2896/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3676 - val_loss: 0.4136\n",
            "Epoch 2897/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3663 - val_loss: 0.4132\n",
            "Epoch 2898/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3663 - val_loss: 0.4098\n",
            "Epoch 2899/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3664 - val_loss: 0.4138\n",
            "Epoch 2900/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3662 - val_loss: 0.4112\n",
            "Epoch 2901/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3652 - val_loss: 0.4100\n",
            "Epoch 2902/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4103\n",
            "Epoch 2903/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3650 - val_loss: 0.4098\n",
            "Epoch 2904/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4142\n",
            "Epoch 2905/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4103\n",
            "Epoch 2906/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4121\n",
            "Epoch 2907/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4060\n",
            "Epoch 2908/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4123\n",
            "Epoch 2909/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4124\n",
            "Epoch 2910/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4123\n",
            "Epoch 2911/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4115\n",
            "Epoch 2912/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3667 - val_loss: 0.4131\n",
            "Epoch 2913/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4103\n",
            "Epoch 2914/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3656 - val_loss: 0.4094\n",
            "Epoch 2915/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3658 - val_loss: 0.4111\n",
            "Epoch 2916/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3656 - val_loss: 0.4153\n",
            "Epoch 2917/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3656 - val_loss: 0.4065\n",
            "Epoch 2918/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3654 - val_loss: 0.4146\n",
            "Epoch 2919/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3650 - val_loss: 0.4166\n",
            "Epoch 2920/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4115\n",
            "Epoch 2921/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3661 - val_loss: 0.4124\n",
            "Epoch 2922/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3670 - val_loss: 0.4126\n",
            "Epoch 2923/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3659 - val_loss: 0.4104\n",
            "Epoch 2924/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3655 - val_loss: 0.4161\n",
            "Epoch 2925/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4115\n",
            "Epoch 2926/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3660 - val_loss: 0.4138\n",
            "Epoch 2927/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4106\n",
            "Epoch 2928/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3655 - val_loss: 0.4106\n",
            "Epoch 2929/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3664 - val_loss: 0.4130\n",
            "Epoch 2930/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4131\n",
            "Epoch 2931/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3657 - val_loss: 0.4141\n",
            "Epoch 2932/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3660 - val_loss: 0.4122\n",
            "Epoch 2933/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3657 - val_loss: 0.4138\n",
            "Epoch 2934/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3662 - val_loss: 0.4129\n",
            "Epoch 2935/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3653 - val_loss: 0.4115\n",
            "Epoch 2936/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3653 - val_loss: 0.4155\n",
            "Epoch 2937/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3654 - val_loss: 0.4138\n",
            "Epoch 2938/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4091\n",
            "Epoch 2939/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.4099\n",
            "Epoch 2940/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.4158\n",
            "Epoch 2941/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3668 - val_loss: 0.4112\n",
            "Epoch 2942/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3665 - val_loss: 0.4071\n",
            "Epoch 2943/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4120\n",
            "Epoch 2944/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4119\n",
            "Epoch 2945/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4071\n",
            "Epoch 2946/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4119\n",
            "Epoch 2947/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3665 - val_loss: 0.4091\n",
            "Epoch 2948/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3661 - val_loss: 0.4120\n",
            "Epoch 2949/3000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3649 - val_loss: 0.4093\n",
            "Epoch 2950/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3654 - val_loss: 0.4097\n",
            "Epoch 2951/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3653 - val_loss: 0.4124\n",
            "Epoch 2952/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3665 - val_loss: 0.4102\n",
            "Epoch 2953/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4082\n",
            "Epoch 2954/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3674 - val_loss: 0.4108\n",
            "Epoch 2955/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3656 - val_loss: 0.4108\n",
            "Epoch 2956/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4085\n",
            "Epoch 2957/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3661 - val_loss: 0.4170\n",
            "Epoch 2958/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3671 - val_loss: 0.4132\n",
            "Epoch 2959/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4135\n",
            "Epoch 2960/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3657 - val_loss: 0.4115\n",
            "Epoch 2961/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3653 - val_loss: 0.4129\n",
            "Epoch 2962/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3669 - val_loss: 0.4100\n",
            "Epoch 2963/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3675 - val_loss: 0.4113\n",
            "Epoch 2964/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3654 - val_loss: 0.4110\n",
            "Epoch 2965/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3666 - val_loss: 0.4117\n",
            "Epoch 2966/3000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3669 - val_loss: 0.4069\n",
            "Epoch 2967/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3660 - val_loss: 0.4155\n",
            "Epoch 2968/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3659 - val_loss: 0.4098\n",
            "Epoch 2969/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3647 - val_loss: 0.4125\n",
            "Epoch 2970/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3652 - val_loss: 0.4120\n",
            "Epoch 2971/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3655 - val_loss: 0.4137\n",
            "Epoch 2972/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4102\n",
            "Epoch 2973/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3652 - val_loss: 0.4141\n",
            "Epoch 2974/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4111\n",
            "Epoch 2975/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4101\n",
            "Epoch 2976/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3651 - val_loss: 0.4107\n",
            "Epoch 2977/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3657 - val_loss: 0.4160\n",
            "Epoch 2978/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3672 - val_loss: 0.4139\n",
            "Epoch 2979/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3653 - val_loss: 0.4172\n",
            "Epoch 2980/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3666 - val_loss: 0.4062\n",
            "Epoch 2981/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3674 - val_loss: 0.4158\n",
            "Epoch 2982/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3678 - val_loss: 0.4140\n",
            "Epoch 2983/3000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3668 - val_loss: 0.4128\n",
            "Epoch 2984/3000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3659 - val_loss: 0.4103\n",
            "Epoch 2985/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3654 - val_loss: 0.4088\n",
            "Epoch 2986/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4154\n",
            "Epoch 2987/3000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3652 - val_loss: 0.4120\n",
            "Epoch 2988/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3674 - val_loss: 0.4107\n",
            "Epoch 2989/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3651 - val_loss: 0.4086\n",
            "Epoch 2990/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3650 - val_loss: 0.4124\n",
            "Epoch 2991/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3650 - val_loss: 0.4114\n",
            "Epoch 2992/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3649 - val_loss: 0.4129\n",
            "Epoch 2993/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4136\n",
            "Epoch 2994/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3654 - val_loss: 0.4117\n",
            "Epoch 2995/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3671 - val_loss: 0.4079\n",
            "Epoch 2996/3000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3658 - val_loss: 0.4116\n",
            "Epoch 2997/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3663 - val_loss: 0.4175\n",
            "Epoch 2998/3000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3677 - val_loss: 0.4173\n",
            "Epoch 2999/3000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3723 - val_loss: 0.4158\n",
            "Epoch 3000/3000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.4071\n",
            "8/8 [==============================] - 1s 8ms/step - loss: 0.4071\n",
            "8/8 [==============================] - 1s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb090577b20>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=3000;    # training epochs\n",
        "temperature = 0.1    # temprature control the smooth of the probabilities\n",
        "\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(con_value)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "yn1Ny_xiXDfL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZQ1q1t2hfPU"
      },
      "outputs": [],
      "source": [],
      "id": "vZQ1q1t2hfPU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uYwG8d50qQf"
      },
      "source": [
        "## 2.4 Add Hidden Layers before Softmax Layer ##"
      ],
      "id": "2uYwG8d50qQf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gJxJD_M41H_v",
        "outputId": "20423409-44ac-42b5-828c-014532e04786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "70/70 [==============================] - 21s 16ms/step - loss: 0.7285 - val_loss: 0.4696\n",
            "Epoch 2/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4842 - val_loss: 0.4503\n",
            "Epoch 3/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4717 - val_loss: 0.4372\n",
            "Epoch 4/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4666 - val_loss: 0.4616\n",
            "Epoch 5/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4655 - val_loss: 0.4390\n",
            "Epoch 6/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4628 - val_loss: 0.4297\n",
            "Epoch 7/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4556 - val_loss: 0.4411\n",
            "Epoch 8/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4534 - val_loss: 0.4369\n",
            "Epoch 9/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4516 - val_loss: 0.4200\n",
            "Epoch 10/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4470 - val_loss: 0.4287\n",
            "Epoch 11/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4524 - val_loss: 0.4225\n",
            "Epoch 12/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4426 - val_loss: 0.4269\n",
            "Epoch 13/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4443 - val_loss: 0.4138\n",
            "Epoch 14/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4400 - val_loss: 0.4145\n",
            "Epoch 15/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4389 - val_loss: 0.4321\n",
            "Epoch 16/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4342 - val_loss: 0.4115\n",
            "Epoch 17/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4332 - val_loss: 0.4044\n",
            "Epoch 18/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4395 - val_loss: 0.4117\n",
            "Epoch 19/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4325 - val_loss: 0.4118\n",
            "Epoch 20/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4331 - val_loss: 0.4140\n",
            "Epoch 21/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4303 - val_loss: 0.4028\n",
            "Epoch 22/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4260 - val_loss: 0.4004\n",
            "Epoch 23/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4276 - val_loss: 0.4109\n",
            "Epoch 24/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4253 - val_loss: 0.4006\n",
            "Epoch 25/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4223 - val_loss: 0.4031\n",
            "Epoch 26/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4220 - val_loss: 0.4020\n",
            "Epoch 27/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4205 - val_loss: 0.3972\n",
            "Epoch 28/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4186 - val_loss: 0.3961\n",
            "Epoch 29/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4195 - val_loss: 0.3991\n",
            "Epoch 30/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4176 - val_loss: 0.3985\n",
            "Epoch 31/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4161 - val_loss: 0.3910\n",
            "Epoch 32/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4193 - val_loss: 0.3988\n",
            "Epoch 33/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4189 - val_loss: 0.3910\n",
            "Epoch 34/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4131 - val_loss: 0.4149\n",
            "Epoch 35/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4215 - val_loss: 0.3935\n",
            "Epoch 36/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4155 - val_loss: 0.3962\n",
            "Epoch 37/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4160 - val_loss: 0.3930\n",
            "Epoch 38/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4105 - val_loss: 0.3988\n",
            "Epoch 39/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4114 - val_loss: 0.3905\n",
            "Epoch 40/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4113 - val_loss: 0.3943\n",
            "Epoch 41/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4202 - val_loss: 0.3958\n",
            "Epoch 42/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4102 - val_loss: 0.4107\n",
            "Epoch 43/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4139 - val_loss: 0.3937\n",
            "Epoch 44/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4159 - val_loss: 0.3903\n",
            "Epoch 45/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4077 - val_loss: 0.3889\n",
            "Epoch 46/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4063 - val_loss: 0.3879\n",
            "Epoch 47/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4111 - val_loss: 0.3870\n",
            "Epoch 48/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4130 - val_loss: 0.3919\n",
            "Epoch 49/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4122 - val_loss: 0.3955\n",
            "Epoch 50/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4073 - val_loss: 0.3875\n",
            "Epoch 51/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4057 - val_loss: 0.3886\n",
            "Epoch 52/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4049 - val_loss: 0.3885\n",
            "Epoch 53/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4068 - val_loss: 0.3896\n",
            "Epoch 54/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4050 - val_loss: 0.3829\n",
            "Epoch 55/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4059 - val_loss: 0.3849\n",
            "Epoch 56/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4033 - val_loss: 0.3844\n",
            "Epoch 57/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4071 - val_loss: 0.3930\n",
            "Epoch 58/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4045 - val_loss: 0.3868\n",
            "Epoch 59/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4069 - val_loss: 0.3895\n",
            "Epoch 60/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4018 - val_loss: 0.3899\n",
            "Epoch 61/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4057 - val_loss: 0.3887\n",
            "Epoch 62/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4043 - val_loss: 0.3831\n",
            "Epoch 63/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4024 - val_loss: 0.3838\n",
            "Epoch 64/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4030 - val_loss: 0.3841\n",
            "Epoch 65/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4056 - val_loss: 0.3875\n",
            "Epoch 66/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4021 - val_loss: 0.3840\n",
            "Epoch 67/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4006 - val_loss: 0.3841\n",
            "Epoch 68/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4112 - val_loss: 0.3931\n",
            "Epoch 69/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4043 - val_loss: 0.3857\n",
            "Epoch 70/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4034 - val_loss: 0.3861\n",
            "Epoch 71/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4028 - val_loss: 0.3831\n",
            "Epoch 72/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4015 - val_loss: 0.3815\n",
            "Epoch 73/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4044 - val_loss: 0.3899\n",
            "Epoch 74/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4003 - val_loss: 0.3841\n",
            "Epoch 75/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4007 - val_loss: 0.3820\n",
            "Epoch 76/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4002 - val_loss: 0.3835\n",
            "Epoch 77/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4006 - val_loss: 0.3822\n",
            "Epoch 78/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4040 - val_loss: 0.3805\n",
            "Epoch 79/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3992 - val_loss: 0.3839\n",
            "Epoch 80/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3988 - val_loss: 0.3846\n",
            "Epoch 81/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4036 - val_loss: 0.3782\n",
            "Epoch 82/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4029 - val_loss: 0.4041\n",
            "Epoch 83/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4002 - val_loss: 0.3823\n",
            "Epoch 84/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4007 - val_loss: 0.3826\n",
            "Epoch 85/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3980 - val_loss: 0.3824\n",
            "Epoch 86/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3987 - val_loss: 0.3831\n",
            "Epoch 87/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4014 - val_loss: 0.3852\n",
            "Epoch 88/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4040 - val_loss: 0.3825\n",
            "Epoch 89/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4000 - val_loss: 0.3824\n",
            "Epoch 90/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3994 - val_loss: 0.3821\n",
            "Epoch 91/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4004 - val_loss: 0.3802\n",
            "Epoch 92/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3981 - val_loss: 0.3903\n",
            "Epoch 93/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4000 - val_loss: 0.3893\n",
            "Epoch 94/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4005 - val_loss: 0.3843\n",
            "Epoch 95/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3992 - val_loss: 0.3847\n",
            "Epoch 96/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4016 - val_loss: 0.3846\n",
            "Epoch 97/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3984 - val_loss: 0.3773\n",
            "Epoch 98/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3971 - val_loss: 0.3789\n",
            "Epoch 99/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3963 - val_loss: 0.3853\n",
            "Epoch 100/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3990 - val_loss: 0.3875\n",
            "Epoch 101/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3991 - val_loss: 0.3766\n",
            "Epoch 102/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3981 - val_loss: 0.3834\n",
            "Epoch 103/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3974 - val_loss: 0.3797\n",
            "Epoch 104/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3985 - val_loss: 0.3802\n",
            "Epoch 105/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4003 - val_loss: 0.3859\n",
            "Epoch 106/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3981 - val_loss: 0.3827\n",
            "Epoch 107/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3987 - val_loss: 0.3769\n",
            "Epoch 108/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3982 - val_loss: 0.3835\n",
            "Epoch 109/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3982 - val_loss: 0.3883\n",
            "Epoch 110/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4018 - val_loss: 0.3782\n",
            "Epoch 111/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3954 - val_loss: 0.3812\n",
            "Epoch 112/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3967 - val_loss: 0.3780\n",
            "Epoch 113/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3982 - val_loss: 0.3868\n",
            "Epoch 114/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.4023 - val_loss: 0.3804\n",
            "Epoch 115/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3966 - val_loss: 0.3822\n",
            "Epoch 116/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4026 - val_loss: 0.3819\n",
            "Epoch 117/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3973 - val_loss: 0.3832\n",
            "Epoch 118/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3956 - val_loss: 0.3832\n",
            "Epoch 119/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3970 - val_loss: 0.3841\n",
            "Epoch 120/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3942 - val_loss: 0.3839\n",
            "Epoch 121/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3993 - val_loss: 0.3780\n",
            "Epoch 122/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3959 - val_loss: 0.3787\n",
            "Epoch 123/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3960 - val_loss: 0.3818\n",
            "Epoch 124/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3967 - val_loss: 0.3798\n",
            "Epoch 125/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3950 - val_loss: 0.3858\n",
            "Epoch 126/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3932 - val_loss: 0.3808\n",
            "Epoch 127/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.3955 - val_loss: 0.3831\n",
            "Epoch 128/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3962 - val_loss: 0.3816\n",
            "Epoch 129/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3943 - val_loss: 0.3785\n",
            "Epoch 130/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3938 - val_loss: 0.3817\n",
            "Epoch 131/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3949 - val_loss: 0.3771\n",
            "Epoch 132/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3990 - val_loss: 0.3815\n",
            "Epoch 133/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3920 - val_loss: 0.3845\n",
            "Epoch 134/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3924 - val_loss: 0.3796\n",
            "Epoch 135/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3941 - val_loss: 0.3772\n",
            "Epoch 136/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3912 - val_loss: 0.3816\n",
            "Epoch 137/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3919 - val_loss: 0.3779\n",
            "Epoch 138/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3932 - val_loss: 0.3809\n",
            "Epoch 139/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3943 - val_loss: 0.3796\n",
            "Epoch 140/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3949 - val_loss: 0.3842\n",
            "Epoch 141/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3950 - val_loss: 0.3818\n",
            "Epoch 142/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.3793\n",
            "Epoch 143/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3941 - val_loss: 0.3800\n",
            "Epoch 144/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3985 - val_loss: 0.3839\n",
            "Epoch 145/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4012 - val_loss: 0.3837\n",
            "Epoch 146/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3927 - val_loss: 0.3764\n",
            "Epoch 147/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3953 - val_loss: 0.3868\n",
            "Epoch 148/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3946 - val_loss: 0.3791\n",
            "Epoch 149/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3902 - val_loss: 0.3760\n",
            "Epoch 150/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3908 - val_loss: 0.3773\n",
            "Epoch 151/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3921 - val_loss: 0.3788\n",
            "Epoch 152/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3955 - val_loss: 0.3824\n",
            "Epoch 153/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3927 - val_loss: 0.3794\n",
            "Epoch 154/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3919 - val_loss: 0.3799\n",
            "Epoch 155/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3934 - val_loss: 0.3828\n",
            "Epoch 156/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3914 - val_loss: 0.3807\n",
            "Epoch 157/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3911 - val_loss: 0.3838\n",
            "Epoch 158/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3804\n",
            "Epoch 159/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3926 - val_loss: 0.3863\n",
            "Epoch 160/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3909 - val_loss: 0.3879\n",
            "Epoch 161/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3919 - val_loss: 0.3848\n",
            "Epoch 162/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4009 - val_loss: 0.3774\n",
            "Epoch 163/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3905 - val_loss: 0.3837\n",
            "Epoch 164/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3930 - val_loss: 0.3769\n",
            "Epoch 165/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3928 - val_loss: 0.3831\n",
            "Epoch 166/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3776\n",
            "Epoch 167/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3930 - val_loss: 0.3763\n",
            "Epoch 168/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3917 - val_loss: 0.3845\n",
            "Epoch 169/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3903 - val_loss: 0.3773\n",
            "Epoch 170/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3932 - val_loss: 0.3939\n",
            "Epoch 171/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3928 - val_loss: 0.3817\n",
            "Epoch 172/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3904 - val_loss: 0.3780\n",
            "Epoch 173/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3907 - val_loss: 0.3823\n",
            "Epoch 174/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3910 - val_loss: 0.3792\n",
            "Epoch 175/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3890 - val_loss: 0.3805\n",
            "Epoch 176/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.3772\n",
            "Epoch 177/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3898 - val_loss: 0.3778\n",
            "Epoch 178/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3930 - val_loss: 0.3892\n",
            "Epoch 179/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3937 - val_loss: 0.3824\n",
            "Epoch 180/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3927 - val_loss: 0.3748\n",
            "Epoch 181/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3927 - val_loss: 0.3781\n",
            "Epoch 182/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3931 - val_loss: 0.3787\n",
            "Epoch 183/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.3800\n",
            "Epoch 184/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3908 - val_loss: 0.3784\n",
            "Epoch 185/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3903 - val_loss: 0.3757\n",
            "Epoch 186/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3897 - val_loss: 0.3803\n",
            "Epoch 187/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3930 - val_loss: 0.3816\n",
            "Epoch 188/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3890 - val_loss: 0.3774\n",
            "Epoch 189/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3907 - val_loss: 0.3847\n",
            "Epoch 190/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3881 - val_loss: 0.3758\n",
            "Epoch 191/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3819\n",
            "Epoch 192/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3879 - val_loss: 0.3839\n",
            "Epoch 193/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3885 - val_loss: 0.3800\n",
            "Epoch 194/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3895 - val_loss: 0.3811\n",
            "Epoch 195/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3904 - val_loss: 0.3851\n",
            "Epoch 196/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3891 - val_loss: 0.3767\n",
            "Epoch 197/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3881 - val_loss: 0.3785\n",
            "Epoch 198/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3881 - val_loss: 0.3790\n",
            "Epoch 199/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3909 - val_loss: 0.3761\n",
            "Epoch 200/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3902 - val_loss: 0.3749\n",
            "Epoch 201/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3907 - val_loss: 0.3769\n",
            "Epoch 202/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3873 - val_loss: 0.3939\n",
            "Epoch 203/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3932 - val_loss: 0.3779\n",
            "Epoch 204/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3891 - val_loss: 0.3749\n",
            "Epoch 205/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3885 - val_loss: 0.3865\n",
            "Epoch 206/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3909 - val_loss: 0.3810\n",
            "Epoch 207/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3906 - val_loss: 0.3756\n",
            "Epoch 208/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3891 - val_loss: 0.3748\n",
            "Epoch 209/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3881 - val_loss: 0.3760\n",
            "Epoch 210/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3902 - val_loss: 0.3916\n",
            "Epoch 211/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3888 - val_loss: 0.3774\n",
            "Epoch 212/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3803\n",
            "Epoch 213/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3903 - val_loss: 0.3739\n",
            "Epoch 214/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3873 - val_loss: 0.3782\n",
            "Epoch 215/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3869 - val_loss: 0.3789\n",
            "Epoch 216/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3752\n",
            "Epoch 217/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3775\n",
            "Epoch 218/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3812\n",
            "Epoch 219/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3888 - val_loss: 0.3779\n",
            "Epoch 220/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3889 - val_loss: 0.3806\n",
            "Epoch 221/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3874 - val_loss: 0.3738\n",
            "Epoch 222/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3861 - val_loss: 0.3784\n",
            "Epoch 223/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3873 - val_loss: 0.3774\n",
            "Epoch 224/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3893 - val_loss: 0.3799\n",
            "Epoch 225/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3872 - val_loss: 0.3781\n",
            "Epoch 226/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3872 - val_loss: 0.3764\n",
            "Epoch 227/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.3882\n",
            "Epoch 228/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - val_loss: 0.3844\n",
            "Epoch 229/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3891 - val_loss: 0.3768\n",
            "Epoch 230/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3847 - val_loss: 0.3779\n",
            "Epoch 231/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3851 - val_loss: 0.3845\n",
            "Epoch 232/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3907 - val_loss: 0.3778\n",
            "Epoch 233/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3782\n",
            "Epoch 234/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3877 - val_loss: 0.3906\n",
            "Epoch 235/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3924 - val_loss: 0.3803\n",
            "Epoch 236/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3872 - val_loss: 0.3774\n",
            "Epoch 237/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3848 - val_loss: 0.3786\n",
            "Epoch 238/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3847 - val_loss: 0.3778\n",
            "Epoch 239/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3875 - val_loss: 0.3787\n",
            "Epoch 240/1000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3866 - val_loss: 0.3781\n",
            "Epoch 241/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3851 - val_loss: 0.3827\n",
            "Epoch 242/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3873 - val_loss: 0.3804\n",
            "Epoch 243/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3808\n",
            "Epoch 244/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3856 - val_loss: 0.3787\n",
            "Epoch 245/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3829 - val_loss: 0.3860\n",
            "Epoch 246/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3872 - val_loss: 0.3803\n",
            "Epoch 247/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.3817\n",
            "Epoch 248/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3910 - val_loss: 0.3784\n",
            "Epoch 249/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.3748\n",
            "Epoch 250/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3868 - val_loss: 0.3776\n",
            "Epoch 251/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3868 - val_loss: 0.3812\n",
            "Epoch 252/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3886 - val_loss: 0.3824\n",
            "Epoch 253/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3838 - val_loss: 0.3761\n",
            "Epoch 254/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3835 - val_loss: 0.3775\n",
            "Epoch 255/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3871 - val_loss: 0.3861\n",
            "Epoch 256/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3863 - val_loss: 0.3749\n",
            "Epoch 257/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3882 - val_loss: 0.3817\n",
            "Epoch 258/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3893\n",
            "Epoch 259/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3868 - val_loss: 0.3775\n",
            "Epoch 260/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3770\n",
            "Epoch 261/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3860 - val_loss: 0.3758\n",
            "Epoch 262/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3858 - val_loss: 0.3795\n",
            "Epoch 263/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3777\n",
            "Epoch 264/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3829\n",
            "Epoch 265/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3857 - val_loss: 0.3782\n",
            "Epoch 266/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3863 - val_loss: 0.3769\n",
            "Epoch 267/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.3785\n",
            "Epoch 268/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3858 - val_loss: 0.3760\n",
            "Epoch 269/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3852 - val_loss: 0.3834\n",
            "Epoch 270/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3841 - val_loss: 0.3764\n",
            "Epoch 271/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3826 - val_loss: 0.3767\n",
            "Epoch 272/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3852 - val_loss: 0.3769\n",
            "Epoch 273/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3853 - val_loss: 0.3725\n",
            "Epoch 274/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.3757\n",
            "Epoch 275/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - val_loss: 0.3788\n",
            "Epoch 276/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3824 - val_loss: 0.3746\n",
            "Epoch 277/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3862 - val_loss: 0.3787\n",
            "Epoch 278/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3831 - val_loss: 0.3765\n",
            "Epoch 279/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3871\n",
            "Epoch 280/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3861 - val_loss: 0.3982\n",
            "Epoch 281/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3924 - val_loss: 0.3793\n",
            "Epoch 282/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3879 - val_loss: 0.3857\n",
            "Epoch 283/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3859 - val_loss: 0.3853\n",
            "Epoch 284/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.3776\n",
            "Epoch 285/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.3795\n",
            "Epoch 286/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3789\n",
            "Epoch 287/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3854 - val_loss: 0.3779\n",
            "Epoch 288/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3842 - val_loss: 0.3764\n",
            "Epoch 289/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3846 - val_loss: 0.3774\n",
            "Epoch 290/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3848 - val_loss: 0.3849\n",
            "Epoch 291/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3809 - val_loss: 0.3747\n",
            "Epoch 292/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3846 - val_loss: 0.3792\n",
            "Epoch 293/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3815 - val_loss: 0.3753\n",
            "Epoch 294/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3820 - val_loss: 0.3799\n",
            "Epoch 295/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3828 - val_loss: 0.3777\n",
            "Epoch 296/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3825 - val_loss: 0.3752\n",
            "Epoch 297/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3822 - val_loss: 0.3737\n",
            "Epoch 298/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.3765\n",
            "Epoch 299/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3821 - val_loss: 0.3776\n",
            "Epoch 300/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3822 - val_loss: 0.3803\n",
            "Epoch 301/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3856 - val_loss: 0.3825\n",
            "Epoch 302/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3889 - val_loss: 0.3887\n",
            "Epoch 303/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3906 - val_loss: 0.3846\n",
            "Epoch 304/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3922 - val_loss: 0.3785\n",
            "Epoch 305/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3838 - val_loss: 0.3772\n",
            "Epoch 306/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3832 - val_loss: 0.3745\n",
            "Epoch 307/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3821 - val_loss: 0.3796\n",
            "Epoch 308/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3816 - val_loss: 0.3772\n",
            "Epoch 309/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3831 - val_loss: 0.3764\n",
            "Epoch 310/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3873 - val_loss: 0.3754\n",
            "Epoch 311/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3828 - val_loss: 0.3775\n",
            "Epoch 312/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.3786\n",
            "Epoch 313/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3773\n",
            "Epoch 314/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - val_loss: 0.3802\n",
            "Epoch 315/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3838 - val_loss: 0.3777\n",
            "Epoch 316/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3822 - val_loss: 0.3784\n",
            "Epoch 317/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3829 - val_loss: 0.3856\n",
            "Epoch 318/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3818 - val_loss: 0.3771\n",
            "Epoch 319/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3839 - val_loss: 0.3765\n",
            "Epoch 320/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3808 - val_loss: 0.3745\n",
            "Epoch 321/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3799 - val_loss: 0.3750\n",
            "Epoch 322/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3821 - val_loss: 0.3823\n",
            "Epoch 323/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3842 - val_loss: 0.3773\n",
            "Epoch 324/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3840 - val_loss: 0.3733\n",
            "Epoch 325/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3837 - val_loss: 0.3821\n",
            "Epoch 326/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3867 - val_loss: 0.3753\n",
            "Epoch 327/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3819 - val_loss: 0.3798\n",
            "Epoch 328/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3831 - val_loss: 0.3775\n",
            "Epoch 329/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3860 - val_loss: 0.3841\n",
            "Epoch 330/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3843 - val_loss: 0.4007\n",
            "Epoch 331/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3899 - val_loss: 0.3780\n",
            "Epoch 332/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3832 - val_loss: 0.3864\n",
            "Epoch 333/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.3801\n",
            "Epoch 334/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3830 - val_loss: 0.3762\n",
            "Epoch 335/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3816 - val_loss: 0.3740\n",
            "Epoch 336/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3814 - val_loss: 0.3776\n",
            "Epoch 337/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.3774\n",
            "Epoch 338/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.3778\n",
            "Epoch 339/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3831 - val_loss: 0.3775\n",
            "Epoch 340/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3823 - val_loss: 0.3871\n",
            "Epoch 341/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3803 - val_loss: 0.3759\n",
            "Epoch 342/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3835 - val_loss: 0.3755\n",
            "Epoch 343/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.3788\n",
            "Epoch 344/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3869 - val_loss: 0.3758\n",
            "Epoch 345/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3892 - val_loss: 0.3789\n",
            "Epoch 346/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3825 - val_loss: 0.3802\n",
            "Epoch 347/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3802 - val_loss: 0.3759\n",
            "Epoch 348/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3758\n",
            "Epoch 349/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3768\n",
            "Epoch 350/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3823 - val_loss: 0.3735\n",
            "Epoch 351/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3833 - val_loss: 0.3794\n",
            "Epoch 352/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3843 - val_loss: 0.3848\n",
            "Epoch 353/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3831 - val_loss: 0.3741\n",
            "Epoch 354/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3814 - val_loss: 0.3789\n",
            "Epoch 355/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3829 - val_loss: 0.3745\n",
            "Epoch 356/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3834 - val_loss: 0.3770\n",
            "Epoch 357/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3825 - val_loss: 0.3802\n",
            "Epoch 358/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3804 - val_loss: 0.3777\n",
            "Epoch 359/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3809 - val_loss: 0.3749\n",
            "Epoch 360/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3849 - val_loss: 0.3813\n",
            "Epoch 361/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3755\n",
            "Epoch 362/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.3772\n",
            "Epoch 363/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3764\n",
            "Epoch 364/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3807 - val_loss: 0.3814\n",
            "Epoch 365/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3808 - val_loss: 0.3778\n",
            "Epoch 366/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3799 - val_loss: 0.3815\n",
            "Epoch 367/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3791 - val_loss: 0.3820\n",
            "Epoch 368/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3820 - val_loss: 0.3735\n",
            "Epoch 369/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3796\n",
            "Epoch 370/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3808 - val_loss: 0.3778\n",
            "Epoch 371/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - val_loss: 0.3847\n",
            "Epoch 372/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3803 - val_loss: 0.3798\n",
            "Epoch 373/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3823 - val_loss: 0.3896\n",
            "Epoch 374/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3817 - val_loss: 0.3876\n",
            "Epoch 375/1000\n",
            "70/70 [==============================] - 2s 23ms/step - loss: 0.3797 - val_loss: 0.3749\n",
            "Epoch 376/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3799 - val_loss: 0.3779\n",
            "Epoch 377/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3802 - val_loss: 0.3775\n",
            "Epoch 378/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3797 - val_loss: 0.3772\n",
            "Epoch 379/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3837 - val_loss: 0.3746\n",
            "Epoch 380/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3811 - val_loss: 0.3799\n",
            "Epoch 381/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.3753\n",
            "Epoch 382/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3806 - val_loss: 0.3738\n",
            "Epoch 383/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.3790\n",
            "Epoch 384/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3836 - val_loss: 0.3850\n",
            "Epoch 385/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3795 - val_loss: 0.3879\n",
            "Epoch 386/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3786 - val_loss: 0.3796\n",
            "Epoch 387/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3792 - val_loss: 0.3811\n",
            "Epoch 388/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3799 - val_loss: 0.3879\n",
            "Epoch 389/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3804 - val_loss: 0.3874\n",
            "Epoch 390/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3796 - val_loss: 0.3794\n",
            "Epoch 391/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3827 - val_loss: 0.3810\n",
            "Epoch 392/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3829 - val_loss: 0.3832\n",
            "Epoch 393/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3795 - val_loss: 0.3761\n",
            "Epoch 394/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3811 - val_loss: 0.3806\n",
            "Epoch 395/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3796 - val_loss: 0.3752\n",
            "Epoch 396/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3797 - val_loss: 0.3754\n",
            "Epoch 397/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3809 - val_loss: 0.3766\n",
            "Epoch 398/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3790 - val_loss: 0.3771\n",
            "Epoch 399/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.3792\n",
            "Epoch 400/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3816 - val_loss: 0.3879\n",
            "Epoch 401/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3783 - val_loss: 0.3768\n",
            "Epoch 402/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3777 - val_loss: 0.3800\n",
            "Epoch 403/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3904 - val_loss: 0.3810\n",
            "Epoch 404/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3868 - val_loss: 0.3830\n",
            "Epoch 405/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3814 - val_loss: 0.3804\n",
            "Epoch 406/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3798 - val_loss: 0.3780\n",
            "Epoch 407/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3793 - val_loss: 0.3788\n",
            "Epoch 408/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3784 - val_loss: 0.3768\n",
            "Epoch 409/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3792 - val_loss: 0.3796\n",
            "Epoch 410/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3780 - val_loss: 0.3782\n",
            "Epoch 411/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3783 - val_loss: 0.3778\n",
            "Epoch 412/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3791 - val_loss: 0.3842\n",
            "Epoch 413/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3791 - val_loss: 0.3790\n",
            "Epoch 414/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3773 - val_loss: 0.3739\n",
            "Epoch 415/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3778 - val_loss: 0.3793\n",
            "Epoch 416/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3816 - val_loss: 0.3929\n",
            "Epoch 417/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3810 - val_loss: 0.3872\n",
            "Epoch 418/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3833 - val_loss: 0.3787\n",
            "Epoch 419/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3806 - val_loss: 0.3857\n",
            "Epoch 420/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3840 - val_loss: 0.3775\n",
            "Epoch 421/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3784 - val_loss: 0.3807\n",
            "Epoch 422/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3785 - val_loss: 0.3782\n",
            "Epoch 423/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3775 - val_loss: 0.3765\n",
            "Epoch 424/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3786 - val_loss: 0.3741\n",
            "Epoch 425/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3794 - val_loss: 0.3737\n",
            "Epoch 426/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3797 - val_loss: 0.3815\n",
            "Epoch 427/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3859 - val_loss: 0.3802\n",
            "Epoch 428/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.3818\n",
            "Epoch 429/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.3784\n",
            "Epoch 430/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3782 - val_loss: 0.3816\n",
            "Epoch 431/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3791 - val_loss: 0.3771\n",
            "Epoch 432/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3815 - val_loss: 0.3811\n",
            "Epoch 433/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.3849\n",
            "Epoch 434/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3813\n",
            "Epoch 435/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3807\n",
            "Epoch 436/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - val_loss: 0.3763\n",
            "Epoch 437/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3776 - val_loss: 0.3791\n",
            "Epoch 438/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3789 - val_loss: 0.3833\n",
            "Epoch 439/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3785 - val_loss: 0.3781\n",
            "Epoch 440/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3770\n",
            "Epoch 441/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3790 - val_loss: 0.3775\n",
            "Epoch 442/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3788 - val_loss: 0.3819\n",
            "Epoch 443/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3804 - val_loss: 0.3762\n",
            "Epoch 444/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3782 - val_loss: 0.3801\n",
            "Epoch 445/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3779 - val_loss: 0.3760\n",
            "Epoch 446/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3774 - val_loss: 0.3791\n",
            "Epoch 447/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3793 - val_loss: 0.3788\n",
            "Epoch 448/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3788 - val_loss: 0.3851\n",
            "Epoch 449/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3781 - val_loss: 0.3779\n",
            "Epoch 450/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3831\n",
            "Epoch 451/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3795 - val_loss: 0.3774\n",
            "Epoch 452/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3774 - val_loss: 0.3770\n",
            "Epoch 453/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.3837\n",
            "Epoch 454/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3794 - val_loss: 0.3771\n",
            "Epoch 455/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3793 - val_loss: 0.3784\n",
            "Epoch 456/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3786 - val_loss: 0.3793\n",
            "Epoch 457/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3818 - val_loss: 0.3843\n",
            "Epoch 458/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3794 - val_loss: 0.3829\n",
            "Epoch 459/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3789 - val_loss: 0.3822\n",
            "Epoch 460/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3791 - val_loss: 0.3836\n",
            "Epoch 461/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3764 - val_loss: 0.3851\n",
            "Epoch 462/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3778 - val_loss: 0.4038\n",
            "Epoch 463/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3864 - val_loss: 0.3926\n",
            "Epoch 464/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.3888\n",
            "Epoch 465/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3776 - val_loss: 0.3790\n",
            "Epoch 466/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3854 - val_loss: 0.3799\n",
            "Epoch 467/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3802 - val_loss: 0.3767\n",
            "Epoch 468/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3779 - val_loss: 0.3752\n",
            "Epoch 469/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3847\n",
            "Epoch 470/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3775 - val_loss: 0.3733\n",
            "Epoch 471/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3778 - val_loss: 0.3830\n",
            "Epoch 472/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3797 - val_loss: 0.3786\n",
            "Epoch 473/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3781 - val_loss: 0.3807\n",
            "Epoch 474/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3783 - val_loss: 0.3855\n",
            "Epoch 475/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3784 - val_loss: 0.3765\n",
            "Epoch 476/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3769 - val_loss: 0.3751\n",
            "Epoch 477/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3776 - val_loss: 0.3818\n",
            "Epoch 478/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3778 - val_loss: 0.3823\n",
            "Epoch 479/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3790 - val_loss: 0.3891\n",
            "Epoch 480/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3782 - val_loss: 0.3880\n",
            "Epoch 481/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3759 - val_loss: 0.3772\n",
            "Epoch 482/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3776 - val_loss: 0.3772\n",
            "Epoch 483/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3768 - val_loss: 0.3758\n",
            "Epoch 484/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3775 - val_loss: 0.3874\n",
            "Epoch 485/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3785\n",
            "Epoch 486/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3764 - val_loss: 0.3865\n",
            "Epoch 487/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3759 - val_loss: 0.3795\n",
            "Epoch 488/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.3796\n",
            "Epoch 489/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3806\n",
            "Epoch 490/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3781 - val_loss: 0.3789\n",
            "Epoch 491/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3780 - val_loss: 0.3808\n",
            "Epoch 492/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3772 - val_loss: 0.3788\n",
            "Epoch 493/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3760 - val_loss: 0.3855\n",
            "Epoch 494/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3778 - val_loss: 0.3828\n",
            "Epoch 495/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3779 - val_loss: 0.3768\n",
            "Epoch 496/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3799\n",
            "Epoch 497/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3750 - val_loss: 0.3781\n",
            "Epoch 498/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3753\n",
            "Epoch 499/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3768\n",
            "Epoch 500/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3755 - val_loss: 0.3833\n",
            "Epoch 501/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3769 - val_loss: 0.3737\n",
            "Epoch 502/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3763 - val_loss: 0.3807\n",
            "Epoch 503/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - val_loss: 0.3784\n",
            "Epoch 504/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3758 - val_loss: 0.3787\n",
            "Epoch 505/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3792\n",
            "Epoch 506/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3780 - val_loss: 0.3828\n",
            "Epoch 507/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3766 - val_loss: 0.3763\n",
            "Epoch 508/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3814\n",
            "Epoch 509/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3770 - val_loss: 0.3874\n",
            "Epoch 510/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3770 - val_loss: 0.3883\n",
            "Epoch 511/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3790 - val_loss: 0.3772\n",
            "Epoch 512/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3773 - val_loss: 0.3820\n",
            "Epoch 513/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3779 - val_loss: 0.3871\n",
            "Epoch 514/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3775 - val_loss: 0.3810\n",
            "Epoch 515/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3766 - val_loss: 0.3757\n",
            "Epoch 516/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3795 - val_loss: 0.3817\n",
            "Epoch 517/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3781 - val_loss: 0.3813\n",
            "Epoch 518/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3834\n",
            "Epoch 519/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3759 - val_loss: 0.3823\n",
            "Epoch 520/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.3791\n",
            "Epoch 521/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.3811\n",
            "Epoch 522/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3858\n",
            "Epoch 523/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3757 - val_loss: 0.3796\n",
            "Epoch 524/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3752 - val_loss: 0.3773\n",
            "Epoch 525/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.3759\n",
            "Epoch 526/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3759 - val_loss: 0.3792\n",
            "Epoch 527/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3763 - val_loss: 0.3838\n",
            "Epoch 528/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3740 - val_loss: 0.3740\n",
            "Epoch 529/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3792 - val_loss: 0.3825\n",
            "Epoch 530/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3760 - val_loss: 0.3735\n",
            "Epoch 531/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3768 - val_loss: 0.3765\n",
            "Epoch 532/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3759 - val_loss: 0.3777\n",
            "Epoch 533/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3797\n",
            "Epoch 534/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.3826\n",
            "Epoch 535/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3783 - val_loss: 0.3797\n",
            "Epoch 536/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3851\n",
            "Epoch 537/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3767 - val_loss: 0.3753\n",
            "Epoch 538/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3752 - val_loss: 0.3834\n",
            "Epoch 539/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3777 - val_loss: 0.3768\n",
            "Epoch 540/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3771 - val_loss: 0.3825\n",
            "Epoch 541/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3747 - val_loss: 0.3757\n",
            "Epoch 542/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3752 - val_loss: 0.3808\n",
            "Epoch 543/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3747 - val_loss: 0.3779\n",
            "Epoch 544/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3744 - val_loss: 0.3773\n",
            "Epoch 545/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3752 - val_loss: 0.3836\n",
            "Epoch 546/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3767 - val_loss: 0.3786\n",
            "Epoch 547/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3758 - val_loss: 0.3798\n",
            "Epoch 548/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.3837\n",
            "Epoch 549/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3752\n",
            "Epoch 550/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3736 - val_loss: 0.3813\n",
            "Epoch 551/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3747 - val_loss: 0.3812\n",
            "Epoch 552/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3766 - val_loss: 0.3839\n",
            "Epoch 553/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3744 - val_loss: 0.3762\n",
            "Epoch 554/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3764 - val_loss: 0.3798\n",
            "Epoch 555/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3782\n",
            "Epoch 556/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3750 - val_loss: 0.3860\n",
            "Epoch 557/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.3773\n",
            "Epoch 558/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3777 - val_loss: 0.3815\n",
            "Epoch 559/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3774\n",
            "Epoch 560/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3779 - val_loss: 0.3776\n",
            "Epoch 561/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3763 - val_loss: 0.3768\n",
            "Epoch 562/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3771 - val_loss: 0.3951\n",
            "Epoch 563/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3762 - val_loss: 0.3756\n",
            "Epoch 564/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3753 - val_loss: 0.3763\n",
            "Epoch 565/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3749 - val_loss: 0.3765\n",
            "Epoch 566/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3747 - val_loss: 0.3817\n",
            "Epoch 567/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3796\n",
            "Epoch 568/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3760 - val_loss: 0.3815\n",
            "Epoch 569/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3877\n",
            "Epoch 570/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.3766\n",
            "Epoch 571/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3743 - val_loss: 0.3827\n",
            "Epoch 572/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3767 - val_loss: 0.3817\n",
            "Epoch 573/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3769 - val_loss: 0.3807\n",
            "Epoch 574/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3761 - val_loss: 0.3767\n",
            "Epoch 575/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3769 - val_loss: 0.3815\n",
            "Epoch 576/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3761 - val_loss: 0.3745\n",
            "Epoch 577/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3750 - val_loss: 0.3849\n",
            "Epoch 578/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3751 - val_loss: 0.3771\n",
            "Epoch 579/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3732 - val_loss: 0.3775\n",
            "Epoch 580/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3764 - val_loss: 0.3830\n",
            "Epoch 581/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3771 - val_loss: 0.3793\n",
            "Epoch 582/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3763 - val_loss: 0.3730\n",
            "Epoch 583/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3751 - val_loss: 0.3714\n",
            "Epoch 584/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3749\n",
            "Epoch 585/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3770 - val_loss: 0.3825\n",
            "Epoch 586/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3765\n",
            "Epoch 587/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3778\n",
            "Epoch 588/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3827\n",
            "Epoch 589/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3757 - val_loss: 0.3808\n",
            "Epoch 590/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.3848\n",
            "Epoch 591/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3894\n",
            "Epoch 592/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3825 - val_loss: 0.3758\n",
            "Epoch 593/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3762 - val_loss: 0.3767\n",
            "Epoch 594/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3744 - val_loss: 0.3769\n",
            "Epoch 595/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3746 - val_loss: 0.3785\n",
            "Epoch 596/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3745 - val_loss: 0.3822\n",
            "Epoch 597/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3751 - val_loss: 0.3825\n",
            "Epoch 598/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3830\n",
            "Epoch 599/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3740 - val_loss: 0.3758\n",
            "Epoch 600/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3733 - val_loss: 0.3779\n",
            "Epoch 601/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3797\n",
            "Epoch 602/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3796\n",
            "Epoch 603/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3777 - val_loss: 0.3840\n",
            "Epoch 604/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.3802\n",
            "Epoch 605/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3770\n",
            "Epoch 606/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3766\n",
            "Epoch 607/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - val_loss: 0.3804\n",
            "Epoch 608/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3760\n",
            "Epoch 609/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - val_loss: 0.3751\n",
            "Epoch 610/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3765 - val_loss: 0.3772\n",
            "Epoch 611/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3755 - val_loss: 0.3794\n",
            "Epoch 612/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3739 - val_loss: 0.3787\n",
            "Epoch 613/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3748 - val_loss: 0.3788\n",
            "Epoch 614/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3737 - val_loss: 0.3818\n",
            "Epoch 615/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3746\n",
            "Epoch 616/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3762 - val_loss: 0.3835\n",
            "Epoch 617/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3743 - val_loss: 0.3834\n",
            "Epoch 618/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3733 - val_loss: 0.3826\n",
            "Epoch 619/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3731 - val_loss: 0.3780\n",
            "Epoch 620/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3795\n",
            "Epoch 621/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3813\n",
            "Epoch 622/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3770\n",
            "Epoch 623/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3753 - val_loss: 0.3845\n",
            "Epoch 624/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.3779\n",
            "Epoch 625/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3753 - val_loss: 0.3817\n",
            "Epoch 626/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3732 - val_loss: 0.3853\n",
            "Epoch 627/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3726 - val_loss: 0.3819\n",
            "Epoch 628/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3746 - val_loss: 0.3820\n",
            "Epoch 629/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3770 - val_loss: 0.3825\n",
            "Epoch 630/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3749 - val_loss: 0.3810\n",
            "Epoch 631/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3729 - val_loss: 0.3786\n",
            "Epoch 632/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3771\n",
            "Epoch 633/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3741 - val_loss: 0.3754\n",
            "Epoch 634/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3731 - val_loss: 0.3831\n",
            "Epoch 635/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3745 - val_loss: 0.3831\n",
            "Epoch 636/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3744\n",
            "Epoch 637/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3749\n",
            "Epoch 638/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3733 - val_loss: 0.3762\n",
            "Epoch 639/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3792\n",
            "Epoch 640/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3834\n",
            "Epoch 641/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3784\n",
            "Epoch 642/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3741 - val_loss: 0.3765\n",
            "Epoch 643/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3733 - val_loss: 0.3761\n",
            "Epoch 644/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3745 - val_loss: 0.3755\n",
            "Epoch 645/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3753 - val_loss: 0.3839\n",
            "Epoch 646/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3725 - val_loss: 0.3793\n",
            "Epoch 647/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3745 - val_loss: 0.4007\n",
            "Epoch 648/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3913 - val_loss: 0.3887\n",
            "Epoch 649/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3813 - val_loss: 0.3790\n",
            "Epoch 650/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3772 - val_loss: 0.3841\n",
            "Epoch 651/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3752 - val_loss: 0.3784\n",
            "Epoch 652/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3774\n",
            "Epoch 653/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3735 - val_loss: 0.3816\n",
            "Epoch 654/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3823\n",
            "Epoch 655/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3757 - val_loss: 0.3797\n",
            "Epoch 656/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3802\n",
            "Epoch 657/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3830\n",
            "Epoch 658/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3779\n",
            "Epoch 659/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3747 - val_loss: 0.3848\n",
            "Epoch 660/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3726 - val_loss: 0.3755\n",
            "Epoch 661/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3743 - val_loss: 0.3752\n",
            "Epoch 662/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3740 - val_loss: 0.3880\n",
            "Epoch 663/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3756 - val_loss: 0.3753\n",
            "Epoch 664/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3727 - val_loss: 0.3808\n",
            "Epoch 665/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3718 - val_loss: 0.3810\n",
            "Epoch 666/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3754\n",
            "Epoch 667/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3787\n",
            "Epoch 668/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3729 - val_loss: 0.3813\n",
            "Epoch 669/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3723 - val_loss: 0.3825\n",
            "Epoch 670/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3721 - val_loss: 0.3786\n",
            "Epoch 671/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3782\n",
            "Epoch 672/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3808\n",
            "Epoch 673/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3817\n",
            "Epoch 674/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3733 - val_loss: 0.3851\n",
            "Epoch 675/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3731 - val_loss: 0.3763\n",
            "Epoch 676/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3793\n",
            "Epoch 677/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3736 - val_loss: 0.3778\n",
            "Epoch 678/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3724 - val_loss: 0.3814\n",
            "Epoch 679/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3723 - val_loss: 0.3821\n",
            "Epoch 680/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3738 - val_loss: 0.3818\n",
            "Epoch 681/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3727 - val_loss: 0.3735\n",
            "Epoch 682/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3729 - val_loss: 0.3742\n",
            "Epoch 683/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3751 - val_loss: 0.3780\n",
            "Epoch 684/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3745 - val_loss: 0.3731\n",
            "Epoch 685/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3735 - val_loss: 0.3776\n",
            "Epoch 686/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3746 - val_loss: 0.3835\n",
            "Epoch 687/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3740 - val_loss: 0.3760\n",
            "Epoch 688/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3793\n",
            "Epoch 689/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3834\n",
            "Epoch 690/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3734 - val_loss: 0.3771\n",
            "Epoch 691/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3718 - val_loss: 0.3833\n",
            "Epoch 692/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3735 - val_loss: 0.3815\n",
            "Epoch 693/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3741 - val_loss: 0.3869\n",
            "Epoch 694/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3727 - val_loss: 0.3764\n",
            "Epoch 695/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3726 - val_loss: 0.3789\n",
            "Epoch 696/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3738 - val_loss: 0.3842\n",
            "Epoch 697/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3718 - val_loss: 0.3809\n",
            "Epoch 698/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3738 - val_loss: 0.3774\n",
            "Epoch 699/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3763 - val_loss: 0.3855\n",
            "Epoch 700/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3734 - val_loss: 0.3797\n",
            "Epoch 701/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3725 - val_loss: 0.3823\n",
            "Epoch 702/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3720 - val_loss: 0.3791\n",
            "Epoch 703/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3710 - val_loss: 0.3758\n",
            "Epoch 704/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3731 - val_loss: 0.3779\n",
            "Epoch 705/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3739 - val_loss: 0.3803\n",
            "Epoch 706/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3802\n",
            "Epoch 707/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3756 - val_loss: 0.3798\n",
            "Epoch 708/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3751 - val_loss: 0.3786\n",
            "Epoch 709/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3739 - val_loss: 0.3768\n",
            "Epoch 710/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3716 - val_loss: 0.3760\n",
            "Epoch 711/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3718 - val_loss: 0.3730\n",
            "Epoch 712/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3730 - val_loss: 0.3864\n",
            "Epoch 713/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3727 - val_loss: 0.3807\n",
            "Epoch 714/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3762\n",
            "Epoch 715/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3715 - val_loss: 0.3769\n",
            "Epoch 716/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3787\n",
            "Epoch 717/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3723 - val_loss: 0.3781\n",
            "Epoch 718/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3749\n",
            "Epoch 719/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3758\n",
            "Epoch 720/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3782\n",
            "Epoch 721/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3742\n",
            "Epoch 722/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3717 - val_loss: 0.3822\n",
            "Epoch 723/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3825\n",
            "Epoch 724/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3705 - val_loss: 0.3760\n",
            "Epoch 725/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3714 - val_loss: 0.3762\n",
            "Epoch 726/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3729 - val_loss: 0.3803\n",
            "Epoch 727/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3735 - val_loss: 0.3798\n",
            "Epoch 728/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3730 - val_loss: 0.3788\n",
            "Epoch 729/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3721 - val_loss: 0.3782\n",
            "Epoch 730/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3738 - val_loss: 0.3734\n",
            "Epoch 731/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3725 - val_loss: 0.3797\n",
            "Epoch 732/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3709 - val_loss: 0.3804\n",
            "Epoch 733/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3706 - val_loss: 0.3863\n",
            "Epoch 734/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3726 - val_loss: 0.3820\n",
            "Epoch 735/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3715 - val_loss: 0.3772\n",
            "Epoch 736/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3733 - val_loss: 0.3767\n",
            "Epoch 737/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3764\n",
            "Epoch 738/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.3753\n",
            "Epoch 739/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3729 - val_loss: 0.3764\n",
            "Epoch 740/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3730 - val_loss: 0.3769\n",
            "Epoch 741/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.3788\n",
            "Epoch 742/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3727 - val_loss: 0.3782\n",
            "Epoch 743/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3755 - val_loss: 0.3755\n",
            "Epoch 744/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.3788\n",
            "Epoch 745/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3721 - val_loss: 0.3732\n",
            "Epoch 746/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3745 - val_loss: 0.3776\n",
            "Epoch 747/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3813 - val_loss: 0.3882\n",
            "Epoch 748/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3740 - val_loss: 0.3770\n",
            "Epoch 749/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3771\n",
            "Epoch 750/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3728 - val_loss: 0.3758\n",
            "Epoch 751/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3748\n",
            "Epoch 752/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3713 - val_loss: 0.3739\n",
            "Epoch 753/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3722 - val_loss: 0.3816\n",
            "Epoch 754/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3734 - val_loss: 0.3755\n",
            "Epoch 755/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3709 - val_loss: 0.3771\n",
            "Epoch 756/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3710 - val_loss: 0.3792\n",
            "Epoch 757/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3715 - val_loss: 0.3857\n",
            "Epoch 758/1000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3740 - val_loss: 0.3799\n",
            "Epoch 759/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3722 - val_loss: 0.3786\n",
            "Epoch 760/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3728 - val_loss: 0.3837\n",
            "Epoch 761/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3761 - val_loss: 0.3819\n",
            "Epoch 762/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3752 - val_loss: 0.3779\n",
            "Epoch 763/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3722 - val_loss: 0.3738\n",
            "Epoch 764/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3750\n",
            "Epoch 765/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3711 - val_loss: 0.3781\n",
            "Epoch 766/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3712 - val_loss: 0.3765\n",
            "Epoch 767/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3821\n",
            "Epoch 768/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3759\n",
            "Epoch 769/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3709 - val_loss: 0.3765\n",
            "Epoch 770/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3716 - val_loss: 0.3747\n",
            "Epoch 771/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3722 - val_loss: 0.3790\n",
            "Epoch 772/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3741 - val_loss: 0.3820\n",
            "Epoch 773/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3733 - val_loss: 0.3775\n",
            "Epoch 774/1000\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.3729 - val_loss: 0.3852\n",
            "Epoch 775/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3742 - val_loss: 0.3864\n",
            "Epoch 776/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3736 - val_loss: 0.3789\n",
            "Epoch 777/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3707 - val_loss: 0.3805\n",
            "Epoch 778/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3700 - val_loss: 0.3774\n",
            "Epoch 779/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3785\n",
            "Epoch 780/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3717 - val_loss: 0.3808\n",
            "Epoch 781/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3743 - val_loss: 0.3803\n",
            "Epoch 782/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.3745\n",
            "Epoch 783/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3702 - val_loss: 0.3787\n",
            "Epoch 784/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3708 - val_loss: 0.3758\n",
            "Epoch 785/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3766\n",
            "Epoch 786/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3735 - val_loss: 0.3801\n",
            "Epoch 787/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3725 - val_loss: 0.3834\n",
            "Epoch 788/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3719 - val_loss: 0.3779\n",
            "Epoch 789/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3706 - val_loss: 0.3775\n",
            "Epoch 790/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3707 - val_loss: 0.3827\n",
            "Epoch 791/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.3718 - val_loss: 0.3826\n",
            "Epoch 792/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3752 - val_loss: 0.3787\n",
            "Epoch 793/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3733 - val_loss: 0.3788\n",
            "Epoch 794/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3711 - val_loss: 0.3751\n",
            "Epoch 795/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3831\n",
            "Epoch 796/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3719\n",
            "Epoch 797/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3706 - val_loss: 0.3763\n",
            "Epoch 798/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3719 - val_loss: 0.3801\n",
            "Epoch 799/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3759\n",
            "Epoch 800/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3756\n",
            "Epoch 801/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3706 - val_loss: 0.3854\n",
            "Epoch 802/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3730 - val_loss: 0.3822\n",
            "Epoch 803/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3791\n",
            "Epoch 804/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3724 - val_loss: 0.3863\n",
            "Epoch 805/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3724 - val_loss: 0.3799\n",
            "Epoch 806/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3711 - val_loss: 0.3776\n",
            "Epoch 807/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3718 - val_loss: 0.3765\n",
            "Epoch 808/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3711 - val_loss: 0.3741\n",
            "Epoch 809/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3771\n",
            "Epoch 810/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3712 - val_loss: 0.3771\n",
            "Epoch 811/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3792\n",
            "Epoch 812/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3714 - val_loss: 0.3730\n",
            "Epoch 813/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.3757\n",
            "Epoch 814/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3704 - val_loss: 0.3780\n",
            "Epoch 815/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.3744\n",
            "Epoch 816/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3702 - val_loss: 0.3754\n",
            "Epoch 817/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3761\n",
            "Epoch 818/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3747\n",
            "Epoch 819/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3795\n",
            "Epoch 820/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3779\n",
            "Epoch 821/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3701 - val_loss: 0.3776\n",
            "Epoch 822/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3725 - val_loss: 0.3776\n",
            "Epoch 823/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3708 - val_loss: 0.3771\n",
            "Epoch 824/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3701 - val_loss: 0.3776\n",
            "Epoch 825/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3706 - val_loss: 0.3745\n",
            "Epoch 826/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3757\n",
            "Epoch 827/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3768\n",
            "Epoch 828/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3777\n",
            "Epoch 829/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3727 - val_loss: 0.3774\n",
            "Epoch 830/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3742 - val_loss: 0.3858\n",
            "Epoch 831/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3744 - val_loss: 0.3795\n",
            "Epoch 832/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3780\n",
            "Epoch 833/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3737 - val_loss: 0.4280\n",
            "Epoch 834/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3837 - val_loss: 0.3833\n",
            "Epoch 835/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3784\n",
            "Epoch 836/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3747 - val_loss: 0.3772\n",
            "Epoch 837/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3723 - val_loss: 0.3767\n",
            "Epoch 838/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3705 - val_loss: 0.3814\n",
            "Epoch 839/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3712 - val_loss: 0.3796\n",
            "Epoch 840/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3697 - val_loss: 0.3865\n",
            "Epoch 841/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3703 - val_loss: 0.3784\n",
            "Epoch 842/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.3792\n",
            "Epoch 843/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3754\n",
            "Epoch 844/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3692 - val_loss: 0.3774\n",
            "Epoch 845/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3834\n",
            "Epoch 846/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3790\n",
            "Epoch 847/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3769\n",
            "Epoch 848/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3762\n",
            "Epoch 849/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3815\n",
            "Epoch 850/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3784\n",
            "Epoch 851/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3796\n",
            "Epoch 852/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.3744\n",
            "Epoch 853/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3803\n",
            "Epoch 854/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3758 - val_loss: 0.3832\n",
            "Epoch 855/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3734 - val_loss: 0.3843\n",
            "Epoch 856/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3741 - val_loss: 0.3770\n",
            "Epoch 857/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3738 - val_loss: 0.3765\n",
            "Epoch 858/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3725 - val_loss: 0.3810\n",
            "Epoch 859/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3774\n",
            "Epoch 860/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.3857\n",
            "Epoch 861/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3814\n",
            "Epoch 862/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3717 - val_loss: 0.3773\n",
            "Epoch 863/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3818\n",
            "Epoch 864/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3787\n",
            "Epoch 865/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3698 - val_loss: 0.3805\n",
            "Epoch 866/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3716 - val_loss: 0.3842\n",
            "Epoch 867/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.3874\n",
            "Epoch 868/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3732 - val_loss: 0.3782\n",
            "Epoch 869/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3755\n",
            "Epoch 870/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3705 - val_loss: 0.3761\n",
            "Epoch 871/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3700 - val_loss: 0.3814\n",
            "Epoch 872/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3698 - val_loss: 0.3770\n",
            "Epoch 873/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3697 - val_loss: 0.3741\n",
            "Epoch 874/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3696 - val_loss: 0.3775\n",
            "Epoch 875/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3687 - val_loss: 0.3791\n",
            "Epoch 876/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3718 - val_loss: 0.3771\n",
            "Epoch 877/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3703 - val_loss: 0.3752\n",
            "Epoch 878/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3691 - val_loss: 0.3796\n",
            "Epoch 879/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3714 - val_loss: 0.3796\n",
            "Epoch 880/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3734 - val_loss: 0.3808\n",
            "Epoch 881/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3807\n",
            "Epoch 882/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3797\n",
            "Epoch 883/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3814\n",
            "Epoch 884/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3699 - val_loss: 0.3781\n",
            "Epoch 885/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3708 - val_loss: 0.3806\n",
            "Epoch 886/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3736 - val_loss: 0.3799\n",
            "Epoch 887/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3778\n",
            "Epoch 888/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3710 - val_loss: 0.3829\n",
            "Epoch 889/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3711 - val_loss: 0.3773\n",
            "Epoch 890/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3722 - val_loss: 0.3797\n",
            "Epoch 891/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3708 - val_loss: 0.3771\n",
            "Epoch 892/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3764\n",
            "Epoch 893/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3745\n",
            "Epoch 894/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3724 - val_loss: 0.3821\n",
            "Epoch 895/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3780\n",
            "Epoch 896/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3715 - val_loss: 0.3746\n",
            "Epoch 897/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3825\n",
            "Epoch 898/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3721 - val_loss: 0.3801\n",
            "Epoch 899/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3776\n",
            "Epoch 900/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3710 - val_loss: 0.3793\n",
            "Epoch 901/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.3759\n",
            "Epoch 902/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.3757\n",
            "Epoch 903/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3767\n",
            "Epoch 904/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3735\n",
            "Epoch 905/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3711 - val_loss: 0.3775\n",
            "Epoch 906/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3723 - val_loss: 0.3737\n",
            "Epoch 907/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3700 - val_loss: 0.3853\n",
            "Epoch 908/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3716 - val_loss: 0.3762\n",
            "Epoch 909/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3770\n",
            "Epoch 910/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3714 - val_loss: 0.3773\n",
            "Epoch 911/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3805\n",
            "Epoch 912/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3698 - val_loss: 0.3759\n",
            "Epoch 913/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3774\n",
            "Epoch 914/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3773\n",
            "Epoch 915/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3824\n",
            "Epoch 916/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3697 - val_loss: 0.3781\n",
            "Epoch 917/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3692 - val_loss: 0.3829\n",
            "Epoch 918/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3711 - val_loss: 0.3824\n",
            "Epoch 919/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3829\n",
            "Epoch 920/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3698 - val_loss: 0.3759\n",
            "Epoch 921/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3732 - val_loss: 0.3781\n",
            "Epoch 922/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3719 - val_loss: 0.3782\n",
            "Epoch 923/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3714 - val_loss: 0.3844\n",
            "Epoch 924/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3703 - val_loss: 0.3832\n",
            "Epoch 925/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3691 - val_loss: 0.3824\n",
            "Epoch 926/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3699 - val_loss: 0.3802\n",
            "Epoch 927/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3687 - val_loss: 0.3812\n",
            "Epoch 928/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3693 - val_loss: 0.3828\n",
            "Epoch 929/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3774\n",
            "Epoch 930/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - val_loss: 0.3795\n",
            "Epoch 931/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3707 - val_loss: 0.3745\n",
            "Epoch 932/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.3687 - val_loss: 0.3761\n",
            "Epoch 933/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3744\n",
            "Epoch 934/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3847\n",
            "Epoch 935/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3719 - val_loss: 0.3771\n",
            "Epoch 936/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3716 - val_loss: 0.3787\n",
            "Epoch 937/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3694 - val_loss: 0.3784\n",
            "Epoch 938/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3726 - val_loss: 0.3780\n",
            "Epoch 939/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3689 - val_loss: 0.3880\n",
            "Epoch 940/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3709 - val_loss: 0.3803\n",
            "Epoch 941/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3679 - val_loss: 0.3784\n",
            "Epoch 942/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3685 - val_loss: 0.3787\n",
            "Epoch 943/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3697 - val_loss: 0.3807\n",
            "Epoch 944/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.3747\n",
            "Epoch 945/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3693 - val_loss: 0.3782\n",
            "Epoch 946/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3754\n",
            "Epoch 947/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3729\n",
            "Epoch 948/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3689 - val_loss: 0.3755\n",
            "Epoch 949/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3703 - val_loss: 0.3810\n",
            "Epoch 950/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3705 - val_loss: 0.3815\n",
            "Epoch 951/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3720 - val_loss: 0.3798\n",
            "Epoch 952/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3688 - val_loss: 0.3741\n",
            "Epoch 953/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3697 - val_loss: 0.3843\n",
            "Epoch 954/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3704 - val_loss: 0.3788\n",
            "Epoch 955/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3698 - val_loss: 0.3750\n",
            "Epoch 956/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3700 - val_loss: 0.3768\n",
            "Epoch 957/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3713 - val_loss: 0.3758\n",
            "Epoch 958/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3712 - val_loss: 0.3738\n",
            "Epoch 959/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.3694 - val_loss: 0.3774\n",
            "Epoch 960/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3704 - val_loss: 0.3772\n",
            "Epoch 961/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3700 - val_loss: 0.3765\n",
            "Epoch 962/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3701 - val_loss: 0.3746\n",
            "Epoch 963/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3700 - val_loss: 0.3784\n",
            "Epoch 964/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3699 - val_loss: 0.3744\n",
            "Epoch 965/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3716 - val_loss: 0.3712\n",
            "Epoch 966/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3767\n",
            "Epoch 967/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3713 - val_loss: 0.3797\n",
            "Epoch 968/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3702 - val_loss: 0.3755\n",
            "Epoch 969/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3699 - val_loss: 0.3735\n",
            "Epoch 970/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3685 - val_loss: 0.3751\n",
            "Epoch 971/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3687 - val_loss: 0.3801\n",
            "Epoch 972/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3686 - val_loss: 0.3737\n",
            "Epoch 973/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3693 - val_loss: 0.3747\n",
            "Epoch 974/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3709 - val_loss: 0.3795\n",
            "Epoch 975/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3717 - val_loss: 0.3742\n",
            "Epoch 976/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3699 - val_loss: 0.3756\n",
            "Epoch 977/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3676 - val_loss: 0.3750\n",
            "Epoch 978/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.3779\n",
            "Epoch 979/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.3771\n",
            "Epoch 980/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3683 - val_loss: 0.3735\n",
            "Epoch 981/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3696 - val_loss: 0.3799\n",
            "Epoch 982/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3682 - val_loss: 0.3790\n",
            "Epoch 983/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.3791\n",
            "Epoch 984/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3725 - val_loss: 0.3760\n",
            "Epoch 985/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3694 - val_loss: 0.3763\n",
            "Epoch 986/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3689 - val_loss: 0.3817\n",
            "Epoch 987/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3709 - val_loss: 0.3780\n",
            "Epoch 988/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3708 - val_loss: 0.3762\n",
            "Epoch 989/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3690 - val_loss: 0.3765\n",
            "Epoch 990/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3687 - val_loss: 0.3774\n",
            "Epoch 991/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.3695 - val_loss: 0.3749\n",
            "Epoch 992/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3689 - val_loss: 0.3761\n",
            "Epoch 993/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.3688 - val_loss: 0.3775\n",
            "Epoch 994/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.3690 - val_loss: 0.3791\n",
            "Epoch 995/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3684 - val_loss: 0.3776\n",
            "Epoch 996/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3688 - val_loss: 0.3826\n",
            "Epoch 997/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3679 - val_loss: 0.3780\n",
            "Epoch 998/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3684 - val_loss: 0.3796\n",
            "Epoch 999/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.3680 - val_loss: 0.3789\n",
            "Epoch 1000/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.3680 - val_loss: 0.3757\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3757\n",
            "8/8 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae69e22ce0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000;    # training epochs\n",
        "temperature = 0.1    # temprature control the smooth of the probabilities\n",
        "\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(x)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "gJxJD_M41H_v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok-LvipnKoD0"
      },
      "source": [
        "##2.5 Only Use Normalization layer ##"
      ],
      "id": "ok-LvipnKoD0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waeLOQUbYF_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907a31b1-1e8b-49d9-c798-b530b4b7a5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "70/70 [==============================] - 14s 19ms/step - loss: 1.4827 - val_loss: 1.3541\n",
            "Epoch 2/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 1.2777 - val_loss: 1.2333\n",
            "Epoch 3/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 1.1930 - val_loss: 1.1648\n",
            "Epoch 4/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 1.1234 - val_loss: 1.0910\n",
            "Epoch 5/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 1.0781 - val_loss: 1.0653\n",
            "Epoch 6/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0549 - val_loss: 1.0371\n",
            "Epoch 7/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 1.0255 - val_loss: 1.0046\n",
            "Epoch 8/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.9815 - val_loss: 0.9389\n",
            "Epoch 9/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.8841 - val_loss: 0.7842\n",
            "Epoch 10/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.6941 - val_loss: 0.5761\n",
            "Epoch 11/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5917 - val_loss: 0.5340\n",
            "Epoch 12/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5841 - val_loss: 0.5312\n",
            "Epoch 13/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5838 - val_loss: 0.5305\n",
            "Epoch 14/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5834 - val_loss: 0.5302\n",
            "Epoch 15/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5829 - val_loss: 0.5308\n",
            "Epoch 16/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5827 - val_loss: 0.5314\n",
            "Epoch 17/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5823 - val_loss: 0.5298\n",
            "Epoch 18/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5821 - val_loss: 0.5292\n",
            "Epoch 19/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5821 - val_loss: 0.5294\n",
            "Epoch 20/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5815 - val_loss: 0.5295\n",
            "Epoch 21/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5813 - val_loss: 0.5298\n",
            "Epoch 22/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.5813 - val_loss: 0.5302\n",
            "Epoch 23/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5808 - val_loss: 0.5293\n",
            "Epoch 24/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5807 - val_loss: 0.5287\n",
            "Epoch 25/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5805 - val_loss: 0.5292\n",
            "Epoch 26/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5806 - val_loss: 0.5296\n",
            "Epoch 27/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5804 - val_loss: 0.5286\n",
            "Epoch 28/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5800 - val_loss: 0.5278\n",
            "Epoch 29/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5798 - val_loss: 0.5282\n",
            "Epoch 30/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5797 - val_loss: 0.5283\n",
            "Epoch 31/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5799 - val_loss: 0.5291\n",
            "Epoch 32/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5794 - val_loss: 0.5282\n",
            "Epoch 33/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5794 - val_loss: 0.5279\n",
            "Epoch 34/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5790 - val_loss: 0.5285\n",
            "Epoch 35/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5792 - val_loss: 0.5287\n",
            "Epoch 36/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5788 - val_loss: 0.5277\n",
            "Epoch 37/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5786 - val_loss: 0.5283\n",
            "Epoch 38/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5787 - val_loss: 0.5281\n",
            "Epoch 39/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5784 - val_loss: 0.5275\n",
            "Epoch 40/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5783 - val_loss: 0.5274\n",
            "Epoch 41/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5780 - val_loss: 0.5273\n",
            "Epoch 42/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5779 - val_loss: 0.5276\n",
            "Epoch 43/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5779 - val_loss: 0.5271\n",
            "Epoch 44/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5777 - val_loss: 0.5275\n",
            "Epoch 45/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5776 - val_loss: 0.5279\n",
            "Epoch 46/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.5776 - val_loss: 0.5263\n",
            "Epoch 47/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5772 - val_loss: 0.5271\n",
            "Epoch 48/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5774 - val_loss: 0.5264\n",
            "Epoch 49/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5776 - val_loss: 0.5277\n",
            "Epoch 50/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5774 - val_loss: 0.5264\n",
            "Epoch 51/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5768 - val_loss: 0.5266\n",
            "Epoch 52/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5772 - val_loss: 0.5256\n",
            "Epoch 53/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5771 - val_loss: 0.5261\n",
            "Epoch 54/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5765 - val_loss: 0.5268\n",
            "Epoch 55/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5767 - val_loss: 0.5271\n",
            "Epoch 56/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.5762 - val_loss: 0.5264\n",
            "Epoch 57/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.5766 - val_loss: 0.5263\n",
            "Epoch 58/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.5765 - val_loss: 0.5285\n",
            "Epoch 59/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5759 - val_loss: 0.5254\n",
            "Epoch 60/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5759 - val_loss: 0.5254\n",
            "Epoch 61/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5618 - val_loss: 0.4926\n",
            "Epoch 62/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5218 - val_loss: 0.4769\n",
            "Epoch 63/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.5031 - val_loss: 0.4565\n",
            "Epoch 64/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4931 - val_loss: 0.4510\n",
            "Epoch 65/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4924 - val_loss: 0.4468\n",
            "Epoch 66/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4930 - val_loss: 0.4437\n",
            "Epoch 67/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4848 - val_loss: 0.4553\n",
            "Epoch 68/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4888 - val_loss: 0.4534\n",
            "Epoch 69/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4819 - val_loss: 0.4427\n",
            "Epoch 70/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4758 - val_loss: 0.4464\n",
            "Epoch 71/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4785 - val_loss: 0.4701\n",
            "Epoch 72/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4774 - val_loss: 0.4435\n",
            "Epoch 73/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4724 - val_loss: 0.4374\n",
            "Epoch 74/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4747 - val_loss: 0.4425\n",
            "Epoch 75/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4715 - val_loss: 0.4375\n",
            "Epoch 76/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4715 - val_loss: 0.4367\n",
            "Epoch 77/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4694 - val_loss: 0.4342\n",
            "Epoch 78/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4706 - val_loss: 0.4365\n",
            "Epoch 79/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4683 - val_loss: 0.4358\n",
            "Epoch 80/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4678 - val_loss: 0.4371\n",
            "Epoch 81/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4668 - val_loss: 0.4348\n",
            "Epoch 82/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4674 - val_loss: 0.4371\n",
            "Epoch 83/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4663 - val_loss: 0.4416\n",
            "Epoch 84/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4671 - val_loss: 0.4420\n",
            "Epoch 85/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4651 - val_loss: 0.4388\n",
            "Epoch 86/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4667 - val_loss: 0.4417\n",
            "Epoch 87/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4669 - val_loss: 0.4338\n",
            "Epoch 88/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4873 - val_loss: 0.4608\n",
            "Epoch 89/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4794 - val_loss: 0.4512\n",
            "Epoch 90/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4742 - val_loss: 0.4481\n",
            "Epoch 91/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4707 - val_loss: 0.4426\n",
            "Epoch 92/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4716 - val_loss: 0.4454\n",
            "Epoch 93/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4664 - val_loss: 0.4434\n",
            "Epoch 94/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4653 - val_loss: 0.4417\n",
            "Epoch 95/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4658 - val_loss: 0.4420\n",
            "Epoch 96/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4642 - val_loss: 0.4393\n",
            "Epoch 97/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4629 - val_loss: 0.4372\n",
            "Epoch 98/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4663 - val_loss: 0.4366\n",
            "Epoch 99/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4621 - val_loss: 0.4354\n",
            "Epoch 100/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4625 - val_loss: 0.4379\n",
            "Epoch 101/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4633 - val_loss: 0.4367\n",
            "Epoch 102/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4617 - val_loss: 0.4398\n",
            "Epoch 103/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4610 - val_loss: 0.4453\n",
            "Epoch 104/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4669 - val_loss: 0.4408\n",
            "Epoch 105/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4632 - val_loss: 0.4345\n",
            "Epoch 106/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4609 - val_loss: 0.4396\n",
            "Epoch 107/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4604 - val_loss: 0.4393\n",
            "Epoch 108/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4616 - val_loss: 0.4386\n",
            "Epoch 109/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4601 - val_loss: 0.4411\n",
            "Epoch 110/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4602 - val_loss: 0.4373\n",
            "Epoch 111/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4602 - val_loss: 0.4358\n",
            "Epoch 112/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4634 - val_loss: 0.4372\n",
            "Epoch 113/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4595 - val_loss: 0.4347\n",
            "Epoch 114/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4598 - val_loss: 0.4435\n",
            "Epoch 115/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4587 - val_loss: 0.4395\n",
            "Epoch 116/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4593 - val_loss: 0.4384\n",
            "Epoch 117/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4608 - val_loss: 0.4382\n",
            "Epoch 118/1000\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.4597 - val_loss: 0.4359\n",
            "Epoch 119/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4580 - val_loss: 0.4374\n",
            "Epoch 120/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4613 - val_loss: 0.4449\n",
            "Epoch 121/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4581 - val_loss: 0.4444\n",
            "Epoch 122/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4574 - val_loss: 0.4385\n",
            "Epoch 123/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4560 - val_loss: 0.4386\n",
            "Epoch 124/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4577 - val_loss: 0.4437\n",
            "Epoch 125/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4599 - val_loss: 0.4379\n",
            "Epoch 126/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4567 - val_loss: 0.4438\n",
            "Epoch 127/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4596 - val_loss: 0.4403\n",
            "Epoch 128/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4573 - val_loss: 0.4392\n",
            "Epoch 129/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4588 - val_loss: 0.4345\n",
            "Epoch 130/1000\n",
            "70/70 [==============================] - 1s 19ms/step - loss: 0.4597 - val_loss: 0.4383\n",
            "Epoch 131/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4581 - val_loss: 0.4424\n",
            "Epoch 132/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4557 - val_loss: 0.4401\n",
            "Epoch 133/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4569 - val_loss: 0.4352\n",
            "Epoch 134/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.4565 - val_loss: 0.4418\n",
            "Epoch 135/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4575 - val_loss: 0.4433\n",
            "Epoch 136/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4579 - val_loss: 0.4433\n",
            "Epoch 137/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4571 - val_loss: 0.4382\n",
            "Epoch 138/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4565 - val_loss: 0.4372\n",
            "Epoch 139/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4608 - val_loss: 0.4627\n",
            "Epoch 140/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4654 - val_loss: 0.4504\n",
            "Epoch 141/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4615 - val_loss: 0.4406\n",
            "Epoch 142/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4558 - val_loss: 0.4363\n",
            "Epoch 143/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4582 - val_loss: 0.4363\n",
            "Epoch 144/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4558 - val_loss: 0.4383\n",
            "Epoch 145/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4555 - val_loss: 0.4355\n",
            "Epoch 146/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4535 - val_loss: 0.4363\n",
            "Epoch 147/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4546 - val_loss: 0.4388\n",
            "Epoch 148/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4549 - val_loss: 0.4420\n",
            "Epoch 149/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4534 - val_loss: 0.4418\n",
            "Epoch 150/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4562 - val_loss: 0.4372\n",
            "Epoch 151/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4559 - val_loss: 0.4333\n",
            "Epoch 152/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4792 - val_loss: 0.4401\n",
            "Epoch 153/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4648 - val_loss: 0.4404\n",
            "Epoch 154/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4572 - val_loss: 0.4365\n",
            "Epoch 155/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4559 - val_loss: 0.4347\n",
            "Epoch 156/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4541 - val_loss: 0.4406\n",
            "Epoch 157/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4577 - val_loss: 0.4358\n",
            "Epoch 158/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4557 - val_loss: 0.4387\n",
            "Epoch 159/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4549 - val_loss: 0.4403\n",
            "Epoch 160/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4551 - val_loss: 0.4462\n",
            "Epoch 161/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4531 - val_loss: 0.4392\n",
            "Epoch 162/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4570 - val_loss: 0.4436\n",
            "Epoch 163/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4568 - val_loss: 0.4368\n",
            "Epoch 164/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4548 - val_loss: 0.4362\n",
            "Epoch 165/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4554 - val_loss: 0.4357\n",
            "Epoch 166/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4545 - val_loss: 0.4356\n",
            "Epoch 167/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4546 - val_loss: 0.4366\n",
            "Epoch 168/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4512 - val_loss: 0.4394\n",
            "Epoch 169/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4539 - val_loss: 0.4388\n",
            "Epoch 170/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4533 - val_loss: 0.4353\n",
            "Epoch 171/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4513 - val_loss: 0.4379\n",
            "Epoch 172/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4556 - val_loss: 0.4356\n",
            "Epoch 173/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4533 - val_loss: 0.4351\n",
            "Epoch 174/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4507 - val_loss: 0.4447\n",
            "Epoch 175/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4519 - val_loss: 0.4426\n",
            "Epoch 176/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4516 - val_loss: 0.4366\n",
            "Epoch 177/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4509 - val_loss: 0.4363\n",
            "Epoch 178/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4497 - val_loss: 0.4344\n",
            "Epoch 179/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4506 - val_loss: 0.4344\n",
            "Epoch 180/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4494 - val_loss: 0.4385\n",
            "Epoch 181/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4493 - val_loss: 0.4387\n",
            "Epoch 182/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4504 - val_loss: 0.4386\n",
            "Epoch 183/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4500 - val_loss: 0.4378\n",
            "Epoch 184/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4510 - val_loss: 0.4390\n",
            "Epoch 185/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4629 - val_loss: 0.4368\n",
            "Epoch 186/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4512 - val_loss: 0.4438\n",
            "Epoch 187/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4526 - val_loss: 0.4361\n",
            "Epoch 188/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4492 - val_loss: 0.4362\n",
            "Epoch 189/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4498 - val_loss: 0.4359\n",
            "Epoch 190/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4482 - val_loss: 0.4370\n",
            "Epoch 191/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4505 - val_loss: 0.4387\n",
            "Epoch 192/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4500 - val_loss: 0.4362\n",
            "Epoch 193/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4596 - val_loss: 0.4343\n",
            "Epoch 194/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4558 - val_loss: 0.4418\n",
            "Epoch 195/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4523 - val_loss: 0.4429\n",
            "Epoch 196/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4501 - val_loss: 0.4365\n",
            "Epoch 197/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4513 - val_loss: 0.4344\n",
            "Epoch 198/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4498 - val_loss: 0.4365\n",
            "Epoch 199/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4477 - val_loss: 0.4363\n",
            "Epoch 200/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4510 - val_loss: 0.4350\n",
            "Epoch 201/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4534 - val_loss: 0.4316\n",
            "Epoch 202/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4575 - val_loss: 0.4315\n",
            "Epoch 203/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4553 - val_loss: 0.4374\n",
            "Epoch 204/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4512 - val_loss: 0.4351\n",
            "Epoch 205/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4496 - val_loss: 0.4369\n",
            "Epoch 206/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4502 - val_loss: 0.4347\n",
            "Epoch 207/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4476 - val_loss: 0.4381\n",
            "Epoch 208/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4493 - val_loss: 0.4372\n",
            "Epoch 209/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4493 - val_loss: 0.4355\n",
            "Epoch 210/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4474 - val_loss: 0.4346\n",
            "Epoch 211/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4497 - val_loss: 0.4321\n",
            "Epoch 212/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4513 - val_loss: 0.4347\n",
            "Epoch 213/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4683 - val_loss: 0.4448\n",
            "Epoch 214/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4689 - val_loss: 0.4451\n",
            "Epoch 215/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4559 - val_loss: 0.4314\n",
            "Epoch 216/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4506 - val_loss: 0.4265\n",
            "Epoch 217/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4466 - val_loss: 0.4256\n",
            "Epoch 218/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4477 - val_loss: 0.4330\n",
            "Epoch 219/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4462 - val_loss: 0.4355\n",
            "Epoch 220/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4469 - val_loss: 0.4442\n",
            "Epoch 221/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4529 - val_loss: 0.4245\n",
            "Epoch 222/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4416 - val_loss: 0.4324\n",
            "Epoch 223/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4486 - val_loss: 0.4307\n",
            "Epoch 224/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4456 - val_loss: 0.4331\n",
            "Epoch 225/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4446 - val_loss: 0.4319\n",
            "Epoch 226/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4433 - val_loss: 0.4341\n",
            "Epoch 227/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4427 - val_loss: 0.4312\n",
            "Epoch 228/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4382 - val_loss: 0.4361\n",
            "Epoch 229/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4401 - val_loss: 0.4315\n",
            "Epoch 230/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4396 - val_loss: 0.4360\n",
            "Epoch 231/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4362 - val_loss: 0.4340\n",
            "Epoch 232/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4371 - val_loss: 0.4384\n",
            "Epoch 233/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4389 - val_loss: 0.4343\n",
            "Epoch 234/1000\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.4373 - val_loss: 0.4296\n",
            "Epoch 235/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.4359 - val_loss: 0.4305\n",
            "Epoch 236/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4374 - val_loss: 0.4331\n",
            "Epoch 237/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4505 - val_loss: 0.4349\n",
            "Epoch 238/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4453 - val_loss: 0.4343\n",
            "Epoch 239/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4380 - val_loss: 0.4297\n",
            "Epoch 240/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4400 - val_loss: 0.4374\n",
            "Epoch 241/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4378 - val_loss: 0.4289\n",
            "Epoch 242/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4354 - val_loss: 0.4373\n",
            "Epoch 243/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4415 - val_loss: 0.4347\n",
            "Epoch 244/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4455 - val_loss: 0.4394\n",
            "Epoch 245/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4345 - val_loss: 0.4404\n",
            "Epoch 246/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4357 - val_loss: 0.4367\n",
            "Epoch 247/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4328 - val_loss: 0.4496\n",
            "Epoch 248/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4326 - val_loss: 0.4447\n",
            "Epoch 249/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4295 - val_loss: 0.4483\n",
            "Epoch 250/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4312 - val_loss: 0.4333\n",
            "Epoch 251/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4347 - val_loss: 0.4367\n",
            "Epoch 252/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4280 - val_loss: 0.4531\n",
            "Epoch 253/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4341 - val_loss: 0.4342\n",
            "Epoch 254/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4319 - val_loss: 0.4380\n",
            "Epoch 255/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4297 - val_loss: 0.4430\n",
            "Epoch 256/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4311 - val_loss: 0.4462\n",
            "Epoch 257/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4285 - val_loss: 0.4433\n",
            "Epoch 258/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4302 - val_loss: 0.4401\n",
            "Epoch 259/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4292 - val_loss: 0.4434\n",
            "Epoch 260/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4298 - val_loss: 0.4344\n",
            "Epoch 261/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4292 - val_loss: 0.4370\n",
            "Epoch 262/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4285 - val_loss: 0.4396\n",
            "Epoch 263/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4269 - val_loss: 0.4422\n",
            "Epoch 264/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4229 - val_loss: 0.4407\n",
            "Epoch 265/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4289 - val_loss: 0.4560\n",
            "Epoch 266/1000\n",
            "70/70 [==============================] - 2s 23ms/step - loss: 0.4313 - val_loss: 0.4477\n",
            "Epoch 267/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4282 - val_loss: 0.4455\n",
            "Epoch 268/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4256 - val_loss: 0.4350\n",
            "Epoch 269/1000\n",
            "70/70 [==============================] - 1s 19ms/step - loss: 0.4324 - val_loss: 0.4365\n",
            "Epoch 270/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4245 - val_loss: 0.4376\n",
            "Epoch 271/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4279 - val_loss: 0.4368\n",
            "Epoch 272/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4278 - val_loss: 0.4458\n",
            "Epoch 273/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4290 - val_loss: 0.4360\n",
            "Epoch 274/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4218 - val_loss: 0.4346\n",
            "Epoch 275/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4268 - val_loss: 0.4412\n",
            "Epoch 276/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4250 - val_loss: 0.4378\n",
            "Epoch 277/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4249 - val_loss: 0.4295\n",
            "Epoch 278/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4289 - val_loss: 0.4390\n",
            "Epoch 279/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4194 - val_loss: 0.4365\n",
            "Epoch 280/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4234 - val_loss: 0.4487\n",
            "Epoch 281/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4261 - val_loss: 0.4597\n",
            "Epoch 282/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4289 - val_loss: 0.4551\n",
            "Epoch 283/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4364 - val_loss: 0.4376\n",
            "Epoch 284/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4268 - val_loss: 0.4370\n",
            "Epoch 285/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4288 - val_loss: 0.4379\n",
            "Epoch 286/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4358 - val_loss: 0.4362\n",
            "Epoch 287/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4332 - val_loss: 0.4370\n",
            "Epoch 288/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4251 - val_loss: 0.4328\n",
            "Epoch 289/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4219 - val_loss: 0.4365\n",
            "Epoch 290/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4219 - val_loss: 0.4414\n",
            "Epoch 291/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4268 - val_loss: 0.4339\n",
            "Epoch 292/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4232 - val_loss: 0.4358\n",
            "Epoch 293/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4220 - val_loss: 0.4387\n",
            "Epoch 294/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4313 - val_loss: 0.4347\n",
            "Epoch 295/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4237 - val_loss: 0.4380\n",
            "Epoch 296/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4212 - val_loss: 0.4450\n",
            "Epoch 297/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4223 - val_loss: 0.4405\n",
            "Epoch 298/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4239 - val_loss: 0.4478\n",
            "Epoch 299/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4228 - val_loss: 0.4455\n",
            "Epoch 300/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4222 - val_loss: 0.4380\n",
            "Epoch 301/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4297 - val_loss: 0.4389\n",
            "Epoch 302/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4238 - val_loss: 0.4438\n",
            "Epoch 303/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4278 - val_loss: 0.4248\n",
            "Epoch 304/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4257 - val_loss: 0.4420\n",
            "Epoch 305/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4295 - val_loss: 0.4403\n",
            "Epoch 306/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4228 - val_loss: 0.4451\n",
            "Epoch 307/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4199 - val_loss: 0.4395\n",
            "Epoch 308/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4222 - val_loss: 0.4483\n",
            "Epoch 309/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4228 - val_loss: 0.4403\n",
            "Epoch 310/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4206 - val_loss: 0.4459\n",
            "Epoch 311/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4254 - val_loss: 0.4380\n",
            "Epoch 312/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4230 - val_loss: 0.4338\n",
            "Epoch 313/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4184 - val_loss: 0.4475\n",
            "Epoch 314/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.4193 - val_loss: 0.4324\n",
            "Epoch 315/1000\n",
            "70/70 [==============================] - 2s 22ms/step - loss: 0.4157 - val_loss: 0.4394\n",
            "Epoch 316/1000\n",
            "70/70 [==============================] - 2s 26ms/step - loss: 0.4198 - val_loss: 0.4425\n",
            "Epoch 317/1000\n",
            "70/70 [==============================] - 2s 30ms/step - loss: 0.4205 - val_loss: 0.4287\n",
            "Epoch 318/1000\n",
            "70/70 [==============================] - 2s 22ms/step - loss: 0.4274 - val_loss: 0.4235\n",
            "Epoch 319/1000\n",
            "70/70 [==============================] - 1s 20ms/step - loss: 0.4181 - val_loss: 0.4420\n",
            "Epoch 320/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4214 - val_loss: 0.4456\n",
            "Epoch 321/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.4175 - val_loss: 0.4316\n",
            "Epoch 322/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4230 - val_loss: 0.4339\n",
            "Epoch 323/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4184 - val_loss: 0.4357\n",
            "Epoch 324/1000\n",
            "70/70 [==============================] - 1s 16ms/step - loss: 0.4280 - val_loss: 0.4338\n",
            "Epoch 325/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4284 - val_loss: 0.4371\n",
            "Epoch 326/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4272 - val_loss: 0.4298\n",
            "Epoch 327/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4216 - val_loss: 0.4483\n",
            "Epoch 328/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4239 - val_loss: 0.4410\n",
            "Epoch 329/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4184 - val_loss: 0.4514\n",
            "Epoch 330/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4146 - val_loss: 0.4441\n",
            "Epoch 331/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4196 - val_loss: 0.4524\n",
            "Epoch 332/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4183 - val_loss: 0.4476\n",
            "Epoch 333/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4164 - val_loss: 0.4332\n",
            "Epoch 334/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4154 - val_loss: 0.4428\n",
            "Epoch 335/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4186 - val_loss: 0.4435\n",
            "Epoch 336/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4198 - val_loss: 0.4436\n",
            "Epoch 337/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4158 - val_loss: 0.4434\n",
            "Epoch 338/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4254 - val_loss: 0.4299\n",
            "Epoch 339/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4229 - val_loss: 0.4520\n",
            "Epoch 340/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4217 - val_loss: 0.4398\n",
            "Epoch 341/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4283 - val_loss: 0.4315\n",
            "Epoch 342/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4312 - val_loss: 0.4356\n",
            "Epoch 343/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4226 - val_loss: 0.4326\n",
            "Epoch 344/1000\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.4206 - val_loss: 0.4410\n",
            "Epoch 345/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4315 - val_loss: 0.4335\n",
            "Epoch 346/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4172 - val_loss: 0.4399\n",
            "Epoch 347/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4173 - val_loss: 0.4579\n",
            "Epoch 348/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4170 - val_loss: 0.4303\n",
            "Epoch 349/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4206 - val_loss: 0.4478\n",
            "Epoch 350/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4167 - val_loss: 0.4289\n",
            "Epoch 351/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4213 - val_loss: 0.4448\n",
            "Epoch 352/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4151 - val_loss: 0.4472\n",
            "Epoch 353/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4155 - val_loss: 0.4420\n",
            "Epoch 354/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4175 - val_loss: 0.4405\n",
            "Epoch 355/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4196 - val_loss: 0.4395\n",
            "Epoch 356/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4175 - val_loss: 0.4348\n",
            "Epoch 357/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4178 - val_loss: 0.4459\n",
            "Epoch 358/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4131 - val_loss: 0.4448\n",
            "Epoch 359/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4150 - val_loss: 0.4451\n",
            "Epoch 360/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4159 - val_loss: 0.4510\n",
            "Epoch 361/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4162 - val_loss: 0.4387\n",
            "Epoch 362/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4173 - val_loss: 0.4476\n",
            "Epoch 363/1000\n",
            "70/70 [==============================] - 1s 13ms/step - loss: 0.4140 - val_loss: 0.4415\n",
            "Epoch 364/1000\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.4154 - val_loss: 0.4343\n",
            "Epoch 365/1000\n",
            "70/70 [==============================] - 1s 15ms/step - loss: 0.4185 - val_loss: 0.4463\n",
            "Epoch 366/1000\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.4202 - val_loss: 0.4418\n",
            "Epoch 367/1000\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.4228 - val_loss: 0.4310\n",
            "Epoch 368/1000\n",
            "67/70 [===========================>..] - ETA: 0s - loss: 0.4141"
          ]
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number\n",
        "nb_epochs=1000;    # training epochs\n",
        "temperature = 0.1    # temprature control the smooth of the probabilities\n",
        "\n",
        "\n",
        "##### Data Augument ##### \n",
        "# Add noise\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()    # generate a random deviation: standard deviation of the normal distribution\n",
        "    noise = np.random.normal(0, deviation, vec.shape)    # generate random noise based on deviation\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)    # apply add_noise() function to each input for trianing\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)    # generate batches of noisy training data\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "# replace softmax\n",
        "x = Dense(3, activation=\"sigmoid\")(x)\n",
        "\n",
        "# add normalization layer\n",
        "pred = layers.Lambda(lambda x: x/(K.sum(x)+K.epsilon()))(x)\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "waeLOQUbYF_I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adzI5AVDMv70"
      },
      "source": [
        "#3. Data Splitting Test for Euclidean distance loss#\n",
        "In thie part, the data will be splitted to:\n",
        "- 3.1 train on one part of the color space and test on an the other part\n",
        "- 3.2 train on data under 1 light and test on 1 light condition \n",
        "- 3.3 train on data under 3 light and test on 1 light condition \n",
        "  \n",
        "In total, 200 color pairs and 4 lights."
      ],
      "id": "adzI5AVDMv70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk6qZ4HcObF_"
      },
      "source": [
        "##3.1 Train on part of the color space and test on the rest part ##\n",
        "\n",
        "loss: 0.0094 - val_loss: 0.0064"
      ],
      "id": "qk6qZ4HcObF_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4s2eIDL2Npp0",
        "outputId": "bbca9808-6297-43a5-fa4e-b64f4248d20b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "70/70 [==============================] - 6s 17ms/step - loss: 0.0204 - val_loss: 0.0066\n",
            "Epoch 2/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0109 - val_loss: 0.0064\n",
            "Epoch 3/500\n",
            "70/70 [==============================] - 1s 14ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 4/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0064\n",
            "Epoch 5/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 6/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 7/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0065\n",
            "Epoch 8/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0062\n",
            "Epoch 9/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0062\n",
            "Epoch 10/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0065\n",
            "Epoch 11/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0064\n",
            "Epoch 12/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0064\n",
            "Epoch 13/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0064\n",
            "Epoch 14/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 15/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0064\n",
            "Epoch 16/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 17/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 18/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0108 - val_loss: 0.0071\n",
            "Epoch 19/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.0110 - val_loss: 0.0063\n",
            "Epoch 20/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 21/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 22/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0108 - val_loss: 0.0064\n",
            "Epoch 23/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0065\n",
            "Epoch 24/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 25/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 26/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 27/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0064\n",
            "Epoch 28/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0062\n",
            "Epoch 29/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 30/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0062\n",
            "Epoch 31/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 32/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0066\n",
            "Epoch 33/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0109 - val_loss: 0.0066\n",
            "Epoch 34/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 35/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0062\n",
            "Epoch 36/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0107 - val_loss: 0.0064\n",
            "Epoch 37/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 38/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0109 - val_loss: 0.0065\n",
            "Epoch 39/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0107 - val_loss: 0.0064\n",
            "Epoch 40/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0107 - val_loss: 0.0064\n",
            "Epoch 41/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0107 - val_loss: 0.0062\n",
            "Epoch 42/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 43/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0064\n",
            "Epoch 44/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0064\n",
            "Epoch 45/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 46/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0064\n",
            "Epoch 47/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 48/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0064\n",
            "Epoch 49/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0065\n",
            "Epoch 50/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0066\n",
            "Epoch 51/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 52/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0062\n",
            "Epoch 53/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 54/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0064\n",
            "Epoch 55/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 56/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 57/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0064\n",
            "Epoch 58/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0064\n",
            "Epoch 59/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 60/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0108 - val_loss: 0.0064\n",
            "Epoch 61/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 62/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 63/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 64/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 65/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 66/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0065\n",
            "Epoch 67/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0063\n",
            "Epoch 68/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 69/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0062\n",
            "Epoch 70/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 71/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 72/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0064\n",
            "Epoch 73/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0064\n",
            "Epoch 74/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 75/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0066\n",
            "Epoch 76/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 77/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0062\n",
            "Epoch 78/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0064\n",
            "Epoch 79/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 80/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 81/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 82/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0105 - val_loss: 0.0063\n",
            "Epoch 83/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0106 - val_loss: 0.0064\n",
            "Epoch 84/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0062\n",
            "Epoch 85/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 86/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 87/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 88/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0064\n",
            "Epoch 89/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 90/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 91/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0066\n",
            "Epoch 92/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0063\n",
            "Epoch 93/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 94/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0063\n",
            "Epoch 95/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0069\n",
            "Epoch 96/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0066\n",
            "Epoch 97/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0066\n",
            "Epoch 98/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 99/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0062\n",
            "Epoch 100/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0066\n",
            "Epoch 101/500\n",
            "70/70 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 102/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 103/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0104 - val_loss: 0.0070\n",
            "Epoch 104/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 105/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0067\n",
            "Epoch 106/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 107/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0103 - val_loss: 0.0066\n",
            "Epoch 108/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0103 - val_loss: 0.0066\n",
            "Epoch 109/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 110/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0062\n",
            "Epoch 111/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 112/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0062\n",
            "Epoch 113/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 114/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0103 - val_loss: 0.0065\n",
            "Epoch 115/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 116/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0104 - val_loss: 0.0066\n",
            "Epoch 117/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 118/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0104 - val_loss: 0.0062\n",
            "Epoch 119/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0103 - val_loss: 0.0063\n",
            "Epoch 120/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0103 - val_loss: 0.0063\n",
            "Epoch 121/500\n",
            "70/70 [==============================] - 1s 11ms/step - loss: 0.0103 - val_loss: 0.0067\n",
            "Epoch 122/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 123/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 124/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0062\n",
            "Epoch 125/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0064\n",
            "Epoch 126/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0063\n",
            "Epoch 127/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 128/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0062\n",
            "Epoch 129/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0064\n",
            "Epoch 130/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0061\n",
            "Epoch 131/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0103 - val_loss: 0.0072\n",
            "Epoch 132/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0062\n",
            "Epoch 133/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0068\n",
            "Epoch 134/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0065\n",
            "Epoch 135/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 136/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0068\n",
            "Epoch 137/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0062\n",
            "Epoch 138/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0062\n",
            "Epoch 139/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0062\n",
            "Epoch 140/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0062\n",
            "Epoch 141/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0104 - val_loss: 0.0061\n",
            "Epoch 142/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 143/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0104 - val_loss: 0.0061\n",
            "Epoch 144/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0066\n",
            "Epoch 145/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 146/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0104 - val_loss: 0.0061\n",
            "Epoch 147/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0065\n",
            "Epoch 148/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0067\n",
            "Epoch 149/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0063\n",
            "Epoch 150/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0069\n",
            "Epoch 151/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0103 - val_loss: 0.0066\n",
            "Epoch 152/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0105 - val_loss: 0.0063\n",
            "Epoch 153/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0068\n",
            "Epoch 154/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0064\n",
            "Epoch 155/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0063\n",
            "Epoch 156/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "Epoch 157/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 158/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0063\n",
            "Epoch 159/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0065\n",
            "Epoch 160/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0104 - val_loss: 0.0062\n",
            "Epoch 161/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0062\n",
            "Epoch 162/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0103 - val_loss: 0.0061\n",
            "Epoch 163/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 164/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0105 - val_loss: 0.0062\n",
            "Epoch 165/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0104 - val_loss: 0.0061\n",
            "Epoch 166/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0102 - val_loss: 0.0067\n",
            "Epoch 167/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0062\n",
            "Epoch 168/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0063\n",
            "Epoch 169/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0063\n",
            "Epoch 170/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0064\n",
            "Epoch 171/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0105 - val_loss: 0.0061\n",
            "Epoch 172/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0065\n",
            "Epoch 173/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0065\n",
            "Epoch 174/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0062\n",
            "Epoch 175/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0104 - val_loss: 0.0061\n",
            "Epoch 176/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 177/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 178/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0061\n",
            "Epoch 179/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0062\n",
            "Epoch 180/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0065\n",
            "Epoch 181/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0063\n",
            "Epoch 182/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0062\n",
            "Epoch 183/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 184/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0063\n",
            "Epoch 185/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0064\n",
            "Epoch 186/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0100 - val_loss: 0.0064\n",
            "Epoch 187/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 188/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0103 - val_loss: 0.0065\n",
            "Epoch 189/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0103 - val_loss: 0.0064\n",
            "Epoch 190/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 191/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0066\n",
            "Epoch 192/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0062\n",
            "Epoch 193/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "Epoch 194/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0066\n",
            "Epoch 195/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0063\n",
            "Epoch 196/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0064\n",
            "Epoch 197/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0065\n",
            "Epoch 198/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 199/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0104 - val_loss: 0.0062\n",
            "Epoch 200/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0062\n",
            "Epoch 201/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 202/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "Epoch 203/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 204/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 205/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 206/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0061\n",
            "Epoch 207/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 208/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 209/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0101 - val_loss: 0.0062\n",
            "Epoch 210/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0100 - val_loss: 0.0061\n",
            "Epoch 211/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0100 - val_loss: 0.0065\n",
            "Epoch 212/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0101 - val_loss: 0.0061\n",
            "Epoch 213/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 214/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0062\n",
            "Epoch 215/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0062\n",
            "Epoch 216/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0063\n",
            "Epoch 217/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "Epoch 218/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0064\n",
            "Epoch 219/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0062\n",
            "Epoch 220/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "Epoch 221/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0065\n",
            "Epoch 222/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 223/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0068\n",
            "Epoch 224/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0068\n",
            "Epoch 225/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "Epoch 226/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0062\n",
            "Epoch 227/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 228/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0062\n",
            "Epoch 229/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0064\n",
            "Epoch 230/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0064\n",
            "Epoch 231/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0100 - val_loss: 0.0061\n",
            "Epoch 232/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0097 - val_loss: 0.0068\n",
            "Epoch 233/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 234/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0061\n",
            "Epoch 235/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0063\n",
            "Epoch 236/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 237/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0067\n",
            "Epoch 238/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0100 - val_loss: 0.0061\n",
            "Epoch 239/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0060\n",
            "Epoch 240/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0061\n",
            "Epoch 241/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0064\n",
            "Epoch 242/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0061\n",
            "Epoch 243/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0061\n",
            "Epoch 244/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0068\n",
            "Epoch 245/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0065\n",
            "Epoch 246/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0105 - val_loss: 0.0061\n",
            "Epoch 247/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0064\n",
            "Epoch 248/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "Epoch 249/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0066\n",
            "Epoch 250/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0097 - val_loss: 0.0062\n",
            "Epoch 251/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0100 - val_loss: 0.0061\n",
            "Epoch 252/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 253/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0107 - val_loss: 0.0062\n",
            "Epoch 254/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 255/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0063\n",
            "Epoch 256/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0109 - val_loss: 0.0063\n",
            "Epoch 257/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 258/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0062\n",
            "Epoch 259/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0070\n",
            "Epoch 260/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0061\n",
            "Epoch 261/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0061\n",
            "Epoch 262/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0062\n",
            "Epoch 263/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "Epoch 264/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0063\n",
            "Epoch 265/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 266/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 267/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0097 - val_loss: 0.0073\n",
            "Epoch 268/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0063\n",
            "Epoch 269/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0066\n",
            "Epoch 270/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 271/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0100 - val_loss: 0.0065\n",
            "Epoch 272/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0098 - val_loss: 0.0067\n",
            "Epoch 273/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0098 - val_loss: 0.0065\n",
            "Epoch 274/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0060\n",
            "Epoch 275/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 276/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0064\n",
            "Epoch 277/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 278/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0061\n",
            "Epoch 279/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 280/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0063\n",
            "Epoch 281/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 282/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 283/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 284/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 285/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 286/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0068\n",
            "Epoch 287/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 288/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 289/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0067\n",
            "Epoch 290/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0069\n",
            "Epoch 291/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0067\n",
            "Epoch 292/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 293/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0099 - val_loss: 0.0064\n",
            "Epoch 294/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0097 - val_loss: 0.0069\n",
            "Epoch 295/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 296/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0099 - val_loss: 0.0067\n",
            "Epoch 297/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0065\n",
            "Epoch 298/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 299/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0067\n",
            "Epoch 300/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0063\n",
            "Epoch 301/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0063\n",
            "Epoch 302/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0101 - val_loss: 0.0062\n",
            "Epoch 303/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0062\n",
            "Epoch 304/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0065\n",
            "Epoch 305/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 306/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0096 - val_loss: 0.0066\n",
            "Epoch 307/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 308/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0062\n",
            "Epoch 309/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 310/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0060\n",
            "Epoch 311/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 312/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 313/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 314/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0061\n",
            "Epoch 315/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0062\n",
            "Epoch 316/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0095 - val_loss: 0.0065\n",
            "Epoch 317/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0099 - val_loss: 0.0064\n",
            "Epoch 318/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0097 - val_loss: 0.0065\n",
            "Epoch 319/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 320/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0061\n",
            "Epoch 321/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0064\n",
            "Epoch 322/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0106 - val_loss: 0.0063\n",
            "Epoch 323/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0064\n",
            "Epoch 324/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0062\n",
            "Epoch 325/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0104 - val_loss: 0.0062\n",
            "Epoch 326/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 327/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0063\n",
            "Epoch 328/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0064\n",
            "Epoch 329/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 330/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 331/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0061\n",
            "Epoch 332/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0104 - val_loss: 0.0060\n",
            "Epoch 333/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 334/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 335/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 336/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0064\n",
            "Epoch 337/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0097 - val_loss: 0.0066\n",
            "Epoch 338/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0095 - val_loss: 0.0062\n",
            "Epoch 339/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 340/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0100 - val_loss: 0.0063\n",
            "Epoch 341/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0063\n",
            "Epoch 342/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 343/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 344/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0062\n",
            "Epoch 345/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0067\n",
            "Epoch 346/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0066\n",
            "Epoch 347/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 348/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 349/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0097 - val_loss: 0.0061\n",
            "Epoch 350/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0063\n",
            "Epoch 351/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0067\n",
            "Epoch 352/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0062\n",
            "Epoch 353/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0065\n",
            "Epoch 354/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0062\n",
            "Epoch 355/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 356/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0061\n",
            "Epoch 357/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0095 - val_loss: 0.0065\n",
            "Epoch 358/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0098 - val_loss: 0.0060\n",
            "Epoch 359/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0100 - val_loss: 0.0061\n",
            "Epoch 360/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0063\n",
            "Epoch 361/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0063\n",
            "Epoch 362/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0062\n",
            "Epoch 363/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0066\n",
            "Epoch 364/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0062\n",
            "Epoch 365/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0060\n",
            "Epoch 366/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0067\n",
            "Epoch 367/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0062\n",
            "Epoch 368/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0092 - val_loss: 0.0073\n",
            "Epoch 369/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0064\n",
            "Epoch 370/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0063\n",
            "Epoch 371/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 372/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0070\n",
            "Epoch 373/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0066\n",
            "Epoch 374/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0065\n",
            "Epoch 375/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0065\n",
            "Epoch 376/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 377/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0064\n",
            "Epoch 378/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0062\n",
            "Epoch 379/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0098 - val_loss: 0.0065\n",
            "Epoch 380/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0096 - val_loss: 0.0063\n",
            "Epoch 381/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0095 - val_loss: 0.0068\n",
            "Epoch 382/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 383/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 384/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0064\n",
            "Epoch 385/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 386/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 387/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0093 - val_loss: 0.0063\n",
            "Epoch 388/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0066\n",
            "Epoch 389/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 390/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 391/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 392/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0065\n",
            "Epoch 393/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0065\n",
            "Epoch 394/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0066\n",
            "Epoch 395/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0096 - val_loss: 0.0064\n",
            "Epoch 396/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0095 - val_loss: 0.0060\n",
            "Epoch 397/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 398/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 399/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0093 - val_loss: 0.0065\n",
            "Epoch 400/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 401/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0099 - val_loss: 0.0062\n",
            "Epoch 402/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0062\n",
            "Epoch 403/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 404/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0097 - val_loss: 0.0065\n",
            "Epoch 405/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 406/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0061\n",
            "Epoch 407/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0061\n",
            "Epoch 408/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0101 - val_loss: 0.0065\n",
            "Epoch 409/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0061\n",
            "Epoch 410/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0096 - val_loss: 0.0063\n",
            "Epoch 411/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0064\n",
            "Epoch 412/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0070\n",
            "Epoch 413/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 414/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0067\n",
            "Epoch 415/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0065\n",
            "Epoch 416/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0094 - val_loss: 0.0074\n",
            "Epoch 417/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0062\n",
            "Epoch 418/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0072\n",
            "Epoch 419/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0094 - val_loss: 0.0063\n",
            "Epoch 420/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0096 - val_loss: 0.0061\n",
            "Epoch 421/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0096 - val_loss: 0.0065\n",
            "Epoch 422/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0066\n",
            "Epoch 423/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0084\n",
            "Epoch 424/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0066\n",
            "Epoch 425/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0095 - val_loss: 0.0067\n",
            "Epoch 426/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0065\n",
            "Epoch 427/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0096 - val_loss: 0.0068\n",
            "Epoch 428/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0067\n",
            "Epoch 429/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0065\n",
            "Epoch 430/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0066\n",
            "Epoch 431/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0064\n",
            "Epoch 432/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0064\n",
            "Epoch 433/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0063\n",
            "Epoch 434/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0062\n",
            "Epoch 435/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0067\n",
            "Epoch 436/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0064\n",
            "Epoch 437/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 438/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0062\n",
            "Epoch 439/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0063\n",
            "Epoch 440/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0096 - val_loss: 0.0069\n",
            "Epoch 441/500\n",
            "70/70 [==============================] - 1s 10ms/step - loss: 0.0096 - val_loss: 0.0065\n",
            "Epoch 442/500\n",
            "70/70 [==============================] - 1s 8ms/step - loss: 0.0096 - val_loss: 0.0063\n",
            "Epoch 443/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0093 - val_loss: 0.0071\n",
            "Epoch 444/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0067\n",
            "Epoch 445/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0068\n",
            "Epoch 446/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0068\n",
            "Epoch 447/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0065\n",
            "Epoch 448/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0063\n",
            "Epoch 449/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0066\n",
            "Epoch 450/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 451/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0065\n",
            "Epoch 452/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0093 - val_loss: 0.0068\n",
            "Epoch 453/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0092 - val_loss: 0.0065\n",
            "Epoch 454/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0102 - val_loss: 0.0063\n",
            "Epoch 455/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0103 - val_loss: 0.0064\n",
            "Epoch 456/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0060\n",
            "Epoch 457/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0100 - val_loss: 0.0061\n",
            "Epoch 458/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0067\n",
            "Epoch 459/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0062\n",
            "Epoch 460/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0096 - val_loss: 0.0065\n",
            "Epoch 461/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0096 - val_loss: 0.0065\n",
            "Epoch 462/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0093 - val_loss: 0.0072\n",
            "Epoch 463/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0065\n",
            "Epoch 464/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0065\n",
            "Epoch 465/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0092 - val_loss: 0.0068\n",
            "Epoch 466/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0067\n",
            "Epoch 467/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0062\n",
            "Epoch 468/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0095 - val_loss: 0.0068\n",
            "Epoch 469/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0068\n",
            "Epoch 470/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0092 - val_loss: 0.0062\n",
            "Epoch 471/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0064\n",
            "Epoch 472/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0066\n",
            "Epoch 473/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 474/500\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0063\n",
            "Epoch 475/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0063\n",
            "Epoch 476/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0062\n",
            "Epoch 477/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0095 - val_loss: 0.0067\n",
            "Epoch 478/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0064\n",
            "Epoch 479/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0064\n",
            "Epoch 480/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0066\n",
            "Epoch 481/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0093 - val_loss: 0.0063\n",
            "Epoch 482/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0096 - val_loss: 0.0061\n",
            "Epoch 483/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0092 - val_loss: 0.0064\n",
            "Epoch 484/500\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0092 - val_loss: 0.0069\n",
            "Epoch 485/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0091 - val_loss: 0.0073\n",
            "Epoch 486/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0063\n",
            "Epoch 487/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0066\n",
            "Epoch 488/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0061\n",
            "Epoch 489/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0062\n",
            "Epoch 490/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0065\n",
            "Epoch 491/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0064\n",
            "Epoch 492/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0095 - val_loss: 0.0066\n",
            "Epoch 493/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0097 - val_loss: 0.0065\n",
            "Epoch 494/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0092 - val_loss: 0.0066\n",
            "Epoch 495/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0060\n",
            "Epoch 496/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0091\n",
            "Epoch 497/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0063\n",
            "Epoch 498/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0069\n",
            "Epoch 499/500\n",
            "70/70 [==============================] - 1s 7ms/step - loss: 0.0096 - val_loss: 0.0065\n",
            "Epoch 500/500\n",
            "70/70 [==============================] - 0s 7ms/step - loss: 0.0094 - val_loss: 0.0065\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "8/8 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f59c21e2260>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=512    # training units number set as 256\n",
        "nb_epochs=500;    # training epochs change to 1000\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "# Randomly choose indices for training and validation\n",
        "train_indices = random.sample(range(200), k=160)\n",
        "val_indices = [i for i in range(200) if i not in train_indices]\n",
        "\n",
        "# Construct the training set\n",
        "train_data = pd.concat([condition1.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition2.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition3.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition4.iloc[train_indices[0]:train_indices[-1]+1, :]])\n",
        "\n",
        "# Construct the validation set\n",
        "val_data = pd.concat([condition1.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition2.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition3.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition4.iloc[val_indices[0]:val_indices[-1]+1, :]])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = np.asarray(train_set).astype(np.float32)\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-4))    # Learning rate changes 1e-5 to 1e-4\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "4s2eIDL2Npp0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWLs2EQZi7OW"
      },
      "source": [
        "##3.2 Train on data under 1 light and test on 1 light condition ##\n",
        "Randomly choose one lighting condition for testing and the 1 condition for training.\n",
        "\n",
        "loss: 0.0216 - val_loss: 0.6389  \n",
        "Significantly higher than naive prediction(0.11)"
      ],
      "id": "oWLs2EQZi7OW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fDPx2kPSow29",
        "outputId": "dfd636dc-7c53-4c07-a2fa-90771d66634c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "25/25 [==============================] - 4s 17ms/step - loss: 2.1460 - val_loss: 1.9866\n",
            "Epoch 2/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 1.9185 - val_loss: 1.8915\n",
            "Epoch 3/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 1.6983 - val_loss: 1.7905\n",
            "Epoch 4/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 1.5107 - val_loss: 1.6821\n",
            "Epoch 5/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 1.3406 - val_loss: 1.5765\n",
            "Epoch 6/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 1.1806 - val_loss: 1.4711\n",
            "Epoch 7/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 1.0238 - val_loss: 1.3734\n",
            "Epoch 8/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.9355 - val_loss: 1.2910\n",
            "Epoch 9/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.8394 - val_loss: 1.2137\n",
            "Epoch 10/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.7574 - val_loss: 1.1519\n",
            "Epoch 11/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.7135 - val_loss: 1.0960\n",
            "Epoch 12/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.6831 - val_loss: 1.0540\n",
            "Epoch 13/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6547 - val_loss: 1.0223\n",
            "Epoch 14/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6314 - val_loss: 0.9868\n",
            "Epoch 15/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5907 - val_loss: 0.9663\n",
            "Epoch 16/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5702 - val_loss: 0.9433\n",
            "Epoch 17/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5462 - val_loss: 0.9289\n",
            "Epoch 18/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5276 - val_loss: 0.8969\n",
            "Epoch 19/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5332 - val_loss: 0.8836\n",
            "Epoch 20/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5156 - val_loss: 0.8686\n",
            "Epoch 21/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.5181 - val_loss: 0.8562\n",
            "Epoch 22/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4819 - val_loss: 0.8339\n",
            "Epoch 23/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.4979 - val_loss: 0.8171\n",
            "Epoch 24/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.4516 - val_loss: 0.8064\n",
            "Epoch 25/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.4495 - val_loss: 0.7886\n",
            "Epoch 26/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4510 - val_loss: 0.7738\n",
            "Epoch 27/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4494 - val_loss: 0.7549\n",
            "Epoch 28/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.4330 - val_loss: 0.7449\n",
            "Epoch 29/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4235 - val_loss: 0.7302\n",
            "Epoch 30/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4022 - val_loss: 0.7155\n",
            "Epoch 31/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4001 - val_loss: 0.7107\n",
            "Epoch 32/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4049 - val_loss: 0.6929\n",
            "Epoch 33/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4010 - val_loss: 0.6896\n",
            "Epoch 34/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3913 - val_loss: 0.6691\n",
            "Epoch 35/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3687 - val_loss: 0.6777\n",
            "Epoch 36/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3816 - val_loss: 0.6561\n",
            "Epoch 37/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3697 - val_loss: 0.6456\n",
            "Epoch 38/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3570 - val_loss: 0.6365\n",
            "Epoch 39/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3571 - val_loss: 0.6279\n",
            "Epoch 40/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3590 - val_loss: 0.6203\n",
            "Epoch 41/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3579 - val_loss: 0.6104\n",
            "Epoch 42/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3751 - val_loss: 0.6028\n",
            "Epoch 43/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3523 - val_loss: 0.5989\n",
            "Epoch 44/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3361 - val_loss: 0.5918\n",
            "Epoch 45/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3527 - val_loss: 0.5832\n",
            "Epoch 46/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3339 - val_loss: 0.5773\n",
            "Epoch 47/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.3408 - val_loss: 0.5749\n",
            "Epoch 48/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.3201 - val_loss: 0.5732\n",
            "Epoch 49/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.3258 - val_loss: 0.5689\n",
            "Epoch 50/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3277 - val_loss: 0.5669\n",
            "Epoch 51/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.3561 - val_loss: 0.5619\n",
            "Epoch 52/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.3586 - val_loss: 0.5554\n",
            "Epoch 53/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.3311 - val_loss: 0.5494\n",
            "Epoch 54/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3302 - val_loss: 0.5465\n",
            "Epoch 55/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3531 - val_loss: 0.5468\n",
            "Epoch 56/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3288 - val_loss: 0.5394\n",
            "Epoch 57/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3425 - val_loss: 0.5373\n",
            "Epoch 58/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3427 - val_loss: 0.5391\n",
            "Epoch 59/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3326 - val_loss: 0.5306\n",
            "Epoch 60/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3135 - val_loss: 0.5276\n",
            "Epoch 61/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3284 - val_loss: 0.5284\n",
            "Epoch 62/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3412 - val_loss: 0.5263\n",
            "Epoch 63/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3111 - val_loss: 0.5236\n",
            "Epoch 64/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3211 - val_loss: 0.5197\n",
            "Epoch 65/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3214 - val_loss: 0.5196\n",
            "Epoch 66/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3261 - val_loss: 0.5183\n",
            "Epoch 67/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3124 - val_loss: 0.5192\n",
            "Epoch 68/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3140 - val_loss: 0.5136\n",
            "Epoch 69/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3266 - val_loss: 0.5127\n",
            "Epoch 70/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3077 - val_loss: 0.5097\n",
            "Epoch 71/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3318 - val_loss: 0.5132\n",
            "Epoch 72/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3127 - val_loss: 0.5122\n",
            "Epoch 73/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3115 - val_loss: 0.5123\n",
            "Epoch 74/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3031 - val_loss: 0.5082\n",
            "Epoch 75/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3303 - val_loss: 0.5147\n",
            "Epoch 76/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3149 - val_loss: 0.5100\n",
            "Epoch 77/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3250 - val_loss: 0.5084\n",
            "Epoch 78/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3044 - val_loss: 0.5108\n",
            "Epoch 79/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3167 - val_loss: 0.5065\n",
            "Epoch 80/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3021 - val_loss: 0.5049\n",
            "Epoch 81/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3232 - val_loss: 0.5072\n",
            "Epoch 82/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2895 - val_loss: 0.5098\n",
            "Epoch 83/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3151 - val_loss: 0.5094\n",
            "Epoch 84/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2922 - val_loss: 0.5094\n",
            "Epoch 85/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.3309 - val_loss: 0.5085\n",
            "Epoch 86/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.3199 - val_loss: 0.5015\n",
            "Epoch 87/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3136 - val_loss: 0.5003\n",
            "Epoch 88/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3010 - val_loss: 0.5020\n",
            "Epoch 89/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3079 - val_loss: 0.5006\n",
            "Epoch 90/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3136 - val_loss: 0.4979\n",
            "Epoch 91/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2853 - val_loss: 0.4960\n",
            "Epoch 92/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2947 - val_loss: 0.4979\n",
            "Epoch 93/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2886 - val_loss: 0.4955\n",
            "Epoch 94/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2966 - val_loss: 0.4930\n",
            "Epoch 95/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2950 - val_loss: 0.4931\n",
            "Epoch 96/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3067 - val_loss: 0.4952\n",
            "Epoch 97/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.3060 - val_loss: 0.4944\n",
            "Epoch 98/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2959 - val_loss: 0.4914\n",
            "Epoch 99/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2923 - val_loss: 0.4900\n",
            "Epoch 100/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3034 - val_loss: 0.4921\n",
            "Epoch 101/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2910 - val_loss: 0.4881\n",
            "Epoch 102/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2976 - val_loss: 0.4913\n",
            "Epoch 103/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2817 - val_loss: 0.4903\n",
            "Epoch 104/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2980 - val_loss: 0.4885\n",
            "Epoch 105/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2961 - val_loss: 0.4898\n",
            "Epoch 106/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3036 - val_loss: 0.4919\n",
            "Epoch 107/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2995 - val_loss: 0.4840\n",
            "Epoch 108/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3134 - val_loss: 0.4816\n",
            "Epoch 109/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3182 - val_loss: 0.4834\n",
            "Epoch 110/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2760 - val_loss: 0.4817\n",
            "Epoch 111/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2906 - val_loss: 0.4865\n",
            "Epoch 112/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2958 - val_loss: 0.4827\n",
            "Epoch 113/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3013 - val_loss: 0.4824\n",
            "Epoch 114/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3024 - val_loss: 0.4784\n",
            "Epoch 115/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2974 - val_loss: 0.4831\n",
            "Epoch 116/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3084 - val_loss: 0.4838\n",
            "Epoch 117/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2972 - val_loss: 0.4774\n",
            "Epoch 118/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2954 - val_loss: 0.4751\n",
            "Epoch 119/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2951 - val_loss: 0.4748\n",
            "Epoch 120/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2597 - val_loss: 0.4769\n",
            "Epoch 121/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3240 - val_loss: 0.4732\n",
            "Epoch 122/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2986 - val_loss: 0.4759\n",
            "Epoch 123/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3383 - val_loss: 0.4754\n",
            "Epoch 124/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3008 - val_loss: 0.4723\n",
            "Epoch 125/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2913 - val_loss: 0.4699\n",
            "Epoch 126/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2956 - val_loss: 0.4723\n",
            "Epoch 127/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2912 - val_loss: 0.4728\n",
            "Epoch 128/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2940 - val_loss: 0.4741\n",
            "Epoch 129/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2847 - val_loss: 0.4730\n",
            "Epoch 130/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2801 - val_loss: 0.4750\n",
            "Epoch 131/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3000 - val_loss: 0.4745\n",
            "Epoch 132/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2841 - val_loss: 0.4721\n",
            "Epoch 133/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2916 - val_loss: 0.4724\n",
            "Epoch 134/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2803 - val_loss: 0.4731\n",
            "Epoch 135/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3066 - val_loss: 0.4638\n",
            "Epoch 136/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2700 - val_loss: 0.4594\n",
            "Epoch 137/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2971 - val_loss: 0.4631\n",
            "Epoch 138/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2950 - val_loss: 0.4654\n",
            "Epoch 139/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2694 - val_loss: 0.4619\n",
            "Epoch 140/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2749 - val_loss: 0.4600\n",
            "Epoch 141/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2757 - val_loss: 0.4659\n",
            "Epoch 142/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2842 - val_loss: 0.4711\n",
            "Epoch 143/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2845 - val_loss: 0.4701\n",
            "Epoch 144/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2915 - val_loss: 0.4692\n",
            "Epoch 145/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2880 - val_loss: 0.4683\n",
            "Epoch 146/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3025 - val_loss: 0.4689\n",
            "Epoch 147/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2748 - val_loss: 0.4771\n",
            "Epoch 148/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2963 - val_loss: 0.4716\n",
            "Epoch 149/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2848 - val_loss: 0.4641\n",
            "Epoch 150/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2575 - val_loss: 0.4672\n",
            "Epoch 151/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2720 - val_loss: 0.4652\n",
            "Epoch 152/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2625 - val_loss: 0.4536\n",
            "Epoch 153/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2877 - val_loss: 0.4611\n",
            "Epoch 154/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2769 - val_loss: 0.4644\n",
            "Epoch 155/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2808 - val_loss: 0.4602\n",
            "Epoch 156/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3180 - val_loss: 0.4625\n",
            "Epoch 157/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2934 - val_loss: 0.4638\n",
            "Epoch 158/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2784 - val_loss: 0.4505\n",
            "Epoch 159/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2796 - val_loss: 0.4509\n",
            "Epoch 160/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2853 - val_loss: 0.4513\n",
            "Epoch 161/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2875 - val_loss: 0.4566\n",
            "Epoch 162/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2982 - val_loss: 0.4567\n",
            "Epoch 163/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2907 - val_loss: 0.4572\n",
            "Epoch 164/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2754 - val_loss: 0.4577\n",
            "Epoch 165/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2716 - val_loss: 0.4565\n",
            "Epoch 166/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2671 - val_loss: 0.4572\n",
            "Epoch 167/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2803 - val_loss: 0.4553\n",
            "Epoch 168/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2908 - val_loss: 0.4532\n",
            "Epoch 169/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2747 - val_loss: 0.4518\n",
            "Epoch 170/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2812 - val_loss: 0.4438\n",
            "Epoch 171/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2767 - val_loss: 0.4446\n",
            "Epoch 172/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2944 - val_loss: 0.4480\n",
            "Epoch 173/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2859 - val_loss: 0.4450\n",
            "Epoch 174/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2767 - val_loss: 0.4513\n",
            "Epoch 175/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2719 - val_loss: 0.4545\n",
            "Epoch 176/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2769 - val_loss: 0.4466\n",
            "Epoch 177/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2924 - val_loss: 0.4481\n",
            "Epoch 178/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2850 - val_loss: 0.4474\n",
            "Epoch 179/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2663 - val_loss: 0.4505\n",
            "Epoch 180/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2725 - val_loss: 0.4490\n",
            "Epoch 181/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2789 - val_loss: 0.4487\n",
            "Epoch 182/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2673 - val_loss: 0.4498\n",
            "Epoch 183/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2714 - val_loss: 0.4495\n",
            "Epoch 184/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2627 - val_loss: 0.4466\n",
            "Epoch 185/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2865 - val_loss: 0.4426\n",
            "Epoch 186/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2806 - val_loss: 0.4397\n",
            "Epoch 187/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2979 - val_loss: 0.4412\n",
            "Epoch 188/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2777 - val_loss: 0.4395\n",
            "Epoch 189/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2695 - val_loss: 0.4389\n",
            "Epoch 190/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2786 - val_loss: 0.4390\n",
            "Epoch 191/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2767 - val_loss: 0.4388\n",
            "Epoch 192/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2662 - val_loss: 0.4404\n",
            "Epoch 193/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2687 - val_loss: 0.4481\n",
            "Epoch 194/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2643 - val_loss: 0.4449\n",
            "Epoch 195/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2760 - val_loss: 0.4483\n",
            "Epoch 196/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2767 - val_loss: 0.4501\n",
            "Epoch 197/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2508 - val_loss: 0.4435\n",
            "Epoch 198/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2623 - val_loss: 0.4449\n",
            "Epoch 199/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2697 - val_loss: 0.4395\n",
            "Epoch 200/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2633 - val_loss: 0.4323\n",
            "Epoch 201/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2689 - val_loss: 0.4352\n",
            "Epoch 202/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2765 - val_loss: 0.4338\n",
            "Epoch 203/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2745 - val_loss: 0.4288\n",
            "Epoch 204/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2580 - val_loss: 0.4284\n",
            "Epoch 205/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2645 - val_loss: 0.4310\n",
            "Epoch 206/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2496 - val_loss: 0.4323\n",
            "Epoch 207/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2619 - val_loss: 0.4301\n",
            "Epoch 208/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2755 - val_loss: 0.4293\n",
            "Epoch 209/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2709 - val_loss: 0.4314\n",
            "Epoch 210/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2526 - val_loss: 0.4285\n",
            "Epoch 211/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2596 - val_loss: 0.4296\n",
            "Epoch 212/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2641 - val_loss: 0.4317\n",
            "Epoch 213/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2644 - val_loss: 0.4347\n",
            "Epoch 214/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2621 - val_loss: 0.4366\n",
            "Epoch 215/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2573 - val_loss: 0.4335\n",
            "Epoch 216/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2665 - val_loss: 0.4316\n",
            "Epoch 217/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2488 - val_loss: 0.4328\n",
            "Epoch 218/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2711 - val_loss: 0.4351\n",
            "Epoch 219/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2569 - val_loss: 0.4340\n",
            "Epoch 220/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2469 - val_loss: 0.4321\n",
            "Epoch 221/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2463 - val_loss: 0.4334\n",
            "Epoch 222/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2699 - val_loss: 0.4366\n",
            "Epoch 223/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2565 - val_loss: 0.4347\n",
            "Epoch 224/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2540 - val_loss: 0.4353\n",
            "Epoch 225/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2443 - val_loss: 0.4286\n",
            "Epoch 226/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2602 - val_loss: 0.4265\n",
            "Epoch 227/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2605 - val_loss: 0.4284\n",
            "Epoch 228/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2522 - val_loss: 0.4282\n",
            "Epoch 229/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2375 - val_loss: 0.4257\n",
            "Epoch 230/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2535 - val_loss: 0.4288\n",
            "Epoch 231/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2468 - val_loss: 0.4278\n",
            "Epoch 232/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2410 - val_loss: 0.4256\n",
            "Epoch 233/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2596 - val_loss: 0.4233\n",
            "Epoch 234/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2585 - val_loss: 0.4296\n",
            "Epoch 235/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2454 - val_loss: 0.4337\n",
            "Epoch 236/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2687 - val_loss: 0.4273\n",
            "Epoch 237/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2590 - val_loss: 0.4262\n",
            "Epoch 238/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2678 - val_loss: 0.4274\n",
            "Epoch 239/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2370 - val_loss: 0.4233\n",
            "Epoch 240/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2609 - val_loss: 0.4208\n",
            "Epoch 241/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2409 - val_loss: 0.4297\n",
            "Epoch 242/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2717 - val_loss: 0.4276\n",
            "Epoch 243/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2562 - val_loss: 0.4231\n",
            "Epoch 244/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2565 - val_loss: 0.4224\n",
            "Epoch 245/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2481 - val_loss: 0.4227\n",
            "Epoch 246/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2569 - val_loss: 0.4200\n",
            "Epoch 247/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2484 - val_loss: 0.4165\n",
            "Epoch 248/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2553 - val_loss: 0.4162\n",
            "Epoch 249/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2386 - val_loss: 0.4166\n",
            "Epoch 250/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2365 - val_loss: 0.4222\n",
            "Epoch 251/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2385 - val_loss: 0.4236\n",
            "Epoch 252/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2448 - val_loss: 0.4246\n",
            "Epoch 253/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2413 - val_loss: 0.4259\n",
            "Epoch 254/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2386 - val_loss: 0.4297\n",
            "Epoch 255/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2356 - val_loss: 0.4324\n",
            "Epoch 256/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2585 - val_loss: 0.4286\n",
            "Epoch 257/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2371 - val_loss: 0.4241\n",
            "Epoch 258/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2511 - val_loss: 0.4262\n",
            "Epoch 259/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2464 - val_loss: 0.4245\n",
            "Epoch 260/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2477 - val_loss: 0.4222\n",
            "Epoch 261/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2415 - val_loss: 0.4327\n",
            "Epoch 262/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2426 - val_loss: 0.4333\n",
            "Epoch 263/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2271 - val_loss: 0.4273\n",
            "Epoch 264/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2510 - val_loss: 0.4257\n",
            "Epoch 265/1000\n",
            "25/25 [==============================] - 0s 16ms/step - loss: 0.2346 - val_loss: 0.4258\n",
            "Epoch 266/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2409 - val_loss: 0.4286\n",
            "Epoch 267/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2496 - val_loss: 0.4260\n",
            "Epoch 268/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2371 - val_loss: 0.4272\n",
            "Epoch 269/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2371 - val_loss: 0.4329\n",
            "Epoch 270/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2807 - val_loss: 0.4335\n",
            "Epoch 271/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2537 - val_loss: 0.4280\n",
            "Epoch 272/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2456 - val_loss: 0.4286\n",
            "Epoch 273/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2317 - val_loss: 0.4298\n",
            "Epoch 274/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2439 - val_loss: 0.4284\n",
            "Epoch 275/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2453 - val_loss: 0.4280\n",
            "Epoch 276/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2493 - val_loss: 0.4274\n",
            "Epoch 277/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2403 - val_loss: 0.4329\n",
            "Epoch 278/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2264 - val_loss: 0.4265\n",
            "Epoch 279/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2303 - val_loss: 0.4254\n",
            "Epoch 280/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2209 - val_loss: 0.4226\n",
            "Epoch 281/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2336 - val_loss: 0.4254\n",
            "Epoch 282/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2294 - val_loss: 0.4306\n",
            "Epoch 283/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2529 - val_loss: 0.4286\n",
            "Epoch 284/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2485 - val_loss: 0.4277\n",
            "Epoch 285/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2188 - val_loss: 0.4315\n",
            "Epoch 286/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2486 - val_loss: 0.4352\n",
            "Epoch 287/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2221 - val_loss: 0.4339\n",
            "Epoch 288/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2400 - val_loss: 0.4337\n",
            "Epoch 289/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2358 - val_loss: 0.4369\n",
            "Epoch 290/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2285 - val_loss: 0.4359\n",
            "Epoch 291/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2551 - val_loss: 0.4373\n",
            "Epoch 292/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2397 - val_loss: 0.4355\n",
            "Epoch 293/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2198 - val_loss: 0.4365\n",
            "Epoch 294/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2111 - val_loss: 0.4343\n",
            "Epoch 295/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2353 - val_loss: 0.4322\n",
            "Epoch 296/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2523 - val_loss: 0.4342\n",
            "Epoch 297/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2040 - val_loss: 0.4322\n",
            "Epoch 298/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2334 - val_loss: 0.4348\n",
            "Epoch 299/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2414 - val_loss: 0.4373\n",
            "Epoch 300/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2516 - val_loss: 0.4414\n",
            "Epoch 301/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2427 - val_loss: 0.4423\n",
            "Epoch 302/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2250 - val_loss: 0.4401\n",
            "Epoch 303/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2324 - val_loss: 0.4392\n",
            "Epoch 304/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2518 - val_loss: 0.4433\n",
            "Epoch 305/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2277 - val_loss: 0.4416\n",
            "Epoch 306/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2221 - val_loss: 0.4433\n",
            "Epoch 307/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2482 - val_loss: 0.4368\n",
            "Epoch 308/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2270 - val_loss: 0.4429\n",
            "Epoch 309/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2243 - val_loss: 0.4406\n",
            "Epoch 310/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2211 - val_loss: 0.4355\n",
            "Epoch 311/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2340 - val_loss: 0.4381\n",
            "Epoch 312/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2133 - val_loss: 0.4478\n",
            "Epoch 313/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2276 - val_loss: 0.4402\n",
            "Epoch 314/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2365 - val_loss: 0.4497\n",
            "Epoch 315/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2335 - val_loss: 0.4447\n",
            "Epoch 316/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2204 - val_loss: 0.4398\n",
            "Epoch 317/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2221 - val_loss: 0.4475\n",
            "Epoch 318/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2105 - val_loss: 0.4513\n",
            "Epoch 319/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2154 - val_loss: 0.4499\n",
            "Epoch 320/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2327 - val_loss: 0.4520\n",
            "Epoch 321/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2019 - val_loss: 0.4553\n",
            "Epoch 322/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2140 - val_loss: 0.4510\n",
            "Epoch 323/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2154 - val_loss: 0.4546\n",
            "Epoch 324/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2191 - val_loss: 0.4529\n",
            "Epoch 325/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2124 - val_loss: 0.4555\n",
            "Epoch 326/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2092 - val_loss: 0.4562\n",
            "Epoch 327/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2234 - val_loss: 0.4599\n",
            "Epoch 328/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2208 - val_loss: 0.4611\n",
            "Epoch 329/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2127 - val_loss: 0.4624\n",
            "Epoch 330/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2137 - val_loss: 0.4613\n",
            "Epoch 331/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2241 - val_loss: 0.4624\n",
            "Epoch 332/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2196 - val_loss: 0.4630\n",
            "Epoch 333/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2173 - val_loss: 0.4665\n",
            "Epoch 334/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2163 - val_loss: 0.4722\n",
            "Epoch 335/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2266 - val_loss: 0.4652\n",
            "Epoch 336/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2027 - val_loss: 0.4689\n",
            "Epoch 337/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2170 - val_loss: 0.4711\n",
            "Epoch 338/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2166 - val_loss: 0.4793\n",
            "Epoch 339/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2035 - val_loss: 0.4750\n",
            "Epoch 340/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2365 - val_loss: 0.4743\n",
            "Epoch 341/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2060 - val_loss: 0.4690\n",
            "Epoch 342/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2294 - val_loss: 0.4756\n",
            "Epoch 343/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2139 - val_loss: 0.4758\n",
            "Epoch 344/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2171 - val_loss: 0.4789\n",
            "Epoch 345/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2201 - val_loss: 0.4799\n",
            "Epoch 346/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2031 - val_loss: 0.4808\n",
            "Epoch 347/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1975 - val_loss: 0.4797\n",
            "Epoch 348/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2004 - val_loss: 0.4801\n",
            "Epoch 349/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2386 - val_loss: 0.4851\n",
            "Epoch 350/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2154 - val_loss: 0.4830\n",
            "Epoch 351/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2147 - val_loss: 0.4832\n",
            "Epoch 352/1000\n",
            "25/25 [==============================] - 0s 16ms/step - loss: 0.2122 - val_loss: 0.4831\n",
            "Epoch 353/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2046 - val_loss: 0.4895\n",
            "Epoch 354/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2011 - val_loss: 0.4893\n",
            "Epoch 355/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2253 - val_loss: 0.4876\n",
            "Epoch 356/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2013 - val_loss: 0.4810\n",
            "Epoch 357/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2249 - val_loss: 0.4891\n",
            "Epoch 358/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2041 - val_loss: 0.4905\n",
            "Epoch 359/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2178 - val_loss: 0.4902\n",
            "Epoch 360/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1893 - val_loss: 0.4944\n",
            "Epoch 361/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2098 - val_loss: 0.4993\n",
            "Epoch 362/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1910 - val_loss: 0.4968\n",
            "Epoch 363/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2049 - val_loss: 0.4969\n",
            "Epoch 364/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2209 - val_loss: 0.4984\n",
            "Epoch 365/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1972 - val_loss: 0.4961\n",
            "Epoch 366/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1973 - val_loss: 0.4980\n",
            "Epoch 367/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1879 - val_loss: 0.5110\n",
            "Epoch 368/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1980 - val_loss: 0.5079\n",
            "Epoch 369/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2162 - val_loss: 0.5058\n",
            "Epoch 370/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1896 - val_loss: 0.5127\n",
            "Epoch 371/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1931 - val_loss: 0.5154\n",
            "Epoch 372/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2258 - val_loss: 0.5092\n",
            "Epoch 373/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2109 - val_loss: 0.5048\n",
            "Epoch 374/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1978 - val_loss: 0.5047\n",
            "Epoch 375/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1880 - val_loss: 0.5042\n",
            "Epoch 376/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1955 - val_loss: 0.5104\n",
            "Epoch 377/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1976 - val_loss: 0.5205\n",
            "Epoch 378/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2224 - val_loss: 0.5195\n",
            "Epoch 379/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1802 - val_loss: 0.5181\n",
            "Epoch 380/1000\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2124 - val_loss: 0.5238\n",
            "Epoch 381/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1947 - val_loss: 0.5267\n",
            "Epoch 382/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2098 - val_loss: 0.5256\n",
            "Epoch 383/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2188 - val_loss: 0.5277\n",
            "Epoch 384/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2193 - val_loss: 0.5234\n",
            "Epoch 385/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2014 - val_loss: 0.5294\n",
            "Epoch 386/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2174 - val_loss: 0.5272\n",
            "Epoch 387/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1782 - val_loss: 0.5283\n",
            "Epoch 388/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1928 - val_loss: 0.5397\n",
            "Epoch 389/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1796 - val_loss: 0.5422\n",
            "Epoch 390/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2104 - val_loss: 0.5351\n",
            "Epoch 391/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1957 - val_loss: 0.5373\n",
            "Epoch 392/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1991 - val_loss: 0.5368\n",
            "Epoch 393/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2064 - val_loss: 0.5395\n",
            "Epoch 394/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1931 - val_loss: 0.5421\n",
            "Epoch 395/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1957 - val_loss: 0.5428\n",
            "Epoch 396/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1880 - val_loss: 0.5538\n",
            "Epoch 397/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1775 - val_loss: 0.5478\n",
            "Epoch 398/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1923 - val_loss: 0.5471\n",
            "Epoch 399/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2306 - val_loss: 0.5510\n",
            "Epoch 400/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1885 - val_loss: 0.5539\n",
            "Epoch 401/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1901 - val_loss: 0.5608\n",
            "Epoch 402/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1849 - val_loss: 0.5555\n",
            "Epoch 403/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1768 - val_loss: 0.5559\n",
            "Epoch 404/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2108 - val_loss: 0.5577\n",
            "Epoch 405/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2088 - val_loss: 0.5546\n",
            "Epoch 406/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2091 - val_loss: 0.5633\n",
            "Epoch 407/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1835 - val_loss: 0.5663\n",
            "Epoch 408/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1921 - val_loss: 0.5652\n",
            "Epoch 409/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2016 - val_loss: 0.5737\n",
            "Epoch 410/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1878 - val_loss: 0.5733\n",
            "Epoch 411/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1678 - val_loss: 0.5742\n",
            "Epoch 412/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1973 - val_loss: 0.5668\n",
            "Epoch 413/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1999 - val_loss: 0.5606\n",
            "Epoch 414/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1899 - val_loss: 0.5592\n",
            "Epoch 415/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1683 - val_loss: 0.5585\n",
            "Epoch 416/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1894 - val_loss: 0.5592\n",
            "Epoch 417/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1947 - val_loss: 0.5638\n",
            "Epoch 418/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1823 - val_loss: 0.5644\n",
            "Epoch 419/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2036 - val_loss: 0.5684\n",
            "Epoch 420/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1832 - val_loss: 0.5692\n",
            "Epoch 421/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1942 - val_loss: 0.5654\n",
            "Epoch 422/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1702 - val_loss: 0.5662\n",
            "Epoch 423/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1772 - val_loss: 0.5638\n",
            "Epoch 424/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1741 - val_loss: 0.5601\n",
            "Epoch 425/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1967 - val_loss: 0.5599\n",
            "Epoch 426/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1790 - val_loss: 0.5678\n",
            "Epoch 427/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1783 - val_loss: 0.5703\n",
            "Epoch 428/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1798 - val_loss: 0.5720\n",
            "Epoch 429/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1934 - val_loss: 0.5661\n",
            "Epoch 430/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1909 - val_loss: 0.5639\n",
            "Epoch 431/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2008 - val_loss: 0.5663\n",
            "Epoch 432/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1839 - val_loss: 0.5718\n",
            "Epoch 433/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1995 - val_loss: 0.5760\n",
            "Epoch 434/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1892 - val_loss: 0.5832\n",
            "Epoch 435/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1787 - val_loss: 0.5943\n",
            "Epoch 436/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1730 - val_loss: 0.5828\n",
            "Epoch 437/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1816 - val_loss: 0.5841\n",
            "Epoch 438/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1813 - val_loss: 0.5817\n",
            "Epoch 439/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1968 - val_loss: 0.5937\n",
            "Epoch 440/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1781 - val_loss: 0.5974\n",
            "Epoch 441/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1854 - val_loss: 0.5919\n",
            "Epoch 442/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1860 - val_loss: 0.5886\n",
            "Epoch 443/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1786 - val_loss: 0.5950\n",
            "Epoch 444/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1747 - val_loss: 0.5894\n",
            "Epoch 445/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1741 - val_loss: 0.5858\n",
            "Epoch 446/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1911 - val_loss: 0.5829\n",
            "Epoch 447/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1880 - val_loss: 0.5908\n",
            "Epoch 448/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1707 - val_loss: 0.5925\n",
            "Epoch 449/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1725 - val_loss: 0.5950\n",
            "Epoch 450/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1775 - val_loss: 0.6051\n",
            "Epoch 451/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1694 - val_loss: 0.6093\n",
            "Epoch 452/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1812 - val_loss: 0.6137\n",
            "Epoch 453/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1804 - val_loss: 0.6163\n",
            "Epoch 454/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1654 - val_loss: 0.6156\n",
            "Epoch 455/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1833 - val_loss: 0.6159\n",
            "Epoch 456/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1961 - val_loss: 0.6204\n",
            "Epoch 457/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1818 - val_loss: 0.6129\n",
            "Epoch 458/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1605 - val_loss: 0.6144\n",
            "Epoch 459/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1830 - val_loss: 0.6062\n",
            "Epoch 460/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1654 - val_loss: 0.6089\n",
            "Epoch 461/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1805 - val_loss: 0.6063\n",
            "Epoch 462/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1855 - val_loss: 0.6061\n",
            "Epoch 463/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1814 - val_loss: 0.6023\n",
            "Epoch 464/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1760 - val_loss: 0.6055\n",
            "Epoch 465/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1956 - val_loss: 0.6022\n",
            "Epoch 466/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1775 - val_loss: 0.5976\n",
            "Epoch 467/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1698 - val_loss: 0.6000\n",
            "Epoch 468/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1862 - val_loss: 0.6111\n",
            "Epoch 469/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1744 - val_loss: 0.6115\n",
            "Epoch 470/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1569 - val_loss: 0.6069\n",
            "Epoch 471/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1789 - val_loss: 0.6033\n",
            "Epoch 472/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1727 - val_loss: 0.6079\n",
            "Epoch 473/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1603 - val_loss: 0.6144\n",
            "Epoch 474/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1714 - val_loss: 0.6169\n",
            "Epoch 475/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1811 - val_loss: 0.6231\n",
            "Epoch 476/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1553 - val_loss: 0.6152\n",
            "Epoch 477/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1842 - val_loss: 0.6117\n",
            "Epoch 478/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1648 - val_loss: 0.6174\n",
            "Epoch 479/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1694 - val_loss: 0.6213\n",
            "Epoch 480/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1856 - val_loss: 0.6213\n",
            "Epoch 481/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1681 - val_loss: 0.6196\n",
            "Epoch 482/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1657 - val_loss: 0.6211\n",
            "Epoch 483/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1705 - val_loss: 0.6220\n",
            "Epoch 484/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1696 - val_loss: 0.6095\n",
            "Epoch 485/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1667 - val_loss: 0.6120\n",
            "Epoch 486/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1760 - val_loss: 0.6140\n",
            "Epoch 487/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1766 - val_loss: 0.6159\n",
            "Epoch 488/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1519 - val_loss: 0.6224\n",
            "Epoch 489/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1607 - val_loss: 0.6251\n",
            "Epoch 490/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1514 - val_loss: 0.6225\n",
            "Epoch 491/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1943 - val_loss: 0.6148\n",
            "Epoch 492/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1585 - val_loss: 0.6247\n",
            "Epoch 493/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1612 - val_loss: 0.6207\n",
            "Epoch 494/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1570 - val_loss: 0.6173\n",
            "Epoch 495/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1689 - val_loss: 0.6176\n",
            "Epoch 496/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1741 - val_loss: 0.6204\n",
            "Epoch 497/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1881 - val_loss: 0.6219\n",
            "Epoch 498/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1550 - val_loss: 0.6103\n",
            "Epoch 499/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1506 - val_loss: 0.6126\n",
            "Epoch 500/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1555 - val_loss: 0.6234\n",
            "Epoch 501/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1861 - val_loss: 0.6271\n",
            "Epoch 502/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1586 - val_loss: 0.6252\n",
            "Epoch 503/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1647 - val_loss: 0.6255\n",
            "Epoch 504/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1539 - val_loss: 0.6305\n",
            "Epoch 505/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1847 - val_loss: 0.6288\n",
            "Epoch 506/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1439 - val_loss: 0.6300\n",
            "Epoch 507/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1644 - val_loss: 0.6180\n",
            "Epoch 508/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1548 - val_loss: 0.6144\n",
            "Epoch 509/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1590 - val_loss: 0.6205\n",
            "Epoch 510/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1823 - val_loss: 0.6322\n",
            "Epoch 511/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1516 - val_loss: 0.6399\n",
            "Epoch 512/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1740 - val_loss: 0.6382\n",
            "Epoch 513/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1651 - val_loss: 0.6399\n",
            "Epoch 514/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1648 - val_loss: 0.6441\n",
            "Epoch 515/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1579 - val_loss: 0.6455\n",
            "Epoch 516/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1641 - val_loss: 0.6374\n",
            "Epoch 517/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1702 - val_loss: 0.6370\n",
            "Epoch 518/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1627 - val_loss: 0.6308\n",
            "Epoch 519/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1442 - val_loss: 0.6321\n",
            "Epoch 520/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1652 - val_loss: 0.6325\n",
            "Epoch 521/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1684 - val_loss: 0.6321\n",
            "Epoch 522/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1800 - val_loss: 0.6318\n",
            "Epoch 523/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1807 - val_loss: 0.6417\n",
            "Epoch 524/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1669 - val_loss: 0.6309\n",
            "Epoch 525/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1583 - val_loss: 0.6364\n",
            "Epoch 526/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1585 - val_loss: 0.6266\n",
            "Epoch 527/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1540 - val_loss: 0.6230\n",
            "Epoch 528/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1705 - val_loss: 0.6300\n",
            "Epoch 529/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1680 - val_loss: 0.6290\n",
            "Epoch 530/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1636 - val_loss: 0.6328\n",
            "Epoch 531/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1568 - val_loss: 0.6412\n",
            "Epoch 532/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1546 - val_loss: 0.6460\n",
            "Epoch 533/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1639 - val_loss: 0.6460\n",
            "Epoch 534/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1750 - val_loss: 0.6428\n",
            "Epoch 535/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1565 - val_loss: 0.6490\n",
            "Epoch 536/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1506 - val_loss: 0.6448\n",
            "Epoch 537/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1566 - val_loss: 0.6446\n",
            "Epoch 538/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1611 - val_loss: 0.6421\n",
            "Epoch 539/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1632 - val_loss: 0.6354\n",
            "Epoch 540/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1549 - val_loss: 0.6335\n",
            "Epoch 541/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1457 - val_loss: 0.6420\n",
            "Epoch 542/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1615 - val_loss: 0.6392\n",
            "Epoch 543/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1506 - val_loss: 0.6325\n",
            "Epoch 544/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1467 - val_loss: 0.6414\n",
            "Epoch 545/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1656 - val_loss: 0.6354\n",
            "Epoch 546/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1695 - val_loss: 0.6344\n",
            "Epoch 547/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1598 - val_loss: 0.6313\n",
            "Epoch 548/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1515 - val_loss: 0.6294\n",
            "Epoch 549/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1780 - val_loss: 0.6155\n",
            "Epoch 550/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1567 - val_loss: 0.6235\n",
            "Epoch 551/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1754 - val_loss: 0.6320\n",
            "Epoch 552/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1614 - val_loss: 0.6303\n",
            "Epoch 553/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1601 - val_loss: 0.6375\n",
            "Epoch 554/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1420 - val_loss: 0.6323\n",
            "Epoch 555/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1408 - val_loss: 0.6282\n",
            "Epoch 556/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1549 - val_loss: 0.6262\n",
            "Epoch 557/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1419 - val_loss: 0.6325\n",
            "Epoch 558/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1647 - val_loss: 0.6252\n",
            "Epoch 559/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1794 - val_loss: 0.6257\n",
            "Epoch 560/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1503 - val_loss: 0.6220\n",
            "Epoch 561/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1334 - val_loss: 0.6231\n",
            "Epoch 562/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1717 - val_loss: 0.6255\n",
            "Epoch 563/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1645 - val_loss: 0.6286\n",
            "Epoch 564/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1696 - val_loss: 0.6296\n",
            "Epoch 565/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1610 - val_loss: 0.6193\n",
            "Epoch 566/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1542 - val_loss: 0.6285\n",
            "Epoch 567/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1458 - val_loss: 0.6341\n",
            "Epoch 568/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1470 - val_loss: 0.6400\n",
            "Epoch 569/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1620 - val_loss: 0.6418\n",
            "Epoch 570/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1453 - val_loss: 0.6452\n",
            "Epoch 571/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1545 - val_loss: 0.6392\n",
            "Epoch 572/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1662 - val_loss: 0.6386\n",
            "Epoch 573/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1436 - val_loss: 0.6373\n",
            "Epoch 574/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1501 - val_loss: 0.6321\n",
            "Epoch 575/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1500 - val_loss: 0.6370\n",
            "Epoch 576/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1517 - val_loss: 0.6511\n",
            "Epoch 577/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1541 - val_loss: 0.6436\n",
            "Epoch 578/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1571 - val_loss: 0.6353\n",
            "Epoch 579/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1625 - val_loss: 0.6474\n",
            "Epoch 580/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1642 - val_loss: 0.6494\n",
            "Epoch 581/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1630 - val_loss: 0.6397\n",
            "Epoch 582/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1398 - val_loss: 0.6386\n",
            "Epoch 583/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1364 - val_loss: 0.6496\n",
            "Epoch 584/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1567 - val_loss: 0.6340\n",
            "Epoch 585/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1434 - val_loss: 0.6258\n",
            "Epoch 586/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1479 - val_loss: 0.6307\n",
            "Epoch 587/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1604 - val_loss: 0.6366\n",
            "Epoch 588/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1471 - val_loss: 0.6262\n",
            "Epoch 589/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1661 - val_loss: 0.6330\n",
            "Epoch 590/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1476 - val_loss: 0.6334\n",
            "Epoch 591/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1549 - val_loss: 0.6224\n",
            "Epoch 592/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1472 - val_loss: 0.6276\n",
            "Epoch 593/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1476 - val_loss: 0.6328\n",
            "Epoch 594/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1502 - val_loss: 0.6389\n",
            "Epoch 595/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1611 - val_loss: 0.6383\n",
            "Epoch 596/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1615 - val_loss: 0.6396\n",
            "Epoch 597/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1557 - val_loss: 0.6381\n",
            "Epoch 598/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1538 - val_loss: 0.6251\n",
            "Epoch 599/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1790 - val_loss: 0.6201\n",
            "Epoch 600/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1351 - val_loss: 0.6237\n",
            "Epoch 601/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1373 - val_loss: 0.6283\n",
            "Epoch 602/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1594 - val_loss: 0.6435\n",
            "Epoch 603/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1519 - val_loss: 0.6460\n",
            "Epoch 604/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1301 - val_loss: 0.6442\n",
            "Epoch 605/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1483 - val_loss: 0.6436\n",
            "Epoch 606/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1462 - val_loss: 0.6393\n",
            "Epoch 607/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1577 - val_loss: 0.6382\n",
            "Epoch 608/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1700 - val_loss: 0.6287\n",
            "Epoch 609/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1523 - val_loss: 0.6250\n",
            "Epoch 610/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1492 - val_loss: 0.6269\n",
            "Epoch 611/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1678 - val_loss: 0.6226\n",
            "Epoch 612/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1364 - val_loss: 0.6269\n",
            "Epoch 613/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1758 - val_loss: 0.6218\n",
            "Epoch 614/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1411 - val_loss: 0.6164\n",
            "Epoch 615/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1501 - val_loss: 0.6235\n",
            "Epoch 616/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1419 - val_loss: 0.6225\n",
            "Epoch 617/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1647 - val_loss: 0.6217\n",
            "Epoch 618/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1265 - val_loss: 0.6217\n",
            "Epoch 619/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1565 - val_loss: 0.6313\n",
            "Epoch 620/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1316 - val_loss: 0.6403\n",
            "Epoch 621/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1467 - val_loss: 0.6350\n",
            "Epoch 622/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1759 - val_loss: 0.6330\n",
            "Epoch 623/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1598 - val_loss: 0.6298\n",
            "Epoch 624/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1410 - val_loss: 0.6345\n",
            "Epoch 625/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1621 - val_loss: 0.6213\n",
            "Epoch 626/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1272 - val_loss: 0.6239\n",
            "Epoch 627/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1308 - val_loss: 0.6303\n",
            "Epoch 628/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1501 - val_loss: 0.6285\n",
            "Epoch 629/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1548 - val_loss: 0.6224\n",
            "Epoch 630/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1555 - val_loss: 0.6176\n",
            "Epoch 631/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1390 - val_loss: 0.6277\n",
            "Epoch 632/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1480 - val_loss: 0.6206\n",
            "Epoch 633/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1622 - val_loss: 0.6118\n",
            "Epoch 634/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1485 - val_loss: 0.6096\n",
            "Epoch 635/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1458 - val_loss: 0.6105\n",
            "Epoch 636/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1567 - val_loss: 0.6187\n",
            "Epoch 637/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1573 - val_loss: 0.6173\n",
            "Epoch 638/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1412 - val_loss: 0.6218\n",
            "Epoch 639/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1512 - val_loss: 0.6186\n",
            "Epoch 640/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1575 - val_loss: 0.6137\n",
            "Epoch 641/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1274 - val_loss: 0.6092\n",
            "Epoch 642/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1365 - val_loss: 0.6132\n",
            "Epoch 643/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1396 - val_loss: 0.6179\n",
            "Epoch 644/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1211 - val_loss: 0.6190\n",
            "Epoch 645/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1370 - val_loss: 0.6082\n",
            "Epoch 646/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1445 - val_loss: 0.6009\n",
            "Epoch 647/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1450 - val_loss: 0.6099\n",
            "Epoch 648/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1578 - val_loss: 0.6102\n",
            "Epoch 649/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1591 - val_loss: 0.6202\n",
            "Epoch 650/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1384 - val_loss: 0.6052\n",
            "Epoch 651/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1499 - val_loss: 0.6108\n",
            "Epoch 652/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1382 - val_loss: 0.6050\n",
            "Epoch 653/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1363 - val_loss: 0.6088\n",
            "Epoch 654/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1288 - val_loss: 0.6131\n",
            "Epoch 655/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1230 - val_loss: 0.6213\n",
            "Epoch 656/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1482 - val_loss: 0.6150\n",
            "Epoch 657/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1401 - val_loss: 0.6206\n",
            "Epoch 658/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1329 - val_loss: 0.6189\n",
            "Epoch 659/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1406 - val_loss: 0.6180\n",
            "Epoch 660/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1453 - val_loss: 0.6245\n",
            "Epoch 661/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1410 - val_loss: 0.6211\n",
            "Epoch 662/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1570 - val_loss: 0.6187\n",
            "Epoch 663/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1381 - val_loss: 0.6117\n",
            "Epoch 664/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1451 - val_loss: 0.6143\n",
            "Epoch 665/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1461 - val_loss: 0.6089\n",
            "Epoch 666/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1516 - val_loss: 0.6104\n",
            "Epoch 667/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1436 - val_loss: 0.6169\n",
            "Epoch 668/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1481 - val_loss: 0.6177\n",
            "Epoch 669/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1568 - val_loss: 0.6193\n",
            "Epoch 670/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1534 - val_loss: 0.6266\n",
            "Epoch 671/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1658 - val_loss: 0.6171\n",
            "Epoch 672/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1455 - val_loss: 0.6180\n",
            "Epoch 673/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1382 - val_loss: 0.6206\n",
            "Epoch 674/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1518 - val_loss: 0.6245\n",
            "Epoch 675/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1412 - val_loss: 0.6127\n",
            "Epoch 676/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1421 - val_loss: 0.6045\n",
            "Epoch 677/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1250 - val_loss: 0.6031\n",
            "Epoch 678/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1402 - val_loss: 0.6146\n",
            "Epoch 679/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1431 - val_loss: 0.6103\n",
            "Epoch 680/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1346 - val_loss: 0.6124\n",
            "Epoch 681/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1312 - val_loss: 0.6056\n",
            "Epoch 682/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1376 - val_loss: 0.6102\n",
            "Epoch 683/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1279 - val_loss: 0.6140\n",
            "Epoch 684/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1589 - val_loss: 0.6021\n",
            "Epoch 685/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1468 - val_loss: 0.6050\n",
            "Epoch 686/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1332 - val_loss: 0.6092\n",
            "Epoch 687/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1200 - val_loss: 0.6019\n",
            "Epoch 688/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1433 - val_loss: 0.5951\n",
            "Epoch 689/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1463 - val_loss: 0.6048\n",
            "Epoch 690/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1438 - val_loss: 0.5980\n",
            "Epoch 691/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1459 - val_loss: 0.5952\n",
            "Epoch 692/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1458 - val_loss: 0.5975\n",
            "Epoch 693/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1209 - val_loss: 0.5939\n",
            "Epoch 694/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1532 - val_loss: 0.5983\n",
            "Epoch 695/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1493 - val_loss: 0.6014\n",
            "Epoch 696/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1439 - val_loss: 0.6003\n",
            "Epoch 697/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1196 - val_loss: 0.6053\n",
            "Epoch 698/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1190 - val_loss: 0.6024\n",
            "Epoch 699/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1462 - val_loss: 0.6095\n",
            "Epoch 700/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1463 - val_loss: 0.6174\n",
            "Epoch 701/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1372 - val_loss: 0.6121\n",
            "Epoch 702/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1464 - val_loss: 0.6071\n",
            "Epoch 703/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1285 - val_loss: 0.6161\n",
            "Epoch 704/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1386 - val_loss: 0.6186\n",
            "Epoch 705/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1343 - val_loss: 0.6244\n",
            "Epoch 706/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1321 - val_loss: 0.6290\n",
            "Epoch 707/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1439 - val_loss: 0.6263\n",
            "Epoch 708/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1190 - val_loss: 0.6357\n",
            "Epoch 709/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1410 - val_loss: 0.6390\n",
            "Epoch 710/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1382 - val_loss: 0.6392\n",
            "Epoch 711/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1339 - val_loss: 0.6184\n",
            "Epoch 712/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1354 - val_loss: 0.6115\n",
            "Epoch 713/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1310 - val_loss: 0.6150\n",
            "Epoch 714/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1347 - val_loss: 0.6034\n",
            "Epoch 715/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1417 - val_loss: 0.6052\n",
            "Epoch 716/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1328 - val_loss: 0.6131\n",
            "Epoch 717/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1363 - val_loss: 0.6118\n",
            "Epoch 718/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1116 - val_loss: 0.6093\n",
            "Epoch 719/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1175 - val_loss: 0.6161\n",
            "Epoch 720/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1272 - val_loss: 0.6088\n",
            "Epoch 721/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1277 - val_loss: 0.5993\n",
            "Epoch 722/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1372 - val_loss: 0.6055\n",
            "Epoch 723/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1342 - val_loss: 0.6017\n",
            "Epoch 724/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1284 - val_loss: 0.6049\n",
            "Epoch 725/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1332 - val_loss: 0.6049\n",
            "Epoch 726/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1479 - val_loss: 0.6010\n",
            "Epoch 727/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1439 - val_loss: 0.6001\n",
            "Epoch 728/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1509 - val_loss: 0.5991\n",
            "Epoch 729/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1340 - val_loss: 0.5897\n",
            "Epoch 730/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1397 - val_loss: 0.5913\n",
            "Epoch 731/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1297 - val_loss: 0.5977\n",
            "Epoch 732/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1392 - val_loss: 0.5942\n",
            "Epoch 733/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1341 - val_loss: 0.5960\n",
            "Epoch 734/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1310 - val_loss: 0.5842\n",
            "Epoch 735/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1466 - val_loss: 0.5848\n",
            "Epoch 736/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1165 - val_loss: 0.5839\n",
            "Epoch 737/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1340 - val_loss: 0.5806\n",
            "Epoch 738/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1248 - val_loss: 0.5922\n",
            "Epoch 739/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1291 - val_loss: 0.5976\n",
            "Epoch 740/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1557 - val_loss: 0.5913\n",
            "Epoch 741/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1402 - val_loss: 0.5951\n",
            "Epoch 742/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1268 - val_loss: 0.5997\n",
            "Epoch 743/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1346 - val_loss: 0.6022\n",
            "Epoch 744/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1283 - val_loss: 0.6026\n",
            "Epoch 745/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1282 - val_loss: 0.6065\n",
            "Epoch 746/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1255 - val_loss: 0.6130\n",
            "Epoch 747/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1408 - val_loss: 0.6096\n",
            "Epoch 748/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1152 - val_loss: 0.6043\n",
            "Epoch 749/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1044 - val_loss: 0.5964\n",
            "Epoch 750/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1332 - val_loss: 0.5998\n",
            "Epoch 751/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1325 - val_loss: 0.5971\n",
            "Epoch 752/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1318 - val_loss: 0.5953\n",
            "Epoch 753/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1246 - val_loss: 0.5969\n",
            "Epoch 754/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1335 - val_loss: 0.5961\n",
            "Epoch 755/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1184 - val_loss: 0.5974\n",
            "Epoch 756/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1392 - val_loss: 0.5970\n",
            "Epoch 757/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1388 - val_loss: 0.6053\n",
            "Epoch 758/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1312 - val_loss: 0.5978\n",
            "Epoch 759/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1125 - val_loss: 0.5954\n",
            "Epoch 760/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1324 - val_loss: 0.5986\n",
            "Epoch 761/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1401 - val_loss: 0.5933\n",
            "Epoch 762/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1089 - val_loss: 0.5864\n",
            "Epoch 763/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1309 - val_loss: 0.5931\n",
            "Epoch 764/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1262 - val_loss: 0.5924\n",
            "Epoch 765/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1310 - val_loss: 0.5921\n",
            "Epoch 766/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1390 - val_loss: 0.5946\n",
            "Epoch 767/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1240 - val_loss: 0.5924\n",
            "Epoch 768/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1270 - val_loss: 0.5871\n",
            "Epoch 769/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1269 - val_loss: 0.5856\n",
            "Epoch 770/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1236 - val_loss: 0.5860\n",
            "Epoch 771/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1230 - val_loss: 0.5894\n",
            "Epoch 772/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1209 - val_loss: 0.5912\n",
            "Epoch 773/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1144 - val_loss: 0.5865\n",
            "Epoch 774/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1259 - val_loss: 0.5812\n",
            "Epoch 775/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1098 - val_loss: 0.5830\n",
            "Epoch 776/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1343 - val_loss: 0.5832\n",
            "Epoch 777/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1354 - val_loss: 0.5879\n",
            "Epoch 778/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1182 - val_loss: 0.5914\n",
            "Epoch 779/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1320 - val_loss: 0.5971\n",
            "Epoch 780/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1270 - val_loss: 0.5943\n",
            "Epoch 781/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1464 - val_loss: 0.5919\n",
            "Epoch 782/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1168 - val_loss: 0.5989\n",
            "Epoch 783/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1468 - val_loss: 0.5961\n",
            "Epoch 784/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1266 - val_loss: 0.5940\n",
            "Epoch 785/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1529 - val_loss: 0.5949\n",
            "Epoch 786/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1166 - val_loss: 0.5944\n",
            "Epoch 787/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1325 - val_loss: 0.5930\n",
            "Epoch 788/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1163 - val_loss: 0.5821\n",
            "Epoch 789/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1272 - val_loss: 0.5900\n",
            "Epoch 790/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1210 - val_loss: 0.5836\n",
            "Epoch 791/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1108 - val_loss: 0.5867\n",
            "Epoch 792/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1132 - val_loss: 0.5833\n",
            "Epoch 793/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1326 - val_loss: 0.5855\n",
            "Epoch 794/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1305 - val_loss: 0.5895\n",
            "Epoch 795/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1275 - val_loss: 0.5975\n",
            "Epoch 796/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1377 - val_loss: 0.5900\n",
            "Epoch 797/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1169 - val_loss: 0.5790\n",
            "Epoch 798/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1332 - val_loss: 0.5860\n",
            "Epoch 799/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1270 - val_loss: 0.5936\n",
            "Epoch 800/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1053 - val_loss: 0.5846\n",
            "Epoch 801/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1480 - val_loss: 0.5925\n",
            "Epoch 802/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1287 - val_loss: 0.5946\n",
            "Epoch 803/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1226 - val_loss: 0.5932\n",
            "Epoch 804/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1245 - val_loss: 0.5901\n",
            "Epoch 805/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1194 - val_loss: 0.5900\n",
            "Epoch 806/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1145 - val_loss: 0.5888\n",
            "Epoch 807/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1195 - val_loss: 0.5940\n",
            "Epoch 808/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1267 - val_loss: 0.5987\n",
            "Epoch 809/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1104 - val_loss: 0.6021\n",
            "Epoch 810/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1310 - val_loss: 0.5937\n",
            "Epoch 811/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1306 - val_loss: 0.5850\n",
            "Epoch 812/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1197 - val_loss: 0.5893\n",
            "Epoch 813/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1125 - val_loss: 0.5850\n",
            "Epoch 814/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1147 - val_loss: 0.5902\n",
            "Epoch 815/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1440 - val_loss: 0.5928\n",
            "Epoch 816/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1162 - val_loss: 0.5917\n",
            "Epoch 817/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1265 - val_loss: 0.5862\n",
            "Epoch 818/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1257 - val_loss: 0.5823\n",
            "Epoch 819/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1345 - val_loss: 0.5857\n",
            "Epoch 820/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1180 - val_loss: 0.5784\n",
            "Epoch 821/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1072 - val_loss: 0.5860\n",
            "Epoch 822/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1121 - val_loss: 0.5860\n",
            "Epoch 823/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1456 - val_loss: 0.5781\n",
            "Epoch 824/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1209 - val_loss: 0.5720\n",
            "Epoch 825/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1168 - val_loss: 0.5808\n",
            "Epoch 826/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1216 - val_loss: 0.5876\n",
            "Epoch 827/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1254 - val_loss: 0.5843\n",
            "Epoch 828/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1358 - val_loss: 0.5811\n",
            "Epoch 829/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1281 - val_loss: 0.5767\n",
            "Epoch 830/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1309 - val_loss: 0.5775\n",
            "Epoch 831/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1231 - val_loss: 0.5819\n",
            "Epoch 832/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1146 - val_loss: 0.5781\n",
            "Epoch 833/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1220 - val_loss: 0.5741\n",
            "Epoch 834/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1074 - val_loss: 0.5819\n",
            "Epoch 835/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1161 - val_loss: 0.5839\n",
            "Epoch 836/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1097 - val_loss: 0.5786\n",
            "Epoch 837/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1148 - val_loss: 0.5777\n",
            "Epoch 838/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1202 - val_loss: 0.5826\n",
            "Epoch 839/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1324 - val_loss: 0.5841\n",
            "Epoch 840/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1223 - val_loss: 0.5781\n",
            "Epoch 841/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1254 - val_loss: 0.5824\n",
            "Epoch 842/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1314 - val_loss: 0.5811\n",
            "Epoch 843/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1206 - val_loss: 0.5838\n",
            "Epoch 844/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1298 - val_loss: 0.5783\n",
            "Epoch 845/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1159 - val_loss: 0.5833\n",
            "Epoch 846/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1257 - val_loss: 0.5877\n",
            "Epoch 847/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1192 - val_loss: 0.5822\n",
            "Epoch 848/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1275 - val_loss: 0.5811\n",
            "Epoch 849/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1258 - val_loss: 0.5801\n",
            "Epoch 850/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1214 - val_loss: 0.5878\n",
            "Epoch 851/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1255 - val_loss: 0.5887\n",
            "Epoch 852/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1325 - val_loss: 0.5911\n",
            "Epoch 853/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1257 - val_loss: 0.5920\n",
            "Epoch 854/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1126 - val_loss: 0.5854\n",
            "Epoch 855/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1228 - val_loss: 0.5835\n",
            "Epoch 856/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1285 - val_loss: 0.5865\n",
            "Epoch 857/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1396 - val_loss: 0.5908\n",
            "Epoch 858/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1441 - val_loss: 0.6045\n",
            "Epoch 859/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1251 - val_loss: 0.5985\n",
            "Epoch 860/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1060 - val_loss: 0.5976\n",
            "Epoch 861/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1194 - val_loss: 0.6014\n",
            "Epoch 862/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1233 - val_loss: 0.6027\n",
            "Epoch 863/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1019 - val_loss: 0.5965\n",
            "Epoch 864/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1184 - val_loss: 0.5926\n",
            "Epoch 865/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1221 - val_loss: 0.5851\n",
            "Epoch 866/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1102 - val_loss: 0.5819\n",
            "Epoch 867/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1255 - val_loss: 0.5828\n",
            "Epoch 868/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1066 - val_loss: 0.5901\n",
            "Epoch 869/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1218 - val_loss: 0.5826\n",
            "Epoch 870/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1215 - val_loss: 0.5802\n",
            "Epoch 871/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1256 - val_loss: 0.5765\n",
            "Epoch 872/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1098 - val_loss: 0.5832\n",
            "Epoch 873/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1418 - val_loss: 0.5692\n",
            "Epoch 874/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1188 - val_loss: 0.5722\n",
            "Epoch 875/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1048 - val_loss: 0.5825\n",
            "Epoch 876/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1309 - val_loss: 0.5860\n",
            "Epoch 877/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1116 - val_loss: 0.5786\n",
            "Epoch 878/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1288 - val_loss: 0.5813\n",
            "Epoch 879/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1387 - val_loss: 0.5722\n",
            "Epoch 880/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1184 - val_loss: 0.5692\n",
            "Epoch 881/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1302 - val_loss: 0.5778\n",
            "Epoch 882/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1253 - val_loss: 0.5823\n",
            "Epoch 883/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1317 - val_loss: 0.5874\n",
            "Epoch 884/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1106 - val_loss: 0.5885\n",
            "Epoch 885/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1375 - val_loss: 0.5809\n",
            "Epoch 886/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1132 - val_loss: 0.5876\n",
            "Epoch 887/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1248 - val_loss: 0.5882\n",
            "Epoch 888/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1050 - val_loss: 0.5910\n",
            "Epoch 889/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1309 - val_loss: 0.5842\n",
            "Epoch 890/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1149 - val_loss: 0.5749\n",
            "Epoch 891/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1068 - val_loss: 0.5806\n",
            "Epoch 892/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1217 - val_loss: 0.5880\n",
            "Epoch 893/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1126 - val_loss: 0.6002\n",
            "Epoch 894/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1313 - val_loss: 0.5936\n",
            "Epoch 895/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1287 - val_loss: 0.5867\n",
            "Epoch 896/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1200 - val_loss: 0.5861\n",
            "Epoch 897/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1112 - val_loss: 0.5890\n",
            "Epoch 898/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1169 - val_loss: 0.5887\n",
            "Epoch 899/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1050 - val_loss: 0.5998\n",
            "Epoch 900/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1098 - val_loss: 0.6016\n",
            "Epoch 901/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1203 - val_loss: 0.5965\n",
            "Epoch 902/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1138 - val_loss: 0.5991\n",
            "Epoch 903/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1227 - val_loss: 0.5957\n",
            "Epoch 904/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.0991 - val_loss: 0.5949\n",
            "Epoch 905/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0970 - val_loss: 0.5997\n",
            "Epoch 906/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.0984 - val_loss: 0.6027\n",
            "Epoch 907/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1153 - val_loss: 0.6023\n",
            "Epoch 908/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1268 - val_loss: 0.5978\n",
            "Epoch 909/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1109 - val_loss: 0.5884\n",
            "Epoch 910/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1001 - val_loss: 0.5871\n",
            "Epoch 911/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1159 - val_loss: 0.5958\n",
            "Epoch 912/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1141 - val_loss: 0.5955\n",
            "Epoch 913/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1163 - val_loss: 0.5872\n",
            "Epoch 914/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1152 - val_loss: 0.5892\n",
            "Epoch 915/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1152 - val_loss: 0.5866\n",
            "Epoch 916/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1165 - val_loss: 0.5831\n",
            "Epoch 917/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1099 - val_loss: 0.5972\n",
            "Epoch 918/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1188 - val_loss: 0.6029\n",
            "Epoch 919/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1194 - val_loss: 0.6056\n",
            "Epoch 920/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1079 - val_loss: 0.6002\n",
            "Epoch 921/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1079 - val_loss: 0.5970\n",
            "Epoch 922/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1038 - val_loss: 0.6000\n",
            "Epoch 923/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1201 - val_loss: 0.6020\n",
            "Epoch 924/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1315 - val_loss: 0.5944\n",
            "Epoch 925/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1290 - val_loss: 0.5947\n",
            "Epoch 926/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1149 - val_loss: 0.5809\n",
            "Epoch 927/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1128 - val_loss: 0.5812\n",
            "Epoch 928/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1101 - val_loss: 0.5926\n",
            "Epoch 929/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1319 - val_loss: 0.5848\n",
            "Epoch 930/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1384 - val_loss: 0.5889\n",
            "Epoch 931/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1161 - val_loss: 0.5899\n",
            "Epoch 932/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1161 - val_loss: 0.5859\n",
            "Epoch 933/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1095 - val_loss: 0.5852\n",
            "Epoch 934/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1163 - val_loss: 0.5824\n",
            "Epoch 935/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1079 - val_loss: 0.5844\n",
            "Epoch 936/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1022 - val_loss: 0.5855\n",
            "Epoch 937/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1273 - val_loss: 0.5950\n",
            "Epoch 938/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1111 - val_loss: 0.5902\n",
            "Epoch 939/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1202 - val_loss: 0.5972\n",
            "Epoch 940/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1358 - val_loss: 0.5977\n",
            "Epoch 941/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1003 - val_loss: 0.5894\n",
            "Epoch 942/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1151 - val_loss: 0.5909\n",
            "Epoch 943/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1219 - val_loss: 0.5870\n",
            "Epoch 944/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1038 - val_loss: 0.5926\n",
            "Epoch 945/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1094 - val_loss: 0.5881\n",
            "Epoch 946/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1012 - val_loss: 0.5967\n",
            "Epoch 947/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1262 - val_loss: 0.6036\n",
            "Epoch 948/1000\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0912 - val_loss: 0.5957\n",
            "Epoch 949/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1083 - val_loss: 0.5887\n",
            "Epoch 950/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1176 - val_loss: 0.5920\n",
            "Epoch 951/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1038 - val_loss: 0.5943\n",
            "Epoch 952/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1191 - val_loss: 0.5954\n",
            "Epoch 953/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1105 - val_loss: 0.6070\n",
            "Epoch 954/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1127 - val_loss: 0.6097\n",
            "Epoch 955/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1057 - val_loss: 0.5982\n",
            "Epoch 956/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1191 - val_loss: 0.5996\n",
            "Epoch 957/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1115 - val_loss: 0.5968\n",
            "Epoch 958/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1079 - val_loss: 0.5855\n",
            "Epoch 959/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1167 - val_loss: 0.5891\n",
            "Epoch 960/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1297 - val_loss: 0.5877\n",
            "Epoch 961/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1173 - val_loss: 0.5841\n",
            "Epoch 962/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1083 - val_loss: 0.5802\n",
            "Epoch 963/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1185 - val_loss: 0.5847\n",
            "Epoch 964/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1201 - val_loss: 0.5997\n",
            "Epoch 965/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1047 - val_loss: 0.5965\n",
            "Epoch 966/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1111 - val_loss: 0.5902\n",
            "Epoch 967/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1088 - val_loss: 0.5931\n",
            "Epoch 968/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1122 - val_loss: 0.5850\n",
            "Epoch 969/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1155 - val_loss: 0.5736\n",
            "Epoch 970/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1066 - val_loss: 0.5784\n",
            "Epoch 971/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1016 - val_loss: 0.5762\n",
            "Epoch 972/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1082 - val_loss: 0.5828\n",
            "Epoch 973/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1145 - val_loss: 0.5907\n",
            "Epoch 974/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1083 - val_loss: 0.5899\n",
            "Epoch 975/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1035 - val_loss: 0.5814\n",
            "Epoch 976/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1164 - val_loss: 0.5849\n",
            "Epoch 977/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1058 - val_loss: 0.5790\n",
            "Epoch 978/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1027 - val_loss: 0.5795\n",
            "Epoch 979/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1416 - val_loss: 0.5717\n",
            "Epoch 980/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1260 - val_loss: 0.5785\n",
            "Epoch 981/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1030 - val_loss: 0.5735\n",
            "Epoch 982/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1043 - val_loss: 0.5633\n",
            "Epoch 983/1000\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1166 - val_loss: 0.5673\n",
            "Epoch 984/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1168 - val_loss: 0.5702\n",
            "Epoch 985/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1118 - val_loss: 0.5772\n",
            "Epoch 986/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1158 - val_loss: 0.5831\n",
            "Epoch 987/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1233 - val_loss: 0.5759\n",
            "Epoch 988/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1181 - val_loss: 0.5766\n",
            "Epoch 989/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1190 - val_loss: 0.5642\n",
            "Epoch 990/1000\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.1063 - val_loss: 0.5627\n",
            "Epoch 991/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1143 - val_loss: 0.5618\n",
            "Epoch 992/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1145 - val_loss: 0.5632\n",
            "Epoch 993/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1121 - val_loss: 0.5718\n",
            "Epoch 994/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1166 - val_loss: 0.5691\n",
            "Epoch 995/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.0932 - val_loss: 0.5659\n",
            "Epoch 996/1000\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1084 - val_loss: 0.5746\n",
            "Epoch 997/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1081 - val_loss: 0.5708\n",
            "Epoch 998/1000\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.1089 - val_loss: 0.5769\n",
            "Epoch 999/1000\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.1206 - val_loss: 0.5807\n",
            "Epoch 1000/1000\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.1271 - val_loss: 0.5738\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5738\n",
            "7/7 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae6a02b160>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHMCAYAAAAzqWlnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACDPElEQVR4nO3dd3gUVcMF8DOzJb13EkhCQui9KEVBsCAgRWmCDVRQsaFiBQUpigXLZ5cmVsoLCgpYUER6772EAElIQnrdMvf7Y5JNlmwgIdndJJzf8/iQnZmdvXsTsye3SkIIASIiIqJ6SnZ2AYiIiIjsiWGHiIiI6jWGHSIiIqrXGHaIiIioXmPYISIionqNYYeIiIjqNYYdIiIiqtcYdoiIiKheY9ghIiKieo1hh8iJJElCr169qn2fXr16QZKk6heonqmp+iWiuo1hh65rkiRV6b+FCxc6u8hkB7Xh52DhwoXXfO+SchGRbVpnF4DImd54441yxz788ENkZWXhmWeega+vr9W5du3a1ejrHzlyBO7u7tW+z6JFi5Cfn18DJbo+OfvngIjsS+JGoETWoqKicPbsWZw5cwZRUVHOLg5VgyRJ6NmzJ9avX1/l5zr652DhwoUYM2YMFixYgIceeqhKzy1p1eGvcyLb2I1FVEkl42IMBgPefPNNNG3aFC4uLpYPpqysLLz77rvo3bs3IiIioNfrERQUhIEDB2LLli0272lrTMnUqVMhSRLWr1+PZcuWoUuXLnB3d4e/vz9GjhyJCxcuVFi2stavXw9JkjB16lTs3bsX/fv3h6+vL9zd3dGzZ09s3rzZZpmSkpIwZswYBAcHw83NDe3atcM333xjdb/KqE59pKWlYdy4cQgLC4OLiwtatmyJBQsW2HyOwWDA9OnTERMTAxcXF0RHR2Py5MkoKiqqVDmvxbZt2zB06FCEhoZCr9ejYcOGGD9+PBITE8tde/r0aYwbNw6xsbFwc3ODv78/WrdujcceewyXLl0CoH7/xowZAwAYM2aMVZdZfHx8jZa9qKgIb7/9Nlq3bg13d3d4e3vjpptuwpIlS2xev3LlSvTp08fyvWjQoAF69uyJzz77rMrvs6wff/wRt9xyC3x9feHq6ormzZtjxowZNr9v//33H+666y5ERETAxcUFoaGhuPHGGzFt2rSaqRSq99iNRVRF99xzD3bs2IE777wTgwcPRnBwMAC1S+q1117DzTffjP79+8PPzw8JCQlYuXIl1qxZg1WrVqFv376Vfp3PPvsMK1euxMCBA9GzZ09s27YNixcvxr59+7B37164uLhU6j47d+7EO++8g65du+KRRx5BQkIC/ve//6FPnz7Yu3cvmjZtark2JSUFXbt2xdmzZ3HzzTejW7duSE5OxhNPPIHbb7+9SvV0rfWRmZmJ7t27Q6/XY+jQoSgqKsLSpUsxduxYyLKMBx980HKtEALDhw/HL7/8gpiYGDz55JMwGAyYP38+Dhw4UKXyVtb8+fMxbtw4uLi4YODAgWjYsCFOnDiBuXPnYtWqVdi6dSsaNWoEQA2OnTt3RnZ2Nvr164d77rkHhYWFOHPmDL799ls8+eSTCAgIwEMPPQRfX1/88ssvGDRokFU32eVdaNVhMBhwxx134N9//0WzZs0wYcIE5OfnY9myZRgxYgT27t2LWbNmWa7/6quvMH78eISGhuKuu+5CYGAgUlJSsH//fixYsABPPPFEld5nibFjx2LBggWIiIjAPffcA19fX2zduhVTpkzBunXr8Oeff0KrVT+e1q5di/79+8Pb2xsDBw5EeHg40tPTceTIEXz22Wc2uyCJyhFEZCUyMlIAEGfOnLE63rNnTwFAtG7dWqSmppZ7XmZmps3j586dE2FhYaJZs2blzgEQPXv2tDr2xhtvCADCy8tL7N+/3+rcvffeKwCIxYsX2yxbWf/8848AIACIBQsWWJ374osvBADx+OOPWx0fO3asACBefPFFq+N79+4Ver1eABBvvPFGufdhy7XWBwDx8MMPC5PJZDl+6NAhodFoRPPmza2u//777wUAceONN4qCggLL8UuXLonGjRvbrN/KsvVzcOzYMaHT6URMTIw4f/681fV//fWXkGVZDB482HLs448/FgDEhx9+WO7+ubm5Ij8/3/J4wYIFNr9XlVFSb1cza9YsAUDceeedwmg0Wo5fvHjR8n43bdpkOd6hQweh1+vFxYsXy92r7Pf2Wt7nkCFDrI4LUfqzX/Y+d999twAg9u7de8UyEF0Ju7GIqmj69OkIDAwsd9zHx8fm8YiICAwdOhRHjx5FQkJCpV/n6aefRuvWra2OPfroowCA7du3V/o+3bt3LzcGZOzYsdBqtVb3MRgM+PHHH+Hj44PJkydbXd+2bVs88MADlX5N4Nrrw93dHXPmzIFGo7Eca9GiBbp3744jR44gNzfXcryka2vWrFlwdXW1HPf398eUKVOqVN7K+Pzzz2E0GvHRRx8hPDzc6lyfPn0wcOBArFq1Cjk5OVbn3Nzcyt3Lw8PD5nF7mj9/PiRJwpw5cywtJwAQHBxsqa+5c+daPUer1UKn05W7l63vbWXe50cffQStVov58+eXu37KlCkICAjA999/X6l72yoDkS3sxiKqoi5dulR4btOmTfjoo4+wZcsWpKSkwGAwWJ2/cOGCpYvjajp16lTuWMOGDQEAGRkZlS6vrfvodDqEhIRY3efYsWMoKChAp06d4OXlVe45PXr0KPdBeDXXUh9NmjSBt7d3uXuVfe+enp4AgN27d0OWZfTo0aPc9fZYX6dkrNG///6LHTt2lDufkpICs9mM48ePo2PHjhg4cCBeffVVTJgwAb///jvuuOMOdO/eHS1atHD4VPGcnBycPHkS4eHhaNasWbnzvXv3BgDs2bPHcmz06NF4/vnn0aJFC4wcORI9e/ZE9+7dERQUZPXcyr7P/Px87Nu3D4GBgfjwww9tltPFxQVHjhyxKsPy5ctxww03YMSIEbjlllvQvXt3REREVKc66DrDsENURaGhoTaPr1ixAkOHDoWrqytuu+02xMTEwMPDA7IsY/369fj333+rNGjW1liNkr/GzWZzte5Tcq+y98nKygIAhISE2Ly+ouMVudb6uFJ5AZQrs7+/v82Wh4q+T9VRMtD23XffveJ1Ja1PkZGR2L59O6ZOnYq1a9di+fLlANTg9sILL+Dpp5+u8TJWpOT7GxYWZvN8yfHMzEzLseeeew6BgYH47LPP8PHHH+PDDz+0zHB79913LUG6su8zIyMDQgikpqZWenDx3XffjV9//RXvv/8+5s+fjy+//BIA0LFjR7z11lu47bbbql4ZdN1h2CGqoor+Ip8yZQr0ej127tyJ5s2bW50bP348/v33X0cU75qVtKZcvHjR5vmKjlfEEfXh4+OD9PR0GI3GcoEnOTm52ve39XqAGhxstT7Z0rx5cyxevBgmkwn79u3DX3/9hf/7v//DM888Aw8PDzz88MM1Xk5bSspeUb0kJSVZXVfigQcewAMPPIDMzExs3rwZK1aswPz583HHHXfg6NGjllaeyrzPknu3b98eu3fvrnTZ+/fvj/79+yMvLw/btm3Dr7/+is8//xwDBgzAnj170KJFiyrXB11fOGaHqIacPHkSLVq0KPfBrigKNm7c6KRSVV6zZs3g5uaG/fv3lxtzAqDK78ER9dGhQ4cK73cta+tczY033ghAnQpdVVqtFh07dsRLL72EH3/8EQDw888/W86XjFGqSqtdVXh5eSEmJgYXLlzAiRMnyp3/559/AKh1aouvry/69euHr7/+Gg899BDS09OxYcOGctdd6X16enqiZcuWOHToENLT06v8Hjw8PNC7d2/MmTMHr776KgwGA9asWVPl+9D1h2GHqIZERUXhxIkTVmutCCEwdepUHD582Iklqxy9Xo8RI0YgKysLM2bMsDq3b98+LFq0qEr3c0R9lKxN89prr6GwsNByPD09vdx7qAlPPvkkdDodJk6ciOPHj5c7bzAYrILQrl27LN1HZZW0kpVdPbtkanZVBrFX1dixYyGEwKRJk6xCVVpaGqZPn265psQ///xjc6HClJQUAKXlr8r7fO6552AwGDB27FirLrMSGRkZVq0+GzZsgMlkqtS9iSrCbiyiGjJx4kQ89thjaN++Pe655x7odDps2rQJhw8fxl133YVVq1Y5u4hX9fbbb+Pvv//GO++8g23btqFbt25ISkrCkiVL0K9fP/z888+Q5cr9jeSI+rj33nuxePFirFy5Eq1atcKgQYNgNBqxbNkydO7cGadOnar2a5TVrFkzzJ8/H2PHjkXLli3Rt29fxMXFwWg0IiEhAf/99x+CgoJw9OhRAMC3336LL7/8Ej169EBMTAz8/Pxw6tQprFq1Ci4uLnj22Wct9+7atSvc3d3x4Ycf4tKlS5YxR0899VS5rqWKXGnl5c8++wwvvPAC1qxZg19++QVt27ZFv379kJ+fj6VLlyIlJQUvvvii1WDvIUOGwNPTEzfeeCOioqIghMB///2HHTt2oGPHjrj11lur/D7Hjh2LXbt24bPPPkNMTAzuuOMONGrUCOnp6Thz5gw2bNiAMWPG4IsvvgCgzkq8cOECunfvjqioKOj1euzatQt///03IiMjMXLkyErVDV3nnDnvnag2uto6O1eyYMEC0bZtW+Hu7i4CAgLE4MGDxf79+y3rh/zzzz9W1+MK6+xcfq0QQpw5c0YAEA8++OBVy1ayzk5F6+JERkaKyMjIcsfPnz8vHnjgAREYGChcXV1F27ZtxcKFC8XSpUsFAPHBBx9csQ7Kqon6KPHggw/a/L4UFRWJadOmiejoaKHX60VkZKR49dVXRWFhYY2vs1Ni//794sEHHxSNGjUSer1e+Pn5iZYtW4px48aJdevWWa7bunWreOyxx0SbNm2En5+fcHV1FTExMeKhhx4SBw4cKHffNWvWiBtvvFF4eHhY1s6x9fqXK7n2Sv9lZGQIIYQoKCgQM2fOFC1bthSurq7C09NTdO/eXfzwww/l7vv555+LwYMHi+joaOHm5ib8/PxEu3btxOzZs0V2dvY1v08hhFi1apXo37+/CAoKEjqdToSEhIjOnTuL1157TRw5csRy3eLFi8XIkSNFbGys8PDwEF5eXqJly5bi1VdfFSkpKVetGyIhhODeWERUKa+99hpmzZqFtWvX4o477nB2cYiIKo1hh4isJCYmokGDBlbHDhw4gG7dukGv1+PChQtWC/gREdV2HLNDRFY6deqE2NhYtGrVCh4eHjhx4gR+++03KIqCL7/8kkGHiOoctuwQkZVp06bh559/Rnx8PHJycuDr64sbb7wRL7zwgl1WJSYisjeGHSIiIqrXuM4OERER1WsMO0RERFSvMewQERFRvcawQ0RERPUap54Xy8jIsLn/SnUFBQUhNTW1xu9L1ljPjsF6dhzWtWOwnh3DHvWs1Wrh5+dXuWtr9JXrMJPJBKPRWKP3lCTJcm9OerMf1rNjsJ4dh3XtGKxnx6gN9cxuLCIiIqrXGHaIiIioXmPYISIionqNYYeIiIjqNQ5QJiKiesdkMiE/P/+q1xUUFMBgMDigRNe3a6lnIQS0Wi08PDyq/foMO0REVK+YTCbk5eXBy8sLsnzlDgydTlfjM3GpvGut57y8PBQVFcHFxaVar89uLCIiqlfy8/MrFXSo9nN3d0dRUVG178OfBCIiqncYdOqHkjV6qos/DURERFSvMewQERFRvcawQ0REVM/ccMMN+Prrr2vkXps3b0Z4eDiysrJq5H7OwNlYREREtcDQoUPRokULvPnmm9W+1+rVq+Hu7l4DpaofGHbsRBiNQE4mTNqaGVxFRETXNyEEzGYztNqrf3QHBAQ4oER1B7ux7OXsSZhfehiprzzm7JIQEVEt9+yzz2LLli2YN28ewsPDER4ejsWLFyM8PBx///03+vbti+joaGzfvh3x8fEYM2YM2rZtiyZNmqBfv37YsGGD1f0u78YKDw/HDz/8gIcffhgxMTHo3r07/vjjj2su72+//YZbbrkF0dHRuOGGG/DFF19YnV+4cCG6d++Oxo0bo23bthg7dqzl3K+//oo+ffogJiYGLVu2xIgRIyq1AGR1sGXHXjQaAIBQFLBth4jIeYQQgMH2Wi1CMast8faid6nU9Ok333wTp0+fRrNmzfDCCy8AAI4dOwYAmDVrFl5//XU0atQIPj4+SExMRO/evfHSSy9Br9dj2bJlGDNmDDZs2IDw8PAKX2POnDmYPHkyJk+ejAULFuDJJ5/Etm3b4OfnV6W3tH//fjz22GN47rnnMHDgQOzcuROvvvoq/Pz8MGLECOzbtw+vv/46Pv74Y3Tq1AmZmZnYuXMnAODixYuYMGECXnvtNdx5553Izc3Ftm3b1O+RHTHs2EvJGg9ms3PLQUR0vTMUQXlyuM1T1V+u7srkT5YALq5Xvc7b2xt6vR6urq4IDg4GAJw8eRIAMGnSJNx8882Wa/38/NCyZUvL4xdffBFr167FH3/8gTFjxlT4GsOHD8fgwYMBAC+//DLmzZuHvXv34pZbbqnSe/rqq6/Qo0cPTJw4EQAQExODEydO4IsvvsCIESNw4cIFuLu749Zbb4WnpyciIiLQvn17GI1GpKSkwGQyoV+/foiIiAAANG/evEqvfy3YjWUvklq1QmHYISKia9emTRurx3l5eXjzzTfRs2dPNG/eHE2aNMGJEydw4cKFK96nbKhwd3eHl5cX0tLSqlyeEydOoHPnzlbHOnfujDNnzsBsNuPmm29GREQEunbtiqeeegrLly+3dFO1aNECPXr0QJ8+fTBu3Dh8//33yMzMrHIZqootO/ZS3I0FRXFuOYiIrnd6F7WFxQa7742lr96eTgDKzap688038d9//2HKlCmIioqCq6srxo0bd9WNNnU6ndVjSZKg2OEzytPTE2vXrsXmzZuxYcMGvPfee5gzZw5+++03+Pj44KeffsLOnTvx77//YsGCBZg9ezZ+/fVXNGrUqMbLUoJhx17YjUVEVCtIklRhV5Kk00GSNQ4ukW06na5S4WPnzp0YNmwY7rzzTgBqS8/58+ftXTyLJk2aYMeOHVbHduzYgcaNG0NT/Ie+VqvFzTffjJtvvhnPPfccmjdvjk2bNqFfv36QJAmdO3dG586dMXHiRHTp0gVr1qzB+PHj7VZmhh17YTcWERFVQcOGDbFnzx6cO3cOHh4eFQaf6OhorFmzBrfddhskScK7775rlxaaiowfPx79+vXDBx98gIEDB2LXrl1YsGABZs2aBQD4888/kZCQgBtuuAG+vr5Yt24dFEVBTEwMdu/ejY0bN6Jnz54IDAzE7t27kZ6ejiZNmti1zAw79sJuLCIiqoLx48fj2WefRa9evVBYWIg5c+bYvO6NN97Ac889h0GDBsHf3x8TJkxAbm6uw8rZunVrfPHFF3jvvffw0UcfITg4GJMmTcKIESMAAD4+PlizZg3mzJmDwsJCREdH48svv0TTpk1x4sQJbNu2DXPnzkVubi7Cw8Px+uuvo3fv3nYtsyTsPd+rjkhNTa3RfltxKQXKy49A0rtA89kyu0+ru55JkoSwsDAkJSWxnu2I9ew4rOvqyc7Ohre3d6WutfuYHQJQvXqu6Pup0+kQFBRUqXtwNpa9sBuLiIioVmA3lr2wG4uIiOqAl156CcuXL7d57u6778bs2bMdXKKax7BjLyWzsRSFzdBERFRrTZo0CY89ZntrIy8vLweXxj4YduxFLtNDqCjWj4mIiGqJwMBABAYGOrsYdsVPYHspu24Du7KIiIichmHHXqxadjhImYiIyFkYduzl8m4sIiIicgqGHXthNxYREVGtwLBjL+zGIiIiqhUYduxEkiRAktQHbNkhIqJa7ty5cwgPD8fBgwedXZQax7BjTzIXFiQiosoZOnQoXn/99Rq737PPPouxY8fW2P3qMoYde7IsLMhuLCIiImepVWFnxYoVeOWVV/DAAw/gkUcewTvvvIPExMSrPm/Lli149tlnMXr0aDz//PPYvXu3A0pbCWVWUSYiIqrIs88+iy1btmDevHkIDw9HeHg4zp07h6NHj+K+++5DkyZN0LZtWzz11FNIT0+3PO/XX39Fnz59EBMTg5YtW2LEiBHIz8/H+++/j6VLl+L333+33G/z5s1VLteWLVvQv39/REdHo3379pg1axZMJtNVXx8ANm/ejP79+yM2NhaxsbEYNGgQzp8/X/3Kuga1agXlw4cP44477kBMTAzMZjN+/PFHzJgxA3PmzIGrq6vN5xw7dgwfffQRRo0ahQ4dOmDjxo149913MXv2bDRq1MjB7+Ay7MYiInI6IQSKzLa37TFDgdFkv9/RLhpJHcN5FW+++SZOnz6NZs2a4YUXXgAAaLVa9O/fH/feey+mTp2KwsJCzJw5E+PHj8fSpUtx8eJFTJgwAa+99hruvPNO5ObmYtu2bRBC4LHHHsOJEyeQm5uLOXPmAAB8fX2rVPakpCTcf//9GD58OD766COcPHkSkyZNgouLC55//vkrvr7JZMLDDz+MUaNG4dNPP4UQAjt27KhUXdhDrQo7r732mtXjCRMm4JFHHsHp06fRokULm89ZvXo12rVrh4EDBwIARo4ciQMHDmDt2rUYN26c3ct8RSUtO2Z2YxEROUuRWWDE4uNOee3FI+Lgqr36B7y3tzf0ej1cXV0RHBwMAPjwww/RqlUrvPLKK5br3n//fXTu3BmnTp1Cfn4+TCYT+vXrh4iICABA8+bNLde6urrCYDBY7ldV33zzDRo0aICZM2dCkiTExsYiOTkZs2bNwsSJE5GSklLh62dkZCA7Oxu33noroqKioNPpEB0dfU3lqAm1KuxcrqQpzNPTs8Jrjh8/jgEDBlgda9u2LXbs2GHzeqPRCKPRaHksSRLc3NwsX9eo4rAjCaV0ZhbVuJLvm7P+YrhesJ4dh3VNgNrbsXnzZjRp0qTcubNnz6Jnz57o0aMH+vTpg549e6Jnz57o379/lVtwKnLy5El07NjR6uewc+fOyMvLQ1JSElq0aFHh6/v5+WH48OEYPXo0brrpJvTq1Qv9+vVDSEjINZWluv8v1NqwoygKFi5ciKZNm16xOyozMxM+Pj5Wx3x8fJCZmWnz+hUrVmDZsmWWx9HR0Zg9ezaCgoJqpNwlFCFwRu8Ok6YQ0f7+0IeF1ej9qbzQ0FBnF+G6wHp2HNb1tSkoKIBOp7M81moFlo9u6ZSyuGgr140FqB/oGo3GUvaCggLcfvvtmDJlSrlrQ0JC4Orqiv/973/Yvn071q9fjwULFuCdd97BmjVrEBkZCVmWIUmSVV1ciVartfyr0+kgSRJkWb6sLkuvudrrf/LJJxg/fjz+/vtv/Pzzz3jrrbewdOlSdOrUqVLlKaHX6xFWzc/QWht25s2bh3PnzuHNN9+s0fsOGTLEqiWo5IcwNTXVatBVdR1OycfLrSciLD8VX6akAG5eNXZvsiZJEkJDQ5GcnAwhbPfLU/Wxnh2HdV09BoPBqgUfADQVXKvT6cpdW5Oq8rGi1Wqteh9atmyJ1atXIywszBIyyiq5rkOHDujQoQOeeeYZdOnSBatWrcL48eOh1WphMpkq/f5KPgNLnhMTE4PVq1fDYDBYPiu3bNkCT09PBAUFXfX1AaBZs2Zo1qwZnnnmGfTt2xfLli1D27ZtK18pUL+fSUlJNuursg0VtTLszJs3D7t378a0adMQEBBwxWt9fX2RlZVldSwrK6vCZjydTldhyq3JXyra4uE6RlkLYTYD/IVld0IIfjA4AOvZcVjX15eGDRtiz549OHfuHDw8PPDQQw/hhx9+wBNPPIEnnngCvr6+iI+Pxy+//IL33nsP+/btw8aNG9GzZ08EBgZi9+7dSE9Pt3R7RUREYP369Th58iT8/f3h5eVV6VYeAHjwwQcxd+5cTJ48GWPGjMGpU6fw/vvvY9y4cZBlGbt3767w9RMSEvD999/jtttuQ2hoKOLj43HmzBkMHTr0muqmuv8f1KqwI4TA/PnzsX37dkydOrVSg6ri4uJw4MAB9O/f33Js//79Nvs4HUknqynYJGsBwdlYRER0ZePHj8ezzz6LXr16obCwEFu3bsXPP/+MWbNmYdSoUSgqKkJERAR69eoFWZbh5eWFbdu2Ye7cucjNzUV4eDhef/119O7dGwAwevRobNmyBf369UNeXh6WLl2Kbt26Vbo8YWFh+PbbbzFjxgzcdttt8PX1xb333otnnnkGAK74+qmpqTh58iSWLl2KjIwMhISE4KGHHsL9999vl7q7GknUoj8b5s6di40bN+LFF19EgwYNLMfd3d2h1+sBAJ988gn8/f0xatQoAOrU86lTp1qmnm/atAkrVqyo8tTz1NTUGm3KPJ9dhAmrzsDDmI8fursCTWzPJqPqkyQJYWFhSEpK4l/BdsR6dhzWdfVkZ2fD29u7UtfauxuLVNWp54q+nzqdrm52Y/3xxx8AgKlTp1odf+KJJ9CrVy8AQFpamtVgr6ZNm+Lpp5/GTz/9hB9//BFhYWGYNGmS09fYKWnZMcpaCEUB51QQERE5R60KO0uWLLnqNZcHIQDo2rUrunbtaocSXTutVTdWzQ18JiIiuhYff/wx/u///s/muRtuuAHfffedg0vkOLUq7NQnOo06QlmRZJhNZlY0ERE51f3334+77rrL5rmKdimoL/gZbCcl3VgAYDQrrGgiInIqPz8/+Pn5ObsYTlGrNgKtT3SaMmHHjvuuEBER0ZUx7NiJRgKk4lkURjPDDhGRo3AGG12OYcdOJEmCDuoGoMYKdtslIqKap9VqkZeXx9BTD5Rdvbk6OJTEjrRCgUECTAr/hyMichQPDw8UFRUhJyfnqtfq9XoYDAYHlOr6dq31LEnSFTcDryyGHTvSiZKWHXZjERE5kouLC1xcXK54DRdvdIzaUM/sxrIjHdSQw24sIiIi52HYsSNL2GE3FhERkdMw7NiRtjjsmBR2YxERETkLw44dlXZjObkgRERE1zGGHTvSoXidHXZjEREROQ3Djh3pLN1YDDtERETOwrBjR6UtO04uCBER0XWMYceOtFLxmB027BARETkNw44dccwOERGR8zHs2JFOYjcWERGRszHs2JEl7LBhh4iIyGkYduyoZOMxtuwQERE5D8OOHZW27FR/e3oiIiK6Ngw7dlQSdkzsxiIiInIahh070hU36LBlh4iIyHkYduxIV1y7HKBMRETkPAw7dqTlmB0iIiKnY9ixI72shhyGHSIiIudh2LGjkjE7JjDsEBEROQvDjh1pLWN2GHaIiIichWHHjiwDlFnNRERETsNPYTvSlozZYTcWERGR0zDs2FFJy46J1UxEROQ0/BS2Ix1nYxERETkdw44dWcIOq5mIiMhp+ClsR5awI7GaiYiInIWfwnak1bBlh4iIyNn4KWxHJS07HKBMRETkPPwUtiOdtqQbS+PkkhAREV2/GHbsSCer1WuUNBCCW58TERE5A8OOHek0pdVrUpxYECIiousYw44dlXRjAYBJYcsOERGRMzDs2JFWLh2rY2TYISIicgqGHTvSaGTIwgwAMJrZj0VEROQMDDv2pJGhU9Sww24sIiIi52DYsSdZA51iAgAYzQw7REREzsCwY0+SDG1J2GHLDhERkVMw7NiRpJGhE2zZISIiciaGHXuSNZYxO2zZISIicg6GHXuSZI7ZISIicjKGHXvSlI7Z4WwsIiIi52DYsSdZA61lnR2GHSIiImdg2LEnuUw3Flt2iIiInIJhx57KhB12YxERETkHw449yRpOPSciInIyhh17kmVoLVPPuTcWERGRMzDs2JPMqedERETOxrBjT2X3xuKYHSIiIqdg2LEnWebUcyIiIidj2LEnq24sjtkhIiJyBoYdeyrbjWVi2CEiInIGhh17KtONxXV2iIiInINhx57YjUVEROR0DDv2VKYby8SwQ0RE5BQMO/bElh0iIiKnY9ixJ049JyIicjqGHTuSJAlaobboMOwQERE5B8OOnenAvbGIiIiciWHHzvRQW3RMbNkhIiJyCoYdO9OiuBuL6+wQERE5BcOOnemKww4XUCYiInIOhh0700lqiw5bdoiIiJyDYcfOdGDYISIiciatswtQ1uHDh7Fy5UqcOXMGGRkZeOGFF9ClS5cKrz906BCmTZtW7vhXX30FX19fO5a08kpbdpxcECIioutUrQo7RUVFiIqKQu/evfHee+9V+nkffvgh3N3dLY+9vb3tUbxrwrBDRETkXLUq7LRv3x7t27ev8vN8fHzg4eFhhxJVn05S/+UAZSIiIueoVWHnWr344oswGo1o2LAhhg0bhmbNmjm7SBb6kpYdISCEgCRJTi4RERHR9aVOhx0/Pz88+uijiImJgdFoxLp16zBt2jTMnDkTjRs3tvkco9EIo9FoeSxJEtzc3Cxf1yRJkqCT1XsKSBCQIDPs1LiS7xuDpH2xnh2Hde0YrGfHqA31XKfDToMGDdCgQQPL46ZNm+LixYv47bff8NRTT9l8zooVK7Bs2TLL4+joaMyePRtBQUF2KWNBmfluAUEhcNNr7PI6BISGhjq7CNcF1rPjsK4dg/XsGM6s5zoddmyJjY3F0aNHKzw/ZMgQDBgwwPK4JGmmpqbCZDLVaFnUlp3Sx+cSk+DlwrBT0yRJQmhoKJKTkyEEp/jbC+vZcVjXjsF6dgx71bNWq610Q0W9Czvx8fHw8/Or8LxOp4NOp7N5zh4/7FqNBpJQICQZBrMCIbi0kb2I4nFRZF+sZ8dhXTsG69kxnFnPtSrsFBYWIjk52fI4JSUF8fHx8PT0RGBgIH744Qekp6fjySefBAD89ttvCA4ORsOGDWEwGPD333/j4MGDmDx5srPeQjmSRgOdYoZBI8No5pQsIiIiR6tVYefUqVNWiwQuWrQIANCzZ09MmDABGRkZSEtLs5w3mUxYtGgR0tPT4eLigsjISEyZMgWtWrVyeNkrImm00AoTDNBxFWUiIiInqFVhp2XLlliyZEmF5ydMmGD1eNCgQRg0aJC9i1U9Gi10ijoWyGRm2CEiInI0DiCxM7UbSw07bNkhIiJyPIYde9NooBVmAGzZISIicgaGHTuTtFq27BARETkRw469yRpoFbVlx8iWHSIiIodj2LEzjtkhIiJyLoYde9NqoRPFYYctO0RERA7HsGNnbNkhIiJyLoYde5O1ljE7JoYdIiIih2PYsTNJq66gDLAbi4iIyBkYduyteG8sgC07REREzsCwY29lx+ywZYeIiMjhGHbsTNKUXVSQu54TERE5GsOOnUkaDcfsEBERORHDjr1xuwgiIiKnYtixM0kuHaDMlh0iIiLHY9ixN40GesUIADAw7BARETkcw46dSVptmbDDAcpERESOxrBjbxot9Ga27BARETkLw469yRroiwcoFzHsEBERORzDjp1ZdWOZ2I1FRETkaAw79lZmgDKnnhMRETkew46dSWVnY5kYdoiIiByNYcfOpDIDlIs4G4uIiMjhGHbsTVM6QJmzsYiIiByPYcfeNFq4cJ0dIiIip2HYsTNJo+UKykRERE7EsGNvGo3VooJCMPAQERE5EsOOnUkajWXXc4DTz4mIiByNYcfeyiwqCHD6ORERkaMx7NiZJGugFWbIQh2czOnnREREjsWwY28aDSSA08+JiIichGHHziStFgCgFww7REREzsCwY2+a4rDDtXaIiIicgmHHziRZAwDQm9myQ0RE5AwMO/ZW0o2lGAAw7BARETkaw46dSRq1ZadkrR2Did1YREREjsSwY2/FYcfFrLbsFLFlh4iIyKEYduxMKhmgbOYAZSIiImdg2LG34pYdbgZKRETkHAw7diZx6jkREZFTMezY2+XdWNwbi4iIyKG01XlyWloa0tLS0KxZM8ux+Ph4/PrrrzAajejevTu6dOlS7ULWZSWzsVyLBygXcjYWERGRQ1WrZWf+/PlYunSp5XFmZiamTZuGbdu24ciRI3j//fexbdu2aheyTrOEnSIADDtERESOVq2wc+rUKbRu3dryeMOGDTAYDHj33XfxxRdfoHXr1li1alW1C1mXSbIMSLKlZaeA3VhEREQOVa2wk5ubCx8fH8vjXbt2oUWLFggNDYUsy+jSpQsuXLhQ7ULWeRoZbmzZISIicopqhR1vb2+kpqYCAPLy8nDixAm0bdvWcl5RFCgKP9yh0ZaO2TGyPoiIiBypWgOUW7dujTVr1sDd3R2HDh2CEMJqQPL58+cREBBQ7ULWebKGY3aIiIicpFphZ9SoUUhKSsK3334LrVaL+++/H8HBwQAAo9GILVu2oHv37jVS0DpNU3bMDsMOERGRI1Ur7Pj6+mL69OnIz8+HXq+HVlt6OyEEpkyZgsDAwGoXss7TaNmyQ0RE5CTVCjsl3N3dyx3T6/WIioqqidvXfbIGrgaO2SEiInKGaoWdAwcO4MyZMxg4cKDl2N9//42lS5fCZDKhe/fueOCBByDL1/lCzWVmY3HqORERkWNVK4UsXboU8fHxlscJCQn4+uuv4e3tjRYtWmDNmjVYuXJldctY95WZjVVkUqAIBh4iIiJHqVbYuXDhAmJiYiyPN2zYADc3N7z55puYOHEi+vTpgw0bNlS7kHVemdlYAtz5nIiIyJGqFXYKCwvh5uZmebx37160a9cOLi4uAIDY2FjLOjzXNVkDvWKCBDXkcNwOERGR41Qr7AQGBuLUqVMAgOTkZJw7dw5t2rSxnM/NzYVOp6teCesDjQYyBFyKa5vTz4mIiBynWgOUe/TogWXLliE9PR3nz5+Hh4cHOnfubDl/+vRphIWFVbuQdV7xZqBuskChInH6ORERkQNVK+zcfffdMJlM2LNnDwIDA/HEE0/Aw8MDgNqqc+jQIfTr169GClqnycU7n0vsxiIiInK0aoUdjUaDe++9F/fee2+5c56envj666+rc/v6o7hlx1VWww67sYiIiBynRhYVBNTBymlpaQDUsTyurq41des6T9JoIAC4SmrIYTcWERGR41Q77Jw8eRLff/89jh49atnhXJZlNGvWDPfdd5/V1PTrlqUbS62fAnZjEREROUy1ws6JEycwdepUaLVa9O7dG+Hh4QDU9Xc2bdqEN954A1OnTkVsbGyNFLbOKu7G8pLMAIBcA8MOERGRo1Qr7Pz000/w9/fH9OnT4evra3Vu2LBhmDJlCn788UdMmTKlOi9T91nCjgkAkF1kdmZpiIiIrivVWmfnxIkTuO2228oFHUDdEf3WW2/FiRMnqvMS9YNsHXZyGHaIiIgcplphR5IkmM0Vf3ArigJJkqrzEvUDW3aIiIicplphp2nTpvj9999tbgmRlpaGP/74A82aNavOS9QPJWEHRgBAjoFhh4iIyFGqNWbn3nvvxRtvvIFnn30WXbp0sayWnJiYiJ07d0KWZZtr8Fx3SrqxRHHYYcsOERGRw1Qr7ERHR2PWrFn48ccfsXPnThgMBgCAXq9Hu3btMGzYMHh5edVIQes0S8uOWj8MO0RERI5T7XV2IiIiMGnSJCiKguzsbACAt7c3ZFnG8uXLsXjxYixevLjaBa3TSsKOUhp2hBAcz0REROQANbaCsizLNmdlEQCNWs1eShEAwKgIFJoE3HQMO0RERPZWrQHKVEmyWs2uihFaWQ047MoiIiJyDIYdRygeoCwpZni5qF9zRhYREZFj1Fg3Vk04fPgwVq5ciTNnziAjIwMvvPACunTpcsXnHDp0CIsWLcK5c+cQEBCAe+65B7169XJMgStLW1zNZhO8PTXIKDBxrR0iIiIHqXLYOX36dKWvTU9Pr9K9i4qKEBUVhd69e+O999676vUpKSl4++23cdttt+Gpp57CwYMH8cUXX8DX1xft2rWr0mvbk6TVQQCAyQTvkpYdhh0iIiKHqHLYeeWVV+xRDgBA+/bt0b59+0pf/8cffyA4OBgPPPAAAHVm2NGjR/Hbb7/VqrADnV7912iwhJ3sIpMTC0RERHT9qHLYefzxx+1Rjmty4sQJtG7d2upY27ZtsXDhwgqfYzQaYTQaLY8lSYKbm5vl65pkuZ++JOwY4e1a0rLDrTRqSkk9sj7ti/XsOKxrx2A9O0ZtqOcqh53aNB4mMzMTPj4+Vsd8fHxQUFAAg8EAfUnIKGPFihVYtmyZ5XF0dDRmz56NoKAgu5XTJyAQGQBcNDIaBPgCyIRZ62JZcZpqRmhoqLOLcF1gPTsO69oxWM+O4cx6rlUDlB1hyJAhGDBggOVxSdJMTU2FyVSzXUuSJCE0NBTZBQUAgKKcHMhG9evk9BwkJSXV6Otdr0rqOTk5GUIIZxen3mI9Ow7r2jFYz45hr3rWarWVbqio02HH19cXWVlZVseysrLg5uZms1UHAHQ6HXQ6nc1z9vphF1r19USZMTsZBUb+z1XDhBCsUwdgPTsO69oxWM+O4cx6rtPr7DRp0gQHDhywOrZ//37ExcU5qUQVKDNAOcRTDT5JOcYrPIGIiIhqSq0KO4WFhYiPj0d8fDwAdWp5fHw80tLSAAA//PADPvnkE8v1t99+O1JSUvDdd9/hwoUL+P3337Flyxb079/fGcWvWEnYMRkR5qV+fanAhCKT4sRCERERXR9qVTfWqVOnMG3aNMvjRYsWAQB69uyJCRMmICMjwxJ8ACA4OBgvv/wyvvnmG6xevRoBAQF47LHHate0c6jr7ACwTD331MvINShIyjEgys/VuYUjIiKq52pV2GnZsiWWLFlS4fkJEybYfM4777xjz2JVX8kYoeIp72Feepy4VIikHCPDDhERkZ3Vqm6seqvMmB0Alq6spByDs0pERER03WDYcQTt5S076uNEhh0iIiK7Y9hxBMsAZTXcNGDLDhERkcMw7DhCyZgdsxnCbC7TjcXp50RERPbGsOMIujILHHL6ORERkUMx7DjCZWHHSy/DRaNuU5FewN3PiYiI7IlhxwEkjQaQi6vaaIAkSfBxVWf9ZxWanVgyIiKi+o9hx1Es08/VcTo+ruoeWVmFbNkhIiKyJ4YdR9GVrqIMAL4lLTtFbNkhIiKyJ4YdR9HabtnJZMsOERGRXTHsOMplLTtB7iW7n3OtHSIiInti2HGUy7aMiPFX98Q6eanQWSUiIiK6LjDsOIplFWW1GysmQA0757MNKDByrR0iIiJ7YdhxlMu6sfzdtAhw00IRwJkMtu4QERHZC8OOoxS37Ahj6RYRscWtOyfTGXaIiIjshWHHUbTWLTsAEMtxO0RERHbHsOMoly0qCLBlh4iIyBEYdhxE0pVv2Wnk6wIASM4xwKQIZxSLiIio3mPYcZTLpp4D6iBlvUaCWQCpecYKnkhERETVwbDjKCUtO6bSUCNLEkI9ubggERGRPTHsOIqNlh0ACPNSjyflsGWHiIjIHhh2HEVbfoAyUDbssGWHiIjIHhh2HMXGAGUACPNiNxYREZE9Mew4io2p5wDQ0FudkXUqvRBCcEYWERFRTWPYcRQbA5QBoEmgK3SyhIxCMy5ks3WHiIiopjHsOErxCsrism4svUa27IB+OqPI4cUiIiKq7xh2HKWCbiwAiPBRz53LYtghIiKqaQw7DmJZQdlUvquqkY86buc8u7GIiIhqHMOOo1ypZcdbPXeeLTtEREQ1jmHHUSqYeg6UdmMl5hhg5h5ZRERENYphx1EqWFQQAII8dHDRSDApQHIuV1ImIiKqSQw7jlLBdhGAukdWOLuyiIiI7IJhx1Gu0I0FAA2LBymfy+IgZSIioprEsOMoLupaOigqsHm6ZEbWyXTb54mIiOjaMOw4irun+q/BAGFj3E7rUHcAwP7kfA5SJiIiqkEMO47i5lb6dUFeudOx/q5w1UrIMypI5KagRERENYZhx0EkWQO4qa03yC8fdjSyhEhftavrDLeNICIiqjEMO47k5qH+a6NlBwAa+6njds5kFDqqRERERPUew44juReHHRstOwAQ7ccNQYmIiGoaw44jFYcdUWHYUVt2TlwqQKFJcVixiIiI6jOGHUfy8FL/zc22ebqxvyuC3LXIMyjYeNb2NURERFQ1DDsOJPn4qV9kZ9g8r5Ul3NhIDURnM9mVRUREVBMYdhzJuzjsZNkOO0DpDugXsjn9nIiIqCYw7DiSjy8AQGRnVnhJyUrKe5PykMJNQYmIiKqNYceBpEq07DQLckNDHz3MAjhw0fZAZiIiIqo8hh1HusqYHUDdAb1FkLr44Mdbk5FnMDuiZERERPUWw44jWVp2MiFExftfuWoly9f7k/PtXSoiIqJ6jWHHkbx91X/NJiA/t8LLWoa4W77OLDTZuVBERET1G8OOA0k6Xenu51cYt9Ml3BMBbloAQEoeBykTERFVB8OOo/lcfZCyJEkY3MIfAHAui+vtEDlLfEYh0guq37p6Kr0Q83ZdxIlLBdidmIstCTko4irpRA6jdXYBrjt+AUDSOYj0NEhXuKxkkPKBiwUwmgV0mitdTURVkZRjwJKDaWgd4oHejX0ghIDBLOCiVf/+O3gxH6/9lWC5/o1bItDQxwWB7loIqBMJruZSvhE/7E9D72gfzN54AVmFZqw8WvpHTv+mfhjXKcTmc/ck5uL1f3ahVaAeg5v7Qafh36VE1cGw42CSfxAEAKSnXvG6xv4u8HHVIKvQjKNp+Wgd4uGQ8hHVZ0IIzN+dYgkdf5/ORosgN+xMzMXXO1PwUPsgDGruj0+2JVk9b9o/5y1ftwhyw8zbGkGWJBy6mA+NLKFZkJvV9RkFJjy7Oh7ZRWb8dSrLZll+O5aB3tE+iA1wxfmsIny/Pw1mRWBQM3/M2ZSIrCIz9l4ADl7Mw5RbGkIrlwas/x26hF2JuXixRzh83Wrfr/GjqQVYceQSujb0Qq9oH2cXh4hhx+ECgtR/rxJ2ZElC+zAPrD+TjZ0X8hh2iKrhQrYBhSYFBpNi1boCAONXnrZ8vXBPKuIzipCUo46Vi/ZzQXqBCVmFpUtAHE4twDv/JaJjAw98si0ZAHBzpDdui/VBm1AP5BvN+GF/KrKLrr5sxPNr48sd23beevLC3uR8rDmegbuaqV3bBrOCRXvV3x8PLj+JNqHueLhDMCJ9XSBVosXJ3nINZrz0x1kAauhh2KHagGHH0fzUsCPS0656aZcIT6w/k42t53LwUPugWvGLjMjZTIqARkK5/x/iMwpxKd8EjSyhdYg7NLKE0+mF+GRbEk6llx/7NuGGUHxaHFbKWh+vbsJ7S7Q3nu3WAGcyCvHyH2dRaCpdLmLLuRxsOZdjebzhbDY2nM3GiNYBWHzgkuX4UzeGYvXxTJxKLwSgbgczoKkfvthx8arvs2mwJ5r667HyaDrm7krB/w5dwnPdGyDnsrW39ifn45nV8QCAbo288EzXMLhqK9ftlVNkRmKOAXEBrpAkCSZFYPGBNDQPcoObVkZjf1dL115lGM0Co5eesDzOLDSjwKjATcduOHIuhh0Hk7y81W6svJyrXYqODTyh10hIzjXiTEYRGvu72r18RM60/XwOTlwqxMjWgdDI5cP9uawivPj7WYR56eCikZFeYIKHXsbAZv74v61JKBnz66GXEeGtx7G0QpuvM+GGUNwe6wu9RsK+5Hz4umqw/HC61TWj26p/mET7uWLu4FhkFZlwOr0Il/KNWLjHdsts2aADAH0a+6BzuCc2xGejT4wP3HUaAMCuxFzsuGC9QvptMT44cakQ8cWbAHds6IfeDV3w27F0mAWQUWjGlHXnLNeHeeksLVAlNifkoLGfC05nFCEhswjPdW+APIMZLloZOlnCjgu5uKdlAH7Yl4r/lXm/I1oHYFSbICw9mIYlB0vfw8Bmfni4o+1xRWWZFYE8o4L/ioNiWRdzDYjy4+8uci6GHUcrmXp+hXV2SrhqZXRs4Ikt53KwKSGHYYfqrbOZRZix/rxlqYWdF3Ix584oS+tNWr4RWknCB5sTkW9UyrXUfLDZeoxNnkGxCjp3NvHFmhOZlsetgtUJAL2ifSzdLA+0C8Ln2y9iV2Iu3rotEkEeOsv1Xi4aeLloEOHtggKjYgk7dzbxxcBm/lh66BL+Pp0FWQKU4gagO2J9IUkSfFy1li6oEo93CUXrsznwctFg6cE0FJkE7m0TiG3nc/HljosIdNfikW5RyM1Iw2s9I/Dm+vO43JM3hOFwaj6+32fdSvxdmcfPrYkHAOhkCcbigiXlGPDPGetQsvjApXJBDQBWHs3ApXwTOod7IinXgO6NvBHp61Luuq93XrSq36aBrsgzKDifbcAzq+PxSMdg/HwkHe3CPPDUjWHlnm/LqfRC5BSZ0S6MXfhUfQw7juZRHHbyrh52ALVZesu5HGxOyMZ9bQPZlUX1jtGs4J3/LlitKXU6owhP/XYGL3RvgIu5Rry14QJsrTnuqpWsupdsaR7khse6hMJNJ2P54XS0CXFHA299ueskScITN4RetbxuOhltQt1xPK0Ag5r7I8xLj2e6huHxLiEwmARyDGb8eybbsnyELQHuOgxqrp7v3dgHZkVAI0vo28QXHjoZcUHu8HLVIRdAx3BP/DyqKf4+nYVPtiXj3taBuDnKG6FeejQLckPHBp7wdtHglT/OIjXf9jT5kqADoFzQsUUrw9JKtilB/WMLUENR/zhfjOkQYpkheinfaBV0YvxdMOu2SPy4Pw3LDqkBau6uFADAX6ey0K2hFxr7u8LXVVPh77M8gxmv/ZmAApOCF29qgO6NvAEAhSYFihCWFrK6QgjhsN/dJkUgt8h8TQPXc4vMmLM5Ed4uGjzTNaxefd4w7DhaSdgpyINQFEjylfuyO4V7QCdLSMxhVxbVT78dz8D5bAPctDJe6NEA04tbMc5lGSxjUcrq2tATOQYFt8X4oEWQOx5fdRp+rhq8fHMEPPUyJAkI8dTjrQ3ncTilAGM6BAMA7msbhGaBbmh62cypa/FazwgUGhWrDxS9RoZeA3i6aDCyTWCV7lfSZSdLEnpG+5T7kJEkCX1ifHFLYx+rae9aWUJM8e+EuUNi8dm2ZPx+MhMtgtzQL84P721KrPA1H+0UjP5xfkjMMeKJVeog7Wg/F7zXNwpaWcILa+Nx4lL5bsDfjmfit+OZmHVrI1zIMZQb9zS4eQC0soQBTf0sYaesklaqnlHeGNc5BJ566+CSZzBj1oYLKChOW+/8l4hv73GHh16D59bEI9+o4MM7o676YS6E2rV2+f0rkpRjwMqj6RjYTA2wNeVUeiFe+eMsgjx0aOjjggfbB1Xp/kUmBScuFaJlsBvS8k3453QWOoZ74lxWEf45k41BzfzQoYH6uZJZYMKk388ivcCInlE+aBPqftUB4rkGM2ZvuIBWIe4wmAV2Jardq3c180eMvyvMikBCVhGiaskA+GsliStt0nQdSU1NhdFYs6sVS5KEsLAwJCUlWfbCEiYjlMfvAQDIH/4AqST8XMHbGy5gy7kcdAjzwBu9G9ZoGesDW/VMNa+69XzwYj5CPHX4eudF5BrMuCXaB70b++ClP87ixKVCjOsUgv5N/ZCYbcDkvxJwycZifn5uWvxf/2h4uZR+gGUWmOCul6G/bC0aIQQUAZtjf2q7a61rIQQu5hoR5KGDRpZwPrsIOYVmvPxngtV1D3cMxsAyXWtCCJxML0QjHxfLgOQtCTmY/d8FuOlkxPi74sDFK+/T17WhJ+IC3DC4hb8lkO1OzMW0f84jxt8FMf6u+OOk7Wn4HcI8MOmmBnj3v0TsTsqzec3lJtwQimAPHcyKQIcGHpYPYrMi8OWOi/j9ZCYAoG2oO/rH+aFzhCdkScLSg2nYm5yP8Z1DYDAL5EpuiHE34ZGfT6HQpKB7Iy88370BCkyVD0pXMmdTIv4tM5apc7gnJveKqPTzv9mTguWH0zG6TSA2JeRYxnSV1b2RFzqFe+JMRmG52Yav9gxHpwaelv8PknIMmPr3ORSZFNzbJgjpBUb8VNyFKQFWLajzh8Tg3/hsfLMnFZ3DPTGpR4MqDVgvYa/f0TqdDkFBQZUrA8OOylFhBwDMTw4Higohz/wSUvDV+6+Tcgx4fOVpCABfDmyM0Br8q6M+YNhxjOrU87ZzOZi14UK542XHuHw9KAbBnqXjZPKNZiw7eAm/HM1A00BXPN+9AfQa2Sro1Fc1/TP916lMuOtkdCvuDqqs9AITdLIELxcNEjKLsPVcDr7fX34m6dgOwRjQ1M9msEzNM8LfTYsis4J3/ktEdpEZNzb0LDfWqE2ou9XGxy/fHI4DF/Px27GKV5svcXOkNzILTWgV4g5PvQZf7bQ92+3t2xvh5T8SbJ67nARgTIdg3NXMD4oA8o0KMgtNaORTfsxSidwiM9LyjXDRyvh8ezJOZxShyKTAYLb+Ht4W4wM/N61lIH7Zbq7z2UU4dLEA8ZmFuJhrtLS0VFerEHWJgg82JyIhy1Cp5wS4aeGmk3E+W70+0F2LN3o3xNydF9Eu1AN3twyo1H0YdmoRh4adlx8BLqVAfmk2pNjmlbrXG3+fw96kPAxvFWCZJUIqhh3HqE49T1mXYPVBdrmRrQNwbxvbP9f5RjPctHKdbkKvqtr8M70hPhvpBUasOZ6JJgGueOrGsGv6a/+jLUn4+7Ttlp7HOofgzjg/FJkU7E/OR3Ku+mEb7eeK5FwD/m9r+SUDKhLkrq1wLFNllQ3lz3ULg5eLuuDriUsF0MgSTl4qhCQBh1IKKrzH9D4NsTkhx2p80w0RnogNcMXq45mI9NFjTIdgTPr9bLlwVFaAuxb3tAhAZqEJioDNrsKvBjVGQqYBM/4tP7D9aibcEIoCo4L5u1Nsno/w1lvCT/dGXnjxpnAA6titf+Oz0S/Oz2rpA7MikG9S0DSqoVPDDsfsOENgCHApBSLtYqXDzq2NfbA3KQ/rTmdhaMuAa/rlQlTTjGYBjXzl7RMMZgXnLvtLsnsjL8ug11h/1wqDDoA6Nxi1vrs5Sm0dGty8cn/VV+TRTsFoH+YBkyLw0ZbS2XRTekWgU7jave+ildE5wrqrv1WIO7KLzPj9RCZe6xWBH/enYXOC9VIe0X4umH17JPQaCTkGBfcvO4GKjOsWDWHIhyIEsgvNWGojPJQZ3405l838q4w7m/iiTagHfFy1VmFn2/lcyyKSGQUmm2PUSuhkCSGeOjzXvYFlnFahScHFXAP2J+cjq3gRyzAvHUI89Qjx1CPYQ2dzM2kvFw2eujEU7/6XCKMi8GinYGQXmdE+1APNg92RlGPAwj0pVu+7REnQAdTB64O+P4oXujfAiiPpOJVeiP8duoRvhzaBLEnIN5rx1r8XcCglH3NHeSPAiX+vsGWnmCNbdpSFH0Ns+gvSoFGQB4ys1L0MZgVjV5xCTpEZMf4ueOu2SAaeYrX5r+D6pGw9G80KXi4ea9MqxB0z+jSEJEmIzyiEl4sGAe46HEnNx9oTmXDTypZf8DH+rmgZ7IaxHYJRZBZYfTwDncI9r9g1cD26nn6mhRA4cDEfKw6nQ6eR8NJN4VUeZ5VrMKPIpODz7eq0/SEt/BHiWdrd/9epTGw/n4swLz26N/LCVzsvIjHHgEk9wtGvQyySk5MhhIBZEUjJM0IrS9DKEh5afvKa3pOXXsbYjiGIC3TFmfQidGvkZXlPG+Kz8ffpLOy5wtikUE8d2oV5YG3x/zePdwmxLGVQEbMi8M+ZLET6uqBJgDoIf/2ZLCw7dAltQz1wf7sg/HosA25aGbfH+lx1v7Uik4K9yXn4v63JGN0mENlFZvxgowvTlgfbB+FwSgF2XCiddXxr02A83TmA3VjO5tCw8+tiiF++h9S9D+SHnqn0/Q5ezMfbG84jx6Cgc7gnXusZfl017VfkevpgcKaSej5/IRHPrTmDMxmlAyVn3toI/8ZnWQagtgp2w8HLmvRLBiDT1fFn2r6KTAqMioCXi/aK9awIgVPphfhuXxr2JuXhma5huJhrwIlLhTiZXojRbYIQUjzOLNbfFcm5RsQGVG7GbGqeEY/8fKrc8bgAV7zbNwqAOjg5Nc+IZ7qGOX0z2KQcAx4r3lrlvb6RaOznirErTiKz8OrbogBA8xAvzOoTjpqcL8Cwcw0cGna2/Qsx930grhU0k2ZV6Z57k/Iw7Z9zUAQwrXdDLrgFfjA4yob4bKw4loWiIiMu5FRugGOJADctPh/YmK2RlcSfaceobD3nGsxIzDYgLrD6yxaUdfBiPlYcvoTRbYNgFgI/7k/DPS0C0DLEvUZfp6YoQkBC6VYtGQUmTF9/3rIdyn1tA60WtQQAH1cNXusZgV6tG1ta0GoKx+zUclJgiDq9L63yg+xKtAvzQP84P6w6loElB9MYdsjuVh/PQHxGkWUqb4lwbz2eujEUr687ZxlQ2b+pH/IMZqw/kw0JwLjOITifbUC/Jr4MOlRneeo1NR50AHX8Uasyweb1W2r30iKXj83zc9Pi1Z7hmLvzIvo28UPLYHdkFJqxOSEHzQJdcWecekyncf4EA4YdZwgq3msm4xKEyQhJq7vy9ZcZ3MIfa05k4FBKAT7fnoxxnULq5FoiVPsVGBV8WcGmlW/cEoEQTz0+6BeFrQm5iPJzsQwsvbtFAEyKsAykJKL6KdBdh5dvLl03aFynEIzrdPX91ByNYccZvHwBvQtgKAIupQIhDar09EB3He6I9cVvxzOx9kQmFCEwrlOoZfl2ourILTLj461J8HXVIquodMquLAEfD22HMG0B8o0KvIvXu4nwdsHQVtYDjG3tn0RE5CxsV3YCSZLU6ecAxMFd13SPsR1DEFk8g+WPk1n4dq/tNRGIqup/hy9h2/lc/H4yE1vPqbMpov1c8Pbtkbghyh86jWwJOkREdQHDjrMY1QGe4qevIZTKjWYvSytLmHFbI8vjX45m4I+TmRzMSNckz2DGL0fS8fIfZ7H8cLrVuQk3hOKDO6PQLKh2DpokIroadmM5idS0NURq8QDltBSgEttGXM7bRYNlI5vi8ZWnkJpvwqfbknE0tQBjOwbDQ+f8AWFU+ylC4ODFfHy8JancKrPPdA1Dl3BPeLIVh4jquFoZdtauXYtVq1YhMzMTkZGRGDt2LGJjY21eu379enz22WdWx3Q6Hb7//ntHFPWaSYPvg9j4JwBA+XQmNNM+uab76DQSpvVphM+3J+PAxXysO52FdaezMKxlAIa08EehSUFyrhEtg/lXOakUIfBffDYa+rioK5vuKt8FOqlHA/SIrNo+SkREtVWtCzubN2/GokWL8Oijj6JJkyb47bffMHPmTHz44Yfw8bG9Vb2bmxs++ugjB5e0eiQfPyC2BXDyMJBYuY3pKhLurceMWxth6cE0yxoHSw9dslr2PMBdi/vaBuFirgE+rlp0begFP7da9+0nO1t8IM2yCqqPqwZB7qUzASd2C0OUrwtCPPVw07GHm4jqj1r3G+3XX39Fnz59cMsttyAiIgKPPvoo9Ho9/vnnnwqfI0kSfH19rf6rC+ShD1m+vpZxO5cb1ioQy+9tih6RXuXOXco34aMtSfjpwCV8ueMinlsTj7OZRTbuQvVVRoHJarn3rEIzThYvBvZ+3yj0ivZBlJ8rgw4R1Tu16k97k8mE06dPY/DgwZZjsiyjdevWOH78eIXPKywsxBNPPAEhBKKjo3HvvfeiYUPbizMZjUarlZIlSYKbm5vl65pUcr8K7xvVpPTr/Tshtb+x2q+p1Uh48aYI3ByVg+xCE3Ym5uFoar5lSe+4AFeczihEeoEJT/92Bq5aCY91CUXbUA946jV1cuG3q9bzdW79mSysPZGBIxXsyNyhgQeaVGLBNNaz47CuHYP17Bi1oZ5r1XYR6enpeOyxxzBjxgzExcVZjn/33Xc4fPgwZs0qv7XC8ePHkZSUhMjISOTn52PlypU4cuQI5syZg4CA8rvyLlmyBMuWLbM8jo6OxuzZs+3zhirhXP9Olq8b/rbTbq9TaDTj6MUctArzxnc7E/DphtM2r2se4oXYIE+EebtCp5Vwf+dILlhYB51MzYW7XoPtZzMw8/ejVucGt2mAF/o0wfGUXJy5lIcejQPh76Gv4E5ERHVfrWrZuRZxcXFWwSguLg4TJ07En3/+iZEjy+8oPmTIEAwYMMDyuCRppqamwmQylbu+OiRJQmhoaKX3A0lKSqrR179ciAZITSnAHY1c0GdUM+xJzMWczYnIMyiWa45czMGRizmWx/8dT4ZelnB3y0AAAlG+rrVudk5V67m+EkJgy7kc7Difi3Wns8qdb+ijR1ygG4Y29UB6agoCJSAwUEJR9iUkZV/9/qxnx2FdOwbr2THsVc9arbZu7o3l7e0NWZaRmZlpdTwzM7PS43C0Wi2io6ORnGx73ymdTgedzvb2DPb6YRdCVOrejvyfTSMBncI98cOwOCTlGJBTZMavxzLwb7z1p97+5HwAwM7EPADqXi4vdG8AX1dNpZskU/OM8HXV2H3X3srWc32x7lQm3PUatA/zwNKDl/DPmSxcyi8f2CO89XjjloYI9iz9ua9OPV1v9exMrGvHYD07hjPruVaFHa1Wi8aNG+PgwYPo0qULAEBRFBw8eBB9+/at1D0URUFCQgLat29vz6LWGKl7H4hN6wCoPwjO6NMM89IjzAt4LtANz3QNgywBWUVmvPZnAs5nW+9uffBiPh5afhIA0MhHj5bB7jhwMR8uWgnRfq64JdoHrULcoQiB89kGZBeaMWVdAjo28MTkXhG2Xp6q6HxWEfYm5+HrnRWvmh3l64LkXANcNDLevj0SXrWsNY6IyJFqVdgBgAEDBuDTTz9F48aNERsbi9WrV6OoqAi9evUCAHzyySfw9/fHqFGjAADLli1DkyZNEBoairy8PKxcuRKpqano06ePE99F5UkjH7WEHRgMgItz9xQqGZ/j66rFx/2jYVIEBIDlhy9hx/lcnM4oncGVkGVAQlZpGDqVXoSt53Lw+i0NsTkhBz8fKV2Jd8eFXMzZlIjHuoTARSNzHNA1yjea8eLvZ5FnVMqdG902EA29XXBDQ0/IkoRcgxmKAIMOEV33al3Y6datG7Kzs7FkyRJkZmYiKioKr776qqUbKy0tzar1Izc3F19++SUyMzPh4eGBxo0bY8aMGYiIqCOtCC5ugCQDQgEKcp0edsrSyJIllIxqE4RRbYKQmG3AiUsFuJBjwB8nsxDioUNcoCuMZoE1JzKRa1Dw4u9nbd7v3/hsSzeZl4sGH/WLgr+bFooAw89V7EvOw6qjGTh5qcAq6LhpZRSaFEy/tSFah3hYPcdTz5BDRATUstlYzpSammo1Jb0mSJKEsLAwJCUlXbGf0vzMvUB+HuQ3P4UUZnvKfG10ebfbF9uTseZEpuVxXIArHukUgou5Rry/KdHmPTx0MvKMCoI9tBjaMhC3x/ogOdeInRdy0beJb6XG+VS2nuuaAxfzsOTAJZzLNiCjwHosjlYGpvRqiNYh7sgzmOHtav+/W+prPddGrGvHYD07hr3qWafT1c0BytctNw8gPw/K6xMgv/kZpLC60Sp1+fii+9oGocCkWFpr+jbxRZiXHk0D3dA00BXjfik/3b2klSIlz4TPtifj95OZOFW80N3cXSmIC1AXuWsV4o6hLQMs3TP/O3QJ8RlF6BTuiQHN/O3/Zh0gKceAb/akwFUr40hqAZJzrcN36xB39IzyRrswDwR5lA42dkTQISKqy/hbshaQWneCWL8aACBWL4X08EQnl+jaeLpoMLFbA5vnQjz1eKRjMLSyhCg/F7z8h7pFRv84X/SN88OfJzOx8miGJeiUOH5JfbwvOR+7E/PwfPcGWHrwEn4/mQkA2J2Uh4u5Rky+q+obqTrTtvM5OJxSgITMIqTkGdEkwBX/nKl4/vf4ziHo28QXMhc/IyKqMoadWkAa+ShgKILYvA7iQryzi2M3d5VpgZk7OAYaWYJ/8f5cD7UPxrbzubhYpjWjSXGrTqC7Dn+fzsKR1AI88vOpcvf95Wg62kcnob1/afOoSRH4/UQm3HUyAj200EgS9iTlwd9Ni1tjfKHTlIaGfKMZ7rrKj29RhLCEDrMioAhY3a9EkUnBzsRcHEjOR3KuEW46GbfH+uJMRiG+2ZNqde3ls94A4N42gRjYzA+uWpkhh4ioGhh2agFJowH6D4fYvA5IOg9hNqvH6rGy3TCAOkB59u2RSMs3opGPS7ltK4I9tPjpQOnGph0beOCZrmF4aPlJKAKYuuYIACDUUwc/Ny1OXiqEUbHdN7z/Yj4GNPWDBGBPUh6WHLyEid3C0NDHBUk5BnRr5GUVLjadzcbyw+l4sH0Qtp7LwZoTmRjROhDRvi6YtzsFF3ON6BfniyKTwK7EXEzs1gBH0wqw4vAlFJqsy7A5IQeV0TncA8NaBnDgNhFRDeAA5WLOHKAMAEJRoEy8D8jPhfzSbEixzWu0LPVFdpEZRrM6LkiSJOy8kIvp68/X6Gt0DvfEwx2D4e2iwW/HM/D9vrSrP+kqIrz1yCoyI6dI3aPszia+eLRTCGQJOJJagLPF3VmDm/vDp5aOweFgTsdhXTsG69kxOECZLCRZhtSyPcSO/yCO7GPYqYC3iwZAaatXp3BPLLg7FpuSTPjnWDJ8XTXYl5yHZoFuuK9tENx0Mtx1GpgUAX93Ld7flIjt53Ov+Bo7LuRiX3IeDObK/U+plYEGXnqrNYdKTOwWhl7RPgDU2WspeUbIkmTVstUi2B0tgt0r9VpERFR1DDu1SXgksOM/IMW+e2TVNwHuOozv0QgDY9yu+lfDqzeHo8CkILPAjEKTgg3x2bijiS/2JOXhmz2pkAAYFWEJOrIEPHVjGFoGu+HH/WlIyDKgX5wvGvq4wE0rw99Na7VX2MVcA37Yl4bWoe64NcbX6rUlSUKIJzfcJCJyNIad2iQoFAAgtv4DcddISMF1a4ZRXSBJEtx1GsuA5Mb+rgDULTPuiPWFRpaQbzTj650XcS7LgAFN/SwtM89WMNOsrBBPPSZ2v/p1RETkOAw7tYgUGIKSdgnl7RchT/sUkpe3U8t0PSkZDOyu0+CZrgwsRET1hX23oaaqCY8CfIunZ+dkQezc6NTiEBER1QcMO7WI5OIC+ZX3gMhYAIA4tNvJJSIiIqr7GHZqGck/EPLQh9QH+7ZDcLAyERFRtTDs1EYBwZYvlXdehki+AOWvXyBqeB0gIiKi6wEHKNdGfgGlX2dlQJn+LGAoAkwmSH3vcVqxiIiI6iK27NRCktZ6KwUYigAA4uh+J5SGiIiobmPYqaXk52eUP6hzAQCIzEsw/990KL+vcHCpiIiI6h6GnVpKatYGUr/h1gf3boWy5R8o334G7N8BsWwBzNOegcjPc04hiYiI6gCGnVpMumMwpGFjgHY3Wo6J+R8Al1JKLzp/BuKnryHMZm5kR0REZAPDTi0muXtCvn0I5DFPW5+4cNbqodjyN5RJD0F54UGIc2ccUjZl+waIk4cd8lpERETVwbBTB0junle/KCcLyM6EsupHAIDIyYKy+W+I/FyI9DTLZUIIiLMnIfKuvPP3lYgLZyG+fg/K7Jev+R5ERESOwqnndYR094MQy7+5+oVnT6lr8iyeBwCWvbakR56H1Pkm4MxxKG+/CIQ1hObNT6+pLGVbj4TZDEmjucLVREREzsWwU0fId94D0bQVlLcmXfnC9FRL0ClLzH0f4ttPgaJC9UDSOQhDESS9yxVvJ1KTgYAgSLIaaITJBDFvTukF+XlAFTcrFdkZgLsXJC1//IiIyP7YjVWXNGxs+VJ+fgY0X6+E/MnSyj+/JOgUEyu+gzhR8bgbZc3/oLw6DqLsFPfcbOuL8i57fBXiYiKU5x+E8uEbVXoeERHRtWLYqUMknQ7yc9MhPzUFUrM26jEXFyA67pruJ/76Rd2OwlAEcewARNL50nNCWLrNxPo1ECYTlJU/Qhw/aH2T3JzS5+TlQhw/dMVZYWLrevWLYweuqcxERERVxX6EOkZq3rbcMfmJVyD+XAmxa5O6Y/qlFODsyUrfU2xaB/HDF+qD0AjI41+E2PRX6QU6PZTH77b95LwcCMUMJJ6D8vV7QGIC5MdeAjp2V+99dD/Evh2QhtyndpmV6TYTRiMknc72fYmIiGoIw049IPkGqOvxDBtjOabs2AicPgbx1y+AmztQkF/h8y1BBwCSz0OZdtlU94sXKnyu8t1nkJq3hdjyT+mxL2ZDfnsepIAgKO9PVl9jx3+Q311g/bqb10FZvRTyg09BatFOPZaSBEBACm5gfW1eDpQ5r0Pq2A1yv2EVloeIiOhyDDv1lNy5B9C5B8TNdwA+voAAlGdHlV5wlQBUaZnpVkGnhPL1u5Bu7lt6IEu9Tpw9YTkkvvtMvfaD1yGNmwQc2g2xbQNgMkLqcxfE3m2Qn58BKSgUYt2vQMIpiIRTAMMOERFVAcfs1HNSWAQkd09IHp6Af5DluPzKe+UvDgq1fu6o8aVf33wH5JfervwLnzoKseBDq0NiwYfArs02LxdfvQuxaR1gMqqP160CLqVA+e5ziMICiOL1gwC1+6u2EEJAGA3OLgYREV0Bw851RJ40C9LwhyF/+L0agkY8rJ7Qu0D+6EfIMz6HdO84y/VS2y6lX7fqCCm2hfX93p4L6Zb+9i304T1QZr1gdUh552WIlCR10cSkcxBmMwq2rIfIzYYoKizuCnMM8d3nUCbeB5F20WGvSUREVcNurOuIFBgC6bZBpY97D4DUoTsk/8DSi3rdqW5HERENeJZZP6dBo/L3CwgGho2BOHsSOH3M9ov6+ANZ6bbPubmr55PP2z5fIumc9eP4E1BeU1udhI8fpCYtkbZzIxDWEAhpAOzbAfmpyZBad4JQFIj1qyFFxwGFBUDjppBcXK1uJ4pbk1BYALHmf5C694Fk4/3aIjasVf9d+z9I9z1RqecQEZFjMexcxyRZA5QNOsXHpPsnlD6++wHAaIQU0uDyp6vndXrIL80GJAli3UrrBQ1jmkEe/xKUF8dYP2fwfeqsMv9AiMN7IRZ8VHrSywfQaIDMCgLS5bIyIHZuVL9OOmcJRsrHb0K6fwLED18CZlPpStKdb1LHBxUTBflQnrkXKDNdXvzzGzSfLavc65cwsCuLiKi2YtihK5LvHHrVayS5uDe0551A0nmIlCRIAcGQRj+urg307kIokx4qvb7XnZA8vNQHXXtD7N4C7N8B+ZX3IEU3gcjNBo4dhPL7cuDM8dIXio5TW5sO7KxU2cW35bfDEDv+gzkrHVKHbpA63wTlgzesgg4AwGiAuJQKKaB0jJPYtwPw9IIU08z2ixmKKlUmIiJyPIYdqhLp5r4QG9ZCKl5Hx+qcTm/VKmTh7VP6dXADoMzGppIkQX7iFSAnG5KPn3rM0xvo2A1ykxZAbjbEuTMQf/wMecwzkMIaQlxMhFizzHotoKo4fkhd/PCnryu8RHlZHc8kz54HmM1QPpmuPv7oR7X7Lfm81YBvcVnYEUf2QaSnQerWG8jKgPLxNEg9blO7xzRaQK8HfPyAc2eA2BaQ3Nyv7b0QEdFVSeJKy91eR1JTU2Gs4Vk+kiQhLCwMSUlJV1xVuC4RhiLg8B6gWVtIrm6Vfp7y+3Ig4xKkEY9AkqQaKYv50YEAAKnrLQgZ8xSSx1kvfFgSzKpD6nUnEBUHsbBMV1vL9sChPdYXengBoeGAjx/kx16G8sQ9gMmknmveFjiy78qvM3AU5LtGVqus9lYff55rK9a1Y7CeHcNe9azT6RAUFHT1C8GWHaoiSe8CtLuxys+T76hgBebqlGXcixBb/oY84hHowhtBHvEIlFU/qeOMdDpIXXsDEBAbfr/m1xDr1wBYY33w8qADAHk5wKmj6tcnj5QGHeCqQQcAxMofIPreDUmnv+ayAsWbrOpdqxREiYjqO4YdqrNKFk4saSmSbxsE6daB1heNfgxSv+HqIGadDuLofmDPVutrmrQAymyIKj/+CsSJw+rq09dA+fm7a3qe+PtXKKePA0YD5LHPqjvKe/tWGFyE0QDlkxmQ4lpB7j9cXWX6+QcBHz9o3vvmmspARFQfMexQvSbJGiAgCNIdQ9QDvQdApKdCeecVoFFjyL0HQGrWBsqiTyD++wPS0DGQOnQFmrWB2LcNSE22vt9tgyD+vEoIunyz1EoSyxZavla+fAc4uh+IjIVm8pzy1yZfgDhxCDi8V53R1qixOosNUGeoGQ2ARls6ePzy5wsBFBVAcq3cWCFxeC/MS+ejaOJUwCegqm+NiMipOGanGMfs1F01Uc9CCMBQZLUGjzAagbxswMNbXUcoMgbQaKA8oc5Qk0Y8AqnrLepKz5+9Balpa4jN62rkPZUlf/ULxMofgIJ8iMQEIDtTXQvpsjWMpHGTIL56V/36oWcs44zkp6YArTup3WsRUZDc3KF8/znEf39AnvIhxIWzkDQaSB27Q5jNkEpCUxkl46NkX3/I733Dn2c74+8Ox2A9O0ZtGLPDsFOMYafucnQ9K8VbWUjDxloNthZGozow2Zag0HKtRHBxBYoK7VjSUtJdIyFW/QS07gSpU4/SrTwaRqszwgB1P7J1qwAA8kuzIcU2tzy/JOwAgHbuKv482xl/dzgG69kxGHZqEYaduqs21bMwFEEsmQepVQcoK38Ezp2B/MxUAALKR9OsrpW/+gXIz1X3BPPwsp7xVQtI/YZBun0IJA9Pq7Aj9bkL8PGDdPMdpeslUY2qTT/T9Rnr2TFqQ9jhmB2iGiTpXSzbRshtOqvHZE35zUs1WrVVyMML0u2DAQDK+TMQ+7ZDfnQSlLdeKL/YIaCGjBbtIbb8bc+3AQAQq5dCrF6qzm4re7y49QeJ54AHJlhmkIncbIidGyF1vlndeJaIqJZg2CGyE0kuHfsi6XSQ354L5Y2n1IHBN/Ysd7084hFgxCPq189Nh/L+ZPVE87aQwhqqLSqBIeoKz5eHnbJr+fgFAhlpNfY+xPJFto9v/Qdi279AZAykDl0hTh0F9m2HOLJffS+pSUBUEygznweSzqkranfrDRzZDzRrXW6PMgAQ+XlAQZ667xoAoZiB3BxI3r419n6I6PrDsEPkIFJAMOQZn0Ps3Qrpxl5XvrZZG0gPPAmx7V/I4yapq0qXcHGFNPQhIOE0EBymDpIuLIAyfSIAQB75CMTuLWoQsTehAPEnIOJPlB7bvRnK7s3lL/3+c+DCWXVj1p59gVHjIbZvgBTXClLxatTKJ9OBk0cgT/8cUkgDiHkfQuz4D/IbH0MKv/LmrEIIKB9OBTQayE9NqbHFK4mo7mPYIXIgydcfUq9+lbpWvul24KbbbZ+zsUij9PBzgKKo+3516AY88jzE7i1QPn+r8uUb9yLEV+9U+vqqEutXq//+uxYoKlJbhyQZ8v/9pHbbFa93JDb8DtEwGmK7GtiUqU9CfvMzSGERap//6WMQaRfVWWSdeqg3z0pXV/cGgLMnYf7qXUjtbwQgqddkpQNtOgOSxCBEdJ1h2CGqJ2RbrUWtOwKNmwKnj0Ge+CaUn75Wd4aPjoN0Q091l3qhFN9AhtSsNcqOFJKnfw5lyuN2Ka/Y+k/xFwqUuXOAvaWLPYo/VpS7Xnn9CcgvzFQ3bzWrK1QLAHJcK8DTCygoKL127hwgNRnij5+t7id16wOxbzuke8dBvqG0K1H56xeIk0cgD3vYagPYCsuekwVkZUCKiFIfm4wQf/8KqXVnSGERVakG6/sqCsTW9ZBjmwNhYdd8HyKyxrBDVI9JOj00r7xreSy//iEg1DFEACBuuh3K528DB3dB6n2X1SatAIDgMHXPr+QLgCSVDpp2cQOKClApwQ2AlMQrX7N365XPF1Pee63cMbFvO8SKb4GcrNKDFy/YfH7JOkhi7vsQzdtC8vaFSE9VQx8AERgKDBoNSaeDOHYQytplkO+fYOlms5Rj2tNAVgbQMBry5DkQq5dBrPoRYukCyO8sgOR3bQsvii3/QCz8CGZZBlZtt37NeR9AZKVDfnYqJFkD5dtPIc6egvziW+o2LkRUIYYdouuIpNVZP9a7QH70BXW15tYd1W6hYWMhtv4DedhYSLIMecJkKD98Ac3AUfBMS0LW7z9Dfm46xP4dEAs/trqf/PQbUD4unWIv9bkL0sB7AQEoz45SD4ZHqmsMnT5WI+9JLPrkmp6nPP8A4OVjfa9Nf6mzzVp1tAQwZfE8aB5/WT2fcQnKl7PVoAOoaxSdPAqx47/S+744BmjVAZpnpkIoCnB0HxDTApKLi9oFpyiWhRtFZjpwMRFS01bqkw/sLL6J2tomcrJgfu81wGgAUpLUc6eOQTRuatnzTezbAalzj2uqA1tEXi6QnQEprGGN3ZPI2Rh2iK5zkrsH0KGr5bF8+2CgeDo8AEih4dA8Nx2SJMH75j7I69pH/dCOirPq8kJsc/W/4DAgPxfy6x9btXDIL8yCSD6vrs8jSRBnTkAc3gPx+3KgIN9m2eS3vla73vZtt3m+2sq2BgFAbrb6b9mWpt2bYZ72DODrDxzcVe4W4uAudVXrsg7uhrLmfxDL1T3KpN4DgP7DoXz4BpCSpO59FtscyqSHAADypFkQaRchdm2y3MKUdhHKXyvV1bLLUN55GYiMLX39r96BiIyBFBwG5b8/gMx0SANGlBuXJIqKgMJ8SD5+V6wSZc5kIOE05CkfQmrUuPT5hiLAaOSyAlQncVHBYlxUsO5iPTuGrXoWh/eo21bk5QIRkZAu7warJFGYD7FrC8SODaW7yjdtDfn5GeqYnvFDbJepS0+I/Bzg4O5rel0rGg1gNlf/PhWQuvau2vpIGg0QFQecOnL1a909IXW5CWL9GuvjDaMhPzcd0GihzJgIpF6E/Np7kMqEJeW3JRDHDkCeMBli53+W1jqp33DIQ+6zXGee8RyQdA7y2/MgeXmrPwPJF4CQMKtlFqpDJJyG2LwO0l0ja2zBSpFxCfDxtVlG/u5wDC4qSER1mtSifc3cx9UdUvc+QPc+6rT5jDTIfe4qPqmBNPJRiJ++BnwDgMxLAAD5ySmQ2qoLNypfvWvVlXRNZbjnIYgl86p1jyup8kKQZnPlgg6grsR9edABgHNn1G4uNzdLN5gy83nIH3ynrsvU7gaIn79Tjz85zLq8q5dACQ6D3L0PhNkMnD2pHt+7Feh8E8SBnepebK07QX7sJaCoCHBxqdT4IXE+HsqX70Bq2R7yyEfVRTcNRVDefw3IzwOyMyGNm1S5917RaxQVQuzaDLHgQ0g39IT0yPPlrzl+CEZDPqC/8oa4QgjO4Kvj2LJTjC07dRfr2TGcWc9CCOBSChAQDCSfh0g4DanLzZYPIHHhLJSpT6nlHPMspBZtIXZuglg8F1L3WyE2/VV6MzcPwD8Q8pQPoUx+DEi7CADQfL0S4uAuiCP71Ods3wBxeC9w5nj5Amm1QMsO6nMv62a6Eum2QRB//lLxBSWDwWuQNOIRiFU/qiGihKsbUFi5AebSgBFAYGiltzORn5sOsfEvdS2pVh3UPeEK8yE1igEACLMZymOlLXXynO+gfDRVHVReUiadHvKnS9WAFhQKSZatXkOYjKWrkNsg8nJLx4gV03y90vqa9DQoL421fW7nRjUkPjABMJmgTH8WUrM2kB94slJ1YHWvS6kQyxZA6tkXUrM21ufy84CTh4EW7SFp617bg8jOABLPlXtfl2PLDhFRJUiSpK4eDQBhDcsNnpXCI9WwUmbXdunWgRAt2gEh4ZC69Yby7quAjx/kdxYAEJBkDaS77rX85Q8AUquOkFp1VL8efB9Ev2FqS9KlVHXKu1Ag9b0H0pD7IMkamD+daTPsSANGQvz6k9Ux+aXZQESU7bCj0ULq3R/SkAcq3kz2GonFc8sfvFLQiYy1tOIAgPh1cZVeT5kzRX3e9n+BsIZA8nlACEgPPKmOK/rLOlgoz91X/iZGA8QfP6sh4e4HIN05FCLhFJRPZgLuHuoA8dgWkAKCIJLPQ54wuXSG4YUEy/pMtoj8PIj/LQRCyywRkJ0JlFmlW/myeK2p4DD19VKTIVKTIW4bBOTnQYppVvn6eOclID0N4nw8NNM/sz43933gwE51o16zGSL5AuRHn4dY8S2kxs0gdexW6de5YhnWLAMunFX/ENBo1JXJ92wDmrSo1urkyruvAcnnIU94DVK7G2qkrPbCsENE9UZJ0LE8blC86nJcKzVshDSwaiWQut6irszcwPbqzJLeRZ06H9wAmq9+LndeHjQaisFQupih5fgoYNAodTZW0nn1XiUrQHt4AXk5QGCIulL05X/R+/iVzvYCIL/6PsR/v0P898fVK8DdU11eICXJEjoswiOv3ArVtgvknncCLdpBmf5slVqsKpR0zvKlWPQJhJdP+UHhFRDLFqj/Ll8Ec9ktS0q2Qtm7tXSA/MnDQPO2EIf2qIPAbd3v8F6ISym2Z++lJALevuoA7zIta2LlD1aXKa9PAABIdwyBPHSMemzFtxBH9gGe3pDadYF00x2A2Qyx4EO1BS29uLzJ58u/bvHsO7GqNBgrj6urjwsUb8bb5WaInZsAX3/L4P7KEAmnIP5aCalDN8uWL1LPO4EmLdQg+b9vgJhm0LxsexFRIYQ6Fi4qFtJlsxYtit+TsvFPaK4QdkTyBaevG8VurGLsxqq7WM+OwXq+MnFkH5RPpkMa/gjknn0rvi4rA2Lz35C69bY5M0rZ+CewcxPEIXXQtfzRj4AkqdtsdOoB5dVxlmul+56A+E5tLZBGPw6pa29ILuqYGWXpfMuiikBJ9147KN/8n9WsMvm56YCnt9piVhy8RF4uxN6t5ZYWsPD0AnJzKlcxFanM+ktVIA0aDfHL99f25NAItRVn/45KP0V+42PAxw/Kc/dX/jlPTQGCG6jhJjisXOvfVZ8/4VVI7W6EyLwEaHQQG9ZCHNgJ+ek31FmVZZg/nQns3WZ1THrkeSA1CeKX0hAn/99iKF+/B6lFe8h9BliOK5v/VgNbdBw0r75nszzmRweqX0TGQjN5DsTZk4Cnj9XCnOJ8PJRpT8OldQeYnnwduKxLsjqq0o3FsFOMYafuYj07Buv56sp2o1WHJEnwPrIbmVnZli62EsqCj9QZS3c/AOnWQVDefFpt0XluutXmqiI1Gcprj1lWyJZffgdSTDOI08egvKUO/pVnfQUpKNT2e8nKgPLCg2p5hj4E8duS0iUC9HrAYLAu8429ILIzIXl4WQaLy89NBwryoXz3mXWLTquO6vpOFxMhdmy48jimWqzceDB7Cw0HgsLUFqEyG/5K/YdD6nwzYDZC/LUKcHWF+Gd1+fJ262NZWNMWqVsfILwRYDBYB8fGTSGPfBSIiAK0OnXpiMx0y9IJ0Ggh3fOgZYC//ORkIKoJIBSIXxdD/LsWbt37wDjmWaeN2WHYKcawU3exnh2D9ew4V6proShAbhYkb7/Sx0C5QbxA8eaos18CcnPUzVSLx7Uovy4GJAly/+FXLIc4cxzQu0AKj1Sft/wbiDX/UzepPbQb2L9TXfAQgPTAk5Bvut3qQ1D+8mdIsgzzs6PVrjuoLQmSq5vV65jfedmyL5r8yRL1r/8zx9VxVgAQFKoOdL6ciyukPgMhEs+Wa8WwF6nPXerCk9cjrRYICVf3matMy56LK6DTA7nZCJr+CTLCIjlAmYiIrk6SZcDbz/pxRddKEuQX30bJgOwS8oARlXut6Djrx4Pvg3TjLWqXV4/bAJMJuHQRuJAAFC8DIPn6q6/p6lZatqatgd2bgQaNygUdAJDvfxLKj19C7j/c0jolYpuXXuDuCfnV9yCOH7KM5QGgjsUqXgtImf8BxJbi/dYaRkO6426I3VsghUZArF5S+hxXN0hD7of48Su1vG7uEBFRlrBlpd2NVgtMSg8/B6lVh3JhR357LqDVqZvuuntCfngikJIMZVbxdPfKruEU20Idf1RbmUxVG8tVVKj+p9FA36ItkJFpt6JdDcMOEVE9dqUwVPV7aawHc+t06niXsjObAEhNWlg9loeNgWjQEFLJ2kmX3zcsAprnppd/rRJePpCi4yBFx0GER0L5YwVw5jjkoQ+VXv/g04CHF8TpY6VdeiVdgEPugzhxGGLdKnXQb6PGMK9eBmSlw6V5GxjHvQhx6iiUXxerA4wL89Xuw8ICKIf3ABoN5BlfWGYuya9/BOXNZ9TXHToGUkAwAFgP9o32gvzMVCAwGGLt/yA22eg+8vFXW0lK6mnSTODYQSgrf7QOPT5+kEeNV/exq4A0cpzabVR29l2TFpCatIQ4dRTSTbdDzH2/wufXBOmR59X1sEpWIi853u4GyK5uADLt+vpXwm6sYuzGqrtYz47BenYc1rVKnD0JZdVPkEc+Cqlk6YGSc4q5Wis3i4uJEOtXI/T+x5BqNFdYzyIxAdDpy41tEknnAS9vSJ7eV3+twgKIP1aoA5PDGqqLMF5KgdS6kzpLbNu/kEY8bLUCudi5EeLYQUjN2wItO0BycYGy4z91ociURGDXZqvXkN/8DFJYBIQQEBv/hFi/BvKwMVZr4IiUJCgznwdimpXuw3YZafTjajdpxx5AQFDxMgC/AO7ugJcPxIKPSmcUlvD0gvzYy5CatoZIOgdcOFs6fR+A5qtf0CA83Knr7DDsFGPYqbtYz47BenYc1rVjOLqeRU4WoHe1zJi75vsknIby6QzAxx/yXSPVdYzadK7ccwvzAb0LkHBaXV+oeVsoTwwFAEg33X7VhRNF0nkgMASSTgdhMqlddEC5KfHmWS+oC3LGNof25Xe4qCAREdH1oML1aqp6n0aN1T3KrmELC8m1eGuMqCalx+5+EGLjn5AG3nv154eVdlleadVn+dEX1OUSbh1U5TLaA8MOERFRHVOTe3XJd94D3FmzK3dLQaGQho2t0XtWR82NXCMiIiKqhRh2iIiIqF5j2CEiIqJ6jWGHiIiI6jWGHSIiIqrXGHaIiIioXmPYISIionqNYYeIiIjqNYYdIiIiqtdq5QrKa9euxapVq5CZmYnIyEiMHTsWsbGxFV6/ZcsWLF68GKmpqQgNDcXo0aPRoUMHB5aYiIiIaqta17KzefNmLFq0CEOHDsXs2bMRGRmJmTNnIisry+b1x44dw0cffYTevXtj9uzZ6Ny5M959910kJCQ4uORERERUG9W6sPPrr7+iT58+uOWWWxAREYFHH30Uer0e//zzj83rV69ejXbt2mHgwIGIiIjAyJEj0bhxY6xdu9bBJSciIqLaqFZ1Y5lMJpw+fRqDBw+2HJNlGa1bt8bx48dtPuf48eMYMGCA1bG2bdtix44dNq83Go0wGo2Wx5Ikwc3NzfJ1TSq5X03fl6yxnh2D9ew4rGvHYD07Rm2o51oVdrKzs6EoCnx9fa2O+/r6IjEx0eZzMjMz4ePjY3XMx8cHmZmZNq9fsWIFli1bZnkcHR2N2bNnIygoqFplv5LQ0FC73ZtKsZ4dg/XsOKxrx2A9O4Yz67lWhR1HGDJkiFVLUEnSzMjIgMlkqtHXkiQJgYGBSEtLgxCiRu9NpVjPjsF6dhzWtWOwnh3DXvWs1Wrh5+dXuWtr7FVrgLe3N2RZLtcqk5mZWa61p4Svr2+5wctZWVkVXq/T6aDT6codr2yFXYvAwEC73ZtKsZ4dg/XsOKxrx2A9O4Yz67lWDVDWarVo3LgxDh48aDmmKAoOHjyIuLg4m8+Ji4vDgQMHrI7t378fTZo0sWtZK6OgoAAvvfQSCgoKnF2Ueo317BisZ8dhXTsG69kxakM916qwAwADBgzAunXrsH79epw/fx5z585FUVERevXqBQD45JNP8MMPP1iu79evH/bt24dVq1bhwoULWLJkCU6dOoW+ffs66R2UEkLgzJkzbB61M9azY7CeHYd17RisZ8eoDfVcq7qxAKBbt27Izs7GkiVLkJmZiaioKLz66quWbqm0tDSrEd1NmzbF008/jZ9++gk//vgjwsLCMGnSJDRq1MhJ74CIiIhqk1oXdgCgb9++FbbMTJ06tdyxrl27omvXrnYuFREREdVFta4bqz7R6XQYOnSozQHRVHNYz47BenYc1rVjsJ4dozbUsyTYWUlERET1GFt2iIiIqF5j2CEiIqJ6jWGHiIiI6jWGHSIiIqrXauXU8/pg7dq1WLVqFTIzMxEZGYmxY8ciNjbW2cWqM1asWIHt27fjwoUL0Ov1iIuLw3333YcGDRpYrjEYDFi0aBE2b94Mo9GItm3b4pFHHrHaKiQtLQ1ff/01Dh06BFdXV/Ts2ROjRo2CRqNxwruq/X7++Wf88MMP6NevHx566CEArOeakp6eju+++w579+5FUVERQkND8cQTTyAmJgaAuvDakiVLsG7dOuTl5aFZs2Z45JFHEBYWZrlHbm4u5s+fj127dkGSJNxwww0YM2YMXF1dnfW2ah1FUbBkyRL8999/yMzMhL+/P3r27Il77rnHskYb67rqDh8+jJUrV+LMmTPIyMjACy+8gC5duljO11Sdnj17FvPmzcOpU6fg7e2Nvn37YtCgQdUuP1t27GDz5s1YtGgRhg4ditmzZyMyMhIzZ84st4cXVezw4cO44447MHPmTEyePBlmsxkzZsxAYWGh5ZpvvvkGu3btwnPPPYdp06YhIyMD77//vuW8oih46623YDKZMGPGDEyYMAHr16/H4sWLnfGWar2TJ0/izz//RGRkpNVx1nP15ebmYsqUKdBqtXj11VfxwQcf4IEHHoCHh4flml9++QVr1qzBo48+ilmzZsHFxQUzZ86EwWCwXPPxxx/j3LlzmDx5Ml5++WUcOXIEX375pTPeUq31888/488//8TDDz+MDz74AKNHj8bKlSuxZs0ayzWs66orKipCVFQUHn74YZvna6JO8/PzMWPGDAQGBuLtt9/Gfffdh6VLl+Kvv/6q/hsQVONeeeUVMXfuXMtjs9ksxo0bJ1asWOG8QtVxWVlZYtiwYeLQoUNCCCHy8vLEyJEjxZYtWyzXnD9/XgwbNkwcO3ZMCCHE7t27xfDhw0VGRoblmt9//1088MADwmg0OrT8tV1BQYF4+umnxb59+8Qbb7whFixYIIRgPdeU7777TkyZMqXC84qiiEcffVT88ssvlmN5eXli1KhRYuPGjUIIIc6dOyeGDRsmTp48ablmz549Yvjw4eLSpUv2K3wd89Zbb4nPPvvM6ti7774rPvroIyEE67omDBs2TGzbts3yuKbq9PfffxcPPfSQ1e+N7777TjzzzDPVLjNbdmqYyWTC6dOn0bp1a8sxWZbRunVrHD9+3Iklq9vy8/MBAJ6engCA06dPw2w2W9VzeHg4AgMDLfV8/PhxNGrUyKq7pV27digoKMC5c+ccV/g6YO7cuWjfvj3atGljdZz1XDN27tyJxo0bY86cOXjkkUfw4osvWv21mpKSgszMTKv6d3d3R2xsrFU9e3h4WLq9AKB169aQJAknT5503Jup5eLi4nDw4EEkJiYCAOLj43Hs2DG0b98eAOvaHmqqTo8fP47mzZtDqy0dYdO2bVskJiYiNze3WmXkmJ0alp2dDUVRrH7xA4Cvr6/lfz6qGkVRsHDhQjRt2tSy51lmZia0Wq1VNwAA+Pj4IDMz03LN5d8HHx8fyzlSbdq0CWfOnMFbb71V7hzruWakpKTgzz//RP/+/TFkyBCcOnUKCxYsgFarRa9evSz1VFJvJS6vZ29vb6vzGo0Gnp6erOcyBg8ejIKCAkycOBGyLENRFIwcORI33XQTALCu7aCm6jQzMxPBwcFW15T8bsnMzLT8sXstGHao1ps3bx7OnTuHN99809lFqXfS0tKwcOFCTJ48GXq93tnFqbcURUFMTAxGjRoFAIiOjkZCQgL+/PNP9OrVy7mFq2e2bNmCjRs34umnn0bDhg0RHx+PhQsXws/Pj3V9HWPYqWHe3t6QZblc+rf11y9d3bx587B7925MmzYNAQEBluO+vr4wmUzIy8uzanXIysqy1LOvr2+5JueSQeL8XqhOnz6NrKwsvPTSS5ZjiqLgyJEjWLt2LV577TXWcw3w8/NDRESE1bGIiAhs27YNQGk9ZWVlwc/Pz3JNVlYWoqKiLNdkZ2db3cNsNiM3N5f1XMZ3332HQYMGoXv37gCARo0aITU1FT///DN69erFuraDmqpTX19fm5+dZV/jWnHMTg3TarVo3LgxDh48aDmmKAoOHjyIuLg4J5asbhFCYN68edi+fTtef/31ck2bjRs3hkajwYEDByzHEhMTkZaWZqnnuLg4JCQkWM2C279/P9zc3Mp98FyvWrdujffeew/vvPOO5b+YmBj06NHD8jXrufqaNm1arhs7MTERQUFBAIDg4GD4+vpa1XN+fj5OnjxpVc95eXk4ffq05ZqDBw9CCMFlLcooKiqCLFt/tMmyDFG8DSTruubVVJ3GxcXhyJEjMJlMlmv279+PBg0aVKsLC2DLjl0MGDAAn376KRo3bozY2FisXr0aRUVFbEKtgnnz5mHjxo148cUX4ebmZkn37u7u0Ov1cHd3R+/evbFo0SJ4enrC3d0d8+fPR1xcnOV/rrZt2yIiIgKffPIJRo8ejczMTPz000+44447uMtxMTc3N8s4qBIuLi7w8vKyHGc9V1///v0xZcoULF++HN26dcPJkyexbt06jBs3DgAgSRL69euH5cuXIywsDMHBwfjpp5/g5+eHzp07A1Bbgtq1a4cvv/wSjz76KEwmE+bPn49u3brB39/fmW+vVunYsSOWL1+OwMBAREREID4+Hr/++ituueUWAKzra1VYWIjk5GTL45SUFMTHx8PT0xOBgYE1Uqc9evTA0qVL8cUXX2DQoEE4d+4c1qxZgwcffLDa5eeu53aydu1arFy5EpmZmYiKisKYMWPQpEkTZxerzhg+fLjN40888YQlNJYsdrdp0yaYTCabi92lpqZi7ty5OHToEFxcXNCzZ0+MHj2ai91dwdSpUxEVFVVuUUHWc/Xs2rULP/zwA5KTkxEcHIz+/fvj1ltvtZwXxYuy/fXXX8jPz0ezZs3w8MMPWy2kmZubi3nz5lktyjZ27NjrdqE7WwoKCrB48WJs374dWVlZ8Pf3R/fu3TF06FDLLB/WddUdOnQI06ZNK3e8Z8+emDBhQo3VadlFBb28vNC3b18MHjy42uVn2CEiIqJ6jWN2iIiIqF5j2CEiIqJ6jWGHiIiI6jWGHSIiIqrXGHaIiIioXmPYISIionqNYYeIiIjqNYYdIrourV+/HsOHD8epU6ecXRQisjNuF0FEdrF+/Xp89tlnFZ6fMWNGvdovbseOHXj//fexcOFCuLq6YsGCBTh79iymTp3q7KIRXfcYdojIroYPH15uI1cACA0NdUJp7OfEiRNo1KiRZen748ePo1WrVk4uFREBDDtEZGft27dHTEyMs4thd6dOnbLsf2cwGBAfH48hQ4Y4uVREBDDsEJGTpaSk4Mknn8R9990HWZaxevVqZGVlITY2Fg8//HC5XdkPHjyIJUuW4MyZM9BoNGjRogVGjRqFiIgIq+vS09OxePFi7N27Fzk5OfDz80O7du0wZswYy4aQAGA0GvHNN99gw4YNMBgMaNOmDcaPHw9vb++rlj07O9vy9alTp9CpUydkZ2fj1KlTMJvNCAkJQXZ2NlxcXODi4lLNmiKia8WNQInILkrG7EyZMgWRkZFW5yRJgpeXF4DSsNOoUSMUFBTg9ttvh9FoxOrVqyHLMt577z3LDuv79+/HW2+9heDgYPTp0wcGgwFr1qyBoiiYPXu2pbssPT0dr7zyCvLz89GnTx+Eh4cjPT0dW7duxYwZM+Dh4WEpX3R0NDw8PNClSxekpKRg9erVuOGGGzBx4sSrvsfhw4dXqi6GDh1a6WuJqOaxZYeI7Gr69Onljul0Onz//fdWx5KTk/Hxxx/D398fANCuXTu8+uqr+OWXX/Dggw8CAL777jt4enpi5syZ8PT0BAB07twZL774IpYsWYInn3wSAPDDDz8gMzMTs2bNsupCGzFiBC7/+87T0xOTJ0+GJEkAACEE1qxZg/z8fLi7u1/xvU2ePBkAsHXrVuzYsQNPPfUUAOD777+Hn58f+vXrBwAICQmpRE0Rkb0w7BCRXT388MMICwuzOibL5Ve96Ny5syXoAEBsbCyaNGmCPXv24MEHH0RGRgbi4+MxcOBAS9ABgMjISLRp0wZ79uwBACiKgh07dqBjx442xwqVhJoSt956q9Wx5s2b47fffkNqamq5FqnLtWnTBgDwxx9/oFWrVmjTpg0URUFycjLuvPNOy3kici6GHSKyq9jY2EoNUL48EJUc27JlCwAgNTUVANCgQYNy14WHh2Pfvn0oLCxEYWEhCgoKyo31qUhgYKDVYw8PDwBAXl7eFZ+Xm5sLRVEAAIcPH8bdd9+N7OxsJCQkWF4/Ozsber3eMkOLiJyDYYeIrmu2WpkAlOvuutxLL71kCWAAsGjRIixatMjy+OWXXwYA9OzZExMmTKiBkhLRtWLYIaJaISkpyeaxoKAgALD8m5iYWO66xMREeHl5wdXVFXq9Hm5ubkhISLBreZ966ikYDAbs2LEDW7ZswdNPPw0A+Omnn+Dl5YX+/fsDgFXXHBE5B7eLIKJaYceOHUhPT7c8PnnyJE6cOIF27doBAPz8/BAVFYV///3XqospISEB+/btQ/v27QGoLTWdO3fGrl27bG4FUVMTUJs1a4Y2bdqgoKAAcXFxaNOmDdq0aYO0tDR07NjR8vjyKfFE5Hhs2SEiu9qzZw8uXLhQ7njTpk2tZimFhoZiypQpVlPPvby8MGjQIMs19913H9566y1MnjwZt9xyCwwGA9auXQt3d3erqd2jRo3C/v37MXXqVPTp0wcRERHIyMjA1q1b8eabb1rG5dSEY8eO4dZbbwUAXLx4EZmZmWjatGmN3Z+Iqo9hh4jsasmSJTaPP/HEE1Zh5+abb4Ysy/jtt9+QnZ2N2NhYjB07Fn5+fpZr2rRpg1dffRVLlizBkiVLLIsKjh492mpLCn9/f8yaNQs//fQTNm7ciIKCAvj7+6Ndu3Y1urhfZmYmLl68aAk3x48fh5ubGxo2bFhjr0FE1cdFBYnIqcquoDxw4EBnF4eI6iGO2SEiIqJ6jWGHiIiI6jWGHSIiIqrXOGaHiIiI6jW27BAREVG9xrBDRERE9RrDDhEREdVrDDtERERUrzHsEBERUb3GsENERET1GsMOERER1WsMO0RERFSvMewQERFRvfb/jAUz/+6iCt8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number set as 256\n",
        "nb_epochs=1000;    # training epochs change to 1000\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = random.choice([condition1, condition2, condition3, condition4])    # randomly choose 1 lighting condition for test set\n",
        "train_conditions = random.choice([c for c in [condition1, condition2, condition3, condition4] if c is not val_condition])   # randomly choose 1 left condition for training\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_conditions.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=5e-6))    # Learning rate changes 1e-5 to 3e-6\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "fDPx2kPSow29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vqXNWwoTkqf"
      },
      "source": [
        "##3.3 Train on data under 3 light and test on 1 light condition ##"
      ],
      "id": "4vqXNWwoTkqf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uHGpIjBAGC8"
      },
      "source": [
        "###3.3.1 Randomly Choose###\n",
        "Randomly choose one lighting condition for testing and the left 3 conditions will be conditions for training.\n",
        "\n",
        "--> 4 fold cross validation...\n",
        "\n",
        "loss: 0.0301 - val_loss: 0.3302  \n",
        "Significantly higher than naive prediction(0.11)"
      ],
      "id": "9uHGpIjBAGC8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "me4W9XdNTpUM",
        "outputId": "fffdc09c-c58b-4c95-d784-85334aed2876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 4s 9ms/step - loss: 2.0647 - val_loss: 2.1753\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.9710 - val_loss: 2.1016\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.8817 - val_loss: 2.0255\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.7821 - val_loss: 1.9488\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 1.6925 - val_loss: 1.8702\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.5877 - val_loss: 1.7930\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.4994 - val_loss: 1.7141\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.4214 - val_loss: 1.6359\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 6ms/step - loss: 1.3281 - val_loss: 1.5540\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.2553 - val_loss: 1.4765\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.1698 - val_loss: 1.3962\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.1103 - val_loss: 1.3198\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.0375 - val_loss: 1.2456\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.9750 - val_loss: 1.1732\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.9239 - val_loss: 1.1058\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.8627 - val_loss: 1.0402\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.8231 - val_loss: 0.9802\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7770 - val_loss: 0.9215\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7326 - val_loss: 0.8705\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6924 - val_loss: 0.8231\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6651 - val_loss: 0.7798\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6380 - val_loss: 0.7423\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6062 - val_loss: 0.7079\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5967 - val_loss: 0.6798\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5736 - val_loss: 0.6541\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5591 - val_loss: 0.6315\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5452 - val_loss: 0.6111\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.5296 - val_loss: 0.5939\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5330 - val_loss: 0.5791\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5204 - val_loss: 0.5647\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4957 - val_loss: 0.5535\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5068 - val_loss: 0.5446\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4917 - val_loss: 0.5363\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4787 - val_loss: 0.5309\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4910 - val_loss: 0.5252\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4766 - val_loss: 0.5195\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4863 - val_loss: 0.5174\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4621 - val_loss: 0.5122\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4732 - val_loss: 0.5082\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4622 - val_loss: 0.5057\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4586 - val_loss: 0.5024\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4550 - val_loss: 0.5000\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4513 - val_loss: 0.4983\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4392 - val_loss: 0.4966\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4598 - val_loss: 0.4966\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4431 - val_loss: 0.4941\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4457 - val_loss: 0.4941\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4287 - val_loss: 0.4943\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4320 - val_loss: 0.4938\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4306 - val_loss: 0.4904\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4138 - val_loss: 0.4900\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4249 - val_loss: 0.4888\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4279 - val_loss: 0.4858\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4276 - val_loss: 0.4859\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4170 - val_loss: 0.4862\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4229 - val_loss: 0.4856\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4154 - val_loss: 0.4846\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4239 - val_loss: 0.4843\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4087 - val_loss: 0.4842\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4133 - val_loss: 0.4839\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4124 - val_loss: 0.4829\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4192 - val_loss: 0.4837\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4268 - val_loss: 0.4816\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4079 - val_loss: 0.4814\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4127 - val_loss: 0.4830\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4136 - val_loss: 0.4822\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4071 - val_loss: 0.4817\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4210 - val_loss: 0.4810\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4160 - val_loss: 0.4822\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4013 - val_loss: 0.4817\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3985 - val_loss: 0.4816\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3997 - val_loss: 0.4826\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4113 - val_loss: 0.4820\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3950 - val_loss: 0.4798\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3919 - val_loss: 0.4807\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3877 - val_loss: 0.4787\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3856 - val_loss: 0.4807\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3871 - val_loss: 0.4811\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4084 - val_loss: 0.4813\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3806 - val_loss: 0.4811\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4008 - val_loss: 0.4800\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3908 - val_loss: 0.4783\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3889 - val_loss: 0.4803\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3914 - val_loss: 0.4788\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3994 - val_loss: 0.4771\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3927 - val_loss: 0.4770\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3965 - val_loss: 0.4760\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3885 - val_loss: 0.4754\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3889 - val_loss: 0.4756\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3912 - val_loss: 0.4761\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3885 - val_loss: 0.4764\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3868 - val_loss: 0.4738\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3805 - val_loss: 0.4742\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3874 - val_loss: 0.4748\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3840 - val_loss: 0.4745\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3827 - val_loss: 0.4731\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3890 - val_loss: 0.4730\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3700 - val_loss: 0.4723\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3736 - val_loss: 0.4737\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3773 - val_loss: 0.4733\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3795 - val_loss: 0.4731\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3721 - val_loss: 0.4732\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3693 - val_loss: 0.4732\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3755 - val_loss: 0.4714\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3851 - val_loss: 0.4724\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3657 - val_loss: 0.4710\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3692 - val_loss: 0.4716\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3763 - val_loss: 0.4725\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3869 - val_loss: 0.4696\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3721 - val_loss: 0.4700\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3660 - val_loss: 0.4701\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3664 - val_loss: 0.4712\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3662 - val_loss: 0.4699\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3640 - val_loss: 0.4702\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3763 - val_loss: 0.4694\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3657 - val_loss: 0.4687\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3699 - val_loss: 0.4703\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3568 - val_loss: 0.4704\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3700 - val_loss: 0.4693\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3664 - val_loss: 0.4687\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3792 - val_loss: 0.4665\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3583 - val_loss: 0.4667\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3675 - val_loss: 0.4640\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3572 - val_loss: 0.4657\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3552 - val_loss: 0.4620\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3653 - val_loss: 0.4614\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3566 - val_loss: 0.4616\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3675 - val_loss: 0.4615\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3642 - val_loss: 0.4610\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3588 - val_loss: 0.4632\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3600 - val_loss: 0.4618\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3621 - val_loss: 0.4624\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3523 - val_loss: 0.4603\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3582 - val_loss: 0.4610\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3664 - val_loss: 0.4590\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3604 - val_loss: 0.4605\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3561 - val_loss: 0.4584\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3628 - val_loss: 0.4609\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3663 - val_loss: 0.4618\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3655 - val_loss: 0.4600\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3621 - val_loss: 0.4624\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3627 - val_loss: 0.4612\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3534 - val_loss: 0.4601\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3571 - val_loss: 0.4568\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3667 - val_loss: 0.4560\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3503 - val_loss: 0.4565\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3451 - val_loss: 0.4571\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3571 - val_loss: 0.4552\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3533 - val_loss: 0.4561\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3558 - val_loss: 0.4541\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3537 - val_loss: 0.4537\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3551 - val_loss: 0.4534\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3449 - val_loss: 0.4532\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3505 - val_loss: 0.4532\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3439 - val_loss: 0.4525\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3418 - val_loss: 0.4523\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3446 - val_loss: 0.4542\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3426 - val_loss: 0.4548\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3422 - val_loss: 0.4564\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3488 - val_loss: 0.4547\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3445 - val_loss: 0.4559\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3416 - val_loss: 0.4554\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3424 - val_loss: 0.4517\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3406 - val_loss: 0.4515\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3476 - val_loss: 0.4527\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3582 - val_loss: 0.4498\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3407 - val_loss: 0.4493\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3469 - val_loss: 0.4489\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3435 - val_loss: 0.4481\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3413 - val_loss: 0.4470\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3425 - val_loss: 0.4462\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3484 - val_loss: 0.4461\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3507 - val_loss: 0.4470\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3352 - val_loss: 0.4486\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3306 - val_loss: 0.4484\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3351 - val_loss: 0.4458\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3392 - val_loss: 0.4448\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3307 - val_loss: 0.4450\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3398 - val_loss: 0.4443\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3410 - val_loss: 0.4427\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3321 - val_loss: 0.4432\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3404 - val_loss: 0.4412\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3332 - val_loss: 0.4413\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3332 - val_loss: 0.4439\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3288 - val_loss: 0.4449\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3422 - val_loss: 0.4421\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3367 - val_loss: 0.4437\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3292 - val_loss: 0.4421\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3359 - val_loss: 0.4421\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3246 - val_loss: 0.4417\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3346 - val_loss: 0.4430\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3310 - val_loss: 0.4417\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3254 - val_loss: 0.4430\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3304 - val_loss: 0.4419\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3224 - val_loss: 0.4434\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3413 - val_loss: 0.4408\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3276 - val_loss: 0.4388\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3356 - val_loss: 0.4394\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3356 - val_loss: 0.4388\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3370 - val_loss: 0.4378\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3406 - val_loss: 0.4339\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3400 - val_loss: 0.4356\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3271 - val_loss: 0.4350\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3203 - val_loss: 0.4348\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3366 - val_loss: 0.4342\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3302 - val_loss: 0.4335\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3307 - val_loss: 0.4352\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3300 - val_loss: 0.4353\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3401 - val_loss: 0.4355\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3344 - val_loss: 0.4328\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3244 - val_loss: 0.4352\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3336 - val_loss: 0.4353\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3330 - val_loss: 0.4335\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3213 - val_loss: 0.4326\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3234 - val_loss: 0.4317\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3249 - val_loss: 0.4323\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3281 - val_loss: 0.4324\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3260 - val_loss: 0.4311\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3271 - val_loss: 0.4297\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3295 - val_loss: 0.4287\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3317 - val_loss: 0.4299\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3293 - val_loss: 0.4284\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3196 - val_loss: 0.4286\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3291 - val_loss: 0.4286\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3166 - val_loss: 0.4273\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3265 - val_loss: 0.4265\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3244 - val_loss: 0.4263\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3224 - val_loss: 0.4290\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3310 - val_loss: 0.4277\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3317 - val_loss: 0.4257\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3249 - val_loss: 0.4239\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3235 - val_loss: 0.4242\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3256 - val_loss: 0.4251\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3248 - val_loss: 0.4234\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3148 - val_loss: 0.4247\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3204 - val_loss: 0.4253\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3128 - val_loss: 0.4260\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3241 - val_loss: 0.4248\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3128 - val_loss: 0.4248\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3172 - val_loss: 0.4249\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3348 - val_loss: 0.4255\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3217 - val_loss: 0.4215\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3186 - val_loss: 0.4214\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3205 - val_loss: 0.4215\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3202 - val_loss: 0.4176\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3154 - val_loss: 0.4181\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3111 - val_loss: 0.4166\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3075 - val_loss: 0.4192\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3177 - val_loss: 0.4193\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3167 - val_loss: 0.4177\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3170 - val_loss: 0.4177\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3147 - val_loss: 0.4189\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3255 - val_loss: 0.4173\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3089 - val_loss: 0.4177\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3200 - val_loss: 0.4144\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3156 - val_loss: 0.4129\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3038 - val_loss: 0.4122\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3092 - val_loss: 0.4115\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3161 - val_loss: 0.4137\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3113 - val_loss: 0.4126\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3091 - val_loss: 0.4134\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3191 - val_loss: 0.4109\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3015 - val_loss: 0.4113\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3106 - val_loss: 0.4101\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3051 - val_loss: 0.4093\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3214 - val_loss: 0.4102\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3015 - val_loss: 0.4098\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3145 - val_loss: 0.4102\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3082 - val_loss: 0.4119\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3117 - val_loss: 0.4126\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3139 - val_loss: 0.4099\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3132 - val_loss: 0.4115\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3234 - val_loss: 0.4113\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3149 - val_loss: 0.4064\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3106 - val_loss: 0.4059\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3090 - val_loss: 0.4046\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2986 - val_loss: 0.4061\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3080 - val_loss: 0.4067\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3030 - val_loss: 0.4078\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3119 - val_loss: 0.4065\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3068 - val_loss: 0.4045\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3146 - val_loss: 0.4048\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3133 - val_loss: 0.4051\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3092 - val_loss: 0.4047\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2958 - val_loss: 0.4035\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3014 - val_loss: 0.4040\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3118 - val_loss: 0.4020\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3040 - val_loss: 0.4019\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3140 - val_loss: 0.4020\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2978 - val_loss: 0.4031\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2975 - val_loss: 0.4041\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3116 - val_loss: 0.4036\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3028 - val_loss: 0.3997\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3080 - val_loss: 0.4001\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2901 - val_loss: 0.3995\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3034 - val_loss: 0.3973\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3023 - val_loss: 0.3984\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3019 - val_loss: 0.3980\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3012 - val_loss: 0.3978\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2962 - val_loss: 0.3987\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2945 - val_loss: 0.3977\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2982 - val_loss: 0.3984\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3006 - val_loss: 0.3985\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2911 - val_loss: 0.3990\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2994 - val_loss: 0.3962\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2979 - val_loss: 0.3967\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3080 - val_loss: 0.3970\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3090 - val_loss: 0.3961\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2980 - val_loss: 0.3956\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2941 - val_loss: 0.3941\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2995 - val_loss: 0.3944\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3058 - val_loss: 0.3938\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2973 - val_loss: 0.3957\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2923 - val_loss: 0.3952\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3023 - val_loss: 0.3929\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3032 - val_loss: 0.3910\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2976 - val_loss: 0.3934\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3015 - val_loss: 0.3931\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3034 - val_loss: 0.3892\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3078 - val_loss: 0.3907\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3047 - val_loss: 0.3891\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2941 - val_loss: 0.3902\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2925 - val_loss: 0.3912\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2974 - val_loss: 0.3902\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2888 - val_loss: 0.3908\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2955 - val_loss: 0.3905\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3072 - val_loss: 0.3906\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2965 - val_loss: 0.3902\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2946 - val_loss: 0.3887\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2976 - val_loss: 0.3871\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3175 - val_loss: 0.3839\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2894 - val_loss: 0.3867\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2887 - val_loss: 0.3852\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2875 - val_loss: 0.3850\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2981 - val_loss: 0.3855\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2916 - val_loss: 0.3862\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2817 - val_loss: 0.3849\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2941 - val_loss: 0.3839\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2968 - val_loss: 0.3819\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2979 - val_loss: 0.3831\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3064 - val_loss: 0.3836\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3024 - val_loss: 0.3800\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2874 - val_loss: 0.3807\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2822 - val_loss: 0.3807\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2947 - val_loss: 0.3800\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2902 - val_loss: 0.3787\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2850 - val_loss: 0.3786\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2779 - val_loss: 0.3775\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2864 - val_loss: 0.3780\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2844 - val_loss: 0.3785\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2790 - val_loss: 0.3761\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2933 - val_loss: 0.3777\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2937 - val_loss: 0.3766\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2867 - val_loss: 0.3758\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2842 - val_loss: 0.3761\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2976 - val_loss: 0.3747\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2847 - val_loss: 0.3739\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2864 - val_loss: 0.3741\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2920 - val_loss: 0.3701\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2796 - val_loss: 0.3735\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2821 - val_loss: 0.3739\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2975 - val_loss: 0.3729\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2855 - val_loss: 0.3763\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2809 - val_loss: 0.3755\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2798 - val_loss: 0.3741\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2878 - val_loss: 0.3713\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2909 - val_loss: 0.3736\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2830 - val_loss: 0.3723\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2875 - val_loss: 0.3721\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2850 - val_loss: 0.3687\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2835 - val_loss: 0.3700\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2796 - val_loss: 0.3683\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2766 - val_loss: 0.3701\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2891 - val_loss: 0.3670\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2861 - val_loss: 0.3647\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2867 - val_loss: 0.3670\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2720 - val_loss: 0.3681\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2831 - val_loss: 0.3685\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2810 - val_loss: 0.3695\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2828 - val_loss: 0.3685\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2871 - val_loss: 0.3668\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2837 - val_loss: 0.3664\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2697 - val_loss: 0.3660\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2757 - val_loss: 0.3635\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2798 - val_loss: 0.3650\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2767 - val_loss: 0.3640\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2789 - val_loss: 0.3644\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2801 - val_loss: 0.3637\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2862 - val_loss: 0.3641\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2904 - val_loss: 0.3637\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2822 - val_loss: 0.3625\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2739 - val_loss: 0.3640\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2920 - val_loss: 0.3656\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2960 - val_loss: 0.3639\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2839 - val_loss: 0.3620\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2883 - val_loss: 0.3618\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2884 - val_loss: 0.3610\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2813 - val_loss: 0.3607\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2874 - val_loss: 0.3595\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2758 - val_loss: 0.3596\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2789 - val_loss: 0.3575\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2872 - val_loss: 0.3572\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2773 - val_loss: 0.3583\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2816 - val_loss: 0.3577\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2833 - val_loss: 0.3576\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2726 - val_loss: 0.3566\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2683 - val_loss: 0.3579\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2787 - val_loss: 0.3574\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2743 - val_loss: 0.3571\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2798 - val_loss: 0.3558\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2823 - val_loss: 0.3559\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2797 - val_loss: 0.3552\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2710 - val_loss: 0.3536\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2838 - val_loss: 0.3566\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2761 - val_loss: 0.3562\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2766 - val_loss: 0.3556\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2766 - val_loss: 0.3526\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2750 - val_loss: 0.3523\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2802 - val_loss: 0.3550\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2792 - val_loss: 0.3537\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2795 - val_loss: 0.3550\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2695 - val_loss: 0.3561\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.2746 - val_loss: 0.3545\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2577 - val_loss: 0.3578\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2703 - val_loss: 0.3555\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2881 - val_loss: 0.3532\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2767 - val_loss: 0.3531\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2679 - val_loss: 0.3525\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2713 - val_loss: 0.3527\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2641 - val_loss: 0.3556\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2680 - val_loss: 0.3526\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2737 - val_loss: 0.3536\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2772 - val_loss: 0.3534\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2738 - val_loss: 0.3535\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2771 - val_loss: 0.3531\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2597 - val_loss: 0.3533\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2564 - val_loss: 0.3524\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2644 - val_loss: 0.3536\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2723 - val_loss: 0.3531\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2676 - val_loss: 0.3530\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2651 - val_loss: 0.3529\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2646 - val_loss: 0.3546\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2665 - val_loss: 0.3509\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2591 - val_loss: 0.3512\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2648 - val_loss: 0.3510\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2736 - val_loss: 0.3524\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2774 - val_loss: 0.3500\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2714 - val_loss: 0.3505\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2612 - val_loss: 0.3488\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2741 - val_loss: 0.3472\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2607 - val_loss: 0.3471\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2562 - val_loss: 0.3475\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2671 - val_loss: 0.3468\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2722 - val_loss: 0.3484\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2654 - val_loss: 0.3457\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2488 - val_loss: 0.3455\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2638 - val_loss: 0.3445\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2611 - val_loss: 0.3442\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2705 - val_loss: 0.3440\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2595 - val_loss: 0.3455\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2734 - val_loss: 0.3467\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2640 - val_loss: 0.3464\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2562 - val_loss: 0.3448\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2591 - val_loss: 0.3465\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2678 - val_loss: 0.3462\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2681 - val_loss: 0.3448\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.3444\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2639 - val_loss: 0.3424\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2579 - val_loss: 0.3440\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2592 - val_loss: 0.3434\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2661 - val_loss: 0.3442\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2585 - val_loss: 0.3437\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2630 - val_loss: 0.3434\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2682 - val_loss: 0.3433\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2658 - val_loss: 0.3441\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2606 - val_loss: 0.3452\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2657 - val_loss: 0.3436\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2621 - val_loss: 0.3447\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2567 - val_loss: 0.3418\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2654 - val_loss: 0.3435\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2616 - val_loss: 0.3444\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2621 - val_loss: 0.3433\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2592 - val_loss: 0.3415\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2584 - val_loss: 0.3403\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2596 - val_loss: 0.3408\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2647 - val_loss: 0.3412\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2613 - val_loss: 0.3406\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2669 - val_loss: 0.3410\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2616 - val_loss: 0.3405\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2600 - val_loss: 0.3400\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2535 - val_loss: 0.3393\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2604 - val_loss: 0.3383\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2530 - val_loss: 0.3358\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2506 - val_loss: 0.3378\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2600 - val_loss: 0.3391\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2427 - val_loss: 0.3384\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2632 - val_loss: 0.3397\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2486 - val_loss: 0.3415\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2423 - val_loss: 0.3406\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2659 - val_loss: 0.3405\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2608 - val_loss: 0.3390\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2562 - val_loss: 0.3379\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2592 - val_loss: 0.3397\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2614 - val_loss: 0.3400\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2687 - val_loss: 0.3380\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2602 - val_loss: 0.3362\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2601 - val_loss: 0.3353\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2543 - val_loss: 0.3347\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2537 - val_loss: 0.3348\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2559 - val_loss: 0.3365\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2563 - val_loss: 0.3349\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2487 - val_loss: 0.3344\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2509 - val_loss: 0.3353\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2537 - val_loss: 0.3345\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2539 - val_loss: 0.3344\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2505 - val_loss: 0.3327\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2642 - val_loss: 0.3297\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2543 - val_loss: 0.3311\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2580 - val_loss: 0.3313\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2419 - val_loss: 0.3355\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2461 - val_loss: 0.3337\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2622 - val_loss: 0.3326\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2524 - val_loss: 0.3319\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2561 - val_loss: 0.3306\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2491 - val_loss: 0.3319\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2526 - val_loss: 0.3316\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2574 - val_loss: 0.3329\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2561 - val_loss: 0.3350\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2599 - val_loss: 0.3318\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2508 - val_loss: 0.3326\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2573 - val_loss: 0.3327\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2513 - val_loss: 0.3327\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2472 - val_loss: 0.3342\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2524 - val_loss: 0.3345\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2550 - val_loss: 0.3348\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2534 - val_loss: 0.3320\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2569 - val_loss: 0.3324\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2511 - val_loss: 0.3350\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2442 - val_loss: 0.3337\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2591 - val_loss: 0.3347\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2459 - val_loss: 0.3340\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2446 - val_loss: 0.3341\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2401 - val_loss: 0.3325\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2557 - val_loss: 0.3330\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2571 - val_loss: 0.3323\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2630 - val_loss: 0.3323\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2583 - val_loss: 0.3326\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2379 - val_loss: 0.3325\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2576 - val_loss: 0.3331\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2493 - val_loss: 0.3340\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2387 - val_loss: 0.3328\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2497 - val_loss: 0.3338\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2510 - val_loss: 0.3346\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2514 - val_loss: 0.3327\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.3335\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2554 - val_loss: 0.3304\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2521 - val_loss: 0.3333\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2497 - val_loss: 0.3332\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2442 - val_loss: 0.3306\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2423 - val_loss: 0.3336\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2453 - val_loss: 0.3331\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2493 - val_loss: 0.3328\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2369 - val_loss: 0.3341\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2445 - val_loss: 0.3352\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2462 - val_loss: 0.3339\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2422 - val_loss: 0.3356\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2540 - val_loss: 0.3312\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2363 - val_loss: 0.3314\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2417 - val_loss: 0.3306\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2496 - val_loss: 0.3313\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2457 - val_loss: 0.3320\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2423 - val_loss: 0.3310\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2444 - val_loss: 0.3315\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2475 - val_loss: 0.3333\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.3311\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2602 - val_loss: 0.3325\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2549 - val_loss: 0.3338\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2461 - val_loss: 0.3323\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2442 - val_loss: 0.3331\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2403 - val_loss: 0.3316\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2393 - val_loss: 0.3327\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2580 - val_loss: 0.3319\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2374 - val_loss: 0.3323\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2423 - val_loss: 0.3301\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2526 - val_loss: 0.3315\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2415 - val_loss: 0.3294\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2369 - val_loss: 0.3293\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2482 - val_loss: 0.3289\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2370 - val_loss: 0.3314\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2390 - val_loss: 0.3313\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2429 - val_loss: 0.3324\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2472 - val_loss: 0.3333\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2431 - val_loss: 0.3318\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2375 - val_loss: 0.3321\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2351 - val_loss: 0.3309\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2378 - val_loss: 0.3333\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2467 - val_loss: 0.3319\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2453 - val_loss: 0.3311\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2409 - val_loss: 0.3305\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2420 - val_loss: 0.3303\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2423 - val_loss: 0.3297\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2462 - val_loss: 0.3305\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2298 - val_loss: 0.3310\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2477 - val_loss: 0.3293\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2385 - val_loss: 0.3261\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.3275\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2451 - val_loss: 0.3278\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2482 - val_loss: 0.3268\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2326 - val_loss: 0.3271\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2432 - val_loss: 0.3274\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2412 - val_loss: 0.3277\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2331 - val_loss: 0.3292\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2425 - val_loss: 0.3271\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2380 - val_loss: 0.3260\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2276 - val_loss: 0.3272\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2380 - val_loss: 0.3279\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2321 - val_loss: 0.3286\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2455 - val_loss: 0.3290\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2467 - val_loss: 0.3280\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2525 - val_loss: 0.3278\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2485 - val_loss: 0.3311\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2419 - val_loss: 0.3318\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2331 - val_loss: 0.3313\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2313 - val_loss: 0.3290\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2426 - val_loss: 0.3267\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2416 - val_loss: 0.3293\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2444 - val_loss: 0.3305\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2363 - val_loss: 0.3295\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2413 - val_loss: 0.3288\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.2391 - val_loss: 0.3265\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2446 - val_loss: 0.3283\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2310 - val_loss: 0.3313\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2467 - val_loss: 0.3309\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.2362 - val_loss: 0.3294\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.2408 - val_loss: 0.3304\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.2295 - val_loss: 0.3308\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.2365 - val_loss: 0.3338\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.2340 - val_loss: 0.3325\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.2306 - val_loss: 0.3310\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.2419 - val_loss: 0.3322\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.2313 - val_loss: 0.3306\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.2345 - val_loss: 0.3297\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2307 - val_loss: 0.3298\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2449 - val_loss: 0.3262\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2382 - val_loss: 0.3289\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2397 - val_loss: 0.3294\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2448 - val_loss: 0.3305\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2202 - val_loss: 0.3304\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2360 - val_loss: 0.3277\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2428 - val_loss: 0.3261\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2305 - val_loss: 0.3278\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2415 - val_loss: 0.3282\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2377 - val_loss: 0.3300\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2279 - val_loss: 0.3302\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2236 - val_loss: 0.3316\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2344 - val_loss: 0.3282\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2404 - val_loss: 0.3295\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2328 - val_loss: 0.3274\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2387 - val_loss: 0.3276\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2260 - val_loss: 0.3278\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2305 - val_loss: 0.3297\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2264 - val_loss: 0.3291\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2319 - val_loss: 0.3269\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 12ms/step - loss: 0.2277 - val_loss: 0.3267\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2476 - val_loss: 0.3247\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2221 - val_loss: 0.3263\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2392 - val_loss: 0.3260\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2364 - val_loss: 0.3259\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2279 - val_loss: 0.3261\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2400 - val_loss: 0.3272\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.3273\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2314 - val_loss: 0.3293\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2364 - val_loss: 0.3307\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2363 - val_loss: 0.3291\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2276 - val_loss: 0.3305\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2243 - val_loss: 0.3302\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2313 - val_loss: 0.3329\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2346 - val_loss: 0.3333\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2267 - val_loss: 0.3339\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2455 - val_loss: 0.3314\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2316 - val_loss: 0.3314\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2317 - val_loss: 0.3313\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2307 - val_loss: 0.3300\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2285 - val_loss: 0.3311\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2337 - val_loss: 0.3303\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2283 - val_loss: 0.3294\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2225 - val_loss: 0.3291\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2309 - val_loss: 0.3300\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2265 - val_loss: 0.3300\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2302 - val_loss: 0.3304\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2260 - val_loss: 0.3297\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2287 - val_loss: 0.3266\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2253 - val_loss: 0.3276\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2277 - val_loss: 0.3285\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2266 - val_loss: 0.3282\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2338 - val_loss: 0.3279\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2226 - val_loss: 0.3301\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2121 - val_loss: 0.3298\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2229 - val_loss: 0.3290\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2256 - val_loss: 0.3289\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2192 - val_loss: 0.3278\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2299 - val_loss: 0.3316\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2288 - val_loss: 0.3294\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2254 - val_loss: 0.3297\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2267 - val_loss: 0.3294\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2244 - val_loss: 0.3307\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2336 - val_loss: 0.3281\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.3273\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.3268\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2334 - val_loss: 0.3273\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2229 - val_loss: 0.3270\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.3290\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2319 - val_loss: 0.3303\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2295 - val_loss: 0.3286\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2276 - val_loss: 0.3270\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2239 - val_loss: 0.3285\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2297 - val_loss: 0.3291\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2243 - val_loss: 0.3303\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2227 - val_loss: 0.3288\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2275 - val_loss: 0.3281\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2196 - val_loss: 0.3288\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.3293\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.3273\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2256 - val_loss: 0.3279\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2270 - val_loss: 0.3282\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2281 - val_loss: 0.3268\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2237 - val_loss: 0.3289\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2185 - val_loss: 0.3296\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2272 - val_loss: 0.3308\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2201 - val_loss: 0.3298\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2240 - val_loss: 0.3295\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2421 - val_loss: 0.3292\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2062 - val_loss: 0.3297\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2241 - val_loss: 0.3304\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2206 - val_loss: 0.3302\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2280 - val_loss: 0.3321\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2152 - val_loss: 0.3324\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2244 - val_loss: 0.3318\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2330 - val_loss: 0.3332\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2187 - val_loss: 0.3336\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2382 - val_loss: 0.3322\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2205 - val_loss: 0.3328\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.3299\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.3286\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2312 - val_loss: 0.3287\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2257 - val_loss: 0.3310\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.3312\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.3322\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2213 - val_loss: 0.3320\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2250 - val_loss: 0.3300\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2258 - val_loss: 0.3296\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2128 - val_loss: 0.3306\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.3308\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2150 - val_loss: 0.3316\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2245 - val_loss: 0.3309\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2139 - val_loss: 0.3330\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2238 - val_loss: 0.3327\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2059 - val_loss: 0.3349\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2162 - val_loss: 0.3346\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2196 - val_loss: 0.3370\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2268 - val_loss: 0.3365\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2190 - val_loss: 0.3355\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2190 - val_loss: 0.3329\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2217 - val_loss: 0.3347\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2193 - val_loss: 0.3324\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2149 - val_loss: 0.3329\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2131 - val_loss: 0.3339\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2217 - val_loss: 0.3312\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2300 - val_loss: 0.3317\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2148 - val_loss: 0.3311\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.3318\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.3305\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2225 - val_loss: 0.3318\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2267 - val_loss: 0.3320\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2257 - val_loss: 0.3327\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2128 - val_loss: 0.3356\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2159 - val_loss: 0.3309\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2200 - val_loss: 0.3307\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2099 - val_loss: 0.3319\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2189 - val_loss: 0.3322\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2130 - val_loss: 0.3320\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2300 - val_loss: 0.3332\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2190 - val_loss: 0.3327\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2223 - val_loss: 0.3339\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2317 - val_loss: 0.3321\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2307 - val_loss: 0.3311\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2265 - val_loss: 0.3309\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.3319\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.3279\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2183 - val_loss: 0.3274\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2154 - val_loss: 0.3289\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2234 - val_loss: 0.3296\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2311 - val_loss: 0.3309\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2179 - val_loss: 0.3307\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2298 - val_loss: 0.3325\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2253 - val_loss: 0.3308\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2143 - val_loss: 0.3324\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2189 - val_loss: 0.3300\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.3301\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.3312\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2254 - val_loss: 0.3298\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2149 - val_loss: 0.3303\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2105 - val_loss: 0.3307\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2321 - val_loss: 0.3291\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2238 - val_loss: 0.3312\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2240 - val_loss: 0.3337\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2185 - val_loss: 0.3331\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2123 - val_loss: 0.3317\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2272 - val_loss: 0.3333\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2164 - val_loss: 0.3300\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2272 - val_loss: 0.3300\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2132 - val_loss: 0.3315\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2158 - val_loss: 0.3295\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2094 - val_loss: 0.3318\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2282 - val_loss: 0.3345\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2304 - val_loss: 0.3329\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2221 - val_loss: 0.3325\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2144 - val_loss: 0.3338\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2190 - val_loss: 0.3330\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.3340\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2251 - val_loss: 0.3345\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2194 - val_loss: 0.3363\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.3362\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.3347\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2071 - val_loss: 0.3351\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2209 - val_loss: 0.3329\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2120 - val_loss: 0.3334\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.3349\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2159 - val_loss: 0.3346\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2127 - val_loss: 0.3339\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2175 - val_loss: 0.3306\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2078 - val_loss: 0.3324\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2198 - val_loss: 0.3335\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2139 - val_loss: 0.3338\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2201 - val_loss: 0.3363\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2307 - val_loss: 0.3338\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2188 - val_loss: 0.3288\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2145 - val_loss: 0.3268\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2155 - val_loss: 0.3284\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2083 - val_loss: 0.3294\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2126 - val_loss: 0.3291\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2123 - val_loss: 0.3302\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2101 - val_loss: 0.3317\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2206 - val_loss: 0.3317\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2140 - val_loss: 0.3333\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2169 - val_loss: 0.3347\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2068 - val_loss: 0.3331\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.3325\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2130 - val_loss: 0.3321\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2083 - val_loss: 0.3319\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2079 - val_loss: 0.3324\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2134 - val_loss: 0.3335\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2071 - val_loss: 0.3336\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2035 - val_loss: 0.3329\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2127 - val_loss: 0.3331\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2197 - val_loss: 0.3318\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2248 - val_loss: 0.3293\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2150 - val_loss: 0.3282\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2161 - val_loss: 0.3315\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2064 - val_loss: 0.3283\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2244 - val_loss: 0.3289\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2156 - val_loss: 0.3308\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2121 - val_loss: 0.3292\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2092 - val_loss: 0.3314\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2079 - val_loss: 0.3307\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2078 - val_loss: 0.3318\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2103 - val_loss: 0.3300\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2063 - val_loss: 0.3284\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2105 - val_loss: 0.3295\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2137 - val_loss: 0.3320\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2106 - val_loss: 0.3316\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1961 - val_loss: 0.3311\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2071 - val_loss: 0.3325\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2189 - val_loss: 0.3322\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2097 - val_loss: 0.3326\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2133 - val_loss: 0.3333\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1989 - val_loss: 0.3310\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2166 - val_loss: 0.3331\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2096 - val_loss: 0.3309\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2255 - val_loss: 0.3303\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2152 - val_loss: 0.3328\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2060 - val_loss: 0.3339\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1944 - val_loss: 0.3338\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2077 - val_loss: 0.3346\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2199 - val_loss: 0.3345\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2163 - val_loss: 0.3331\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2155 - val_loss: 0.3357\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2175 - val_loss: 0.3346\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.3335\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2110 - val_loss: 0.3321\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2119 - val_loss: 0.3327\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2079 - val_loss: 0.3356\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2095 - val_loss: 0.3334\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2194 - val_loss: 0.3323\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2080 - val_loss: 0.3334\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.3327\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2166 - val_loss: 0.3349\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2063 - val_loss: 0.3346\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2098 - val_loss: 0.3365\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2118 - val_loss: 0.3392\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.3380\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2055 - val_loss: 0.3380\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2061 - val_loss: 0.3377\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2088 - val_loss: 0.3382\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2023 - val_loss: 0.3410\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2106 - val_loss: 0.3388\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2129 - val_loss: 0.3389\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2142 - val_loss: 0.3389\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2055 - val_loss: 0.3380\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2141 - val_loss: 0.3364\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2214 - val_loss: 0.3372\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2061 - val_loss: 0.3379\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2071 - val_loss: 0.3371\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2132 - val_loss: 0.3352\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2008 - val_loss: 0.3366\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2045 - val_loss: 0.3371\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1991 - val_loss: 0.3364\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.3342\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2070 - val_loss: 0.3337\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2026 - val_loss: 0.3340\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1988 - val_loss: 0.3330\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2069 - val_loss: 0.3332\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.3334\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.3354\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2048 - val_loss: 0.3369\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1984 - val_loss: 0.3358\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1998 - val_loss: 0.3364\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.3334\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2097 - val_loss: 0.3346\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2114 - val_loss: 0.3358\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2003 - val_loss: 0.3332\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 13ms/step - loss: 0.1959 - val_loss: 0.3357\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2042 - val_loss: 0.3364\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2169 - val_loss: 0.3363\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2083 - val_loss: 0.3378\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1993 - val_loss: 0.3376\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2046 - val_loss: 0.3359\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2046 - val_loss: 0.3341\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.3355\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1990 - val_loss: 0.3352\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2069 - val_loss: 0.3339\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.3339\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2108 - val_loss: 0.3339\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2095 - val_loss: 0.3361\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2065 - val_loss: 0.3374\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2006 - val_loss: 0.3367\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2042 - val_loss: 0.3386\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2036 - val_loss: 0.3391\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1967 - val_loss: 0.3377\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1999 - val_loss: 0.3375\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2022 - val_loss: 0.3338\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1959 - val_loss: 0.3348\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1969 - val_loss: 0.3362\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2057 - val_loss: 0.3371\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1970 - val_loss: 0.3384\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2076 - val_loss: 0.3379\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2046 - val_loss: 0.3356\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1955 - val_loss: 0.3333\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2187 - val_loss: 0.3333\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2094 - val_loss: 0.3358\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.3365\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2036 - val_loss: 0.3383\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2012 - val_loss: 0.3374\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1900 - val_loss: 0.3377\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1978 - val_loss: 0.3369\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2022 - val_loss: 0.3382\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1979 - val_loss: 0.3393\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1933 - val_loss: 0.3369\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2115 - val_loss: 0.3378\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2023 - val_loss: 0.3390\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1987 - val_loss: 0.3400\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2127 - val_loss: 0.3383\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2104 - val_loss: 0.3386\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2001 - val_loss: 0.3364\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2026 - val_loss: 0.3359\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2001 - val_loss: 0.3380\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2064 - val_loss: 0.3375\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2141 - val_loss: 0.3359\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1942 - val_loss: 0.3337\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.3334\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2067 - val_loss: 0.3336\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1987 - val_loss: 0.3347\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1994 - val_loss: 0.3379\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2052 - val_loss: 0.3339\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2032 - val_loss: 0.3330\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1979 - val_loss: 0.3351\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1990 - val_loss: 0.3349\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2058 - val_loss: 0.3355\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1999 - val_loss: 0.3343\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1888 - val_loss: 0.3367\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1931 - val_loss: 0.3381\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1973 - val_loss: 0.3365\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1937 - val_loss: 0.3378\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.3330\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2061 - val_loss: 0.3376\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2057 - val_loss: 0.3378\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1881 - val_loss: 0.3370\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2015 - val_loss: 0.3361\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2043 - val_loss: 0.3357\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1976 - val_loss: 0.3339\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3339\n",
            "7/7 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae6a973850>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number set as 256\n",
        "nb_epochs=1000;    # training epochs change to 1000\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = random.choice([condition1, condition2, condition3, condition4])    # randomly choose 1 lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "me4W9XdNTpUM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NraLzXmAPKk"
      },
      "source": [
        "###3.3.2 1st light condition as test###  "
      ],
      "id": "0NraLzXmAPKk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SsLHq1n3Aduz",
        "outputId": "83e38cd7-661b-44cd-ede8-1f45d1d5330c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 5s 14ms/step - loss: 2.3550 - val_loss: 2.0191\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 2.2750 - val_loss: 1.9542\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 2.1915 - val_loss: 1.8874\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 2.0953 - val_loss: 1.8175\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 2.0067 - val_loss: 1.7481\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 1.9132 - val_loss: 1.6802\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.8134 - val_loss: 1.6119\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.7214 - val_loss: 1.5435\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.6284 - val_loss: 1.4777\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.5361 - val_loss: 1.4125\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.4500 - val_loss: 1.3488\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.3713 - val_loss: 1.2871\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 1.2910 - val_loss: 1.2251\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.2177 - val_loss: 1.1630\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.1436 - val_loss: 1.1015\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 1.0653 - val_loss: 1.0401\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.9969 - val_loss: 0.9805\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.9413 - val_loss: 0.9227\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8979 - val_loss: 0.8697\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.8323 - val_loss: 0.8148\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7939 - val_loss: 0.7646\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.7507 - val_loss: 0.7178\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7184 - val_loss: 0.6744\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6963 - val_loss: 0.6330\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6702 - val_loss: 0.5949\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6292 - val_loss: 0.5612\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6228 - val_loss: 0.5305\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6037 - val_loss: 0.5026\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5923 - val_loss: 0.4785\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5726 - val_loss: 0.4561\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5524 - val_loss: 0.4368\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5587 - val_loss: 0.4205\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5499 - val_loss: 0.4063\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5360 - val_loss: 0.3914\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.5266 - val_loss: 0.3781\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5318 - val_loss: 0.3692\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5163 - val_loss: 0.3595\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5215 - val_loss: 0.3505\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5007 - val_loss: 0.3430\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4975 - val_loss: 0.3356\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5038 - val_loss: 0.3299\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4939 - val_loss: 0.3251\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4856 - val_loss: 0.3203\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4872 - val_loss: 0.3154\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4803 - val_loss: 0.3130\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4666 - val_loss: 0.3086\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4724 - val_loss: 0.3060\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4804 - val_loss: 0.3025\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4581 - val_loss: 0.3018\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4639 - val_loss: 0.2990\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4692 - val_loss: 0.2977\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4638 - val_loss: 0.2956\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4535 - val_loss: 0.2933\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4521 - val_loss: 0.2913\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4492 - val_loss: 0.2903\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4542 - val_loss: 0.2886\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4531 - val_loss: 0.2878\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4530 - val_loss: 0.2859\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4473 - val_loss: 0.2851\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4413 - val_loss: 0.2847\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4337 - val_loss: 0.2830\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4432 - val_loss: 0.2814\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4407 - val_loss: 0.2802\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4383 - val_loss: 0.2800\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4257 - val_loss: 0.2802\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4227 - val_loss: 0.2809\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4166 - val_loss: 0.2798\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4087 - val_loss: 0.2796\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4272 - val_loss: 0.2797\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4232 - val_loss: 0.2785\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4219 - val_loss: 0.2793\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4219 - val_loss: 0.2793\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4244 - val_loss: 0.2790\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4273 - val_loss: 0.2784\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4165 - val_loss: 0.2777\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4152 - val_loss: 0.2787\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4115 - val_loss: 0.2775\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4052 - val_loss: 0.2773\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4023 - val_loss: 0.2766\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4044 - val_loss: 0.2762\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4020 - val_loss: 0.2770\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.4028 - val_loss: 0.2775\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4117 - val_loss: 0.2763\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3991 - val_loss: 0.2772\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3918 - val_loss: 0.2758\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3986 - val_loss: 0.2772\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3991 - val_loss: 0.2768\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3973 - val_loss: 0.2760\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4001 - val_loss: 0.2764\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3990 - val_loss: 0.2765\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3982 - val_loss: 0.2757\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3919 - val_loss: 0.2751\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3881 - val_loss: 0.2755\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3851 - val_loss: 0.2745\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3696 - val_loss: 0.2740\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3889 - val_loss: 0.2737\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3983 - val_loss: 0.2729\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3722 - val_loss: 0.2740\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3854 - val_loss: 0.2733\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3708 - val_loss: 0.2735\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3755 - val_loss: 0.2735\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3812 - val_loss: 0.2735\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3922 - val_loss: 0.2722\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3698 - val_loss: 0.2736\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3767 - val_loss: 0.2730\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3843 - val_loss: 0.2733\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3674 - val_loss: 0.2726\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3646 - val_loss: 0.2728\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3679 - val_loss: 0.2725\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3836 - val_loss: 0.2707\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3780 - val_loss: 0.2718\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3760 - val_loss: 0.2707\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3611 - val_loss: 0.2706\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3630 - val_loss: 0.2695\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3684 - val_loss: 0.2694\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3622 - val_loss: 0.2698\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3581 - val_loss: 0.2691\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3726 - val_loss: 0.2685\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3754 - val_loss: 0.2680\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3687 - val_loss: 0.2667\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3764 - val_loss: 0.2660\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3664 - val_loss: 0.2661\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3542 - val_loss: 0.2652\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3580 - val_loss: 0.2647\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3564 - val_loss: 0.2645\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3645 - val_loss: 0.2649\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3539 - val_loss: 0.2649\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3577 - val_loss: 0.2641\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3607 - val_loss: 0.2641\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3582 - val_loss: 0.2632\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3556 - val_loss: 0.2635\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3515 - val_loss: 0.2641\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3478 - val_loss: 0.2644\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3446 - val_loss: 0.2632\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3565 - val_loss: 0.2625\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3623 - val_loss: 0.2627\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3502 - val_loss: 0.2626\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3480 - val_loss: 0.2623\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3642 - val_loss: 0.2611\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3352 - val_loss: 0.2618\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3591 - val_loss: 0.2622\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3499 - val_loss: 0.2617\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3521 - val_loss: 0.2604\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3348 - val_loss: 0.2602\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3388 - val_loss: 0.2597\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3453 - val_loss: 0.2581\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3487 - val_loss: 0.2580\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3501 - val_loss: 0.2574\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3511 - val_loss: 0.2565\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3523 - val_loss: 0.2570\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3433 - val_loss: 0.2566\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3401 - val_loss: 0.2560\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3405 - val_loss: 0.2556\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3408 - val_loss: 0.2558\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3490 - val_loss: 0.2567\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3354 - val_loss: 0.2555\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3399 - val_loss: 0.2550\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3452 - val_loss: 0.2537\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3341 - val_loss: 0.2533\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3319 - val_loss: 0.2528\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3452 - val_loss: 0.2532\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3451 - val_loss: 0.2535\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3330 - val_loss: 0.2528\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3311 - val_loss: 0.2520\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3343 - val_loss: 0.2521\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3274 - val_loss: 0.2517\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3292 - val_loss: 0.2512\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3262 - val_loss: 0.2500\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3411 - val_loss: 0.2502\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3336 - val_loss: 0.2504\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3279 - val_loss: 0.2499\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3340 - val_loss: 0.2505\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3260 - val_loss: 0.2508\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3369 - val_loss: 0.2503\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3342 - val_loss: 0.2501\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3378 - val_loss: 0.2508\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3224 - val_loss: 0.2511\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3301 - val_loss: 0.2503\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3267 - val_loss: 0.2499\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3260 - val_loss: 0.2488\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3267 - val_loss: 0.2486\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3358 - val_loss: 0.2477\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3263 - val_loss: 0.2466\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3395 - val_loss: 0.2455\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3294 - val_loss: 0.2452\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3177 - val_loss: 0.2460\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3282 - val_loss: 0.2467\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3333 - val_loss: 0.2464\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3325 - val_loss: 0.2457\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3316 - val_loss: 0.2447\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3247 - val_loss: 0.2440\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3240 - val_loss: 0.2444\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3212 - val_loss: 0.2443\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3283 - val_loss: 0.2436\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3240 - val_loss: 0.2437\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3262 - val_loss: 0.2431\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3199 - val_loss: 0.2422\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3222 - val_loss: 0.2429\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3151 - val_loss: 0.2429\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3226 - val_loss: 0.2429\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.3215 - val_loss: 0.2433\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3167 - val_loss: 0.2430\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3252 - val_loss: 0.2424\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3244 - val_loss: 0.2423\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3296 - val_loss: 0.2420\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3150 - val_loss: 0.2421\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3185 - val_loss: 0.2416\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3162 - val_loss: 0.2411\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3101 - val_loss: 0.2416\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3078 - val_loss: 0.2422\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3249 - val_loss: 0.2414\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2979 - val_loss: 0.2411\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3198 - val_loss: 0.2411\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3178 - val_loss: 0.2416\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3141 - val_loss: 0.2413\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3223 - val_loss: 0.2412\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3083 - val_loss: 0.2417\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3100 - val_loss: 0.2423\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3118 - val_loss: 0.2416\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3178 - val_loss: 0.2415\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3020 - val_loss: 0.2411\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3096 - val_loss: 0.2398\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3171 - val_loss: 0.2392\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3115 - val_loss: 0.2402\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3138 - val_loss: 0.2388\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3096 - val_loss: 0.2395\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3154 - val_loss: 0.2397\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3002 - val_loss: 0.2387\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3147 - val_loss: 0.2380\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3079 - val_loss: 0.2382\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3150 - val_loss: 0.2388\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3027 - val_loss: 0.2397\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3145 - val_loss: 0.2393\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3105 - val_loss: 0.2383\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3086 - val_loss: 0.2395\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3196 - val_loss: 0.2403\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3099 - val_loss: 0.2398\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3062 - val_loss: 0.2392\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3129 - val_loss: 0.2390\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3036 - val_loss: 0.2389\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3143 - val_loss: 0.2392\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3078 - val_loss: 0.2392\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3065 - val_loss: 0.2398\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3088 - val_loss: 0.2398\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3054 - val_loss: 0.2403\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2956 - val_loss: 0.2399\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3072 - val_loss: 0.2398\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2989 - val_loss: 0.2389\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3097 - val_loss: 0.2388\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3006 - val_loss: 0.2395\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3111 - val_loss: 0.2389\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2938 - val_loss: 0.2381\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3055 - val_loss: 0.2376\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3000 - val_loss: 0.2369\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3043 - val_loss: 0.2377\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2904 - val_loss: 0.2377\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2974 - val_loss: 0.2368\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2976 - val_loss: 0.2367\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3020 - val_loss: 0.2366\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2956 - val_loss: 0.2375\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2989 - val_loss: 0.2379\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3011 - val_loss: 0.2370\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3054 - val_loss: 0.2387\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.2997 - val_loss: 0.2372\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3061 - val_loss: 0.2387\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2944 - val_loss: 0.2384\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3095 - val_loss: 0.2381\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2999 - val_loss: 0.2388\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3047 - val_loss: 0.2389\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2961 - val_loss: 0.2376\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3064 - val_loss: 0.2373\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3053 - val_loss: 0.2371\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3053 - val_loss: 0.2371\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2984 - val_loss: 0.2364\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2913 - val_loss: 0.2373\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2963 - val_loss: 0.2364\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2840 - val_loss: 0.2367\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2914 - val_loss: 0.2367\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2898 - val_loss: 0.2373\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2851 - val_loss: 0.2378\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2896 - val_loss: 0.2377\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3010 - val_loss: 0.2367\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2940 - val_loss: 0.2373\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2972 - val_loss: 0.2366\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2886 - val_loss: 0.2363\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3030 - val_loss: 0.2367\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3051 - val_loss: 0.2362\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2881 - val_loss: 0.2365\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2912 - val_loss: 0.2369\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.2871 - val_loss: 0.2366\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2934 - val_loss: 0.2358\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2935 - val_loss: 0.2359\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2959 - val_loss: 0.2361\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3038 - val_loss: 0.2364\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2883 - val_loss: 0.2362\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2917 - val_loss: 0.2367\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2834 - val_loss: 0.2361\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2906 - val_loss: 0.2362\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2844 - val_loss: 0.2358\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2965 - val_loss: 0.2357\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2840 - val_loss: 0.2354\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2919 - val_loss: 0.2364\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2829 - val_loss: 0.2359\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3011 - val_loss: 0.2357\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3024 - val_loss: 0.2358\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2996 - val_loss: 0.2357\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2783 - val_loss: 0.2353\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2890 - val_loss: 0.2356\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2893 - val_loss: 0.2365\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2859 - val_loss: 0.2363\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2979 - val_loss: 0.2363\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2907 - val_loss: 0.2370\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2859 - val_loss: 0.2364\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2781 - val_loss: 0.2350\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2909 - val_loss: 0.2358\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2871 - val_loss: 0.2353\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2847 - val_loss: 0.2342\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2921 - val_loss: 0.2343\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2903 - val_loss: 0.2331\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2756 - val_loss: 0.2349\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2848 - val_loss: 0.2360\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2861 - val_loss: 0.2356\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2845 - val_loss: 0.2352\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2877 - val_loss: 0.2357\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2812 - val_loss: 0.2346\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2821 - val_loss: 0.2349\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2838 - val_loss: 0.2349\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2944 - val_loss: 0.2358\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2710 - val_loss: 0.2350\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2866 - val_loss: 0.2362\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2915 - val_loss: 0.2364\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2708 - val_loss: 0.2366\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2881 - val_loss: 0.2374\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2838 - val_loss: 0.2367\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2884 - val_loss: 0.2379\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2825 - val_loss: 0.2374\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2803 - val_loss: 0.2369\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2832 - val_loss: 0.2376\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2876 - val_loss: 0.2374\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2789 - val_loss: 0.2366\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2884 - val_loss: 0.2370\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2862 - val_loss: 0.2363\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2818 - val_loss: 0.2352\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2896 - val_loss: 0.2352\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2878 - val_loss: 0.2356\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2828 - val_loss: 0.2352\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.2362\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2849 - val_loss: 0.2363\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2804 - val_loss: 0.2359\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2713 - val_loss: 0.2357\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2857 - val_loss: 0.2357\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2859 - val_loss: 0.2354\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2871 - val_loss: 0.2355\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2745 - val_loss: 0.2341\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2842 - val_loss: 0.2346\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2818 - val_loss: 0.2353\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2824 - val_loss: 0.2351\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2789 - val_loss: 0.2348\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2794 - val_loss: 0.2357\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.2359\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.2353\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2746 - val_loss: 0.2359\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2768 - val_loss: 0.2338\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2740 - val_loss: 0.2343\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2719 - val_loss: 0.2339\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2699 - val_loss: 0.2344\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2825 - val_loss: 0.2338\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2705 - val_loss: 0.2346\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2747 - val_loss: 0.2364\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2733 - val_loss: 0.2347\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2769 - val_loss: 0.2351\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2759 - val_loss: 0.2355\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2883 - val_loss: 0.2350\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2750 - val_loss: 0.2343\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.2361\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2781 - val_loss: 0.2363\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2839 - val_loss: 0.2363\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2888 - val_loss: 0.2353\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2718 - val_loss: 0.2352\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2657 - val_loss: 0.2352\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2777 - val_loss: 0.2350\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2635 - val_loss: 0.2363\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2747 - val_loss: 0.2354\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2627 - val_loss: 0.2357\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2770 - val_loss: 0.2348\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2701 - val_loss: 0.2357\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2740 - val_loss: 0.2353\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.2358\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2679 - val_loss: 0.2352\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2713 - val_loss: 0.2357\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2674 - val_loss: 0.2368\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2645 - val_loss: 0.2364\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2814 - val_loss: 0.2367\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2752 - val_loss: 0.2369\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2761 - val_loss: 0.2364\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2785 - val_loss: 0.2366\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2745 - val_loss: 0.2370\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2727 - val_loss: 0.2359\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2692 - val_loss: 0.2367\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2610 - val_loss: 0.2369\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.2372\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2747 - val_loss: 0.2372\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2817 - val_loss: 0.2371\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2611 - val_loss: 0.2356\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2613 - val_loss: 0.2357\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2746 - val_loss: 0.2358\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2669 - val_loss: 0.2358\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2658 - val_loss: 0.2363\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2734 - val_loss: 0.2374\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2604 - val_loss: 0.2373\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2809 - val_loss: 0.2369\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2781 - val_loss: 0.2381\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2586 - val_loss: 0.2377\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2625 - val_loss: 0.2372\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2701 - val_loss: 0.2368\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2617 - val_loss: 0.2366\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2605 - val_loss: 0.2365\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2795 - val_loss: 0.2373\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.2377\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2663 - val_loss: 0.2369\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2755 - val_loss: 0.2377\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2793 - val_loss: 0.2376\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2672 - val_loss: 0.2373\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2710 - val_loss: 0.2375\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.2372\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2633 - val_loss: 0.2375\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2576 - val_loss: 0.2375\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2653 - val_loss: 0.2376\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2761 - val_loss: 0.2381\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2776 - val_loss: 0.2386\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2682 - val_loss: 0.2394\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2597 - val_loss: 0.2399\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2805 - val_loss: 0.2399\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.2386\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2786 - val_loss: 0.2378\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2648 - val_loss: 0.2372\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2658 - val_loss: 0.2383\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2738 - val_loss: 0.2398\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2609 - val_loss: 0.2395\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2647 - val_loss: 0.2393\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2580 - val_loss: 0.2388\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.2387\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2665 - val_loss: 0.2390\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2577 - val_loss: 0.2398\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2700 - val_loss: 0.2379\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2577 - val_loss: 0.2374\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2677 - val_loss: 0.2382\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2635 - val_loss: 0.2382\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2539 - val_loss: 0.2376\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2690 - val_loss: 0.2377\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2647 - val_loss: 0.2375\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2790 - val_loss: 0.2376\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.2376\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2721 - val_loss: 0.2384\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2561 - val_loss: 0.2371\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2681 - val_loss: 0.2375\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2587 - val_loss: 0.2378\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2535 - val_loss: 0.2382\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2683 - val_loss: 0.2399\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2589 - val_loss: 0.2399\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2588 - val_loss: 0.2385\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2583 - val_loss: 0.2368\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2564 - val_loss: 0.2367\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2580 - val_loss: 0.2373\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2642 - val_loss: 0.2402\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2452 - val_loss: 0.2380\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2610 - val_loss: 0.2376\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2607 - val_loss: 0.2377\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2655 - val_loss: 0.2386\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.2381\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2533 - val_loss: 0.2374\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2660 - val_loss: 0.2383\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2571 - val_loss: 0.2380\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2618 - val_loss: 0.2385\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2506 - val_loss: 0.2371\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.2368\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2557 - val_loss: 0.2385\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2513 - val_loss: 0.2375\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2616 - val_loss: 0.2381\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2531 - val_loss: 0.2381\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2600 - val_loss: 0.2385\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2491 - val_loss: 0.2390\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2489 - val_loss: 0.2394\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2587 - val_loss: 0.2387\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2577 - val_loss: 0.2390\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2580 - val_loss: 0.2391\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2564 - val_loss: 0.2404\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2601 - val_loss: 0.2407\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2579 - val_loss: 0.2399\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2645 - val_loss: 0.2396\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2622 - val_loss: 0.2399\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2692 - val_loss: 0.2390\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2563 - val_loss: 0.2402\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2539 - val_loss: 0.2399\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2629 - val_loss: 0.2393\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2475 - val_loss: 0.2390\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2587 - val_loss: 0.2390\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2600 - val_loss: 0.2387\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2499 - val_loss: 0.2404\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2503 - val_loss: 0.2399\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2481 - val_loss: 0.2406\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2549 - val_loss: 0.2396\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2588 - val_loss: 0.2406\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2538 - val_loss: 0.2383\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2468 - val_loss: 0.2389\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2653 - val_loss: 0.2392\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2610 - val_loss: 0.2387\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2540 - val_loss: 0.2400\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2677 - val_loss: 0.2392\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2554 - val_loss: 0.2401\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2502 - val_loss: 0.2393\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2504 - val_loss: 0.2395\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2605 - val_loss: 0.2397\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2412 - val_loss: 0.2386\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2459 - val_loss: 0.2374\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2573 - val_loss: 0.2385\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2546 - val_loss: 0.2390\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2467 - val_loss: 0.2410\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2443 - val_loss: 0.2391\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2540 - val_loss: 0.2401\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2645 - val_loss: 0.2394\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2455 - val_loss: 0.2406\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2548 - val_loss: 0.2399\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2499 - val_loss: 0.2389\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2540 - val_loss: 0.2390\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2560 - val_loss: 0.2395\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2655 - val_loss: 0.2386\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2569 - val_loss: 0.2382\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2527 - val_loss: 0.2399\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2468 - val_loss: 0.2401\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2559 - val_loss: 0.2398\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2373 - val_loss: 0.2401\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2528 - val_loss: 0.2399\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2510 - val_loss: 0.2403\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2576 - val_loss: 0.2400\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2549 - val_loss: 0.2407\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2552 - val_loss: 0.2386\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2525 - val_loss: 0.2407\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2518 - val_loss: 0.2402\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2465 - val_loss: 0.2402\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2609 - val_loss: 0.2401\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2470 - val_loss: 0.2412\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2505 - val_loss: 0.2412\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2482 - val_loss: 0.2404\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2547 - val_loss: 0.2411\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2473 - val_loss: 0.2394\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2510 - val_loss: 0.2392\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2495 - val_loss: 0.2409\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2502 - val_loss: 0.2393\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.2407\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2455 - val_loss: 0.2386\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2517 - val_loss: 0.2407\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2559 - val_loss: 0.2413\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2573 - val_loss: 0.2418\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2429 - val_loss: 0.2404\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2511 - val_loss: 0.2397\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2412 - val_loss: 0.2405\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2475 - val_loss: 0.2401\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.2394\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2482 - val_loss: 0.2400\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2327 - val_loss: 0.2396\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.2409\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2409 - val_loss: 0.2402\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2513 - val_loss: 0.2423\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2521 - val_loss: 0.2413\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2604 - val_loss: 0.2416\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2516 - val_loss: 0.2415\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2473 - val_loss: 0.2400\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2436 - val_loss: 0.2403\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2456 - val_loss: 0.2407\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2469 - val_loss: 0.2397\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2466 - val_loss: 0.2408\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2483 - val_loss: 0.2402\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2495 - val_loss: 0.2399\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2423 - val_loss: 0.2406\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2481 - val_loss: 0.2415\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2503 - val_loss: 0.2426\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2526 - val_loss: 0.2426\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2556 - val_loss: 0.2416\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2472 - val_loss: 0.2419\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2583 - val_loss: 0.2429\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2418 - val_loss: 0.2415\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2391 - val_loss: 0.2412\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2530 - val_loss: 0.2413\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.2425\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2499 - val_loss: 0.2421\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2448 - val_loss: 0.2402\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2359 - val_loss: 0.2408\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2374 - val_loss: 0.2418\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2268 - val_loss: 0.2421\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2449 - val_loss: 0.2415\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2406 - val_loss: 0.2426\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2415 - val_loss: 0.2411\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2447 - val_loss: 0.2410\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2424 - val_loss: 0.2402\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2338 - val_loss: 0.2396\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2436 - val_loss: 0.2401\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2460 - val_loss: 0.2403\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2545 - val_loss: 0.2405\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2492 - val_loss: 0.2398\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.2381\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2443 - val_loss: 0.2391\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2407 - val_loss: 0.2389\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2404 - val_loss: 0.2382\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2599 - val_loss: 0.2390\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2391 - val_loss: 0.2388\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2442 - val_loss: 0.2386\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2524 - val_loss: 0.2393\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.2392\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2376 - val_loss: 0.2391\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.2380\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2332 - val_loss: 0.2386\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2456 - val_loss: 0.2391\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2411 - val_loss: 0.2388\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2445 - val_loss: 0.2387\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2431 - val_loss: 0.2381\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2442 - val_loss: 0.2388\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2457 - val_loss: 0.2393\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2382 - val_loss: 0.2390\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2409 - val_loss: 0.2384\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2477 - val_loss: 0.2390\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2483 - val_loss: 0.2393\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2419 - val_loss: 0.2388\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2479 - val_loss: 0.2373\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2277 - val_loss: 0.2381\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2435 - val_loss: 0.2370\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2382 - val_loss: 0.2374\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2460 - val_loss: 0.2377\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2380 - val_loss: 0.2377\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2456 - val_loss: 0.2389\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2336 - val_loss: 0.2370\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2443 - val_loss: 0.2389\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2339 - val_loss: 0.2385\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2386 - val_loss: 0.2387\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2512 - val_loss: 0.2391\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2309 - val_loss: 0.2406\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.2399\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2400 - val_loss: 0.2401\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2339 - val_loss: 0.2410\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2473 - val_loss: 0.2409\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2349 - val_loss: 0.2407\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2402 - val_loss: 0.2400\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2296 - val_loss: 0.2400\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2317 - val_loss: 0.2411\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2436 - val_loss: 0.2393\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2335 - val_loss: 0.2395\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2349 - val_loss: 0.2408\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.2407\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2380 - val_loss: 0.2392\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2428 - val_loss: 0.2404\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2347 - val_loss: 0.2420\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2442 - val_loss: 0.2400\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2368 - val_loss: 0.2409\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2377 - val_loss: 0.2409\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2280 - val_loss: 0.2412\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2405 - val_loss: 0.2406\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2280 - val_loss: 0.2407\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2337 - val_loss: 0.2419\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2383 - val_loss: 0.2417\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2446 - val_loss: 0.2415\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2316 - val_loss: 0.2408\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2310 - val_loss: 0.2401\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2516 - val_loss: 0.2423\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2354 - val_loss: 0.2434\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2411 - val_loss: 0.2413\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.2415\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2323 - val_loss: 0.2412\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.2398\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.2400\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2314 - val_loss: 0.2411\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2341 - val_loss: 0.2389\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2293 - val_loss: 0.2380\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2341 - val_loss: 0.2376\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.2395\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2337 - val_loss: 0.2387\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2353 - val_loss: 0.2388\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2420 - val_loss: 0.2388\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2302 - val_loss: 0.2391\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2334 - val_loss: 0.2384\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2390 - val_loss: 0.2386\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.2378\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2398 - val_loss: 0.2373\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.2381\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2253 - val_loss: 0.2372\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2346 - val_loss: 0.2374\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2434 - val_loss: 0.2379\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2447 - val_loss: 0.2381\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2303 - val_loss: 0.2383\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2406 - val_loss: 0.2381\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2251 - val_loss: 0.2390\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2392 - val_loss: 0.2390\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2337 - val_loss: 0.2385\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2326 - val_loss: 0.2388\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2364 - val_loss: 0.2389\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.2369\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2318 - val_loss: 0.2369\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2321 - val_loss: 0.2375\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2351 - val_loss: 0.2380\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2332 - val_loss: 0.2379\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2316 - val_loss: 0.2382\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2205 - val_loss: 0.2395\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2299 - val_loss: 0.2396\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2295 - val_loss: 0.2392\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2282 - val_loss: 0.2393\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2295 - val_loss: 0.2399\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2286 - val_loss: 0.2396\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2366 - val_loss: 0.2407\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.2380\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2400 - val_loss: 0.2394\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2337 - val_loss: 0.2383\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2344 - val_loss: 0.2389\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2212 - val_loss: 0.2391\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2254 - val_loss: 0.2392\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2336 - val_loss: 0.2391\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2279 - val_loss: 0.2395\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.2396\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2276 - val_loss: 0.2405\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2292 - val_loss: 0.2393\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.2394\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2272 - val_loss: 0.2397\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2430 - val_loss: 0.2405\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2212 - val_loss: 0.2390\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2244 - val_loss: 0.2398\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2393 - val_loss: 0.2389\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.2396\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2235 - val_loss: 0.2392\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.2389\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2345 - val_loss: 0.2388\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2304 - val_loss: 0.2386\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2224 - val_loss: 0.2380\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2378 - val_loss: 0.2385\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2251 - val_loss: 0.2392\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2351 - val_loss: 0.2381\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2252 - val_loss: 0.2376\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2279 - val_loss: 0.2373\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2348 - val_loss: 0.2384\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2397 - val_loss: 0.2383\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2414 - val_loss: 0.2387\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2334 - val_loss: 0.2391\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2288 - val_loss: 0.2400\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.2399\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2225 - val_loss: 0.2384\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2330 - val_loss: 0.2386\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2366 - val_loss: 0.2386\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2345 - val_loss: 0.2385\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2197 - val_loss: 0.2369\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2320 - val_loss: 0.2382\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2393 - val_loss: 0.2382\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.2379\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2284 - val_loss: 0.2369\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2221 - val_loss: 0.2382\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2315 - val_loss: 0.2374\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2234 - val_loss: 0.2374\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2274 - val_loss: 0.2359\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2182 - val_loss: 0.2349\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2245 - val_loss: 0.2346\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.2334\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.2346\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2276 - val_loss: 0.2357\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2332 - val_loss: 0.2357\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2260 - val_loss: 0.2363\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.2381\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2252 - val_loss: 0.2371\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.2380\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2304 - val_loss: 0.2377\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2316 - val_loss: 0.2379\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2183 - val_loss: 0.2368\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2221 - val_loss: 0.2373\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2254 - val_loss: 0.2384\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2302 - val_loss: 0.2374\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2233 - val_loss: 0.2369\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2385 - val_loss: 0.2366\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2222 - val_loss: 0.2363\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2266 - val_loss: 0.2369\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.2381\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2233 - val_loss: 0.2380\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2205 - val_loss: 0.2371\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2151 - val_loss: 0.2380\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2222 - val_loss: 0.2375\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2270 - val_loss: 0.2375\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.2373\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.2387\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2263 - val_loss: 0.2369\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2211 - val_loss: 0.2376\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2288 - val_loss: 0.2388\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2273 - val_loss: 0.2389\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2254 - val_loss: 0.2371\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2251 - val_loss: 0.2381\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2207 - val_loss: 0.2384\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2243 - val_loss: 0.2369\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2168 - val_loss: 0.2366\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.2361\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2245 - val_loss: 0.2364\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.2357\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2339 - val_loss: 0.2361\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2180 - val_loss: 0.2360\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2148 - val_loss: 0.2350\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2187 - val_loss: 0.2347\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.2361\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.2377\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2247 - val_loss: 0.2369\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.2381\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2290 - val_loss: 0.2373\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2237 - val_loss: 0.2360\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2228 - val_loss: 0.2360\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2152 - val_loss: 0.2358\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2210 - val_loss: 0.2367\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2230 - val_loss: 0.2352\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2213 - val_loss: 0.2364\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2291 - val_loss: 0.2364\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2292 - val_loss: 0.2361\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.2376\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2255 - val_loss: 0.2367\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2224 - val_loss: 0.2373\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2188 - val_loss: 0.2366\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2183 - val_loss: 0.2368\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2096 - val_loss: 0.2366\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2110 - val_loss: 0.2358\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2270 - val_loss: 0.2362\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.2365\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2226 - val_loss: 0.2377\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2180 - val_loss: 0.2351\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2174 - val_loss: 0.2352\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2314 - val_loss: 0.2352\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2061 - val_loss: 0.2356\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2279 - val_loss: 0.2364\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2243 - val_loss: 0.2376\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2165 - val_loss: 0.2371\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2174 - val_loss: 0.2355\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2135 - val_loss: 0.2374\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2190 - val_loss: 0.2368\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2181 - val_loss: 0.2356\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2158 - val_loss: 0.2349\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2212 - val_loss: 0.2367\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2145 - val_loss: 0.2352\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2240 - val_loss: 0.2344\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.2341\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.2343\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2128 - val_loss: 0.2331\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2280 - val_loss: 0.2329\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.2338\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2250 - val_loss: 0.2331\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2176 - val_loss: 0.2341\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2034 - val_loss: 0.2331\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2152 - val_loss: 0.2331\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2111 - val_loss: 0.2333\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2175 - val_loss: 0.2339\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2208 - val_loss: 0.2363\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.2358\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2218 - val_loss: 0.2360\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2226 - val_loss: 0.2360\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.2357\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2187 - val_loss: 0.2354\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2261 - val_loss: 0.2354\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2202 - val_loss: 0.2350\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2160 - val_loss: 0.2349\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2119 - val_loss: 0.2347\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2117 - val_loss: 0.2349\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2109 - val_loss: 0.2346\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2191 - val_loss: 0.2342\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2123 - val_loss: 0.2340\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.2363\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2153 - val_loss: 0.2344\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2252 - val_loss: 0.2353\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2038 - val_loss: 0.2350\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2148 - val_loss: 0.2344\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2035 - val_loss: 0.2346\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2021 - val_loss: 0.2349\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2078 - val_loss: 0.2349\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2179 - val_loss: 0.2365\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2172 - val_loss: 0.2355\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2206 - val_loss: 0.2351\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2146 - val_loss: 0.2357\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2160 - val_loss: 0.2362\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2357 - val_loss: 0.2362\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2252 - val_loss: 0.2346\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2207 - val_loss: 0.2346\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2173 - val_loss: 0.2339\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2090 - val_loss: 0.2351\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2035 - val_loss: 0.2341\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2119 - val_loss: 0.2342\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2131 - val_loss: 0.2345\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2203 - val_loss: 0.2325\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2158 - val_loss: 0.2331\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2103 - val_loss: 0.2319\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2126 - val_loss: 0.2336\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.2342\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.2332\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2219 - val_loss: 0.2325\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2041 - val_loss: 0.2346\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.2350\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2209 - val_loss: 0.2329\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2140 - val_loss: 0.2328\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2062 - val_loss: 0.2315\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2179 - val_loss: 0.2322\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2096 - val_loss: 0.2338\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2159 - val_loss: 0.2330\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2033 - val_loss: 0.2333\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.2339\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2135 - val_loss: 0.2338\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2240 - val_loss: 0.2331\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2060 - val_loss: 0.2318\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2167 - val_loss: 0.2312\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2169 - val_loss: 0.2316\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2083 - val_loss: 0.2328\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.2323\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2104 - val_loss: 0.2328\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2102 - val_loss: 0.2327\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2148 - val_loss: 0.2324\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2110 - val_loss: 0.2325\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2068 - val_loss: 0.2332\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2085 - val_loss: 0.2339\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2032 - val_loss: 0.2331\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1970 - val_loss: 0.2328\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2155 - val_loss: 0.2338\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2109 - val_loss: 0.2338\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2157 - val_loss: 0.2335\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2128 - val_loss: 0.2336\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2034 - val_loss: 0.2330\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2109 - val_loss: 0.2331\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2172 - val_loss: 0.2328\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.1970 - val_loss: 0.2316\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2133 - val_loss: 0.2322\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2097 - val_loss: 0.2321\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2064 - val_loss: 0.2323\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2068 - val_loss: 0.2335\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2339 - val_loss: 0.2322\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2050 - val_loss: 0.2315\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.2316\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2155 - val_loss: 0.2324\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2045 - val_loss: 0.2324\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2036 - val_loss: 0.2319\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2005 - val_loss: 0.2334\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2059 - val_loss: 0.2330\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1952 - val_loss: 0.2323\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2043 - val_loss: 0.2332\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.2326\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2098 - val_loss: 0.2334\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.2337\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2078 - val_loss: 0.2329\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2030 - val_loss: 0.2325\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2131 - val_loss: 0.2318\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2096 - val_loss: 0.2329\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2069 - val_loss: 0.2321\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2071 - val_loss: 0.2318\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2118 - val_loss: 0.2318\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.2322\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2152 - val_loss: 0.2320\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1982 - val_loss: 0.2323\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2041 - val_loss: 0.2326\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2043 - val_loss: 0.2334\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.2332\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2153 - val_loss: 0.2316\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2047 - val_loss: 0.2314\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2089 - val_loss: 0.2308\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2026 - val_loss: 0.2310\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2134 - val_loss: 0.2308\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2015 - val_loss: 0.2312\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2134 - val_loss: 0.2313\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2127 - val_loss: 0.2316\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2004 - val_loss: 0.2326\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2044 - val_loss: 0.2331\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2048 - val_loss: 0.2303\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1973 - val_loss: 0.2297\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2035 - val_loss: 0.2296\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2199 - val_loss: 0.2303\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2022 - val_loss: 0.2301\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2057 - val_loss: 0.2324\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2115 - val_loss: 0.2304\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2039 - val_loss: 0.2311\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2063 - val_loss: 0.2302\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.2300\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2111 - val_loss: 0.2309\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1985 - val_loss: 0.2307\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2089 - val_loss: 0.2302\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2101 - val_loss: 0.2313\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2041 - val_loss: 0.2310\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.2297\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2019 - val_loss: 0.2304\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2034 - val_loss: 0.2312\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2074 - val_loss: 0.2302\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2011 - val_loss: 0.2298\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1948 - val_loss: 0.2297\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2029 - val_loss: 0.2299\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1945 - val_loss: 0.2308\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.2298\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.2298\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2041 - val_loss: 0.2292\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2103 - val_loss: 0.2282\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2082 - val_loss: 0.2288\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2012 - val_loss: 0.2276\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1996 - val_loss: 0.2284\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2097 - val_loss: 0.2294\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1959 - val_loss: 0.2298\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2018 - val_loss: 0.2292\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2077 - val_loss: 0.2291\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.2298\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2139 - val_loss: 0.2290\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2108 - val_loss: 0.2301\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2057 - val_loss: 0.2293\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.2293\n",
            "7/7 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae6a1dc910>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number\n",
        "nb_epochs=1000;    # training epochs change\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition1   # choose 1st lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "SsLHq1n3Aduz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsUDTN5OO9sH"
      },
      "source": [
        "###3.3.3 2nd light condition as test###  "
      ],
      "id": "OsUDTN5OO9sH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GU2yN8WFPGQs",
        "outputId": "46655479-2a10-4f02-b892-66ff096da897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - 4s 9ms/step - loss: 2.2083 - val_loss: 2.1821\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 2.1285 - val_loss: 2.1126\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 2.0406 - val_loss: 2.0435\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.9568 - val_loss: 1.9753\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.8722 - val_loss: 1.9028\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.7833 - val_loss: 1.8279\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.6937 - val_loss: 1.7510\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.6137 - val_loss: 1.6727\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.5224 - val_loss: 1.5935\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.4349 - val_loss: 1.5153\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 1.3517 - val_loss: 1.4381\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 1.2678 - val_loss: 1.3630\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.2037 - val_loss: 1.2896\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 1.1265 - val_loss: 1.2174\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 1.0595 - val_loss: 1.1473\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.9895 - val_loss: 1.0780\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.9325 - val_loss: 1.0155\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.8856 - val_loss: 0.9582\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.8391 - val_loss: 0.9029\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7964 - val_loss: 0.8526\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.7468 - val_loss: 0.8043\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.7197 - val_loss: 0.7630\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6894 - val_loss: 0.7248\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6733 - val_loss: 0.6904\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6318 - val_loss: 0.6617\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6269 - val_loss: 0.6338\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.6159 - val_loss: 0.6100\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5966 - val_loss: 0.5884\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5816 - val_loss: 0.5706\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5724 - val_loss: 0.5533\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.5596 - val_loss: 0.5404\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.5530 - val_loss: 0.5299\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.5411 - val_loss: 0.5182\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5233 - val_loss: 0.5109\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5439 - val_loss: 0.5032\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5146 - val_loss: 0.4954\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5116 - val_loss: 0.4918\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5088 - val_loss: 0.4853\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.5093 - val_loss: 0.4811\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4976 - val_loss: 0.4788\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4931 - val_loss: 0.4767\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4808 - val_loss: 0.4727\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4730 - val_loss: 0.4696\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4654 - val_loss: 0.4687\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4720 - val_loss: 0.4661\n",
            "Epoch 46/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4731 - val_loss: 0.4649\n",
            "Epoch 47/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4638 - val_loss: 0.4623\n",
            "Epoch 48/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4577 - val_loss: 0.4615\n",
            "Epoch 49/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4574 - val_loss: 0.4594\n",
            "Epoch 50/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4447 - val_loss: 0.4584\n",
            "Epoch 51/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4635 - val_loss: 0.4574\n",
            "Epoch 52/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4633 - val_loss: 0.4547\n",
            "Epoch 53/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4446 - val_loss: 0.4548\n",
            "Epoch 54/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4512 - val_loss: 0.4511\n",
            "Epoch 55/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4563 - val_loss: 0.4501\n",
            "Epoch 56/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4557 - val_loss: 0.4519\n",
            "Epoch 57/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4452 - val_loss: 0.4506\n",
            "Epoch 58/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4286 - val_loss: 0.4495\n",
            "Epoch 59/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4366 - val_loss: 0.4483\n",
            "Epoch 60/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4525 - val_loss: 0.4476\n",
            "Epoch 61/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4359 - val_loss: 0.4465\n",
            "Epoch 62/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4390 - val_loss: 0.4465\n",
            "Epoch 63/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4308 - val_loss: 0.4441\n",
            "Epoch 64/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4300 - val_loss: 0.4444\n",
            "Epoch 65/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4216 - val_loss: 0.4428\n",
            "Epoch 66/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4312 - val_loss: 0.4401\n",
            "Epoch 67/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4211 - val_loss: 0.4399\n",
            "Epoch 68/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4318 - val_loss: 0.4397\n",
            "Epoch 69/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4211 - val_loss: 0.4378\n",
            "Epoch 70/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4094 - val_loss: 0.4374\n",
            "Epoch 71/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4189 - val_loss: 0.4378\n",
            "Epoch 72/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4155 - val_loss: 0.4362\n",
            "Epoch 73/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4117 - val_loss: 0.4360\n",
            "Epoch 74/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4115 - val_loss: 0.4353\n",
            "Epoch 75/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4134 - val_loss: 0.4360\n",
            "Epoch 76/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4070 - val_loss: 0.4347\n",
            "Epoch 77/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4034 - val_loss: 0.4341\n",
            "Epoch 78/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4047 - val_loss: 0.4347\n",
            "Epoch 79/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4066 - val_loss: 0.4334\n",
            "Epoch 80/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4184 - val_loss: 0.4325\n",
            "Epoch 81/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4072 - val_loss: 0.4320\n",
            "Epoch 82/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4128 - val_loss: 0.4312\n",
            "Epoch 83/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4101 - val_loss: 0.4291\n",
            "Epoch 84/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.4049 - val_loss: 0.4307\n",
            "Epoch 85/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3989 - val_loss: 0.4285\n",
            "Epoch 86/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4041 - val_loss: 0.4279\n",
            "Epoch 87/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.4067 - val_loss: 0.4260\n",
            "Epoch 88/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3969 - val_loss: 0.4286\n",
            "Epoch 89/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4025 - val_loss: 0.4284\n",
            "Epoch 90/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3964 - val_loss: 0.4283\n",
            "Epoch 91/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.4112 - val_loss: 0.4275\n",
            "Epoch 92/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.4036 - val_loss: 0.4250\n",
            "Epoch 93/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3880 - val_loss: 0.4234\n",
            "Epoch 94/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3968 - val_loss: 0.4222\n",
            "Epoch 95/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3879 - val_loss: 0.4234\n",
            "Epoch 96/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3954 - val_loss: 0.4220\n",
            "Epoch 97/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3822 - val_loss: 0.4217\n",
            "Epoch 98/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3817 - val_loss: 0.4227\n",
            "Epoch 99/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3931 - val_loss: 0.4211\n",
            "Epoch 100/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3887 - val_loss: 0.4208\n",
            "Epoch 101/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3815 - val_loss: 0.4234\n",
            "Epoch 102/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3926 - val_loss: 0.4221\n",
            "Epoch 103/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3930 - val_loss: 0.4212\n",
            "Epoch 104/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3876 - val_loss: 0.4219\n",
            "Epoch 105/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3775 - val_loss: 0.4208\n",
            "Epoch 106/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3804 - val_loss: 0.4200\n",
            "Epoch 107/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3793 - val_loss: 0.4201\n",
            "Epoch 108/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3792 - val_loss: 0.4183\n",
            "Epoch 109/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3725 - val_loss: 0.4187\n",
            "Epoch 110/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3919 - val_loss: 0.4181\n",
            "Epoch 111/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3797 - val_loss: 0.4183\n",
            "Epoch 112/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3750 - val_loss: 0.4154\n",
            "Epoch 113/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3774 - val_loss: 0.4162\n",
            "Epoch 114/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3735 - val_loss: 0.4138\n",
            "Epoch 115/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3765 - val_loss: 0.4145\n",
            "Epoch 116/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3654 - val_loss: 0.4161\n",
            "Epoch 117/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3814 - val_loss: 0.4148\n",
            "Epoch 118/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3696 - val_loss: 0.4151\n",
            "Epoch 119/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3746 - val_loss: 0.4136\n",
            "Epoch 120/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3688 - val_loss: 0.4148\n",
            "Epoch 121/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3683 - val_loss: 0.4142\n",
            "Epoch 122/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3674 - val_loss: 0.4138\n",
            "Epoch 123/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3745 - val_loss: 0.4104\n",
            "Epoch 124/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3637 - val_loss: 0.4118\n",
            "Epoch 125/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3644 - val_loss: 0.4128\n",
            "Epoch 126/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3619 - val_loss: 0.4117\n",
            "Epoch 127/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3554 - val_loss: 0.4106\n",
            "Epoch 128/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3629 - val_loss: 0.4097\n",
            "Epoch 129/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3624 - val_loss: 0.4081\n",
            "Epoch 130/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3579 - val_loss: 0.4092\n",
            "Epoch 131/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3611 - val_loss: 0.4089\n",
            "Epoch 132/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3555 - val_loss: 0.4095\n",
            "Epoch 133/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3592 - val_loss: 0.4067\n",
            "Epoch 134/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3679 - val_loss: 0.4074\n",
            "Epoch 135/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3637 - val_loss: 0.4084\n",
            "Epoch 136/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3414 - val_loss: 0.4074\n",
            "Epoch 137/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3562 - val_loss: 0.4076\n",
            "Epoch 138/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3563 - val_loss: 0.4075\n",
            "Epoch 139/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3592 - val_loss: 0.4084\n",
            "Epoch 140/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3522 - val_loss: 0.4071\n",
            "Epoch 141/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3542 - val_loss: 0.4060\n",
            "Epoch 142/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3538 - val_loss: 0.4059\n",
            "Epoch 143/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3550 - val_loss: 0.4063\n",
            "Epoch 144/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3487 - val_loss: 0.4067\n",
            "Epoch 145/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3661 - val_loss: 0.4058\n",
            "Epoch 146/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3660 - val_loss: 0.4047\n",
            "Epoch 147/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3384 - val_loss: 0.4044\n",
            "Epoch 148/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3417 - val_loss: 0.4058\n",
            "Epoch 149/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3568 - val_loss: 0.4059\n",
            "Epoch 150/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3489 - val_loss: 0.4059\n",
            "Epoch 151/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3549 - val_loss: 0.4065\n",
            "Epoch 152/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3413 - val_loss: 0.4054\n",
            "Epoch 153/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3446 - val_loss: 0.4063\n",
            "Epoch 154/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3396 - val_loss: 0.4062\n",
            "Epoch 155/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3579 - val_loss: 0.4049\n",
            "Epoch 156/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3558 - val_loss: 0.4069\n",
            "Epoch 157/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3491 - val_loss: 0.4053\n",
            "Epoch 158/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3565 - val_loss: 0.4048\n",
            "Epoch 159/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3585 - val_loss: 0.4048\n",
            "Epoch 160/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3314 - val_loss: 0.4047\n",
            "Epoch 161/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3536 - val_loss: 0.4039\n",
            "Epoch 162/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3464 - val_loss: 0.4035\n",
            "Epoch 163/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3429 - val_loss: 0.4021\n",
            "Epoch 164/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3602 - val_loss: 0.4008\n",
            "Epoch 165/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3396 - val_loss: 0.3998\n",
            "Epoch 166/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3391 - val_loss: 0.4012\n",
            "Epoch 167/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3369 - val_loss: 0.4021\n",
            "Epoch 168/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3294 - val_loss: 0.4016\n",
            "Epoch 169/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3353 - val_loss: 0.4025\n",
            "Epoch 170/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3402 - val_loss: 0.4003\n",
            "Epoch 171/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3460 - val_loss: 0.4004\n",
            "Epoch 172/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3359 - val_loss: 0.4009\n",
            "Epoch 173/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3296 - val_loss: 0.4009\n",
            "Epoch 174/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3391 - val_loss: 0.4006\n",
            "Epoch 175/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3436 - val_loss: 0.4008\n",
            "Epoch 176/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3269 - val_loss: 0.3983\n",
            "Epoch 177/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3398 - val_loss: 0.3975\n",
            "Epoch 178/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3361 - val_loss: 0.3978\n",
            "Epoch 179/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3421 - val_loss: 0.4003\n",
            "Epoch 180/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3338 - val_loss: 0.4005\n",
            "Epoch 181/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3339 - val_loss: 0.3985\n",
            "Epoch 182/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3368 - val_loss: 0.3984\n",
            "Epoch 183/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3272 - val_loss: 0.3995\n",
            "Epoch 184/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3319 - val_loss: 0.4004\n",
            "Epoch 185/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3351 - val_loss: 0.3984\n",
            "Epoch 186/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3325 - val_loss: 0.3994\n",
            "Epoch 187/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3204 - val_loss: 0.3973\n",
            "Epoch 188/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3364 - val_loss: 0.3976\n",
            "Epoch 189/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3390 - val_loss: 0.3979\n",
            "Epoch 190/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3284 - val_loss: 0.3970\n",
            "Epoch 191/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3281 - val_loss: 0.3972\n",
            "Epoch 192/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3210 - val_loss: 0.3977\n",
            "Epoch 193/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3267 - val_loss: 0.3974\n",
            "Epoch 194/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3251 - val_loss: 0.3956\n",
            "Epoch 195/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3354 - val_loss: 0.3967\n",
            "Epoch 196/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3286 - val_loss: 0.3965\n",
            "Epoch 197/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3301 - val_loss: 0.3965\n",
            "Epoch 198/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3372 - val_loss: 0.3956\n",
            "Epoch 199/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3240 - val_loss: 0.3955\n",
            "Epoch 200/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3282 - val_loss: 0.3941\n",
            "Epoch 201/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3248 - val_loss: 0.3947\n",
            "Epoch 202/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3232 - val_loss: 0.3944\n",
            "Epoch 203/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3329 - val_loss: 0.3941\n",
            "Epoch 204/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3211 - val_loss: 0.3940\n",
            "Epoch 205/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3289 - val_loss: 0.3939\n",
            "Epoch 206/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3229 - val_loss: 0.3934\n",
            "Epoch 207/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3177 - val_loss: 0.3935\n",
            "Epoch 208/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3179 - val_loss: 0.3928\n",
            "Epoch 209/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3251 - val_loss: 0.3926\n",
            "Epoch 210/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3103 - val_loss: 0.3924\n",
            "Epoch 211/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3217 - val_loss: 0.3937\n",
            "Epoch 212/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3201 - val_loss: 0.3937\n",
            "Epoch 213/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3326 - val_loss: 0.3924\n",
            "Epoch 214/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3304 - val_loss: 0.3928\n",
            "Epoch 215/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3180 - val_loss: 0.3933\n",
            "Epoch 216/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3136 - val_loss: 0.3931\n",
            "Epoch 217/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3159 - val_loss: 0.3936\n",
            "Epoch 218/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3191 - val_loss: 0.3935\n",
            "Epoch 219/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3155 - val_loss: 0.3951\n",
            "Epoch 220/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3172 - val_loss: 0.3953\n",
            "Epoch 221/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3175 - val_loss: 0.3932\n",
            "Epoch 222/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3236 - val_loss: 0.3942\n",
            "Epoch 223/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3116 - val_loss: 0.3941\n",
            "Epoch 224/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3239 - val_loss: 0.3931\n",
            "Epoch 225/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3210 - val_loss: 0.3934\n",
            "Epoch 226/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3186 - val_loss: 0.3934\n",
            "Epoch 227/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3205 - val_loss: 0.3950\n",
            "Epoch 228/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3193 - val_loss: 0.3939\n",
            "Epoch 229/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3101 - val_loss: 0.3945\n",
            "Epoch 230/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3200 - val_loss: 0.3942\n",
            "Epoch 231/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3125 - val_loss: 0.3942\n",
            "Epoch 232/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3014 - val_loss: 0.3958\n",
            "Epoch 233/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3130 - val_loss: 0.3946\n",
            "Epoch 234/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3151 - val_loss: 0.3949\n",
            "Epoch 235/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3197 - val_loss: 0.3944\n",
            "Epoch 236/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3268 - val_loss: 0.3924\n",
            "Epoch 237/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3002 - val_loss: 0.3927\n",
            "Epoch 238/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3219 - val_loss: 0.3938\n",
            "Epoch 239/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3094 - val_loss: 0.3927\n",
            "Epoch 240/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3056 - val_loss: 0.3926\n",
            "Epoch 241/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3091 - val_loss: 0.3932\n",
            "Epoch 242/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3173 - val_loss: 0.3923\n",
            "Epoch 243/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3131 - val_loss: 0.3927\n",
            "Epoch 244/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3096 - val_loss: 0.3920\n",
            "Epoch 245/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3160 - val_loss: 0.3918\n",
            "Epoch 246/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2998 - val_loss: 0.3916\n",
            "Epoch 247/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3019 - val_loss: 0.3913\n",
            "Epoch 248/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3230 - val_loss: 0.3917\n",
            "Epoch 249/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3082 - val_loss: 0.3904\n",
            "Epoch 250/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3217 - val_loss: 0.3914\n",
            "Epoch 251/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3117 - val_loss: 0.3910\n",
            "Epoch 252/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3227 - val_loss: 0.3930\n",
            "Epoch 253/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3126 - val_loss: 0.3924\n",
            "Epoch 254/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3091 - val_loss: 0.3906\n",
            "Epoch 255/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3115 - val_loss: 0.3896\n",
            "Epoch 256/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3037 - val_loss: 0.3917\n",
            "Epoch 257/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3054 - val_loss: 0.3919\n",
            "Epoch 258/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3114 - val_loss: 0.3918\n",
            "Epoch 259/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3139 - val_loss: 0.3918\n",
            "Epoch 260/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3155 - val_loss: 0.3916\n",
            "Epoch 261/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3138 - val_loss: 0.3883\n",
            "Epoch 262/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3103 - val_loss: 0.3885\n",
            "Epoch 263/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3018 - val_loss: 0.3902\n",
            "Epoch 264/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3005 - val_loss: 0.3898\n",
            "Epoch 265/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2950 - val_loss: 0.3897\n",
            "Epoch 266/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3004 - val_loss: 0.3909\n",
            "Epoch 267/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3027 - val_loss: 0.3911\n",
            "Epoch 268/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2983 - val_loss: 0.3903\n",
            "Epoch 269/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2996 - val_loss: 0.3901\n",
            "Epoch 270/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2963 - val_loss: 0.3898\n",
            "Epoch 271/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3078 - val_loss: 0.3896\n",
            "Epoch 272/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3020 - val_loss: 0.3897\n",
            "Epoch 273/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3056 - val_loss: 0.3908\n",
            "Epoch 274/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2996 - val_loss: 0.3918\n",
            "Epoch 275/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2921 - val_loss: 0.3920\n",
            "Epoch 276/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3077 - val_loss: 0.3908\n",
            "Epoch 277/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2978 - val_loss: 0.3907\n",
            "Epoch 278/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3110 - val_loss: 0.3908\n",
            "Epoch 279/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2985 - val_loss: 0.3907\n",
            "Epoch 280/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3017 - val_loss: 0.3887\n",
            "Epoch 281/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2946 - val_loss: 0.3891\n",
            "Epoch 282/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2962 - val_loss: 0.3885\n",
            "Epoch 283/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3005 - val_loss: 0.3887\n",
            "Epoch 284/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3037 - val_loss: 0.3896\n",
            "Epoch 285/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3035 - val_loss: 0.3899\n",
            "Epoch 286/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3033 - val_loss: 0.3890\n",
            "Epoch 287/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2919 - val_loss: 0.3872\n",
            "Epoch 288/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2947 - val_loss: 0.3871\n",
            "Epoch 289/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2991 - val_loss: 0.3892\n",
            "Epoch 290/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3013 - val_loss: 0.3891\n",
            "Epoch 291/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2929 - val_loss: 0.3901\n",
            "Epoch 292/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2939 - val_loss: 0.3906\n",
            "Epoch 293/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2951 - val_loss: 0.3899\n",
            "Epoch 294/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3047 - val_loss: 0.3908\n",
            "Epoch 295/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2974 - val_loss: 0.3895\n",
            "Epoch 296/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3064 - val_loss: 0.3904\n",
            "Epoch 297/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2993 - val_loss: 0.3910\n",
            "Epoch 298/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3022 - val_loss: 0.3890\n",
            "Epoch 299/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2908 - val_loss: 0.3894\n",
            "Epoch 300/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2984 - val_loss: 0.3904\n",
            "Epoch 301/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2807 - val_loss: 0.3891\n",
            "Epoch 302/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2944 - val_loss: 0.3895\n",
            "Epoch 303/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2896 - val_loss: 0.3902\n",
            "Epoch 304/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2922 - val_loss: 0.3887\n",
            "Epoch 305/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2898 - val_loss: 0.3891\n",
            "Epoch 306/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2984 - val_loss: 0.3903\n",
            "Epoch 307/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2970 - val_loss: 0.3901\n",
            "Epoch 308/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2935 - val_loss: 0.3891\n",
            "Epoch 309/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2914 - val_loss: 0.3904\n",
            "Epoch 310/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3037 - val_loss: 0.3917\n",
            "Epoch 311/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2960 - val_loss: 0.3918\n",
            "Epoch 312/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2839 - val_loss: 0.3923\n",
            "Epoch 313/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2920 - val_loss: 0.3920\n",
            "Epoch 314/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2887 - val_loss: 0.3915\n",
            "Epoch 315/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2916 - val_loss: 0.3918\n",
            "Epoch 316/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2970 - val_loss: 0.3904\n",
            "Epoch 317/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3019 - val_loss: 0.3902\n",
            "Epoch 318/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2861 - val_loss: 0.3919\n",
            "Epoch 319/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2960 - val_loss: 0.3919\n",
            "Epoch 320/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2841 - val_loss: 0.3919\n",
            "Epoch 321/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2893 - val_loss: 0.3930\n",
            "Epoch 322/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2886 - val_loss: 0.3924\n",
            "Epoch 323/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2988 - val_loss: 0.3937\n",
            "Epoch 324/1000\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2920 - val_loss: 0.3933\n",
            "Epoch 325/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2895 - val_loss: 0.3925\n",
            "Epoch 326/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2899 - val_loss: 0.3939\n",
            "Epoch 327/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2820 - val_loss: 0.3944\n",
            "Epoch 328/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2960 - val_loss: 0.3930\n",
            "Epoch 329/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2933 - val_loss: 0.3937\n",
            "Epoch 330/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2822 - val_loss: 0.3949\n",
            "Epoch 331/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2893 - val_loss: 0.3940\n",
            "Epoch 332/1000\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.2801 - val_loss: 0.3933\n",
            "Epoch 333/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2942 - val_loss: 0.3928\n",
            "Epoch 334/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2825 - val_loss: 0.3930\n",
            "Epoch 335/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2874 - val_loss: 0.3919\n",
            "Epoch 336/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2799 - val_loss: 0.3909\n",
            "Epoch 337/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2827 - val_loss: 0.3913\n",
            "Epoch 338/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2788 - val_loss: 0.3921\n",
            "Epoch 339/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.3946\n",
            "Epoch 340/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2811 - val_loss: 0.3941\n",
            "Epoch 341/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2869 - val_loss: 0.3929\n",
            "Epoch 342/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2872 - val_loss: 0.3933\n",
            "Epoch 343/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2824 - val_loss: 0.3929\n",
            "Epoch 344/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2855 - val_loss: 0.3947\n",
            "Epoch 345/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2891 - val_loss: 0.3935\n",
            "Epoch 346/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2902 - val_loss: 0.3937\n",
            "Epoch 347/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2836 - val_loss: 0.3920\n",
            "Epoch 348/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2792 - val_loss: 0.3928\n",
            "Epoch 349/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2823 - val_loss: 0.3933\n",
            "Epoch 350/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2857 - val_loss: 0.3944\n",
            "Epoch 351/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2861 - val_loss: 0.3949\n",
            "Epoch 352/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2787 - val_loss: 0.3936\n",
            "Epoch 353/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2752 - val_loss: 0.3934\n",
            "Epoch 354/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2870 - val_loss: 0.3934\n",
            "Epoch 355/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2844 - val_loss: 0.3944\n",
            "Epoch 356/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2759 - val_loss: 0.3918\n",
            "Epoch 357/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2772 - val_loss: 0.3904\n",
            "Epoch 358/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.3916\n",
            "Epoch 359/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.3915\n",
            "Epoch 360/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2740 - val_loss: 0.3915\n",
            "Epoch 361/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2889 - val_loss: 0.3912\n",
            "Epoch 362/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2802 - val_loss: 0.3920\n",
            "Epoch 363/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2773 - val_loss: 0.3933\n",
            "Epoch 364/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2870 - val_loss: 0.3946\n",
            "Epoch 365/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2761 - val_loss: 0.3959\n",
            "Epoch 366/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2785 - val_loss: 0.3961\n",
            "Epoch 367/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2738 - val_loss: 0.3966\n",
            "Epoch 368/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2854 - val_loss: 0.3975\n",
            "Epoch 369/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2706 - val_loss: 0.3963\n",
            "Epoch 370/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2840 - val_loss: 0.3940\n",
            "Epoch 371/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2775 - val_loss: 0.3958\n",
            "Epoch 372/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2795 - val_loss: 0.3965\n",
            "Epoch 373/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2682 - val_loss: 0.3957\n",
            "Epoch 374/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2787 - val_loss: 0.3950\n",
            "Epoch 375/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2769 - val_loss: 0.3948\n",
            "Epoch 376/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2833 - val_loss: 0.3941\n",
            "Epoch 377/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2856 - val_loss: 0.3949\n",
            "Epoch 378/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.3940\n",
            "Epoch 379/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2813 - val_loss: 0.3935\n",
            "Epoch 380/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2756 - val_loss: 0.3927\n",
            "Epoch 381/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2730 - val_loss: 0.3928\n",
            "Epoch 382/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2655 - val_loss: 0.3942\n",
            "Epoch 383/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2702 - val_loss: 0.3937\n",
            "Epoch 384/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2749 - val_loss: 0.3930\n",
            "Epoch 385/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2672 - val_loss: 0.3936\n",
            "Epoch 386/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2718 - val_loss: 0.3933\n",
            "Epoch 387/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2720 - val_loss: 0.3919\n",
            "Epoch 388/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2810 - val_loss: 0.3933\n",
            "Epoch 389/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2744 - val_loss: 0.3934\n",
            "Epoch 390/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2806 - val_loss: 0.3923\n",
            "Epoch 391/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2626 - val_loss: 0.3926\n",
            "Epoch 392/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2742 - val_loss: 0.3927\n",
            "Epoch 393/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2773 - val_loss: 0.3924\n",
            "Epoch 394/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2755 - val_loss: 0.3922\n",
            "Epoch 395/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2711 - val_loss: 0.3908\n",
            "Epoch 396/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2821 - val_loss: 0.3890\n",
            "Epoch 397/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2707 - val_loss: 0.3895\n",
            "Epoch 398/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2775 - val_loss: 0.3904\n",
            "Epoch 399/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2758 - val_loss: 0.3921\n",
            "Epoch 400/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2656 - val_loss: 0.3930\n",
            "Epoch 401/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2794 - val_loss: 0.3925\n",
            "Epoch 402/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2838 - val_loss: 0.3913\n",
            "Epoch 403/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2672 - val_loss: 0.3923\n",
            "Epoch 404/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2703 - val_loss: 0.3913\n",
            "Epoch 405/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2578 - val_loss: 0.3918\n",
            "Epoch 406/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2834 - val_loss: 0.3924\n",
            "Epoch 407/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2813 - val_loss: 0.3928\n",
            "Epoch 408/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2684 - val_loss: 0.3937\n",
            "Epoch 409/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2777 - val_loss: 0.3920\n",
            "Epoch 410/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2706 - val_loss: 0.3922\n",
            "Epoch 411/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2690 - val_loss: 0.3919\n",
            "Epoch 412/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2824 - val_loss: 0.3928\n",
            "Epoch 413/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2611 - val_loss: 0.3941\n",
            "Epoch 414/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2646 - val_loss: 0.3925\n",
            "Epoch 415/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2662 - val_loss: 0.3920\n",
            "Epoch 416/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2762 - val_loss: 0.3921\n",
            "Epoch 417/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2674 - val_loss: 0.3912\n",
            "Epoch 418/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2730 - val_loss: 0.3929\n",
            "Epoch 419/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2753 - val_loss: 0.3925\n",
            "Epoch 420/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2690 - val_loss: 0.3931\n",
            "Epoch 421/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.3951\n",
            "Epoch 422/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2710 - val_loss: 0.3936\n",
            "Epoch 423/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2732 - val_loss: 0.3922\n",
            "Epoch 424/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2737 - val_loss: 0.3939\n",
            "Epoch 425/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2800 - val_loss: 0.3928\n",
            "Epoch 426/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2570 - val_loss: 0.3920\n",
            "Epoch 427/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2780 - val_loss: 0.3933\n",
            "Epoch 428/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2729 - val_loss: 0.3925\n",
            "Epoch 429/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.3957\n",
            "Epoch 430/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2630 - val_loss: 0.3930\n",
            "Epoch 431/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2560 - val_loss: 0.3931\n",
            "Epoch 432/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2688 - val_loss: 0.3930\n",
            "Epoch 433/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2572 - val_loss: 0.3936\n",
            "Epoch 434/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2676 - val_loss: 0.3920\n",
            "Epoch 435/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2780 - val_loss: 0.3909\n",
            "Epoch 436/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2769 - val_loss: 0.3918\n",
            "Epoch 437/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2697 - val_loss: 0.3904\n",
            "Epoch 438/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2653 - val_loss: 0.3899\n",
            "Epoch 439/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2641 - val_loss: 0.3913\n",
            "Epoch 440/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2676 - val_loss: 0.3924\n",
            "Epoch 441/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2643 - val_loss: 0.3931\n",
            "Epoch 442/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.3924\n",
            "Epoch 443/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2651 - val_loss: 0.3935\n",
            "Epoch 444/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2619 - val_loss: 0.3929\n",
            "Epoch 445/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2605 - val_loss: 0.3922\n",
            "Epoch 446/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2649 - val_loss: 0.3907\n",
            "Epoch 447/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2701 - val_loss: 0.3903\n",
            "Epoch 448/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2600 - val_loss: 0.3900\n",
            "Epoch 449/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2643 - val_loss: 0.3882\n",
            "Epoch 450/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2779 - val_loss: 0.3911\n",
            "Epoch 451/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2671 - val_loss: 0.3897\n",
            "Epoch 452/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2686 - val_loss: 0.3911\n",
            "Epoch 453/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2754 - val_loss: 0.3914\n",
            "Epoch 454/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2632 - val_loss: 0.3931\n",
            "Epoch 455/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2579 - val_loss: 0.3907\n",
            "Epoch 456/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2538 - val_loss: 0.3895\n",
            "Epoch 457/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2534 - val_loss: 0.3886\n",
            "Epoch 458/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2680 - val_loss: 0.3877\n",
            "Epoch 459/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2633 - val_loss: 0.3899\n",
            "Epoch 460/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2588 - val_loss: 0.3903\n",
            "Epoch 461/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2676 - val_loss: 0.3888\n",
            "Epoch 462/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2624 - val_loss: 0.3888\n",
            "Epoch 463/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2556 - val_loss: 0.3892\n",
            "Epoch 464/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2639 - val_loss: 0.3900\n",
            "Epoch 465/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2679 - val_loss: 0.3901\n",
            "Epoch 466/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2658 - val_loss: 0.3873\n",
            "Epoch 467/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2666 - val_loss: 0.3880\n",
            "Epoch 468/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2705 - val_loss: 0.3872\n",
            "Epoch 469/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2691 - val_loss: 0.3883\n",
            "Epoch 470/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2600 - val_loss: 0.3895\n",
            "Epoch 471/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2647 - val_loss: 0.3904\n",
            "Epoch 472/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2754 - val_loss: 0.3914\n",
            "Epoch 473/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2630 - val_loss: 0.3918\n",
            "Epoch 474/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2552 - val_loss: 0.3887\n",
            "Epoch 475/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2623 - val_loss: 0.3879\n",
            "Epoch 476/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2652 - val_loss: 0.3884\n",
            "Epoch 477/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2567 - val_loss: 0.3879\n",
            "Epoch 478/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2541 - val_loss: 0.3873\n",
            "Epoch 479/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2608 - val_loss: 0.3881\n",
            "Epoch 480/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2612 - val_loss: 0.3862\n",
            "Epoch 481/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2523 - val_loss: 0.3878\n",
            "Epoch 482/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2550 - val_loss: 0.3880\n",
            "Epoch 483/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2658 - val_loss: 0.3882\n",
            "Epoch 484/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2589 - val_loss: 0.3896\n",
            "Epoch 485/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2481 - val_loss: 0.3905\n",
            "Epoch 486/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2567 - val_loss: 0.3879\n",
            "Epoch 487/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2587 - val_loss: 0.3863\n",
            "Epoch 488/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.3882\n",
            "Epoch 489/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2611 - val_loss: 0.3873\n",
            "Epoch 490/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2554 - val_loss: 0.3861\n",
            "Epoch 491/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2570 - val_loss: 0.3873\n",
            "Epoch 492/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2589 - val_loss: 0.3866\n",
            "Epoch 493/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2460 - val_loss: 0.3873\n",
            "Epoch 494/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2542 - val_loss: 0.3869\n",
            "Epoch 495/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2554 - val_loss: 0.3875\n",
            "Epoch 496/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2739 - val_loss: 0.3866\n",
            "Epoch 497/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2553 - val_loss: 0.3867\n",
            "Epoch 498/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2624 - val_loss: 0.3868\n",
            "Epoch 499/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2628 - val_loss: 0.3865\n",
            "Epoch 500/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2618 - val_loss: 0.3869\n",
            "Epoch 501/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2529 - val_loss: 0.3859\n",
            "Epoch 502/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2595 - val_loss: 0.3860\n",
            "Epoch 503/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2498 - val_loss: 0.3853\n",
            "Epoch 504/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2629 - val_loss: 0.3850\n",
            "Epoch 505/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2582 - val_loss: 0.3839\n",
            "Epoch 506/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2564 - val_loss: 0.3847\n",
            "Epoch 507/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2652 - val_loss: 0.3871\n",
            "Epoch 508/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2455 - val_loss: 0.3870\n",
            "Epoch 509/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2465 - val_loss: 0.3904\n",
            "Epoch 510/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2461 - val_loss: 0.3879\n",
            "Epoch 511/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2567 - val_loss: 0.3885\n",
            "Epoch 512/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2404 - val_loss: 0.3882\n",
            "Epoch 513/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2550 - val_loss: 0.3884\n",
            "Epoch 514/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2521 - val_loss: 0.3896\n",
            "Epoch 515/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2645 - val_loss: 0.3901\n",
            "Epoch 516/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2535 - val_loss: 0.3897\n",
            "Epoch 517/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2557 - val_loss: 0.3895\n",
            "Epoch 518/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2518 - val_loss: 0.3895\n",
            "Epoch 519/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2513 - val_loss: 0.3888\n",
            "Epoch 520/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2536 - val_loss: 0.3916\n",
            "Epoch 521/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2642 - val_loss: 0.3898\n",
            "Epoch 522/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2558 - val_loss: 0.3896\n",
            "Epoch 523/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2513 - val_loss: 0.3896\n",
            "Epoch 524/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2550 - val_loss: 0.3905\n",
            "Epoch 525/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.3885\n",
            "Epoch 526/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2562 - val_loss: 0.3886\n",
            "Epoch 527/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2581 - val_loss: 0.3859\n",
            "Epoch 528/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2549 - val_loss: 0.3866\n",
            "Epoch 529/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2427 - val_loss: 0.3863\n",
            "Epoch 530/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2665 - val_loss: 0.3855\n",
            "Epoch 531/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2601 - val_loss: 0.3877\n",
            "Epoch 532/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2522 - val_loss: 0.3875\n",
            "Epoch 533/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2485 - val_loss: 0.3858\n",
            "Epoch 534/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2530 - val_loss: 0.3848\n",
            "Epoch 535/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2583 - val_loss: 0.3856\n",
            "Epoch 536/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2456 - val_loss: 0.3862\n",
            "Epoch 537/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2492 - val_loss: 0.3871\n",
            "Epoch 538/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2500 - val_loss: 0.3875\n",
            "Epoch 539/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2441 - val_loss: 0.3872\n",
            "Epoch 540/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2510 - val_loss: 0.3879\n",
            "Epoch 541/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2527 - val_loss: 0.3867\n",
            "Epoch 542/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2482 - val_loss: 0.3866\n",
            "Epoch 543/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2416 - val_loss: 0.3865\n",
            "Epoch 544/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2522 - val_loss: 0.3866\n",
            "Epoch 545/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2451 - val_loss: 0.3867\n",
            "Epoch 546/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2662 - val_loss: 0.3876\n",
            "Epoch 547/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2474 - val_loss: 0.3874\n",
            "Epoch 548/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2476 - val_loss: 0.3857\n",
            "Epoch 549/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2618 - val_loss: 0.3851\n",
            "Epoch 550/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.3856\n",
            "Epoch 551/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2432 - val_loss: 0.3854\n",
            "Epoch 552/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2480 - val_loss: 0.3853\n",
            "Epoch 553/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2502 - val_loss: 0.3851\n",
            "Epoch 554/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2587 - val_loss: 0.3845\n",
            "Epoch 555/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2397 - val_loss: 0.3838\n",
            "Epoch 556/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2410 - val_loss: 0.3856\n",
            "Epoch 557/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2439 - val_loss: 0.3852\n",
            "Epoch 558/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2522 - val_loss: 0.3862\n",
            "Epoch 559/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2488 - val_loss: 0.3852\n",
            "Epoch 560/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2394 - val_loss: 0.3844\n",
            "Epoch 561/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2487 - val_loss: 0.3851\n",
            "Epoch 562/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2452 - val_loss: 0.3866\n",
            "Epoch 563/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2583 - val_loss: 0.3858\n",
            "Epoch 564/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2499 - val_loss: 0.3857\n",
            "Epoch 565/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2472 - val_loss: 0.3857\n",
            "Epoch 566/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2366 - val_loss: 0.3861\n",
            "Epoch 567/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2527 - val_loss: 0.3867\n",
            "Epoch 568/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2472 - val_loss: 0.3853\n",
            "Epoch 569/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2486 - val_loss: 0.3846\n",
            "Epoch 570/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2364 - val_loss: 0.3838\n",
            "Epoch 571/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2339 - val_loss: 0.3854\n",
            "Epoch 572/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2532 - val_loss: 0.3875\n",
            "Epoch 573/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.3877\n",
            "Epoch 574/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.3857\n",
            "Epoch 575/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2440 - val_loss: 0.3858\n",
            "Epoch 576/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2450 - val_loss: 0.3855\n",
            "Epoch 577/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2459 - val_loss: 0.3854\n",
            "Epoch 578/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2511 - val_loss: 0.3838\n",
            "Epoch 579/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2496 - val_loss: 0.3838\n",
            "Epoch 580/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2440 - val_loss: 0.3835\n",
            "Epoch 581/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2517 - val_loss: 0.3823\n",
            "Epoch 582/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2506 - val_loss: 0.3841\n",
            "Epoch 583/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2359 - val_loss: 0.3858\n",
            "Epoch 584/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2399 - val_loss: 0.3856\n",
            "Epoch 585/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2476 - val_loss: 0.3862\n",
            "Epoch 586/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2407 - val_loss: 0.3845\n",
            "Epoch 587/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2392 - val_loss: 0.3855\n",
            "Epoch 588/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2476 - val_loss: 0.3850\n",
            "Epoch 589/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2382 - val_loss: 0.3844\n",
            "Epoch 590/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2416 - val_loss: 0.3861\n",
            "Epoch 591/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2475 - val_loss: 0.3864\n",
            "Epoch 592/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2363 - val_loss: 0.3859\n",
            "Epoch 593/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2324 - val_loss: 0.3852\n",
            "Epoch 594/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2368 - val_loss: 0.3848\n",
            "Epoch 595/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2485 - val_loss: 0.3858\n",
            "Epoch 596/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2475 - val_loss: 0.3853\n",
            "Epoch 597/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2457 - val_loss: 0.3846\n",
            "Epoch 598/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2491 - val_loss: 0.3865\n",
            "Epoch 599/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2387 - val_loss: 0.3845\n",
            "Epoch 600/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2437 - val_loss: 0.3850\n",
            "Epoch 601/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2424 - val_loss: 0.3832\n",
            "Epoch 602/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2488 - val_loss: 0.3851\n",
            "Epoch 603/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2296 - val_loss: 0.3860\n",
            "Epoch 604/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2385 - val_loss: 0.3863\n",
            "Epoch 605/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2426 - val_loss: 0.3857\n",
            "Epoch 606/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2466 - val_loss: 0.3850\n",
            "Epoch 607/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2458 - val_loss: 0.3835\n",
            "Epoch 608/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2399 - val_loss: 0.3852\n",
            "Epoch 609/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2402 - val_loss: 0.3822\n",
            "Epoch 610/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2494 - val_loss: 0.3812\n",
            "Epoch 611/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2503 - val_loss: 0.3800\n",
            "Epoch 612/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2387 - val_loss: 0.3810\n",
            "Epoch 613/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2452 - val_loss: 0.3808\n",
            "Epoch 614/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2370 - val_loss: 0.3816\n",
            "Epoch 615/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2379 - val_loss: 0.3816\n",
            "Epoch 616/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2394 - val_loss: 0.3805\n",
            "Epoch 617/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2325 - val_loss: 0.3820\n",
            "Epoch 618/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2459 - val_loss: 0.3813\n",
            "Epoch 619/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2326 - val_loss: 0.3799\n",
            "Epoch 620/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2391 - val_loss: 0.3825\n",
            "Epoch 621/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2312 - val_loss: 0.3822\n",
            "Epoch 622/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.3832\n",
            "Epoch 623/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2325 - val_loss: 0.3816\n",
            "Epoch 624/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2328 - val_loss: 0.3814\n",
            "Epoch 625/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2444 - val_loss: 0.3808\n",
            "Epoch 626/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2444 - val_loss: 0.3796\n",
            "Epoch 627/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2423 - val_loss: 0.3800\n",
            "Epoch 628/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2318 - val_loss: 0.3809\n",
            "Epoch 629/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2492 - val_loss: 0.3811\n",
            "Epoch 630/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2363 - val_loss: 0.3805\n",
            "Epoch 631/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2395 - val_loss: 0.3814\n",
            "Epoch 632/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2353 - val_loss: 0.3796\n",
            "Epoch 633/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2455 - val_loss: 0.3789\n",
            "Epoch 634/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2396 - val_loss: 0.3778\n",
            "Epoch 635/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.3774\n",
            "Epoch 636/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2347 - val_loss: 0.3796\n",
            "Epoch 637/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2470 - val_loss: 0.3788\n",
            "Epoch 638/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2355 - val_loss: 0.3785\n",
            "Epoch 639/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2432 - val_loss: 0.3796\n",
            "Epoch 640/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2443 - val_loss: 0.3776\n",
            "Epoch 641/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2318 - val_loss: 0.3776\n",
            "Epoch 642/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2372 - val_loss: 0.3774\n",
            "Epoch 643/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2349 - val_loss: 0.3780\n",
            "Epoch 644/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2403 - val_loss: 0.3756\n",
            "Epoch 645/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2370 - val_loss: 0.3760\n",
            "Epoch 646/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2370 - val_loss: 0.3766\n",
            "Epoch 647/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.3758\n",
            "Epoch 648/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2429 - val_loss: 0.3765\n",
            "Epoch 649/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2419 - val_loss: 0.3768\n",
            "Epoch 650/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2354 - val_loss: 0.3759\n",
            "Epoch 651/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2268 - val_loss: 0.3769\n",
            "Epoch 652/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2350 - val_loss: 0.3751\n",
            "Epoch 653/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.3762\n",
            "Epoch 654/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.3764\n",
            "Epoch 655/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2495 - val_loss: 0.3756\n",
            "Epoch 656/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2364 - val_loss: 0.3772\n",
            "Epoch 657/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2457 - val_loss: 0.3767\n",
            "Epoch 658/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2389 - val_loss: 0.3757\n",
            "Epoch 659/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2436 - val_loss: 0.3766\n",
            "Epoch 660/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2323 - val_loss: 0.3760\n",
            "Epoch 661/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2400 - val_loss: 0.3755\n",
            "Epoch 662/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2335 - val_loss: 0.3740\n",
            "Epoch 663/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2311 - val_loss: 0.3762\n",
            "Epoch 664/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2330 - val_loss: 0.3763\n",
            "Epoch 665/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2403 - val_loss: 0.3743\n",
            "Epoch 666/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2454 - val_loss: 0.3745\n",
            "Epoch 667/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2280 - val_loss: 0.3766\n",
            "Epoch 668/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.3765\n",
            "Epoch 669/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2420 - val_loss: 0.3780\n",
            "Epoch 670/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2327 - val_loss: 0.3770\n",
            "Epoch 671/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2343 - val_loss: 0.3763\n",
            "Epoch 672/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.3749\n",
            "Epoch 673/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.3748\n",
            "Epoch 674/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2341 - val_loss: 0.3776\n",
            "Epoch 675/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2405 - val_loss: 0.3765\n",
            "Epoch 676/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2428 - val_loss: 0.3743\n",
            "Epoch 677/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2404 - val_loss: 0.3759\n",
            "Epoch 678/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2316 - val_loss: 0.3752\n",
            "Epoch 679/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2464 - val_loss: 0.3724\n",
            "Epoch 680/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2291 - val_loss: 0.3741\n",
            "Epoch 681/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2374 - val_loss: 0.3753\n",
            "Epoch 682/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2322 - val_loss: 0.3769\n",
            "Epoch 683/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.3743\n",
            "Epoch 684/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2354 - val_loss: 0.3756\n",
            "Epoch 685/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2268 - val_loss: 0.3760\n",
            "Epoch 686/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2303 - val_loss: 0.3771\n",
            "Epoch 687/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2421 - val_loss: 0.3763\n",
            "Epoch 688/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2298 - val_loss: 0.3764\n",
            "Epoch 689/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.3772\n",
            "Epoch 690/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2400 - val_loss: 0.3763\n",
            "Epoch 691/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2422 - val_loss: 0.3761\n",
            "Epoch 692/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2261 - val_loss: 0.3755\n",
            "Epoch 693/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2321 - val_loss: 0.3752\n",
            "Epoch 694/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2324 - val_loss: 0.3748\n",
            "Epoch 695/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2400 - val_loss: 0.3766\n",
            "Epoch 696/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2335 - val_loss: 0.3756\n",
            "Epoch 697/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.3755\n",
            "Epoch 698/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2365 - val_loss: 0.3744\n",
            "Epoch 699/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2340 - val_loss: 0.3723\n",
            "Epoch 700/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2389 - val_loss: 0.3719\n",
            "Epoch 701/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2367 - val_loss: 0.3710\n",
            "Epoch 702/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2271 - val_loss: 0.3723\n",
            "Epoch 703/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2401 - val_loss: 0.3733\n",
            "Epoch 704/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2268 - val_loss: 0.3726\n",
            "Epoch 705/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2306 - val_loss: 0.3729\n",
            "Epoch 706/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.3726\n",
            "Epoch 707/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2315 - val_loss: 0.3720\n",
            "Epoch 708/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2256 - val_loss: 0.3709\n",
            "Epoch 709/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2249 - val_loss: 0.3716\n",
            "Epoch 710/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2362 - val_loss: 0.3698\n",
            "Epoch 711/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2301 - val_loss: 0.3680\n",
            "Epoch 712/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2261 - val_loss: 0.3690\n",
            "Epoch 713/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2258 - val_loss: 0.3700\n",
            "Epoch 714/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2263 - val_loss: 0.3709\n",
            "Epoch 715/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2142 - val_loss: 0.3713\n",
            "Epoch 716/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2223 - val_loss: 0.3698\n",
            "Epoch 717/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2243 - val_loss: 0.3709\n",
            "Epoch 718/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2303 - val_loss: 0.3695\n",
            "Epoch 719/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2260 - val_loss: 0.3695\n",
            "Epoch 720/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2345 - val_loss: 0.3701\n",
            "Epoch 721/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.3687\n",
            "Epoch 722/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2216 - val_loss: 0.3696\n",
            "Epoch 723/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2218 - val_loss: 0.3709\n",
            "Epoch 724/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2276 - val_loss: 0.3724\n",
            "Epoch 725/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2270 - val_loss: 0.3714\n",
            "Epoch 726/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2282 - val_loss: 0.3705\n",
            "Epoch 727/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2246 - val_loss: 0.3715\n",
            "Epoch 728/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2277 - val_loss: 0.3712\n",
            "Epoch 729/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.3722\n",
            "Epoch 730/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2282 - val_loss: 0.3703\n",
            "Epoch 731/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2194 - val_loss: 0.3683\n",
            "Epoch 732/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2395 - val_loss: 0.3681\n",
            "Epoch 733/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2192 - val_loss: 0.3691\n",
            "Epoch 734/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2300 - val_loss: 0.3711\n",
            "Epoch 735/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2257 - val_loss: 0.3695\n",
            "Epoch 736/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2344 - val_loss: 0.3667\n",
            "Epoch 737/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.3669\n",
            "Epoch 738/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2321 - val_loss: 0.3669\n",
            "Epoch 739/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.3652\n",
            "Epoch 740/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2323 - val_loss: 0.3656\n",
            "Epoch 741/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2292 - val_loss: 0.3642\n",
            "Epoch 742/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2310 - val_loss: 0.3634\n",
            "Epoch 743/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.3636\n",
            "Epoch 744/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2309 - val_loss: 0.3636\n",
            "Epoch 745/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2108 - val_loss: 0.3658\n",
            "Epoch 746/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2274 - val_loss: 0.3669\n",
            "Epoch 747/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2237 - val_loss: 0.3658\n",
            "Epoch 748/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2310 - val_loss: 0.3678\n",
            "Epoch 749/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.3668\n",
            "Epoch 750/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2267 - val_loss: 0.3662\n",
            "Epoch 751/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2332 - val_loss: 0.3654\n",
            "Epoch 752/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2368 - val_loss: 0.3646\n",
            "Epoch 753/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2230 - val_loss: 0.3642\n",
            "Epoch 754/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2333 - val_loss: 0.3645\n",
            "Epoch 755/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2290 - val_loss: 0.3657\n",
            "Epoch 756/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2352 - val_loss: 0.3667\n",
            "Epoch 757/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2302 - val_loss: 0.3678\n",
            "Epoch 758/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2361 - val_loss: 0.3669\n",
            "Epoch 759/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2221 - val_loss: 0.3694\n",
            "Epoch 760/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2166 - val_loss: 0.3680\n",
            "Epoch 761/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2269 - val_loss: 0.3657\n",
            "Epoch 762/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2177 - val_loss: 0.3644\n",
            "Epoch 763/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2223 - val_loss: 0.3636\n",
            "Epoch 764/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2227 - val_loss: 0.3648\n",
            "Epoch 765/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2163 - val_loss: 0.3635\n",
            "Epoch 766/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2170 - val_loss: 0.3622\n",
            "Epoch 767/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2267 - val_loss: 0.3636\n",
            "Epoch 768/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2276 - val_loss: 0.3626\n",
            "Epoch 769/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2343 - val_loss: 0.3649\n",
            "Epoch 770/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.3644\n",
            "Epoch 771/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2322 - val_loss: 0.3644\n",
            "Epoch 772/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2199 - val_loss: 0.3653\n",
            "Epoch 773/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2202 - val_loss: 0.3647\n",
            "Epoch 774/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2240 - val_loss: 0.3648\n",
            "Epoch 775/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2260 - val_loss: 0.3659\n",
            "Epoch 776/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2233 - val_loss: 0.3647\n",
            "Epoch 777/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.3662\n",
            "Epoch 778/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.3645\n",
            "Epoch 779/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2323 - val_loss: 0.3659\n",
            "Epoch 780/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2173 - val_loss: 0.3651\n",
            "Epoch 781/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2304 - val_loss: 0.3666\n",
            "Epoch 782/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2331 - val_loss: 0.3682\n",
            "Epoch 783/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2230 - val_loss: 0.3680\n",
            "Epoch 784/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2283 - val_loss: 0.3676\n",
            "Epoch 785/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2308 - val_loss: 0.3666\n",
            "Epoch 786/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2258 - val_loss: 0.3652\n",
            "Epoch 787/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2313 - val_loss: 0.3649\n",
            "Epoch 788/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2294 - val_loss: 0.3649\n",
            "Epoch 789/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2248 - val_loss: 0.3642\n",
            "Epoch 790/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2097 - val_loss: 0.3626\n",
            "Epoch 791/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2182 - val_loss: 0.3642\n",
            "Epoch 792/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2227 - val_loss: 0.3650\n",
            "Epoch 793/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2124 - val_loss: 0.3642\n",
            "Epoch 794/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2305 - val_loss: 0.3664\n",
            "Epoch 795/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2109 - val_loss: 0.3665\n",
            "Epoch 796/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2242 - val_loss: 0.3663\n",
            "Epoch 797/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2180 - val_loss: 0.3666\n",
            "Epoch 798/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2227 - val_loss: 0.3659\n",
            "Epoch 799/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2217 - val_loss: 0.3650\n",
            "Epoch 800/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2140 - val_loss: 0.3649\n",
            "Epoch 801/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2244 - val_loss: 0.3645\n",
            "Epoch 802/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.3656\n",
            "Epoch 803/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2307 - val_loss: 0.3648\n",
            "Epoch 804/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2184 - val_loss: 0.3640\n",
            "Epoch 805/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2212 - val_loss: 0.3637\n",
            "Epoch 806/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2123 - val_loss: 0.3636\n",
            "Epoch 807/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2187 - val_loss: 0.3651\n",
            "Epoch 808/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2147 - val_loss: 0.3650\n",
            "Epoch 809/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2157 - val_loss: 0.3642\n",
            "Epoch 810/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2201 - val_loss: 0.3630\n",
            "Epoch 811/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2242 - val_loss: 0.3633\n",
            "Epoch 812/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2237 - val_loss: 0.3641\n",
            "Epoch 813/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2263 - val_loss: 0.3630\n",
            "Epoch 814/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2214 - val_loss: 0.3623\n",
            "Epoch 815/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2181 - val_loss: 0.3645\n",
            "Epoch 816/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2232 - val_loss: 0.3645\n",
            "Epoch 817/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2319 - val_loss: 0.3632\n",
            "Epoch 818/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2175 - val_loss: 0.3633\n",
            "Epoch 819/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2161 - val_loss: 0.3628\n",
            "Epoch 820/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2139 - val_loss: 0.3630\n",
            "Epoch 821/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.3644\n",
            "Epoch 822/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2246 - val_loss: 0.3637\n",
            "Epoch 823/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2275 - val_loss: 0.3622\n",
            "Epoch 824/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2162 - val_loss: 0.3623\n",
            "Epoch 825/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2239 - val_loss: 0.3626\n",
            "Epoch 826/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2192 - val_loss: 0.3632\n",
            "Epoch 827/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2200 - val_loss: 0.3615\n",
            "Epoch 828/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2155 - val_loss: 0.3619\n",
            "Epoch 829/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2196 - val_loss: 0.3615\n",
            "Epoch 830/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2169 - val_loss: 0.3611\n",
            "Epoch 831/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2177 - val_loss: 0.3634\n",
            "Epoch 832/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2265 - val_loss: 0.3610\n",
            "Epoch 833/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2102 - val_loss: 0.3600\n",
            "Epoch 834/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2147 - val_loss: 0.3609\n",
            "Epoch 835/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2257 - val_loss: 0.3605\n",
            "Epoch 836/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2105 - val_loss: 0.3601\n",
            "Epoch 837/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2151 - val_loss: 0.3598\n",
            "Epoch 838/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2158 - val_loss: 0.3592\n",
            "Epoch 839/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2105 - val_loss: 0.3600\n",
            "Epoch 840/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.3612\n",
            "Epoch 841/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.3595\n",
            "Epoch 842/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2122 - val_loss: 0.3596\n",
            "Epoch 843/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2082 - val_loss: 0.3574\n",
            "Epoch 844/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.3582\n",
            "Epoch 845/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2107 - val_loss: 0.3588\n",
            "Epoch 846/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2168 - val_loss: 0.3592\n",
            "Epoch 847/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.2184 - val_loss: 0.3594\n",
            "Epoch 848/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2251 - val_loss: 0.3584\n",
            "Epoch 849/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2092 - val_loss: 0.3594\n",
            "Epoch 850/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2213 - val_loss: 0.3588\n",
            "Epoch 851/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2082 - val_loss: 0.3600\n",
            "Epoch 852/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2190 - val_loss: 0.3609\n",
            "Epoch 853/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2252 - val_loss: 0.3587\n",
            "Epoch 854/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2171 - val_loss: 0.3600\n",
            "Epoch 855/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2190 - val_loss: 0.3606\n",
            "Epoch 856/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2144 - val_loss: 0.3590\n",
            "Epoch 857/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2142 - val_loss: 0.3609\n",
            "Epoch 858/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2234 - val_loss: 0.3595\n",
            "Epoch 859/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2085 - val_loss: 0.3586\n",
            "Epoch 860/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2098 - val_loss: 0.3589\n",
            "Epoch 861/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2118 - val_loss: 0.3571\n",
            "Epoch 862/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2051 - val_loss: 0.3565\n",
            "Epoch 863/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2209 - val_loss: 0.3542\n",
            "Epoch 864/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2158 - val_loss: 0.3542\n",
            "Epoch 865/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2102 - val_loss: 0.3548\n",
            "Epoch 866/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2111 - val_loss: 0.3547\n",
            "Epoch 867/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2208 - val_loss: 0.3553\n",
            "Epoch 868/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2188 - val_loss: 0.3547\n",
            "Epoch 869/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2146 - val_loss: 0.3554\n",
            "Epoch 870/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2055 - val_loss: 0.3554\n",
            "Epoch 871/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2078 - val_loss: 0.3571\n",
            "Epoch 872/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.3569\n",
            "Epoch 873/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2117 - val_loss: 0.3564\n",
            "Epoch 874/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2182 - val_loss: 0.3570\n",
            "Epoch 875/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2212 - val_loss: 0.3571\n",
            "Epoch 876/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2138 - val_loss: 0.3580\n",
            "Epoch 877/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2079 - val_loss: 0.3586\n",
            "Epoch 878/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2127 - val_loss: 0.3551\n",
            "Epoch 879/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2185 - val_loss: 0.3548\n",
            "Epoch 880/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2222 - val_loss: 0.3548\n",
            "Epoch 881/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2144 - val_loss: 0.3550\n",
            "Epoch 882/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2152 - val_loss: 0.3552\n",
            "Epoch 883/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2160 - val_loss: 0.3554\n",
            "Epoch 884/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2137 - val_loss: 0.3549\n",
            "Epoch 885/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2090 - val_loss: 0.3563\n",
            "Epoch 886/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2104 - val_loss: 0.3561\n",
            "Epoch 887/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2027 - val_loss: 0.3547\n",
            "Epoch 888/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2108 - val_loss: 0.3565\n",
            "Epoch 889/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.3546\n",
            "Epoch 890/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2183 - val_loss: 0.3539\n",
            "Epoch 891/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2110 - val_loss: 0.3525\n",
            "Epoch 892/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2140 - val_loss: 0.3509\n",
            "Epoch 893/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2143 - val_loss: 0.3501\n",
            "Epoch 894/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2090 - val_loss: 0.3514\n",
            "Epoch 895/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2113 - val_loss: 0.3506\n",
            "Epoch 896/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2153 - val_loss: 0.3532\n",
            "Epoch 897/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.3539\n",
            "Epoch 898/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2110 - val_loss: 0.3534\n",
            "Epoch 899/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2015 - val_loss: 0.3519\n",
            "Epoch 900/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2121 - val_loss: 0.3507\n",
            "Epoch 901/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.3511\n",
            "Epoch 902/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2162 - val_loss: 0.3517\n",
            "Epoch 903/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2106 - val_loss: 0.3524\n",
            "Epoch 904/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2135 - val_loss: 0.3510\n",
            "Epoch 905/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2133 - val_loss: 0.3505\n",
            "Epoch 906/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2005 - val_loss: 0.3522\n",
            "Epoch 907/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2089 - val_loss: 0.3530\n",
            "Epoch 908/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2091 - val_loss: 0.3502\n",
            "Epoch 909/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2057 - val_loss: 0.3491\n",
            "Epoch 910/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2021 - val_loss: 0.3488\n",
            "Epoch 911/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2093 - val_loss: 0.3512\n",
            "Epoch 912/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2079 - val_loss: 0.3498\n",
            "Epoch 913/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2130 - val_loss: 0.3516\n",
            "Epoch 914/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2068 - val_loss: 0.3533\n",
            "Epoch 915/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2104 - val_loss: 0.3534\n",
            "Epoch 916/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2139 - val_loss: 0.3521\n",
            "Epoch 917/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2104 - val_loss: 0.3492\n",
            "Epoch 918/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2045 - val_loss: 0.3517\n",
            "Epoch 919/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2177 - val_loss: 0.3520\n",
            "Epoch 920/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2079 - val_loss: 0.3517\n",
            "Epoch 921/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2167 - val_loss: 0.3527\n",
            "Epoch 922/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2103 - val_loss: 0.3538\n",
            "Epoch 923/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.3525\n",
            "Epoch 924/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2163 - val_loss: 0.3516\n",
            "Epoch 925/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2076 - val_loss: 0.3505\n",
            "Epoch 926/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2088 - val_loss: 0.3490\n",
            "Epoch 927/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2025 - val_loss: 0.3479\n",
            "Epoch 928/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2052 - val_loss: 0.3486\n",
            "Epoch 929/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2081 - val_loss: 0.3485\n",
            "Epoch 930/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.3503\n",
            "Epoch 931/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2095 - val_loss: 0.3507\n",
            "Epoch 932/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2072 - val_loss: 0.3513\n",
            "Epoch 933/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2198 - val_loss: 0.3502\n",
            "Epoch 934/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.3497\n",
            "Epoch 935/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2044 - val_loss: 0.3502\n",
            "Epoch 936/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2090 - val_loss: 0.3512\n",
            "Epoch 937/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1951 - val_loss: 0.3503\n",
            "Epoch 938/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2071 - val_loss: 0.3495\n",
            "Epoch 939/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2106 - val_loss: 0.3500\n",
            "Epoch 940/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.3469\n",
            "Epoch 941/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1970 - val_loss: 0.3488\n",
            "Epoch 942/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2103 - val_loss: 0.3474\n",
            "Epoch 943/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2081 - val_loss: 0.3490\n",
            "Epoch 944/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2098 - val_loss: 0.3473\n",
            "Epoch 945/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2153 - val_loss: 0.3484\n",
            "Epoch 946/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2159 - val_loss: 0.3468\n",
            "Epoch 947/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2045 - val_loss: 0.3479\n",
            "Epoch 948/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2005 - val_loss: 0.3475\n",
            "Epoch 949/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2075 - val_loss: 0.3465\n",
            "Epoch 950/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2173 - val_loss: 0.3469\n",
            "Epoch 951/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2049 - val_loss: 0.3465\n",
            "Epoch 952/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2037 - val_loss: 0.3465\n",
            "Epoch 953/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2103 - val_loss: 0.3468\n",
            "Epoch 954/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2138 - val_loss: 0.3455\n",
            "Epoch 955/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2091 - val_loss: 0.3455\n",
            "Epoch 956/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2056 - val_loss: 0.3455\n",
            "Epoch 957/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2029 - val_loss: 0.3456\n",
            "Epoch 958/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2047 - val_loss: 0.3480\n",
            "Epoch 959/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2138 - val_loss: 0.3467\n",
            "Epoch 960/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2053 - val_loss: 0.3463\n",
            "Epoch 961/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1979 - val_loss: 0.3461\n",
            "Epoch 962/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1992 - val_loss: 0.3455\n",
            "Epoch 963/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2105 - val_loss: 0.3453\n",
            "Epoch 964/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1910 - val_loss: 0.3467\n",
            "Epoch 965/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2003 - val_loss: 0.3460\n",
            "Epoch 966/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2010 - val_loss: 0.3452\n",
            "Epoch 967/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2009 - val_loss: 0.3453\n",
            "Epoch 968/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1910 - val_loss: 0.3425\n",
            "Epoch 969/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2021 - val_loss: 0.3431\n",
            "Epoch 970/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2067 - val_loss: 0.3435\n",
            "Epoch 971/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1867 - val_loss: 0.3445\n",
            "Epoch 972/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1968 - val_loss: 0.3465\n",
            "Epoch 973/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1974 - val_loss: 0.3473\n",
            "Epoch 974/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2076 - val_loss: 0.3454\n",
            "Epoch 975/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.1946 - val_loss: 0.3442\n",
            "Epoch 976/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2018 - val_loss: 0.3437\n",
            "Epoch 977/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2048 - val_loss: 0.3419\n",
            "Epoch 978/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2054 - val_loss: 0.3424\n",
            "Epoch 979/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1982 - val_loss: 0.3432\n",
            "Epoch 980/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2033 - val_loss: 0.3426\n",
            "Epoch 981/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2226 - val_loss: 0.3440\n",
            "Epoch 982/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2007 - val_loss: 0.3421\n",
            "Epoch 983/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2024 - val_loss: 0.3431\n",
            "Epoch 984/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1974 - val_loss: 0.3438\n",
            "Epoch 985/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2006 - val_loss: 0.3429\n",
            "Epoch 986/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2039 - val_loss: 0.3452\n",
            "Epoch 987/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2000 - val_loss: 0.3438\n",
            "Epoch 988/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2120 - val_loss: 0.3435\n",
            "Epoch 989/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2050 - val_loss: 0.3434\n",
            "Epoch 990/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2002 - val_loss: 0.3425\n",
            "Epoch 991/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.1953 - val_loss: 0.3425\n",
            "Epoch 992/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2017 - val_loss: 0.3428\n",
            "Epoch 993/1000\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.2022 - val_loss: 0.3437\n",
            "Epoch 994/1000\n",
            "75/75 [==============================] - 1s 11ms/step - loss: 0.1969 - val_loss: 0.3424\n",
            "Epoch 995/1000\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.2028 - val_loss: 0.3426\n",
            "Epoch 996/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2038 - val_loss: 0.3419\n",
            "Epoch 997/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2013 - val_loss: 0.3421\n",
            "Epoch 998/1000\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.2001 - val_loss: 0.3414\n",
            "Epoch 999/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1951 - val_loss: 0.3419\n",
            "Epoch 1000/1000\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1994 - val_loss: 0.3394\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3394\n",
            "7/7 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae1637f610>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number\n",
        "nb_epochs=1000;    # training epochs change\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition2   # choose 2nd lighting condition for test set\n",
        "\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "GU2yN8WFPGQs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNKobK2ePIbf"
      },
      "source": [
        "###3.3.4 3rd light condition as test###  "
      ],
      "id": "cNKobK2ePIbf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2_a5xoIPJTv"
      },
      "outputs": [],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number\n",
        "nb_epochs=1000;    # training epochs change\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition3   # choose 3rd lighting condition for test set\n",
        "\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "B2_a5xoIPJTv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do9GnXSdPMkJ"
      },
      "source": [
        "###3.3.5 4th light condition as test###  "
      ],
      "id": "Do9GnXSdPMkJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qkevaibPMua"
      },
      "outputs": [],
      "source": [
        "bs=8    # batch_size  \n",
        "vnoise=1    # the amount of noise to be added to training data\n",
        "nn=256    # training units number\n",
        "nb_epochs=1000;    # training epochs change\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/BVG_DM.csv', delimiter=';', header=0)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "val_condition = condition4   # choose 4th lighting condition for test set\n",
        "train_conditions = [c for c in [condition1, condition2, condition3, condition4] if c is not val_condition]    # the left will be train condition\n",
        "train_set = pd.concat(train_conditions)    # concatenate the remaining conditions for training set\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = train_set.values.astype(np.float32)\n",
        "val_set = val_condition.values.astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Create the twin Network \n",
        "net = Sequential()\n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the Siamese network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1,Lab2]=layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "siamese = Model(inputs=In, outputs=dist)\n",
        "\n",
        "# Loss and optimizer\n",
        "#datagen_train.fit(X_train)\n",
        "siamese.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-6))    \n",
        "\n",
        "# Training\n",
        "H=siamese.fit(train_generator, epochs=nb_epochs, validation_data=(X_val, Y_val), verbose=1)\n",
        "siamese.evaluate(X_val,Y_val)\n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "4qkevaibPMua"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_AV5HY31ii4"
      },
      "source": [
        "#4. Data Splitting Test for Possibilty Distribution#  \n",
        "In thie part, the data will be splitted to:\n",
        "- 4.1 train on one part of the color space and test on an the other part\n",
        "- 4.2 train on data under 3 light and test on 1 light condition \n",
        "  \n",
        "In total, 200 color pairs and 4 lights."
      ],
      "id": "2_AV5HY31ii4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCb-2QO9B5uZ"
      },
      "source": [
        "## 4.1 Train on part of the color space and test on the rest part ##\n"
      ],
      "id": "lCb-2QO9B5uZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPhentAiHFnB"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "\n",
        "# count occurrences in each row, and make new columns\n",
        "def count_occurrences(row,element):\n",
        "    count = 0\n",
        "    for value in row:\n",
        "        if value == element:\n",
        "            count += 1\n",
        "    return count/20\n",
        "\n",
        "data_df['0occurrences'] = results_subset.apply(count_occurrences, args=(0,), axis=1)\n",
        "data_df['1occurrences'] = results_subset.apply(count_occurrences, args=(1,), axis=1)\n",
        "data_df['2occurrences'] = results_subset.apply(count_occurrences, args=(2,), axis=1)\n",
        "\n",
        "\n",
        "# Data splitting(7:3)\n",
        "data_df = data_df.drop(columns = ['C1', 'C2']+results_cols)    # drop columns (color information and results of participants)\n",
        "train_df, val_df = train_test_split(data_df, test_size=0.3, random_state=42)    # split the data   \n",
        "train_set = train_df.values\n",
        "val_set = val_df.values\n",
        "\n",
        "# Get the Lab data (X) and probability distribution (Y)\n",
        "train_set = np.asarray(train_set).astype(np.float32)    # convert to ndarray, then convert all the components to float32\n",
        "X_train = train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train = train_set[:,6:9]    # probability distribution columns\n",
        "Y_train = Y_train.reshape(-1,3)\n",
        "\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "X_val = val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val = val_set[:,6:9]\n",
        "Y_val = Y_val.reshape(-1,3)\n",
        "print(Y_val)  # example: output to see the possibility distribution for validation set"
      ],
      "id": "FPhentAiHFnB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "IK3zenSIEDL-",
        "outputId": "e5f9c814-41ef-4073-e09f-f2577d27357b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6b3a0d26071e>\u001b[0m in \u001b[0;36m<cell line: 162>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0mH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (8, 1) and (8, 3) are incompatible\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "##### Preprocess Train/Test Sets ##### \n",
        "data_df = pd.read_csv('/content/drive/MyDrive/data/all_BVG.csv', header=0)    # note the data is from all_BVG.csv file\n",
        "\n",
        "# Calculate the most selected data by participants\n",
        "results_cols = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'];\n",
        "results_subset = data_df.loc[:, results_cols]    # select columns of participants evaluation result\n",
        "\n",
        "# count occurrences in each row, and make new columns\n",
        "def count_occurrences(row,element):\n",
        "    count = 0\n",
        "    for value in row:\n",
        "        if value == element:\n",
        "            count += 1\n",
        "    return count/20\n",
        "\n",
        "data_df['0occurrences'] = results_subset.apply(count_occurrences, args=(0,), axis=1)\n",
        "data_df['1occurrences'] = results_subset.apply(count_occurrences, args=(1,), axis=1)\n",
        "data_df['2occurrences'] = results_subset.apply(count_occurrences, args=(2,), axis=1)\n",
        "\n",
        "# Extract data for each lighting condition\n",
        "condition1 = data_df.iloc[0:200, :]\n",
        "condition2 = data_df.iloc[200:400, :]\n",
        "condition3 = data_df.iloc[400:600, :]\n",
        "condition4 = data_df.iloc[600:800, :]\n",
        "\n",
        "# Randomly choose indices for training and validation\n",
        "train_indices = random.sample(range(200), k=160)\n",
        "val_indices = [i for i in range(200) if i not in train_indices]\n",
        "\n",
        "# Construct the training set\n",
        "train_data = pd.concat([condition1.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition2.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition3.iloc[train_indices[0]:train_indices[-1]+1, :],\n",
        "                        condition4.iloc[train_indices[0]:train_indices[-1]+1, :]])\n",
        "\n",
        "# Construct the test set\n",
        "val_data = pd.concat([condition1.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition2.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition3.iloc[val_indices[0]:val_indices[-1]+1, :],\n",
        "                      condition4.iloc[val_indices[0]:val_indices[-1]+1, :]])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_set = np.asarray(train_set).astype(np.float32)\n",
        "val_set = np.asarray(val_set).astype(np.float32)\n",
        "\n",
        "X_train=train_set[:,0:6]\n",
        "X_train = X_train.reshape(-1,6,1,1)\n",
        "Y_train=train_set[:,6]\n",
        "\n",
        "X_val =val_set[:,0:6]\n",
        "X_val = X_val.reshape(-1,6,1,1)\n",
        "Y_val =val_set[:,6]\n",
        "\n",
        "\n",
        "##### Data augmentation #####\n",
        "def add_noise(vec):\n",
        "    deviation = vnoise*random.random()\n",
        "    noise = np.random.normal(0, deviation, vec.shape)\n",
        "    vec += noise\n",
        "    return vec\n",
        "\n",
        "# Prepare data-augmenting data generator\n",
        "datagen_train = ImageDataGenerator(preprocessing_function=add_noise)\n",
        "train_generator = datagen_train.flow(X_train, Y_train, shuffle=True, batch_size=bs)\n",
        "#datagen_val = ImageDataGenerator()\n",
        "\n",
        "\n",
        "##### Neural Network #####\n",
        "# Calculate the euclidean distance between two vectors\n",
        "def euclidean_distance(vec):\n",
        "    x1, x2 = vec\n",
        "    return K.sqrt(K.sum(K.square(x1 - x2), axis=-1)+1e-16)   # \"/2\"： normalize 0~1\n",
        "\n",
        "# Split the input into two parts (Lab1 and Lab2)\n",
        "def split(x):\n",
        "    return [x[:,0:3,:,:],x[:,3:6,:,:]]\n",
        "\n",
        "# Normalize data by using min-max normalization\n",
        "def normalize(data, min_value, max_value):\n",
        "    return (data-min_value)/(max_value-min_value)\n",
        "\n",
        "# Creates sequential neural network model.\n",
        "net = Sequential()  \n",
        "net.add(Flatten())\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))    \n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "net.add(Dense(nn, activation='relu'))\n",
        "\n",
        "# Create the siamese neural network\n",
        "In = Input(shape=(6,1,1))\n",
        "[Lab1, Lab2] = layers.Lambda(split)(In)    # split the input into two parts\n",
        "x1 = net(Lab1)\n",
        "x2 = net(Lab2)\n",
        "\n",
        "\n",
        "dist = layers.Lambda(euclidean_distance)([x1,x2])    # calculate the euclidean distance between the output\n",
        "dist = layers.Reshape((1,), name='output')(dist)  # reshape the output to be 2D\n",
        "\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "# normalize the dist\n",
        "dist_normalized = normalize(dist, 0, 2)\n",
        "\n",
        "# normalize the Lab1 and Lab2\n",
        "L_min, L_max = 0, 100\n",
        "a_min, a_max = -128, 127\n",
        "b_min, b_max = -128, 127\n",
        "\n",
        "Lab1_flat = Flatten()(Lab1)\n",
        "Lab2_flat = Flatten()(Lab2)\n",
        "\n",
        "Lab1_normalized = K.stack([\n",
        "    normalize(Lab1_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab1_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab1_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "Lab2_normalized = K.stack([\n",
        "    normalize(Lab2_flat[:,0], L_min, L_max),\n",
        "    normalize(Lab2_flat[:,1], a_min, a_max),\n",
        "    normalize(Lab2_flat[:,2], b_min, b_max)\n",
        "], axis=-1)\n",
        "\n",
        "\n",
        "# concatenate x1, x2 and dist tensors as the input of softmax layer\n",
        "con_value = K.concatenate([dist_normalized, Lab1_normalized, Lab2_normalized], axis=-1)\n",
        "\n",
        "# temprature control: divide by the temperature parameter\n",
        "con_value = con_value/temperature \n",
        "\n",
        "# add layers\n",
        "x = Dense(32, activation='relu')(con_value)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "x = Dense(3, activation='relu')(x)\n",
        "\n",
        "pred = Dense(3, activation=\"softmax\")(x)    # add a softmax layer, output a probability distribution over the 3 classes.\n",
        "# ------------------------------------------------------------!!!!!!!!!!\n",
        "\n",
        "siamese = Model(inputs = In, outputs = pred)    # create the model: input is the Lab pairs and output is the class\n",
        "\n",
        "# Loss and optimizer\n",
        "siamese.compile(loss = categorical_crossentropy, optimizer=Adam(learning_rate=1e-4))    # The categorical_crossentropy loss and the Learning rate set as 1e-4 (Adam)\n",
        "\n",
        "# Training\n",
        "H=siamese.fit(X_train, Y_train, batch_size = bs, epochs=nb_epochs, validation_data=(X_val, Y_val))\n",
        "siamese.evaluate(X_val, Y_val)    \n",
        "\n",
        "\n",
        "##### Plot the curves #####\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = siamese.predict(x=X_val)\n",
        "N = np.arange(0, nb_epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training and Test Losses\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "IK3zenSIEDL-"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0KweRiGk9SoV",
        "F-OQiTO_KaOV",
        "Q2Quhtsl9cU9",
        "JrEgpgcmMEWp",
        "Yxm_PyRkFQCm",
        "aZHdyLOfNdHT",
        "Xw3rR3HbKElf",
        "Pl0GYtnNXFJC",
        "qk6qZ4HcObF_",
        "OsUDTN5OO9sH",
        "cNKobK2ePIbf"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}